
# Table of Contents

1.  [профессия](#org94d7e19)
    1.  [levels](#orgabfe0ed)
    2.  [project](#org306b5c6)
    3.  [Data Science можно разделить на три основных составляющих:](#orgf983931)
    4.  [Data Science math](#org8ec81dc)
2.  [задаче машинного обучения](#orgaa04a21)
3.  [визуализация данных](#orgd842fce)
    1.  [matplotlib](#orgdc94ed7)
    2.  [plotly](#orga53d0ae)
    3.  [рекомендации](#org738e39a)
4.  [EDA Exploratory Data Analysis](#org9feceb7)
5.  [Задачи оптимизации II](#org0f7bc33)
6.  [Математический анализ в контексте задачи оптимизации. Часть III](#org3fdf903)
    1.  [practic gradient1](#orgdfdb187)
    2.  [practic gradient2](#org94c98cc)
    3.  [practic newton](#org0c9000b)
7.  [Линейное программирование](#org2fcc3e4)
    1.  [scipy example](#orgd576f16)
8.  [https://www.kaggle.com/competitions/nyc-taxi-trip-duration/overview](#orgc5d1aa1)
9.  [Bayes classifier байесовский классификатор](#org78aac92)
    1.  [веротяность, условная вероятность](#org3224d4a)
    2.  [примеры](#org6309651)
    3.  [Разбиение вероятностного пространства](#org20f0a28)
    4.  [Naive Bayes Classifier, NBC Наивный байесовский классификатор НБК](#org37d653a)
        1.  [cons](#orge4a862a)
        2.  [pros](#org5edb69b)
        3.  [sklearn](#orgee52d27)
        4.  [training](#org489e73e)
        5.  [links](#orgb73a5ea)
10. [деревья решений](#orgb1cc95f)

;-**- mode: Org; fill-column: 110;-**-

skillFactory.ru
<https://lms.skillfactory.ru/>

Профессия Data Science


<a id="org94d7e19"></a>

# профессия


<a id="orgabfe0ed"></a>

## levels

-   Junior. Специалист начального уровня, который может успешно выполнять задачи, время от времени прибегая к помощи со стороны коллег.
-   Middle. Специалист среднего уровня, который может самостоятельно реализовать поставленную задачу от начала и до конца.
-   Senior.Специалист с глубокими знаниями в определенной области Data Science, который имеет опыт разработки и внедрения целостных решений, способен предложить несколько способов выполнения задачи и знает, чем они хороши и чем плохи.
-   Lead. Специалист, который руководит командой Data Science, умеет переводить бизнес-задачи в технические, разделять задачи на составляющие и планировать работу команды, а также управлять взаимоотношениями в команде и решать проблемы взаимодействия со смежными командами.

Senior и Lead равнозначные - первый технические навыки, второй управленческие

Чтобы Middle-специалисту стать специалистом уровня Senior, нужно:

-   в деталях разобраться в какой-то области Data Science;
-   получить опыт полного цикла решения задач — от общения с бизнес-заказчиками до измерения эффектов от внедрения.

Middle/Senior → Lead

-   больше развивать менеджерские компетенции — навыки управления проектами и командами;
-   уметь разрешать и предупреждать конфликтные ситуации;
-   научиться брать на себя ответственность за работу других.


<a id="org306b5c6"></a>

## project

Опишем процесс работы над проектом в Data Science по методологии CRISP-DM <https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining>

-   <https://arxiv.org/pdf/1709.09003.pdf>

-   Постановка задачи (Business Understanding)
    -   Какой показатель бизнеса мы сможем улучшить?
    -   Какие рычаги у нас есть, чтобы его улучшать?
    -   Какие данные нам нужны? Доступны ли они?
    -   Какие инсайты или предсказания мы хотим получить от модели/алгоритма?
-   Исследование данных (Data Understanding)
-   Подготовка данных (Data Preparation)
-   Построение модели (Modeling)
-   Оценка модели (Evaluation)
-   Внедрение (Deployment)


<a id="orgf983931"></a>

## Data Science можно разделить на три основных составляющих:

-   Data Engineering (инжинирия данных) - обеспечение надёжности
    обработки данных. Инженеры данных умеют загружать большие объёмы
    данных в базы, настраивать потоки данных между системами и делать
    так, чтобы расчёты производились быстро и с минимальными
    вычислительными ресурсами.
    -   инженеры данных или дата-инженеры
-   Data Analytics (анализ данных) - Практика исследования,
    использования и интерпретации данных для решения задач бизнеса -
    выгружать нужные данные из базы, формировать из них понятные отчёты,
    ставить правильные вопросы и корректно отвечать на них.
    -   аналитики данных или дата-аналитики
-   Machine Learning (машинное обучение) - автоматического принятия решений
    -   дата-сайентисты, профессионал по машинному обучению.


<a id="org8ec81dc"></a>

## Data Science math

-   линейной алгебре;
-   математической статистике;
-   оптимизации.


<a id="orgaa04a21"></a>

# задаче машинного обучения

-   Регрессия
-   Классификация
-   Кластеризация


<a id="orgd842fce"></a>

# визуализация данных

-   pandas
-   matplotlib - старая,  изначально разрабатывалась для научных вычислений
-   seaborn
-   plotly - новая, новым коммерческим продуктом с бесплатной версией, который создавался специально для Data Science


<a id="orgdc94ed7"></a>

## matplotlib

ДОБАВЛЕНИЕ ИНФОРМАТИВНОСТИ В ГРАФИКИ

-   axes.set<sub>title</sub>() — заголовок диаграммы, а также его настройки (например, параметр fontsize отвечает за размер шрифта);
-   axes.set<sub>xlabel</sub>() — название оси абсцисс;
-   axes.set<sub>ylabel</sub>() — название оси ординат;
-   axes.set<sub>xticks</sub>() — установка отметок на оси абсцисс;
-   axes.set<sub>yticks</sub>() — установка отметок на оси ординат;
-   axes.xaxis.set<sub>tick</sub><sub>params</sub>() — управление параметрами отметок на оси абсцисс (например, параметр rotation отвечает за поворот отметок в градусах);
-   axes.yaxis.set<sub>tick</sub><sub>params</sub>() — управление параметрами отметок на оси ординат;
-   axes.legend() — отображение легенды;
-   axes.grid() — установка сетки.


<a id="orga53d0ae"></a>

## plotly

-   позволяет строить интерактивные графики
-   строить интерактивную 3D-визуализацию, карту мира и многое другое.
-   "тяжёлая артиллерия"
-   отрисовываются медленно и очень сильно нагружают видеокарту
-   бесплатную версию библиотеки разрешено применять в коммерческих продуктах.

новых оптимизированных модули и надстройки в Plotly

-   express <https://plotly.com/python/plotly-express/>
-   cufflinks <https://github.com/santosjorge/cufflinks>

С помощью экспресс-режима (px)

-   line() — линейные графики;
-   histogram() — гистограммы;
-   scatter() — диаграммы рассеяния;
-   box() — коробчатые диаграммы;
-   bar() — столбчатые диаграммы;
-   pie() — круговые диаграммы.

treemap() - как тепловая карта московской биржи


<a id="org738e39a"></a>

## рекомендации

linear plot отлично подходит, если набор данных непрерывен (как мы уже видели раньше, обычно это временной
ряд). График используется для определения тенденций во временном ряду и сравнения нескольких рядов между
собой.

-   Не используйте график, если набор данных дискретный (менее 20 наблюдений) — в таком случае лучше воспользуйтесь столбчатой диаграммой.
-   Время всегда отображается по оси абсцисс и разбивается на равные интервалы.
-   Если даты сливаются, используйте наклон в 45 градусов.
-   Не используйте график для сравнения рядов, если их больше 7-10 — график станет нечитабельным. Попробуйте уменьшить число категорий.

Гистограммы histogram) часто применяются для разведывательного анализа данных (EDA), так как они дают информацию о
распределении признака. С их помощью можно сразу определить диапазон изменения признака, его модальное
значение (пик гистограммы), а также найти «пеньки», которые выбиваются от непрерывного распределения
гистограммы, — аномалии.

-   Не стоит строить гистограмму, если наблюдений мало — распределение окажется далёким от действительного и вы просто сделаете ложные выводы. По статистике, для того, чтобы гистограмма хоть как-то оценивала истинное распределение, нужно как минимум 30 наблюдений (на практике нужно хотя бы 100).
-   Попробуйте (ради эксперимента) построить пять-семь гистограмм на одном графике для их сравнения по категориям (например, страны). ⛔ На практике так делать не нужно. Для сравнения параметров распределений по категориям предназначена коробчатая диаграмма (boxplot).
-   Если вы всё же хотите сравнить гистограммы между собой, предварительно обязательно приведите признаки к одной шкале (мы делали это, когда сравнивали ежедневную заболеваемость коронавирусом в процентах от населения страны). Если этого не сделать, распределения окажутся несопоставимыми.

Диаграмма рассеяния (scatter plot) и её производные — jointplot (гистограммы с рассеянием), kdeplot (диаграмма плотностей) и bubble plot (пузырьковая диаграмма) — предназначены для выявления взаимосвязи между двумя (или в случае 3D — тремя) признаками.

-   Не используйте диаграммы рассеяния на маленьком наборе данных. Здесь ситуация та же, что и с гистограммами.
-   Не стоит использовать расцветку и размер точек для признаков с большим числом уникальных категорий.
-   В случае если вы не видите зависимостей в данных, попробуйте использовать логарифмическую шкалу по оси абсцисс (по оси абсцисс и ординат в случае 3D-графика). Во всех библиотеках в методе есть параметр log, значение которого нужно установить на True.

Почему логарифмическая шкала?

-   во-первых, отбрасывает отрицательные значения
-   во-вторых, «приземляет» более высокие значения — зависимость становится более гладкой, и её становится легче
    просматривать. При этом логарифмирование не искажает исходную зависимость: то есть если на исходных данных
    был тренд роста признака А от признака Б, то на логарифмированных данных этот тренд сохранится.

Круговая диаграмма (pie chart) показывает структуру признака, то есть процентную долю каждого из возможных значений признака.

-   Сумма значений в круге всегда должна равняться единице, то есть всегда должно быть целое и его части (например, отношение числа заболевших вирусом по странам к общему количеству населения).
-   С помощью круговой диаграммы нельзя сравнить средний чек в ресторанах — эти средние не являются частью единого целого. Однако можно сравнить число сотрудников в этих ресторанах, так как они являются частью одной совокупности.
-   Не визуализируйте секторы, близкие к 0, — их невозможно сравнить друг с другом.
-   Не делайте больше 6-8 секторов — воспринимать информацию будет сложно. Если компонентов больше, выделите ТОП-6-8, а остальные обозначьте как «прочие».
-   Всегда отображайте легенду либо подписи категорий внутри секторов.
-   Если важно выделить часть графика, «вытащите» его из центра.


<a id="org9feceb7"></a>

# EDA Exploratory Data Analysis


<a id="org0f7bc33"></a>

# TODO Задачи оптимизации II

Оптимизация функций — это задача нахождения набора входных данных целевой функции (то есть её аргументов),
которые приводят к минимуму или максимуму функции.

Задачу на нахождение максимума можно свести к задаче на нахождение минимума


<a id="org3fdf903"></a>

# Математический анализ в контексте задачи оптимизации. Часть III

методы оптимизации

-   методы нулевого порядка (их работа основана на оценке значений самой целевой функции в разных точках);
-   методы первого порядка ( + первые производные);
-   методы второго порядка ( + значение градиента, и гессиан (матрицу Гессе)).

«оракул первого порядка» или «оракул нулевого порядка» - компоненты алгоритма, которые находят информацию на
каждом шаге для метода соответствующего порядка.

momentum - популярная вариация — градиентного спуска

Синаптическая пластичность — это понятие, которое используется для описания того, как формируются и
укрепляются нейронные связи после получения новой информации.

функцией стоимости - оценка еффективности нейронной сети

Обратное распространение (backpropagation) — Операции обратного распространения вычисляют частную производную
функции стоимости по отношению к весам и активациям предыдущего слоя, чтобы определить, какие значения влияют
на градиент функции стоимости.  результат, итеративно корректируются для уменьшения функции стоимости.

скорость обучения (или темп обучения или шаг градиентного спуска) - параметр метода оптимизации.

седловых точках (типа как локальный минимум), где градиент равен нулю.

типу градиентного спуска:

-   Batch Gradient Descent;
-   Stochastic Gradient Descent;
-   Mini-batch Gradient Descent.

Batch Gradient Descent

-   хорошо работает, если мы рассматриваем выпуклые или относительно гладкие функции ошибки
    -   у линейной или логистической регрессии

Stochastic Gradient Descent

-   реализации стохастического спуска вычисляются градиенты не для всей выборки, а только для случайно выбранной единственной точки
-   Сильные колебания в обновлении параметров модели

Mini-batch Gradient Descent

-   Колебания зависят от размера подвыборки (увеличиваются с уменьшением её объема)

AdaGrad - параметры, которые сильно обновляются каждый раз, начинают обновляться слабее.

-   Однако снижение скорости обучения в AdaGrad иногда происходит слишком радикально, и она практически
    обнуляется. Чтобы решить эту проблему, были созданы алгоритмы RMSProp, AdaDelta, Adam и некоторые другие.
-   <https://ruder.io/optimizing-gradient-descent/index.html#adagrad>
-   <https://habr.com/ru/post/318970/>

sklearn.linear<sub>model.LogisticRegression</sub>

-   **newton-cg** - алгоритм работает быстрее, чем градиентный спуск, и тратит меньше времени для достижения
    минимума, однако у него есть и определённые недостатки, о которых мы поговорим позже.
-   **lbfgs**
-   **liblinear**
-   **sag** -  вариациями стохастического градиентного спуска
-   **saga** -  вариациями стохастического градиентного спуска

newton-cg - Метод Ньютона - для x вычисляется f(x), строится касательная, и в точке пересечения касательной с
осью Ox, строится новая точка, к которой также строится касательная, и так далее.

-   Изначально использовался для решения уравнения f(x) = 0
-   уравнение касательной y = f'(xn)(x-xn)+f(xn)
-   нам нужно попасть в y=0
-   0 = f'(xn)\*(xn+1 - xn) + f(xn)
-   xn+1 - xn = - f(xn)/f'(xn)
-   xn+1  = xn - f(xn)/f'(xn)
-   чтобы найти точки минимума/максимума нужно решать f'(x)=0
    -   xn+1 = xn - f'(xn)/f''(xn)
-   Метод Ньютона, если считать в количестве итераций, в многомерном случае (с гессианом) работает быстрее градиентного спуска.
-   легко попасть в максимум или седловую точку. Особенно ярко это видно на невыпуклых функциях с большим количеством переменных, так как у таких функций
    седловые точки встречаются намного чаще экстремумов.
-   подходит для В линейной регрессии или при решении задачи классификации с помощью метода опорных векторов или
    логистической регрессии
-   отсутствия необходимости в настройке гиперпараметра шага

квазиньютоновскими методами - среднее между градиентный спуском и методом Ньютона (методы опртимизации первого порядка)

-   симметричная коррекция ранга 1 (SR1);
-   схема Дэвидона — Флетчера — Пауэлла (DFP);
-   схема Бройдена — Флетчера — Гольдфарба — Шанно (BFGS).
    -   вариации для экономии памяти (так как во время их реализации хранится ограниченное количество информации)
        -   L-BFGS;
        -   L-BFGS-B. - является лишь улучшенной версией L-BFGS для работы с ограничениями.


<a id="orgdfdb187"></a>

## practic gradient1

    import numpy as np
    from matplotlib import pyplot as plt
    
    
    def fun(x, y, a=1, b=1):
        return a * (x ** 2) + b * (y ** 2)
    
    
    def grad(x, y, a=1, b=1):
        return np.array([2 * a * x, 2 * b * y])  # df/dx df/dy
    
    
    def grad_descend(grad, step_size=0.2, num_steps=30):
        lst = []
        x = np.random.uniform(0, 3, size=2)  # array([2.26419079, 0.06810665])
        lst.append(x)
        for i in range(num_steps):
            x = x - step_size * grad(lst[-1][0], lst[-1][1])  # get last two values
            lst.append(x)
        return np.array(lst)
    
    
    def plot_grad(fun, trace):
        fig = plt.figure(figsize=(10, 8))
        x_ = np.linspace(-1, 1, 100)
        y_ = np.linspace(-1, 1, 100)
        x, y = np.meshgrid(x_, y_)  # hack to display function on grid
        z = fun(x, y)
        ax = fig.add_subplot(1, 1, 1, projection='3d')
        ax.plot_surface(x, y, z, alpha=0.6)
        ax.contour(x, y, z, zdir='z', offset=z.min())
        z_trace = fun(trace[:, 0], trace[:, 1])
        ax.plot(trace[:, 0], trace[:, 1], z_trace, "o-")
        ax.set_xlim(x.min(), x.max())
        ax.set_ylim(y.min(), y.max())
        ax.set_zlim(z.min(), z.max())
        plt.show()
    
    
    trace = grad_descend(grad, 0.1, 20)
    plot_grad(fun, trace=trace)


<a id="org94c98cc"></a>

## practic gradient2

    from scipy import optimize
    optimize.minimize(lambda x: f(*x), x0=(0, 0))
    
    from sklearn.linear_model import SGDRegressor


<a id="org0c9000b"></a>

## practic newton

    import numpy as np
    from matplotlib import pyplot as plt
    
    def func1(x):
        return 3*x**2 - 6*x -45
    def func2(x):
        return 6*x - 6
    
    init_value = 42
    iter_count = 0
    x_curr = init_value
    epsilon = 0.0001
    f = func1(x_curr)
    
    while (abs(f) > epsilon):
        f = func1(x_curr)
        f_prime = func2(x_curr)
        x_curr = x_curr - (f)/(f_prime)
        iter_count += 1
        print(x_curr)
    
    from scipy.optimize import newton
    a = newton(func=func1,x0=50,fprime=func2, tol=0.0001)
    print('newton', a)


<a id="org2fcc3e4"></a>

# Линейное программирование

один из наиболее часто встречающихся случаев задачи условной оптимизации — задачу линейного программирования.

Линейное программирование — это метод оптимизации для системы линейных ограничений и линейной целевой функции,
а все переменные неотрицательны.

-   В производстве — чтобы рассчитать человеческие и технические ресурсы и минимизировать стоимость итоговой продукции.
-   При составлении бизнес-планов — чтобы решить, какие продукты продавать и в каком количестве, чтобы максимизировать прибыль.
-   В логистике — чтобы определить, как использовать транспортные ресурсы для выполнения заказов за минимальное время.
-   В сфере общепита — чтобы составить расписание для официантов.

Целочисленным линейным программированием (ЦЛП) называется вариация задачи линейного программирования, когда
все переменные — целые числа.

Пример задачи:

-   Система:
    -   2\*x+4\*y<=220
    -   3\*x+2\*y<=150
    -   x>=0
    -   y>=0
-   целевая функция:
    -   p(x,y) = 4\*x+3\*y - найти максимальное возможное значение этой функции и точка должна удовлетворять системе

Решать можно хоть на коленке с линейкой.

Для решения програмно:

-   формулируйте задачу именно в формате поиска минимума: min(-4\*x-3\*y)
-   c = [-4 ; -3 ]
-   A = [ [2 ; 4] ; [3 ; 2] ]
-   b = [ 220 ; 150 ]
-   задача стала:
    -   min (c<sup>T</sup> \* x)
    -   A\*x<=b

Библиотеки Python:

-   SciPy - не умеет решать задачи целочисленного линейного программирования
    -   from scipy.optimize import linprog
-   CVXPY -
    -


<a id="orgd576f16"></a>

## scipy example

У нас есть 6 товаров с заданными ценами на них и заданной массой.  Вместимость сумки, в которую мы можем
положить товары, заранее известна и равна 15 кг.  Какой товар и в каком объёме необходимо взять, чтобы сумма
всех цен товаров была максимальной?

    values = [4, 2, 1, 7, 3, 6] #стоимости товаров
    weights = [5, 9, 8, 2, 6, 5] #вес товаров
    C = 15 #вместимость сумки
    n = 6 #количество товаров


<a id="orgc5d1aa1"></a>

# <https://www.kaggle.com/competitions/nyc-taxi-trip-duration/overview>

Бизнес-задача: определить характеристики и с их помощью спрогнозировать длительность поездки на такси.

-   построить модель машинного обучения, которая на основе предложенных характеристик клиента будет
    предсказывать числовой признак — время поездки такси, то есть решить задачу регрессии.

OSRM (Open Source Routing Machine) — это открытый бесплатный ресурс, который активно используется во многих
сервисах, предполагающих построение кратчайшего маршрута. Он позволяет не только построить оптимальный
кусочно-линейный путь из точки А в точку B, но также узнать его примерную длительность, длину, а также
подробную информацию о количестве шагов, которые необходимо преодолеть по пути (количество поворотов). У
данного сервиса есть API, с которым вы можете познакомиться в документации.

-   <https://www.thinkdatascience.com/post/2020-03-03-osrm/osrm/>
-   Время поездки, вычисляемое с помощью OSRM, не является истинным временем поездки, так как оно вычисляется по
    кратчайшему пути при идеальных условиях: отсутствие пробок, погодных влияний и других внешних факторов.

расстояние по формуле Хаверсина.

-   <http://espressocode.top/haversine-formula-to-find-distance-between-two-points-on-a-sphere/>
-   Данная формула определяет кратчайшее расстояние между двумя точками на сфере, если известны широта и долгота
    каждой из точек.

функция для вычисления угла направления движения (в градусах).

    def get_haversine_distance(lat1, lng1, lat2, lng2):
        # переводим углы в радианы
        lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))
        # радиус Земли в километрах
        EARTH_RADIUS = 6371
        # считаем кратчайшее расстояние h по формуле Хаверсайна
        lat_delta = lat2 - lat1
        lng_delta = lng2 - lng1
        d = np.sin(lat_delta * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng_delta * 0.5) ** 2
        h = 2 * EARTH_RADIUS * np.arcsin(np.sqrt(d))
        return h
    
    
    def get_angle_direction(lat1, lng1, lat2, lng2):
        # переводим углы в радианы
        lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))
        # считаем угол направления движения alpha по формуле угла пеленга
        lng_delta_rad = lng2 - lng1
        y = np.sin(lng_delta_rad) * np.cos(lat2)
        x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)
        alpha = np.degrees(np.arctan2(y, x))
        return alpha


<a id="org78aac92"></a>

# Bayes classifier байесовский классификатор

    "intersection" "∩"


<a id="org3224d4a"></a>

## веротяность, условная вероятность

условная вероятность и теорема Байеса являются фундаментом для байесовского классификатора

-   случайным экспериментом - эксперимент, результат которого не детерминирован изначально
-   элементарный исход - исхода случайного эксперимента
-   пространством элементарных исходов - омега Ω

P(A) = n/N - **вероятность** А

-   n - равновероятные элементарные исходы, составляющие событие A,
-   N - все возможные элементарные исходы

**Дополнение события А** - это неболагоприятные события для А. У дополнения А и самого А нет общих исходов - они
 взаимоисключающие, называют **несовместные**. (черта над А, у нас ^A)

-   P(A) - P(<sup>A</sup>) = 1
-   1 - 0.7 = 0.3 - 0.7 и 0.3 вероятности 2 взаимоисключающих событий

вероятность двух несовместимых событий в одном эксперименте

-   P(A U B ) = P(A) + P(B) - где А и B - несовместимые

вероятность двух независимых событий в двух разных экспериментах или пересечение в одном

-   P(A ∩ B) = P(A) \* P(B)
-   если равенство выполняется, то события **независимы**

вероятность двух совместимых событий в двух разных экспериментах

-   P(A U B) = P(A) + P(B) - P(A ∩ B)

Разбиение вероятностного пространства — это взаимоисключающие и совместно исчерпывающие события

Услованая вероятность P(B|A) - вероятность B при условии A уже произошло

-   P(B|A) = P(A and B) / P(A)
    -   где P(A and B) - вероятность что произошло А и затем B
-   P(A and B) = P(A) \* P(B|A)
-   если это один эксперимент, то скорее всего P(A and B) = P(A)

Precision = P(real=1&pred=1|pred=1) = tp/(tp+fp)
Recall = P(real=1&pred=1|real=1) = tp/(tp+fn)
Specificity = P(real=0&pred=0|pred=0)

Cобытия A и B называются **независимыми**, если вероятность их пересечения равна произведению вероятностей

-   P(A and B) = P(A) \* P(B|A)

Вероятность, что случайно выбранный человек пьёт кофе, равна сумме вероятности, что человек пьёт кофе и
  является мужчиной, и вероятности, что человек пьет кофе и является женщиной.

Теорема Байеса - позволяет «переставить причину и следствие»

-   P(A|B) = ( P(B|A)\*P(A) ) / P(B)
-   из P(B|A) можно получить P(A|B)
-   P(A and B) = P(A) \* P(B|A) = P(B) \* P(A|B)


<a id="org6309651"></a>

## примеры

34 всего

-   14 ди
-   16 ст
-   4 обе
-   8 ни одной

ди или ст или ди и ст ?

-   P(A U B) = P(A) + P(B) - P(A ∩ B)
-   14/34+16/34-4/34

два раза бросаем монету 00 01 10 11
2/4 = 0.5 = один раз из них орел
0.5\*0.5 = 0.25 = 1/4 - вероятность, что два раза орел

два раза бросаем монету
P(A|B) = 1/2
P(A и B) = P(A) \* P(B|A) = 1/2 \*1/2 = 1/4
Проверка на независимость 1/4 = 1/4 - независимы

Кубик

-   чет 3/6 , >=3 4/6
    -   чет+>=3 = 2/6
    -   3/6\*4/6 = 2/6 -> независимые события
-   чет 3/6, не больше 2 очков 2/6
    -   чет+<=2 = 1/6
    -   3/6\*2/6 = 1/6 -> независимые события
-   чет 3/6 , нечет 3/6
    -   чет+нечет=0 -> взаимоисключающие

вероятность, что при подбрасывании два раза шарика они два окажутся верными = при одинарном бросании
    выполнится оба условия

кубик чет 3/6 , нечет 3/6

-   чет+нечет = 0
-   3/6\*3/6 = 0.25 -
-   0.25 != 0 -> зависимые

Брошены две игральные кости. Найти вероятность того, что сумма выпавших очков равна четырем

-   1 3, 2 2, 3, 1
-   всего исходов 6\*6=36
-   благоприятных = 3

= 


<a id="org20f0a28"></a>

## TODO Разбиение вероятностного пространства

Разбиение вероятностного пространства — это взаимоисключающие и совместно исчерпывающие события


<a id="org37d653a"></a>

## Naive Bayes Classifier, NBC Наивный байесовский классификатор НБК

-   простота, как идейная, так и алгоритмическая.
-   основанный на том, что все признаки модели независимы.
-   bag of words assumption

multinomial naive Bayes classifier - linear classifier

naive Bayes assumpiton: P(f1,f2,f3|c) = P(f1|c)\*P(f2|c)\*P(f3|c)

Вероятность, что документ относится к классу

-   P(c1) = argmax P(c|d) , где argmax - для всех c классов, документа dk
-   P(c1) = argmax P(d|c)\*P(c)
-   P(c1) = argmax P(f1,f2,f3|c)\*P(c) , где argmax - для всех c классов, f1f2 - feature of d
    -   P(f1|c), P(f2|c) - are independent and hence can be 'naively' multiplied (Наивное предположение Байеса)
-   P(c1) = argmax P(c) \* П P(f|c) , где П для всех f features
-   P(c1) = argmax log(P(c)) + ∑log(P(w|c)), argmax for all c, ∑ for all positions of word w


<a id="orge4a862a"></a>

### cons

-   Предположение о независимых признаках не выполняется на практике практически никогда.
-   Если нет обучающего набора данных для какого-то из классов, это приводит к нулевой апостериорной вероятности и модель не может сделать прогноза.


<a id="org5edb69b"></a>

### pros

-   Алгоритм не только прост для понимания и реализации, но также даёт достаточно точные оценки и быстро работает.
-   Наивный Байес имеет низкую вычислительную сложность.
-   Он может эффективно работать с большим набором данных.
-   Его можно использовать с задачами прогнозирования нескольких классов, то есть в задачах мультиклассовой классификации.
-   Если выполнено предположение о независимости признаков, то НБК даёт более высокое качество, чем логистическая регрессия и многие другие модели.


<a id="orgee52d27"></a>

### sklearn

-   GaussianNB — самый простой вариант, работает с непрерывными признаками;
-   MultinomialNB  — работает с категориальными признаками, текстами и несбалансированными выборками;
-   ComplementNB — улучшенная версия MultinomialNB, стабильно показывает более высокое качество в задачах классификации текстов;
-   BernoulliNB — версия для работы с бинарными признаками;
-   CategoricalNB — работает с категориальными признаками, предполагает кодировку данных через OrdinalEncoder.


<a id="org489e73e"></a>

### training

-   P(f1|c),P(f2|c) we’ll assume a feature is just the existence of a word in the document’s bag of

words.

-   P(w1|c) - fraction of times the word w appears among all words in all documents of category c
-   first concatenate all documents with category c into one big “category c” text.
-   we use the frequency of w in this concatenated document
-   P(w1|c) = count(w1,c)/∑count(w,c), ∑ for all w
-   if we dont have w1 in training class -> П - will give 0
    -   solution is the add-one (Laplace) smoothing


<a id="orgb73a5ea"></a>

### links

-   <https://web.stanford.edu/~jurafsky/slp3/4.pdf>


<a id="orgb1cc95f"></a>

# деревья решений

CART (Classification and Regression Tree) — это алгоритм, предназначенный для построения бинарных деревьев
 решений. предназначен как для задач классификации, так и для задач регрессии. в sklearn

решающие пни — деревья глубины 1 (строится на номере признака и решающем значении для него)

корневая вершина является родительской, а две другие — её потомками.

