<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-01-14 Tue 12:40 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org44984eb">1. conferences</a></li>
<li><a href="#org4087d7e">2. best links</a>
<ul>
<li><a href="#org88023d8">2.1. blogs</a></li>
<li><a href="#org44f2785">2.2. papers</a></li>
<li><a href="#orge50e86e">2.3. AI/ML Russian repositories </a></li>
<li><a href="#org126e6a8">2.4. youtube</a></li>
</ul>
</li>
<li><a href="#orge5aca5f">3. most frequent math methods</a>
<ul>
<li><a href="#org3920eff">3.1. layout resolution</a></li>
<li><a href="#org2cc9442">3.2. model size in memory</a></li>
<li><a href="#org39e4e08">3.3. compare two objects by features</a></li>
<li><a href="#org41a1d3f">3.4. distance matrix</a>
<ul>
<li><a href="#org515fc07">3.4.1. calc</a></li>
<li><a href="#org280b95c">3.4.2. find lowest/max</a></li>
<li><a href="#org0df514e">3.4.3. faster</a></li>
</ul>
</li>
<li><a href="#org25fcba0">3.5. interpolation </a></li>
<li><a href="#orgfc373a5">3.6. softmax</a></li>
<li><a href="#orgfb0908b">3.7. minimize the negative log likelihood instead of maximizing the likelihood.</a></li>
</ul>
</li>
<li><a href="#orgc6bbe62">4. common terms</a></li>
<li><a href="#org7fea063">5. rare terms</a></li>
<li><a href="#orgd08f9b1">6. number of parameters calculations</a></li>
<li><a href="#orgae4003d">7. Tasks, problems classification</a>
<ul>
<li><a href="#orgf909ee9">7.1. Classification problem and types</a></li>
<li><a href="#orge381e2d">7.2. discriminative model vs generative models</a>
<ul>
<li><a href="#org2a1049e">7.2.1. Examples</a></li>
<li><a href="#org92b4bbe">7.2.2. applications</a></li>
</ul>
</li>
<li><a href="#orgc93406b">7.3. links</a></li>
</ul>
</li>
<li><a href="#orge91e606">8. Data Analysis [ə'nælɪsɪs]</a>
<ul>
<li><a href="#orgf399947">8.1. <span class="todo TODO">TODO</span> open-source tools</a></li>
<li><a href="#org778fa1c">8.2. dictionary</a></li>
<li><a href="#org080a4a7">8.3. Steps</a>
<ul>
<li><a href="#org9c4153b">8.3.1. стандарт CRISP-DM или Cross-Industry Standard Process for Data Mining/Data Science</a></li>
<li><a href="#org45129ae">8.3.2. ASUM-DM Analytics Solutions Unified Method for Data Mining/Predictive Analytics 2015</a></li>
<li><a href="#org9bae417">8.3.3. Процесс разработки</a></li>
<li><a href="#org29e72c8">8.3.4. Descriptive analytics</a></li>
<li><a href="#orgbec4d63">8.3.5. Анализ временных рядов -</a></li>
</ul>
</li>
<li><a href="#org48dc1f2">8.4. 2019 pro https://habr.com/ru/company/JetBrains-education/blog/438058/</a>
<ul>
<li><a href="#orga0c1334">8.4.1. Часть 1</a></li>
<li><a href="#orgd44e194">8.4.2. Часть 2</a></li>
</ul>
</li>
<li><a href="#orgd5ad633">8.5. EXAMPLES OF ANALYSIS</a>
<ul>
<li><a href="#org6b9b6a8">8.5.1. dobrinin links</a></li>
<li><a href="#org0d6faa3">8.5.2. https://github.com/firmai/industry-machine-learning</a></li>
</ul>
</li>
<li><a href="#orged6ffaa">8.6. EDA Exploratory analysis</a>
<ul>
<li><a href="#org835030b">8.6.1. median, mean value</a></li>
<li><a href="#orgbff9559">8.6.2. types of comparison</a></li>
<li><a href="#org21d343d">8.6.3. skewness and kurtosis</a></li>
<li><a href="#orgd2baa13">8.6.4. <span class="todo TODO">TODO</span> normal distribution test</a></li>
<li><a href="#org38855a7">8.6.5. Analysis for regression model:</a></li>
<li><a href="#org5aef19a">8.6.6. quartile, quantile, percentile</a></li>
</ul>
</li>
<li><a href="#orgea6ee7b">8.7. gradient boostings vs NN</a></li>
<li><a href="#org967a057">8.8. theory</a>
<ul>
<li><a href="#org4bdacb6">8.8.1. types of data, data types :</a></li>
<li><a href="#orgd84edcf">8.8.2. terms</a></li>
<li><a href="#orgca898b5">8.8.3. 1 column describe</a></li>
<li><a href="#orgd961b4a">8.8.4. categories of analysis</a></li>
<li><a href="#org97992f0">8.8.5. methods</a></li>
<li><a href="#org5d87823">8.8.6. correlation</a></li>
<li><a href="#org9ae155e">8.8.7. explanatory/inference vs prediction modeling</a></li>
<li><a href="#org9ec6139">8.8.8. Independence of Irrelevant Alternatives(IIA)</a></li>
</ul>
</li>
<li><a href="#orgcdf0cbc">8.9. Feature Preparation </a>
<ul>
<li><a href="#org5ee476b">8.9.1. terms</a></li>
<li><a href="#org01216b0">8.9.2. Выбросы Outliers</a></li>
<li><a href="#org236ea29">8.9.3. IDs encoding with embaddings</a></li>
<li><a href="#org2bafab5">8.9.4. Categorical encode</a></li>
<li><a href="#org334b8da">8.9.5. imbalanced classes and sampling</a></li>
<li><a href="#org8e116c3">8.9.6. Skewed numerical feature</a></li>
<li><a href="#orgbf06092">8.9.7. missing values: NaN, None</a></li>
<li><a href="#org6289db1">8.9.8. numerical data to bins</a></li>
<li><a href="#org0f6b9fe">8.9.9. Sparse Classes</a></li>
<li><a href="#org838d163">8.9.10. Feature engeering or Feature Creation</a></li>
<li><a href="#org19b4b86">8.9.11. Standardization, Rescale, Normalization</a></li>
<li><a href="#orgd6f699e">8.9.12. feature selection (correlation) - Filter Methods</a></li>
<li><a href="#orge260aa2">8.9.13. отбор признаков feature filtrating or feature selection</a></li>
<li><a href="#org8efbe6c">8.9.14. links</a></li>
</ul>
</li>
<li><a href="#org9a7e62e">8.10. поиск зависимостей между признаками (Finding relationships among variables) или data mining или Интеллектуальный анализ данных</a>
<ul>
<li><a href="#orgbe86692">8.10.1. <span class="todo TODO">TODO</span> нелинейная коррелцяи - поиск через регрессию</a></li>
<li><a href="#orgd02e4da">8.10.2. simple</a></li>
</ul>
</li>
<li><a href="#orgc56949b">8.11. Корреляционный анализ</a>
<ul>
<li><a href="#org90106b5">8.11.1. корреляция Пуассона</a></li>
<li><a href="#org87c7617">8.11.2. pearson vs spearman vs kendall</a></li>
</ul>
</li>
<li><a href="#org93a60f8">8.12. Кластерный анализ</a>
<ul>
<li><a href="#org8a71c92">8.12.1. terms</a></li>
<li><a href="#org316c7c6">8.12.2. steps</a></li>
<li><a href="#org4eb146c">8.12.3. preparation</a></li>
<li><a href="#org526e02c">8.12.4. Цели кластеризации</a></li>
<li><a href="#org5e7adab">8.12.5. Методы кластеризации</a></li>
<li><a href="#orgcef0923">8.12.6. Create similarity metric</a></li>
<li><a href="#orgaf244b3">8.12.7. Measuring Similarity from Embeddings</a></li>
<li><a href="#org79c65c8">8.12.8. cosine-similarity</a></li>
<li><a href="#orgb45fac8">8.12.9. Hierarchical clustering</a></li>
<li><a href="#org4872e03">8.12.10. Automatic clustering</a></li>
<li><a href="#orgf40d40e">8.12.11. mistakes</a></li>
<li><a href="#org023c616">8.12.12. quality, validation, evalutaion</a></li>
<li><a href="#org9cf93de">8.12.13. links</a></li>
</ul>
</li>
<li><a href="#org11388b5">8.13. Регрессивный линейный анализ - linear regression</a>
<ul>
<li><a href="#orgbfa9b86">8.13.1. types</a></li>
<li><a href="#org15b8f4b">8.13.2. parameters estimation methods</a></li>
<li><a href="#orgdc178e4">8.13.3. цели регрессивного анализа</a></li>
<li><a href="#orgfa9ffa4">8.13.4. требования для регрессивного анализа</a></li>
<li><a href="#orgd7bec83">8.13.5. Linear least squares (LLS) - most simple</a></li>
<li><a href="#orgc742ef3">8.13.6. regularization methods</a></li>
<li><a href="#org464af04">8.13.7. logistic regression (or logit regression)</a></li>
<li><a href="#orgb1478df">8.13.8. Linear Regression Vs. Logistic Regression</a></li>
<li><a href="#org82853df">8.13.9. example1</a></li>
<li><a href="#org9da5fbc">8.13.10. example2</a></li>
<li><a href="#org41e6c42">8.13.11. links</a></li>
</ul>
</li>
<li><a href="#orgfec0f29">8.14. Факторный анализ</a></li>
<li><a href="#org817e1e9">8.15. Time Series Analysis</a>
<ul>
<li><a href="#orgd4869b9">8.15.1. terms</a></li>
<li><a href="#orgee55f8f">8.15.2. forecasting methods</a></li>
<li><a href="#orgbfcb412">8.15.3. forecasting loss metrics</a></li>
<li><a href="#org9692cf2">8.15.4. features</a></li>
<li><a href="#orgbd73554">8.15.5. определение стационарности</a></li>
<li><a href="#orgc505f61">8.15.6. rate of change</a></li>
<li><a href="#orgc456d52">8.15.7. one dimension convolution</a></li>
<li><a href="#orga2d6261">8.15.8. graphs</a></li>
<li><a href="#org0c115a8">8.15.9. datasets</a></li>
<li><a href="#org71ef7e5">8.15.10. <span class="todo TODO">TODO</span> forecasting</a></li>
<li><a href="#orgc168633">8.15.11. links</a></li>
</ul>
</li>
<li><a href="#org7c14de4">8.16. Feature Importance</a>
<ul>
<li><a href="#org14643f1">8.16.1. классификационные модели показывающие важность признаков</a></li>
</ul>
</li>
<li><a href="#org949f069">8.17. Малое количество данных</a></li>
<li><a href="#orgcc5c8c2">8.18. Probability Callibration</a>
<ul>
<li><a href="#org868fb0d">8.18.1. prediction intervals</a></li>
</ul>
</li>
<li><a href="#orgf5f391a">8.19. Ensembles</a>
<ul>
<li><a href="#org2c48803">8.19.1. stacking vs bagging vs boosting (old):</a></li>
<li><a href="#org1fd08f1">8.19.2. stacking vs bagging vs boosting</a></li>
<li><a href="#orgc7f53ee">8.19.3. Stacking</a></li>
<li><a href="#orgf005130">8.19.4. bagging (bootstrap aggregation)</a></li>
<li><a href="#org4eee0fb">8.19.5. boosting</a></li>
<li><a href="#orgf1f0648">8.19.6. skillfactory apporach</a></li>
</ul>
</li>
<li><a href="#org26095c8">8.20. Проверка гипотез </a></li>
<li><a href="#org5315607">8.21. Автокорреляция ACF</a>
<ul>
<li><a href="#orga3b42bf">8.21.1. plotting</a></li>
<li><a href="#orga678c57">8.21.2. calc</a></li>
<li><a href="#org03efe35">8.21.3. похожие понятия</a></li>
<li><a href="#org027ddd5">8.21.4. &#x2013; СРАВНЕНИЕ СПОСОБОВ &#x2013; https://stackoverflow.com/questions/643699/how-can-i-use-numpy-correlate-to-do-autocorrelation</a></li>
</ul>
</li>
<li><a href="#org5b08aa5">8.22. Оптимизацинные задачи Mathematical Optimization Математическое программирование</a>
<ul>
<li><a href="#orgda96a74">8.22.1. definition</a></li>
<li><a href="#org20c0270">8.22.2. terms</a></li>
<li><a href="#orgd24fe3d">8.22.3. problem forms</a></li>
<li><a href="#org40a47db">8.22.4. <span class="todo TODO">TODO</span> simplex algorithm</a></li>
<li><a href="#org4b90148">8.22.5. <span class="todo TODO">TODO</span> branch and bound</a></li>
<li><a href="#org30d0c3e">8.22.6. good known problems</a></li>
<li><a href="#orge543d94">8.22.7. Optimization with Calculus</a></li>
<li><a href="#org676b709">8.22.8. имитация отжига</a></li>
<li><a href="#org0fb5efe">8.22.9. Linerization</a></li>
<li><a href="#org867a05b">8.22.10. course</a></li>
<li><a href="#org12a8e4d">8.22.11. scipy</a></li>
<li><a href="#org7d4a266">8.22.12. links</a></li>
</ul>
</li>
<li><a href="#org9700710">8.23. Optimization algorithms</a></li>
<li><a href="#org0dd4c9e">8.24. виды графиков</a>
<ul>
<li><a href="#org67e9391">8.24.1. простые линейные графики с описанием</a></li>
<li><a href="#orgae0ee2b">8.24.2. форматирование axis</a></li>
<li><a href="#org15756c8">8.24.3. гистограмма</a></li>
<li><a href="#org7d70c0d">8.24.4. box plot</a></li>
<li><a href="#org39b317a">8.24.5. bar plot, bar chart</a></li>
<li><a href="#orgdffeab5">8.24.6. Q–Q plot</a></li>
<li><a href="#org0244d11">8.24.7. Scatter plot</a></li>
<li><a href="#orgf41f5a2">8.24.8. Scatter matrix</a></li>
<li><a href="#orgde0da15">8.24.9. Correlation Matrix with heatmap</a></li>
<li><a href="#orgae67901">8.24.10. PDP</a></li>
<li><a href="#org48c0a5a">8.24.11. pie chart</a></li>
<li><a href="#org92dbf1b">8.24.12. sns.lmplot для 2 столбцов (scatter + regression)</a></li>
</ul>
</li>
<li><a href="#org54b2c34">8.25. виды графиков по назначению</a></li>
<li><a href="#org74c6b00">8.26. библиотеки для графиков</a></li>
<li><a href="#orgf8aecda">8.27. тексты</a></li>
<li><a href="#orga207164">8.28. типичное значение</a></li>
<li><a href="#org896af25">8.29. simularity measure - Коэффициент сходства</a></li>
<li><a href="#org602af8e">8.30. libs</a></li>
<li><a href="#org3f88075">8.31. decision tree</a>
<ul>
<li><a href="#orgda1e0ff">8.31.1. how it works</a></li>
</ul>
</li>
<li><a href="#orgd9d8cef">8.32. продуктовая аналитика</a></li>
<li><a href="#orgb085303">8.33. links</a></li>
</ul>
</li>
<li><a href="#org796d28f">9. Information retrieval</a>
<ul>
<li><a href="#orgff5efa6">9.1. measures</a></li>
</ul>
</li>
<li><a href="#orgd16ba8c">10. Recommender system</a>
<ul>
<li><a href="#org40a2a63">10.1. basic</a></li>
<li><a href="#org9422b5b">10.2. algorithms all</a></li>
<li><a href="#org5dad62b">10.3. matrix factorization</a>
<ul>
<li><a href="#org97b8b2d">10.3.1. links</a></li>
</ul>
</li>
<li><a href="#org1f63539">10.4. algoriths</a>
<ul>
<li><a href="#org7ec260a">10.4.1. memory based </a></li>
<li><a href="#org36e1d7b">10.4.2. Model-based</a></li>
<li><a href="#org6114e40">10.4.3. Deep learning</a></li>
<li><a href="#org8d181d3">10.4.4. keras</a></li>
<li><a href="#org70e9083">10.4.5. pyTorch - TorchRec</a></li>
<li><a href="#orgfefbfb8">10.4.6. TensorFlow Recommenders</a></li>
<li><a href="#org0e6ee66">10.4.7. Neural Graph Matching based Collaborative Filtering (GMCF)</a></li>
<li><a href="#org847903a">10.4.8. DLRM vs GMCF</a></li>
<li><a href="#org7618d58">10.4.9. surprise</a></li>
</ul>
</li>
<li><a href="#orgb6db5e3">10.5. datasets</a></li>
<li><a href="#orgb996072">10.6. simularity</a></li>
<li><a href="#org94c8436">10.7. terms</a></li>
<li><a href="#orgeeb3290">10.8. problems</a></li>
<li><a href="#org4dd8ec3">10.9. scikit-surprise</a></li>
<li><a href="#org932111f">10.10. <span class="todo TODO">TODO</span> https://github.com/sb-ai-lab/RePlay</a></li>
<li><a href="#orgc7fe6e5">10.11. TIGER: Transformer Index for GEnerative Recommenders</a></li>
<li><a href="#org1e8845a">10.12. links</a>
<ul>
<li><a href="#orgab9d677">10.12.1. Alternating Least Squares (ALS)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org4d0e414">11. Machine learning</a>
<ul>
<li><a href="#org1bb128b">11.1. steps</a></li>
<li><a href="#org0a88ed9">11.2. ensembles theory</a>
<ul>
<li><a href="#org16a243a">11.2.1. terms</a></li>
<li><a href="#org068ce1d">11.2.2. history</a></li>
<li><a href="#org689b015">11.2.3. Может ли набор слабых обучающих алгоритмов создать сильный обучающий алгоритм</a></li>
<li><a href="#org3e9096a">11.2.4. AdaBoost</a></li>
<li><a href="#org588f21b">11.2.5. Hoeffding's inequality</a></li>
<li><a href="#org2802d04">11.2.6. <span class="todo TODO">TODO</span> Bias-Variance Decompostion, Statistical Computational and Representational, Diversity</a></li>
<li><a href="#orgb37f51d">11.2.7. error rate</a></li>
<li><a href="#org46158d9">11.2.8. fusion strategy or combination methods</a></li>
<li><a href="#org927b592">11.2.9. mixture-of-experts</a></li>
<li><a href="#orga18b386">11.2.10. Sparse mixture-of-expert</a></li>
<li><a href="#org81a0c8f">11.2.11. Mixture-of-Denoisers (MoD)</a></li>
<li><a href="#org9f2501f">11.2.12. links</a></li>
</ul>
</li>
<li><a href="#org2a80553">11.3. Энтропия</a></li>
<li><a href="#org5aa3a11">11.4. Artificial general intelligence AGI or strong AI or full AI</a>
<ul>
<li><a href="#org28aff55">11.4.1. Symbolic AI or Good Old Fashioned AI (GOFAI)</a></li>
<li><a href="#org84c6cd9">11.4.2. Others</a></li>
</ul>
</li>
<li><a href="#org8a8ab62">11.5. Machine learning</a>
<ul>
<li><a href="#orgc919e2e">11.5.1. ML techniques</a></li>
<li><a href="#orgd90e5d4">11.5.2. terms</a></li>
<li><a href="#orgf29ce46">11.5.3. Смещение и дисперсия для анализа переобучения</a></li>
<li><a href="#org5c8671f">11.5.4. Regression vs. classification</a></li>
<li><a href="#org1d1660e">11.5.5. Reducing Loss (loss function) or cost function or residual </a></li>
<li><a href="#org64c7dc6">11.5.6. Regularization Overfeed problem</a></li>
<li><a href="#orgf7d47c8">11.5.7. Sampling</a></li>
<li><a href="#org116d8ef">11.5.8. CRF Conditional random field</a></li>
<li><a href="#orge335edd">11.5.9. типы обучения</a></li>
<li><a href="#orgcc6b6d1">11.5.10. Training, validation, and test sets</a></li>
<li><a href="#orgf23608b">11.5.11. с учителем</a></li>
<li><a href="#orga23523c">11.5.12. без учителя</a></li>
<li><a href="#org8522d4d">11.5.13. Structured prediction </a></li>
<li><a href="#orga906f80">11.5.14. курс ML Воронцов ШАД http://www.machinelearning.ru</a></li>
<li><a href="#org7a6ab49">11.5.15. метрики metrics</a></li>
<li><a href="#orgae5c43a">11.5.16. <span class="todo TODO">TODO</span> problems</a></li>
<li><a href="#org1a2669e">11.5.17. эконом эффективность</a></li>
<li><a href="#orgf8c13f2">11.5.18. Spike-timing-dependent plasticity STDP</a></li>
<li><a href="#org36a50c2">11.5.19. non-linearity</a></li>
<li><a href="#orga613b97">11.5.20. math</a></li>
<li><a href="#org68ef3d8">11.5.21. optimal configuration</a></li>
<li><a href="#org5f2bb33">11.5.22. <span class="todo TODO">TODO</span> merging</a></li>
<li><a href="#org627b999">11.5.23. training, Inference mode, frozen state</a></li>
<li><a href="#orgc7d4de4">11.5.24. MY NOTES</a></li>
<li><a href="#org191ac5c">11.5.25. Spatial Transformer Network (STN)</a></li>
<li><a href="#org1736492">11.5.26. Bayesian model averaging</a></li>
<li><a href="#org1752ef3">11.5.27. residual connection (or skip connection)</a></li>
<li><a href="#org8b9148c">11.5.28. vanishing gradient problem</a></li>
<li><a href="#org9e436b5">11.5.29. Multi-task learning(MTL)</a></li>
<li><a href="#org246a4ba">11.5.30. many classes</a></li>
<li><a href="#org52d937e">11.5.31. super-convergence Fast Training with Large Learnign rate</a></li>
<li><a href="#orga74b7ba">11.5.32. One Shot Learning &amp; Triple loss &amp; triple network</a></li>
<li><a href="#orgb56ef9d">11.5.33. Evaluation Metrices</a></li>
<li><a href="#org4a3a948">11.5.34. forecast</a></li>
<li><a href="#org943d5d0">11.5.35. Machine Learning Crash Course Google https://developers.google.com/machine-learning/crash-course/ml-intro</a></li>
<li><a href="#org108e2e1">11.5.36. Дилемма смещения–дисперсии Bias–variance tradeoff or Approximation-generalization tradeoff</a></li>
<li><a href="#orge55171e">11.5.37. Explainable AI (XAI) and Interpretable Machine Learning (IML) models</a></li>
</ul>
</li>
<li><a href="#orgf315e2a">11.6. Sampling</a>
<ul>
<li><a href="#org3527020">11.6.1. slice sampling</a></li>
</ul>
</li>
<li><a href="#orgba90cc2">11.7. likelihood, the log-likelihood, and the maximum likelihood estimate</a>
<ul>
<li><a href="#orge53a65c">11.7.1. links</a></li>
</ul>
</li>
<li><a href="#orgc08181f">11.8. Reinforcement learning (RL)</a>
<ul>
<li><a href="#orgfac2de1">11.8.1. terms</a></li>
<li><a href="#org5072921">11.8.2. basic</a></li>
<li><a href="#orgb0488d5">11.8.3. Exploration Strategy</a></li>
<li><a href="#org7e55299">11.8.4. RL algorithms</a></li>
<li><a href="#org5d5f6d5">11.8.5. environment is typically stated in the form of a Markov decision process (MDP)</a></li>
<li><a href="#org85c88f0">11.8.6. Dynamic programming</a></li>
<li><a href="#orgc43caad">11.8.7. Markov decision process (MDP)</a></li>
<li><a href="#org21ae7c1">11.8.8. Markov chain</a></li>
<li><a href="#org45e5152">11.8.9. Decision Transfromer</a></li>
<li><a href="#org1c611ca">11.8.10. Auto RL</a></li>
<li><a href="#org333e02a">11.8.11. tools</a></li>
<li><a href="#orgfe05efc">11.8.12. links</a></li>
</ul>
</li>
<li><a href="#orgf1bf663">11.9. Distributed training</a>
<ul>
<li><a href="#orge93e4c2">11.9.1. temrs</a></li>
<li><a href="#org6e6c7e8">11.9.2. all</a></li>
<li><a href="#org31881ce">11.9.3. tips</a></li>
<li><a href="#orgc248615">11.9.4. paradigms</a></li>
<li><a href="#org574300e">11.9.5. serverless computing and machine learning (SPIRT architecture)</a></li>
<li><a href="#org4be89c8">11.9.6. links</a></li>
</ul>
</li>
<li><a href="#orgdb2fdb3">11.10. Federated learning (or collaborative learning)</a></li>
<li><a href="#org020165d">11.11. Statistical classification</a>
<ul>
<li><a href="#org880df58">11.11.1. in satistics</a></li>
</ul>
</li>
<li><a href="#orgbc75a9c">11.12. Тематическое моделирование</a></li>
<li><a href="#org782d437">11.13. Популярные методы</a></li>
<li><a href="#org9851954">11.14. прогнозирование</a></li>
<li><a href="#orga905719">11.15. Сейчас</a>
<ul>
<li><a href="#org48496ee">11.15.1. примеры</a></li>
<li><a href="#orgba884dd">11.15.2. библиотеки</a></li>
</ul>
</li>
<li><a href="#org7cfccd8">11.16. kafka</a></li>
<li><a href="#orgab02882">11.17. в кредитных орг-ях</a></li>
<li><a href="#org3907ccc">11.18. <span class="todo TODO">TODO</span> Сбербанк проекты</a></li>
<li><a href="#orgbfd6a2d">11.19. KDTree simular</a></li>
<li><a href="#org9305ca3">11.20. Применение в банке</a></li>
<li><a href="#org87dbfeb">11.21. вспомогательные математические методы</a></li>
<li><a href="#org99a6ed0">11.22. AutoML</a>
<ul>
<li><a href="#org455f76a">11.22.1. <span class="todo TODO">TODO</span> https://github.com/sb-ai-lab/LightAutoML</a></li>
<li><a href="#orgd2a78d3">11.22.2. Neuton AutoML https://neuton.ai/</a></li>
</ul>
</li>
<li><a href="#orgb5c04fb">11.23. Известные Датасеты</a>
<ul>
<li><a href="#org12c2e77">11.23.1. signatures</a></li>
</ul>
</li>
<li><a href="#org8e7071e">11.24. игрушечные датасеты toy datasets</a>
<ul>
<li><a href="#org7b6c8a3">11.24.1. line with standard deviation</a></li>
<li><a href="#org9d36ed8">11.24.2. two bloabs of Gaussian distributions N(mu,sigma ^2)</a></li>
<li><a href="#orgb2ad11a">11.24.3. cosine with standard deviation</a></li>
<li><a href="#orgebe38e2">11.24.4. normal distribution</a></li>
</ul>
</li>
<li><a href="#org237fbd3">11.25. <span class="todo TODO">TODO</span> Genetic algorithms</a></li>
<li><a href="#org548c011">11.26. Causal inference - причинно следственный вывод</a>
<ul>
<li><a href="#org6a7afc9">11.26.1. terms</a></li>
<li><a href="#org4fa8d66">11.26.2. related topics</a></li>
<li><a href="#orga03fbc7">11.26.3. problems:</a></li>
<li><a href="#orgf9b7c6b">11.26.4. steps:</a></li>
<li><a href="#org6d9e4b0">11.26.5. <span class="todo TODO">TODO</span> frameworks for causal inference</a></li>
<li><a href="#orga93d3a5">11.26.6. methods</a></li>
<li><a href="#org4a86498">11.26.7. links</a></li>
</ul>
</li>
<li><a href="#org161d157">11.27. <span class="todo TODO">TODO</span> Uplift modelling</a>
<ul>
<li><a href="#orgc747856">11.27.1. <span class="todo TODO">TODO</span> Uplift modeling via Gradient Boosting paper</a></li>
<li><a href="#orgae5e8c3">11.27.2. dataset</a></li>
<li><a href="#org7b1b25a">11.27.3. customers segmentation</a></li>
<li><a href="#org9387b6b">11.27.4. metrics</a></li>
<li><a href="#orgd3014f5">11.27.5. mts</a></li>
</ul>
</li>
<li><a href="#orgc4e71a6">11.28. A/B test</a>
<ul>
<li><a href="#org352a0ca">11.28.1. <span class="todo TODO">TODO</span> links</a></li>
</ul>
</li>
<li><a href="#orgfed40cc">11.29. A/B test - Multi-Armed Bandit (MAB) - reinforcement learning problem</a></li>
<li><a href="#org30ecfa0">11.30. Regression</a></li>
<li><a href="#org37c6e11">11.31. Similarity (ˌsiməˈlerədē/)</a>
<ul>
<li><a href="#org6799ebc">11.31.1. Cosine similarity, Orchini similarity, Otsuka–Ochiai similarity</a></li>
</ul>
</li>
<li><a href="#orge47be08">11.32. <span class="todo TODO">TODO</span> Metric learning</a></li>
<li><a href="#org5d373ed">11.33. Compressing Models</a>
<ul>
<li><a href="#orge935861">11.33.1. Knowledge Distillation</a></li>
</ul>
</li>
<li><a href="#orga1f0a3f">11.34. Bayesian network</a></li>
</ul>
</li>
<li><a href="#org6a8d0b4">12. Artificial Neural Network and deep learning </a>
<ul>
<li><a href="#orgafb7694">12.1. <span class="todo TODO">TODO</span> frameworks</a>
<ul>
<li><a href="#org220c4c5">12.1.1. history</a></li>
<li><a href="#org4b890eb">12.1.2. list</a></li>
</ul>
</li>
<li><a href="#org8e2bb86">12.2. History</a>
<ul>
<li><a href="#orgea081cf">12.2.1. Перцептрон</a></li>
</ul>
</li>
<li><a href="#org5ddd1e8">12.3. Evolution of Deep Learning</a></li>
<li><a href="#org6fbf01d">12.4. persons</a></li>
<li><a href="#org2da93f9">12.5. Theory basis</a>
<ul>
<li><a href="#org41784ff">12.5.1. NN definition (stanford)</a></li>
<li><a href="#org121b0a7">12.5.2. activation functions</a></li>
<li><a href="#org86719f0">12.5.3. Regularization</a></li>
<li><a href="#org9e1cace">12.5.4. loss functions</a></li>
<li><a href="#org664f37f">12.5.5. Backpropagation </a></li>
<li><a href="#org2f886bb">12.5.6. limits of NN</a></li>
<li><a href="#orga121ed5">12.5.7. Self-organization</a></li>
<li><a href="#orgb5aa143">12.5.8. <span class="todo TODO">TODO</span> Universal approximation theorem</a></li>
</ul>
</li>
<li><a href="#org2c6349a">12.6. STEPS</a></li>
<li><a href="#orgd868be3">12.7. Конспект универ</a>
<ul>
<li><a href="#orgcb90a09">12.7.1. введение</a></li>
<li><a href="#org43b16b5">12.7.2. Обучение</a></li>
</ul>
</li>
<li><a href="#orga13decc">12.8. Data Augmentation</a>
<ul>
<li><a href="#orge689c6c">12.8.1. image libraries</a></li>
<li><a href="#org0ecef6e">12.8.2. CA conventional augmentation</a></li>
<li><a href="#org4956c65">12.8.3. <span class="todo TODO">TODO</span> AutoAugment method and Fast AutoAugment method</a></li>
<li><a href="#org3cc404f">12.8.4. <span class="todo TODO">TODO</span> RandAugment</a></li>
<li><a href="#org82e2b26">12.8.5. <span class="todo TODO">TODO</span> Self-paced Augmentation</a></li>
<li><a href="#orge5e0144">12.8.6. Data normalization and  Feature scaling </a></li>
<li><a href="#orgbb3d834">12.8.7. Boosting</a></li>
<li><a href="#orgdbc1e10">12.8.8. Input One-Hot Encode Контрастное кодирование</a></li>
</ul>
</li>
<li><a href="#org1dba684">12.9. Major network Architectures</a>
<ul>
<li><a href="#orge62418b">12.9.1. Unet</a></li>
</ul>
</li>
<li><a href="#orga23d0e5">12.10. Activation Functions φ(net)</a>
<ul>
<li><a href="#orgfd64a0b">12.10.1. links</a></li>
</ul>
</li>
<li><a href="#orgfe5b879">12.11. виды сетей и слоев</a>
<ul>
<li><a href="#org4a88c0f">12.11.1. Основные типы:</a></li>
<li><a href="#org5028a3f">12.11.2. Dense layer or fully-connected layer</a></li>
</ul>
</li>
<li><a href="#orga298211">12.12. Layer Normalization and Batch Normalization</a></li>
<li><a href="#orgd185141">12.13. hybrid networks</a></li>
<li><a href="#org8204404">12.14. Dynamic Neural Networks</a></li>
<li><a href="#org0683413">12.15. MLP, CNN, RNN, etc.</a>
<ul>
<li><a href="#org14c242d">12.15.1. LCN</a></li>
<li><a href="#org363632e">12.15.2. CNN </a></li>
<li><a href="#org03ea4e4">12.15.3. RNN recurrent [rɪˈkʌrənt] повторяющийся</a></li>
<li><a href="#orgaf4820e">12.15.4. RNTNs recursive [riːˈkɜːsɪv]</a></li>
<li><a href="#org6d4a97f">12.15.5. LSTM </a></li>
<li><a href="#orgb173af5">12.15.6. Attention, SAN self-attention, Transformer</a></li>
<li><a href="#orge02ff0e">12.15.7. NeRF</a></li>
<li><a href="#orgf4b659c">12.15.8. Autoencoders</a></li>
<li><a href="#org0a72468">12.15.9. Variational Autoencoders (VAE)</a></li>
</ul>
</li>
<li><a href="#orga8c1c96">12.16. batch and batch normalization</a></li>
<li><a href="#org45549ac">12.17. patterns of design</a></li>
<li><a href="#orgb7ddf90">12.18. <span class="todo TODO">TODO</span> MultiModal Machine Learning (MMML)</a>
<ul>
<li><a href="#orgac2b040">12.18.1. theory</a></li>
<li><a href="#org3df113c">12.18.2. real world task for MMML</a></li>
<li><a href="#org2e08cbc">12.18.3. <span class="todo TODO">TODO</span> core challenges in deep MMML</a></li>
<li><a href="#org7424cc6">12.18.4. current major systems</a></li>
<li><a href="#orgb0b78c9">12.18.5. datasets</a></li>
<li><a href="#orgeddca32">12.18.6. multimodal RAG for documents</a></li>
<li><a href="#org7e94ed4">12.18.7. links</a></li>
</ul>
</li>
<li><a href="#orgecfcfa4">12.19. challanges</a></li>
<li><a href="#orgba382f9">12.20. GAN Generative adversarial network </a></li>
<li><a href="#orgc39c8fb">12.21. inerpretation</a></li>
<li><a href="#orge8767df">12.22. multiclass(multi-class) classification problem or large number of classes problem</a></li>
<li><a href="#orgfb01a0f">12.23. Design Patterns for neural networks</a></li>
<li><a href="#org1b70b4b">12.24. Ways to optimize training of neural network</a></li>
</ul>
</li>
<li><a href="#org2586203">13. Natural Language Processing (NLP)</a>
<ul>
<li><a href="#orgdd01816">13.1. history</a></li>
<li><a href="#org65458e6">13.2. NLP pyramid</a></li>
<li><a href="#org68303af">13.3. Tokenization</a></li>
<li><a href="#orgbb87f50">13.4. Sentiment analysis definition (Liu 2010)</a></li>
<li><a href="#orgd8f6893">13.5. Approaches:</a></li>
<li><a href="#org70fcc59">13.6. Machine learning steps:</a></li>
<li><a href="#orgb15792d">13.7. Математические методы анализа текстов</a>
<ul>
<li><a href="#orgcccdb14">13.7.1. Определения:</a></li>
<li><a href="#org12e9f78">13.7.2. схема извлечения ключевых фраз</a></li>
<li><a href="#org0fcffbe">13.7.3. Оценка эффективности извлечения ключевых фраз:</a></li>
<li><a href="#org673acdd">13.7.4. предобработка plain text</a></li>
<li><a href="#org64a32c3">13.7.5. Коллокаци Collocations</a></li>
<li><a href="#org5bb41df">13.7.6. Полезные модули</a></li>
</ul>
</li>
<li><a href="#org30fd8a1">13.8. Извлечение именованных сущностей NER (Named-Entity Recognizing)</a>
<ul>
<li><a href="#orgfc2c3e0">13.8.1. Approaches to NER</a></li>
<li><a href="#orgb86a546">13.8.2. Deep learning</a></li>
<li><a href="#org3f67980">13.8.3. characteristics of the token &amp; text in a surrounding window</a></li>
<li><a href="#org5c6376e">13.8.4. Shape/orthographic features</a></li>
<li><a href="#org1d9475c">13.8.5. Metrics</a></li>
<li><a href="#orgf24cc63">13.8.6. С использованием нейронных сетей (CNN):</a></li>
<li><a href="#org7a33ff5">13.8.7. Apache OpenNLP</a></li>
<li><a href="#org2a4b06f">13.8.8. Natasha</a></li>
<li><a href="#org8538997">13.8.9. UDPipe</a></li>
</ul>
</li>
<li><a href="#org9b498b4">13.9. extracting features</a>
<ul>
<li><a href="#org3b22cae">13.9.1. bag-of-words bag of words</a></li>
</ul>
</li>
<li><a href="#org5844dc5">13.10. preprocessing</a>
<ul>
<li><a href="#orgbd5fd8d">13.10.1. Two existing strategies for applying pre-trained language representations to downstream tasks:</a></li>
<li><a href="#orgfc47e67">13.10.2. <span class="todo TODO">TODO</span> singular-value decomposition (SVD) Сингулярное разложение</a></li>
<li><a href="#org2ba741f">13.10.3. Word embedding</a></li>
</ul>
</li>
<li><a href="#org50df61d">13.11. n-gram</a></li>
<li><a href="#org2d70bcb">13.12. Bleu Score and WER Metrics</a></li>
<li><a href="#orgda4843c">13.13. Levels of analysis:</a>
<ul>
<li><a href="#org0d82a2e">13.13.1. old</a></li>
</ul>
</li>
<li><a href="#orga89adc4">13.14. Universal grammar</a></li>
<li><a href="#org569a0a5">13.15. Корпус языка</a></li>
<li><a href="#org8719f27">13.16. seq2seq model</a></li>
<li><a href="#orgbc00423">13.17. Рукописные цифры анализ</a></li>
<li><a href="#orgd2b428e">13.18. Fully-parallel text generation for neural machine translation</a></li>
<li><a href="#orge2d7f3b">13.19. speaker diarization task</a></li>
<li><a href="#org2651a2b">13.20. keyword extraction</a></li>
<li><a href="#org0720314">13.21. Approximate string matching or fuzzy string searching</a>
<ul>
<li><a href="#org099892d">13.21.1. steps</a></li>
<li><a href="#orga4d494a">13.21.2. agrep</a></li>
<li><a href="#org17a4e13">13.21.3. links</a></li>
</ul>
</li>
<li><a href="#org78f03a2">13.22. pre-training objective</a></li>
<li><a href="#org868691c">13.23. Principle of compositionality or Frege's principle</a></li>
<li><a href="#orgf864c21">13.24. 2023 major development</a></li>
<li><a href="#org7f45ac1">13.25. IntellectDialog - автоматизации взаимодействия с клиентами в мессенджерах</a></li>
<li><a href="#orgc2ddebb">13.26. Transformers applications for NLP</a>
<ul>
<li><a href="#org44dd10e">13.26.1. BERT Bidirectional Encoder Representations  from Transformers</a></li>
</ul>
</li>
<li><a href="#org333f8fd">13.27. metrics</a>
<ul>
<li><a href="#org24811ad">13.27.1. BLEU (bilingual evaluation understudy)</a></li>
<li><a href="#orgdafa00d">13.27.2. Perplexity</a></li>
<li><a href="#org4cad351">13.27.3. NIST - based on the BLEU</a></li>
<li><a href="#org04d428b">13.27.4. Word error rate (WER) or word accuracy (WAcc)</a></li>
</ul>
</li>
<li><a href="#org6a12d81">13.28. RLHF (Reinforcement Learning from Human Feedback) </a>
<ul>
<li><a href="#orgba4effc">13.28.1. classic</a></li>
<li><a href="#orgaae11d3">13.28.2. Direct Preference Optimization (DPO)</a></li>
<li><a href="#orge7bc397">13.28.3. ChatGPT 3 steps</a></li>
<li><a href="#orgc3fc4a7">13.28.4. 2024 RLF with Mixture of Judges Experts</a></li>
<li><a href="#orge9749cd">13.28.5. Multi-Armed Bandit problem </a></li>
<li><a href="#org0e736fe">13.28.6. links</a></li>
</ul>
</li>
<li><a href="#org2715230">13.29. Language Server</a></li>
<li><a href="#org6c018bf">13.30. word2vec - Skip-gram and CBOW</a></li>
<li><a href="#org530127e">13.31. GPT</a></li>
<li><a href="#org350a458">13.32. Text embeddings - neural retrival task</a>
<ul>
<li><a href="#org3259068">13.32.1. history</a></li>
<li><a href="#orge1ac411">13.32.2. terms</a></li>
<li><a href="#org394ae46">13.32.3. banchmarks</a></li>
<li><a href="#org6bc6be4">13.32.4. text encoders</a></li>
<li><a href="#orgda31c86">13.32.5. SLADE</a></li>
</ul>
</li>
<li><a href="#orga12f3e2">13.33. Text to speach</a>
<ul>
<li><a href="#org29b1703">13.33.1. Yandex Alice - news</a></li>
</ul>
</li>
<li><a href="#org4ea3b77">13.34. negative sampleing</a>
<ul>
<li><a href="#orga353f24">13.34.1. selecting negative samples strategies</a></li>
<li><a href="#orgdc6a614">13.34.2. example</a></li>
<li><a href="#orgf362a90">13.34.3. improved performance</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgd4f8e8e">14. LLM, chat bots,  conversational AI, intelligent virtual agents (IVAs)</a>
<ul>
<li><a href="#orge83ddcc">14.1. terms</a></li>
<li><a href="#org0275c13">14.2. complexity</a></li>
<li><a href="#org571525d">14.3. Context window problem</a>
<ul>
<li><a href="#orgdbc8afe">14.3.1. SSM</a></li>
</ul>
</li>
<li><a href="#orga9b2154">14.4. types</a></li>
<li><a href="#orgda090bc">14.5. tools</a></li>
<li><a href="#orgf9ce279">14.6. history</a></li>
<li><a href="#orgae96c1a">14.7. theory</a></li>
<li><a href="#orgd4aa89c">14.8. calculation or RAM required</a>
<ul>
<li><a href="#orgefde4a6">14.8.1. estimation of memory by parameters</a></li>
</ul>
</li>
<li><a href="#org7fe44cd">14.9. Adaptation to task - ICL vs Fine-tuning</a>
<ul>
<li><a href="#orgac8e948">14.9.1. when not enough?</a></li>
<li><a href="#orge46d29e">14.9.2. enhancing ICL</a></li>
</ul>
</li>
<li><a href="#org41a8569">14.10. Prompt engineering: цепочки и деревья команд к LLMs</a>
<ul>
<li><a href="#org0a40694">14.10.1. terms</a></li>
<li><a href="#org6b8a369">14.10.2. general findings</a></li>
<li><a href="#org77cc8c1">14.10.3. Chain of Thoughts (CoT) and Variants</a></li>
<li><a href="#org7d5a4ab">14.10.4. others techs</a></li>
<li><a href="#org5faa6ec">14.10.5. <span class="todo TODO">TODO</span> Automated Prompt Engineering (APE)</a></li>
</ul>
</li>
<li><a href="#orga616f9a">14.11. Fine-tuning</a>
<ul>
<li><a href="#orge67dad0">14.11.1. <b>Parameter-Efficient Finetuning</b> techniques (PEFT)</a></li>
<li><a href="#org93861f5">14.11.2. multi-task learning</a></li>
<li><a href="#org34df80c">14.11.3. links</a></li>
</ul>
</li>
<li><a href="#orgfcaac83">14.12. Hallucinations and checking of reasoning</a>
<ul>
<li><a href="#orgc62d29e">14.12.1. survey</a></li>
<li><a href="#orgaa41ead">14.12.2. selfcheckgpt - black-box</a></li>
<li><a href="#org6581972">14.12.3. detection of hallucinations</a></li>
<li><a href="#orgc956784">14.12.4. checking by LLM problems:</a></li>
<li><a href="#orgf9d041f">14.12.5. stopping hallucinations or mitigation of hallucinations</a></li>
<li><a href="#org3817564">14.12.6. WikiChat stops the hallucination</a></li>
<li><a href="#org82c3d8c">14.12.7. SelfCheck - prompt engineering for enhance <b>correctness of reasoning step</b></a></li>
<li><a href="#org233e0b5">14.12.8. banchmarks</a></li>
<li><a href="#org8f39080">14.12.9. Fact Checking</a></li>
<li><a href="#org1e6fe77">14.12.10. citates</a></li>
<li><a href="#orgf71a736">14.12.11. 2024 Self-Correct via Reinforcement Learning (google)</a></li>
</ul>
</li>
<li><a href="#org50daf95">14.13. choosing LLM model and architecture</a></li>
<li><a href="#org7c2bfbb">14.14. free chatgpt api, cloud models, LLM Providers</a></li>
<li><a href="#org9269488">14.15. instruction-following LLMs</a></li>
<li><a href="#org8cf9ad9">14.16. DISADVANTAGES AND PROBLEMS</a></li>
<li><a href="#org0b60d87">14.17. Advantages for programming</a></li>
<li><a href="#org69c9836">14.18. ability to use context from previous interactions to inform their responses to subsequent questions</a></li>
<li><a href="#orga62002f">14.19. GigaChat Sber</a></li>
<li><a href="#org4f67f66">14.20. GPT - Generative Pre-trained Transformer</a></li>
<li><a href="#org78fad9e">14.21. llama2 </a>
<ul>
<li><a href="#org09cd4aa">14.21.1. theory</a></li>
<li><a href="#org0398bf7">14.21.2. quantization libraries</a></li>
<li><a href="#orgd00ef14">14.21.3. jailbreak</a></li>
<li><a href="#org2d07113">14.21.4. gpt vs llama</a></li>
<li><a href="#orgf877eb8">14.21.5. fine tuning</a></li>
<li><a href="#org8aa0eab">14.21.6. stackllama</a></li>
<li><a href="#orgdae5276">14.21.7. distribute</a></li>
<li><a href="#org0581319">14.21.8. schema trl+deepspeed</a></li>
<li><a href="#org2a08d0f">14.21.9. wiki at work</a></li>
<li><a href="#orgc5a3fb8">14.21.10. links</a></li>
</ul>
</li>
<li><a href="#orga2ad51c">14.22. frameworks to control control LLM</a></li>
<li><a href="#org0a58d4d">14.23. size optimization</a></li>
<li><a href="#orge675c0e">14.24. distribute training - choose framework</a>
<ul>
<li><a href="#orgf96598a">14.24.1. wiki work</a></li>
<li><a href="#orgcbf1ce1">14.24.2. links</a></li>
</ul>
</li>
<li><a href="#org0b09a0e">14.25. <span class="todo TODO">TODO</span> bots</a></li>
<li><a href="#orgb6fa3b1">14.26. Inference optimization</a></li>
<li><a href="#org3e72010">14.27. pipeline</a>
<ul>
<li><a href="#org419d34c">14.27.1. types:</a></li>
<li><a href="#orgb854af3">14.27.2. use cases</a></li>
</ul>
</li>
<li><a href="#org8dbe94d">14.28. Knowledge Graph (KG)</a>
<ul>
<li><a href="#orgeff988c">14.28.1. terms</a></li>
<li><a href="#orga8f3553">14.28.2. types</a></li>
<li><a href="#org6a3a741">14.28.3. levels:</a></li>
<li><a href="#orgcbfbae4">14.28.4. building</a></li>
<li><a href="#org665642f">14.28.5. <span class="todo TODO">TODO</span> problems</a></li>
<li><a href="#orga1d60d2">14.28.6. usage patterns:</a></li>
<li><a href="#orgc36ae8b">14.28.7. Naive RAG, problems and Advanced technique</a></li>
<li><a href="#org590a6c6">14.28.8. <span class="todo TODO">TODO</span> RAG loss</a></li>
<li><a href="#org8833f12">14.28.9. RAG - indexing</a></li>
<li><a href="#orge7068cd">14.28.10. RAG - graph-based database</a></li>
<li><a href="#org5c4990d">14.28.11. RAG - types of graphs</a></li>
<li><a href="#org32524a7">14.28.12. GRAG - alternatives</a></li>
<li><a href="#orgfaaacbd">14.28.13. GRAG - graph-based RAG</a></li>
<li><a href="#org9a98e21">14.28.14. Contriver - contrastive retriver</a></li>
<li><a href="#org87117cc">14.28.15. SBERT and sentence transformers</a></li>
<li><a href="#orgea59377">14.28.16. Reasoning on KG</a></li>
<li><a href="#org45ba7bd">14.28.17. GNN-RAG - GNN + LLM for RAG-based KGQA</a></li>
<li><a href="#org56fbb09">14.28.18. reranking model or cross-encoder - A two-stage retrieval system.</a></li>
<li><a href="#orgaf78970">14.28.19. prompt</a></li>
<li><a href="#org42e07a7">14.28.20. open source RAGs</a></li>
</ul>
</li>
<li><a href="#org20bab2d">14.29. Articles and Research automation</a>
<ul>
<li><a href="#orgec0ff6a">14.29.1. https://github.com/SakanaAI/AI-Scientist</a></li>
<li><a href="#org0e47ed7">14.29.2. https://github.com/stanford-oval/storm</a></li>
</ul>
</li>
<li><a href="#org13fe032">14.30. RAG-пайплайн or framework</a>
<ul>
<li><a href="#orga2147ce">14.30.1. Steps:</a></li>
<li><a href="#org313975a">14.30.2. Original paper</a></li>
<li><a href="#orgb71421d">14.30.3. RAG python inference</a></li>
</ul>
</li>
<li><a href="#org153ec6f">14.31. tools</a></li>
<li><a href="#org45dd0e0">14.32. vector database </a>
<ul>
<li><a href="#org354a8a3">14.32.1. internal implementation</a></li>
<li><a href="#orgace62e2">14.32.2. lmdb vs redis vs redict</a></li>
<li><a href="#org5226218">14.32.3. sqlite vs Redis vs Clickhouse</a></li>
<li><a href="#orgf982c62">14.32.4. Elasticsearch vs edgedb vs taxi vs Chroma vs pgvector vs VQLite vs Weaviate</a></li>
<li><a href="#org220fb95">14.32.5. Faiss</a></li>
<li><a href="#orga1c441c">14.32.6. fastest Qdrant vs Epsilla vs Chroma</a></li>
<li><a href="#orgb33d6d9">14.32.7. Most Used Vectorstores</a></li>
<li><a href="#orga4135e3">14.32.8. Milvus</a></li>
<li><a href="#org0482372">14.32.9. vector database vs hash tables vs tree based</a></li>
</ul>
</li>
<li><a href="#org54e3b22">14.33. LangChain</a>
<ul>
<li><a href="#org2e64bf0">14.33.1. cons:</a></li>
</ul>
</li>
<li><a href="#orgc608dc3">14.34. Promt Engineering vs Train Foundation Models vs Adapters</a></li>
<li><a href="#orga7a75c6">14.35. <span class="todo TODO">TODO</span> Named tensor notation.</a></li>
<li><a href="#orgfcdee08">14.36. Agents, auto-programming</a>
<ul>
<li><a href="#orgbf0d526">14.36.1. terms</a></li>
<li><a href="#orgd314395">14.36.2. theory</a></li>
<li><a href="#org82efb4a">14.36.3. <span class="todo TODO">TODO</span> Development steps</a></li>
<li><a href="#orgfc1136f">14.36.4. Developer tools</a></li>
<li><a href="#orgcd8ffda">14.36.5. links</a></li>
</ul>
</li>
<li><a href="#org56ef4b6">14.37. Jailbreaks</a></li>
<li><a href="#org468eebe">14.38. Spreadsheets</a></li>
<li><a href="#org012bf1e">14.39. USECASES</a></li>
<li><a href="#orgc899a92">14.40. <span class="todo TODO">TODO</span> Alpha telega bot</a></li>
<li><a href="#org0551b8c">14.41. personal IDE, PC helpers</a></li>
<li><a href="#orge27ff74">14.42. private data</a></li>
<li><a href="#org40c52d5">14.43. standardization</a></li>
<li><a href="#orgde9e868">14.44. NLP metrics</a></li>
<li><a href="#org37c2984">14.45. interpretation</a></li>
<li><a href="#orgf02bb86">14.46. links</a></li>
</ul>
</li>
<li><a href="#org669987d">15. Adversarial machine learning</a>
<ul>
<li><a href="#org5ed42db">15.1. linear classifiers - spam - evasion attacks</a></li>
</ul>
</li>
<li><a href="#org16a6808">16. Diffusion NN (DNN)</a>
<ul>
<li><a href="#orgdc984ea">16.1. history</a></li>
<li><a href="#org8a445d7">16.2. DALL-E</a></li>
<li><a href="#orgc260bbb">16.3. Basics: forward and reverse diffusion processes and sampling procedure</a></li>
<li><a href="#orgfdc4d1a">16.4. optimizations</a></li>
<li><a href="#orge981576">16.5. links</a></li>
</ul>
</li>
<li><a href="#org8643923">17. OLD deploy tf keras</a></li>
<li><a href="#org993fea1">18. deeppavlov lections</a></li>
<li><a href="#org9fc68cc">19. passport</a>
<ul>
<li><a href="#org144f8e3">19.1. error</a></li>
<li><a href="#org0f5e93f">19.2. Расчет контрольной суммы</a></li>
<li><a href="#orgc21432d">19.3. passport serial number</a></li>
<li><a href="#org95ba083">19.4. string metric for measuring the difference between two sequences</a></li>
</ul>
</li>
<li><a href="#org97326b1">20. captcha</a>
<ul>
<li><a href="#orgcfa3ec5">20.1. audio capcha</a></li>
<li><a href="#org298446a">20.2. <span class="todo TODO">TODO</span> split file by worlds</a></li>
<li><a href="#orge34b1e2">20.3. reCAPTCHA google</a></li>
<li><a href="#orgcb5b99f">20.4. image captcha</a>
<ul>
<li><a href="#orgdb03097">20.4.1. <span class="todo TODO">TODO</span> remove colour</a></li>
</ul>
</li>
<li><a href="#org5e2b6ea">20.5. tesseract fine-tuning</a></li>
<li><a href="#org375d583">20.6. links</a></li>
</ul>
</li>
<li><a href="#orgd144ae7">21. kaggle</a>
<ul>
<li><a href="#org7d5e4ab">21.1. 1C forecast</a></li>
<li><a href="#orgfd4434f">21.2. Keras measure of intelligence</a>
<ul>
<li><a href="#orged3fdf2">21.2.1. teory</a></li>
<li><a href="#orgc2ea517">21.2.2. new in AI since 2017</a></li>
<li><a href="#org49cbf7c">21.2.3. automatic programming</a></li>
<li><a href="#org82aded6">21.2.4. Data</a></li>
<li><a href="#orgdc27d37">21.2.5. MY programming</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org557bd57">22. ИИ в банках</a>
<ul>
<li><a href="#orgd360038">22.1. 2020 Ассоция российских банков обсудила https://banks.cnews.ru/news/line/2020-01-24_v_assotsiatsii_rossijski</a></li>
</ul>
</li>
<li><a href="#orga80d842">23. MLOps and ModelOps (Machine Learning Operations)</a>
<ul>
<li><a href="#orgbfeaff9">23.1. terms</a></li>
<li><a href="#orgf599d6e">23.2. Deployment Types:</a></li>
<li><a href="#org3884e82">23.3. DevOps</a>
<ul>
<li><a href="#org0184096">23.3.1. DevOps strategies</a></li>
</ul>
</li>
<li><a href="#orgb38fa55">23.4. CRISP-ML. The ML Lifecycle Process. </a>
<ul>
<li><a href="#orgc0c4cc3">23.4.1. CRISP-ML(Q) states main characteristics of mode choose: ⚿</a></li>
<li><a href="#org13f05ac">23.4.2. phases</a></li>
</ul>
</li>
<li><a href="#orgbd878c1">23.5. Challenges with the ML Process:</a></li>
<li><a href="#org3f7c8ca">23.6. implemetation steps:</a></li>
<li><a href="#org1f85a56">23.7. pipeline services or workflow management software (WMS)</a></li>
<li><a href="#org1d84f62">23.8. tasks and tools</a>
<ul>
<li><a href="#org34bb6a7">23.8.1. tasks</a></li>
<li><a href="#orgd0f399a">23.8.2. tools</a></li>
</ul>
</li>
<li><a href="#org3fec1f2">23.9. principles</a></li>
<li><a href="#org5558078">23.10. standard</a>
<ul>
<li><a href="#org2502763">23.10.1. ISO/IEC DIS 5259-1 Artificial intelligence — Data quality for analytics and machine learning (ML) — Part 1: Overview, terminology, and examples</a></li>
</ul>
</li>
<li><a href="#org6a60dbb">23.11. TFX - Tensorflow Extended</a></li>
<li><a href="#orge77c217">23.12. <span class="todo TODO">TODO</span> Kubeflow</a></li>
<li><a href="#org5d332a1">23.13. <span class="todo TODO">TODO</span> Airflow</a></li>
<li><a href="#orgcdb86de">23.14. <span class="todo TODO">TODO</span> - mlmodel service</a></li>
<li><a href="#org73d2c5e">23.15. <span class="todo TODO">TODO</span> continuous training</a></li>
<li><a href="#orgcd19c52">23.16. <span class="todo TODO">TODO</span> Feature attribution or feature importance</a></li>
<li><a href="#orga39a4f1">23.17. Monitoring</a>
<ul>
<li><a href="#org775f0a0">23.17.1. metrics</a></li>
<li><a href="#org7c60644">23.17.2. batch vs online</a></li>
</ul>
</li>
<li><a href="#org6fd739e">23.18. Principles</a>
<ul>
<li><a href="#orga6e47c4">23.18.1. effectivenes metrics</a></li>
</ul>
</li>
<li><a href="#orgb308b19">23.19. links</a></li>
</ul>
</li>
<li><a href="#org77eb9ed">24. Automated machine learning (AutoML)</a>
<ul>
<li><a href="#org18bc08d">24.1. major papers</a></li>
<li><a href="#orge584993">24.2. history</a></li>
<li><a href="#org3d287ca">24.3. tasks</a></li>
<li><a href="#orgee0a05a">24.4. approaches</a></li>
<li><a href="#org42992a1">24.5. banchmark</a></li>
<li><a href="#org87daa64">24.6. opensource frameworks</a></li>
<li><a href="#org21dee2b">24.7. popular web</a>
<ul>
<li><a href="#org50c91c4">24.7.1. ml space horovod + tensorflow</a></li>
</ul>
</li>
<li><a href="#org6e58b8c">24.8. classification of tasks</a></li>
<li><a href="#org9f73318">24.9. automl &amp; blockchain</a></li>
<li><a href="#org16ac881">24.10. books</a></li>
</ul>
</li>
<li><a href="#org26e72e9">25. Big Data</a></li>
<li><a href="#org9f74c6e">26. hard questions</a></li>
<li><a href="#orgeca2b55">27. cloud, clusters</a>
<ul>
<li><a href="#orgb36cbd9">27.1. Data Anonymization, Dataset Privacy, Scrubbing Techniques</a>
<ul>
<li><a href="#orgf85bf92">27.1.1. terms</a></li>
<li><a href="#org5a1ea77">27.1.2. Scrubbing Techniques</a></li>
<li><a href="#org49daad9">27.1.3. tools</a></li>
<li><a href="#org5ea6deb">27.1.4. links</a></li>
</ul>
</li>
<li><a href="#orgff21e1e">27.2. docker NVIDIA Container Toolkit</a></li>
</ul>
</li>
<li><a href="#orgf2cfc87">28. Data Roles - Data team</a>
<ul>
<li><a href="#org27ca940">28.1. Architect -</a></li>
<li><a href="#orgfb081db">28.2. System analyst</a></li>
<li><a href="#orgc59b812">28.3. Data Engineers</a></li>
<li><a href="#orgf0c68c9">28.4. Data Analysts</a></li>
<li><a href="#org7e5588a">28.5. Data Engineer+ Data Analytic</a></li>
<li><a href="#org4eafb7d">28.6. Data Scientist</a></li>
<li><a href="#orgef16602">28.7. Machine Learning Engineers</a></li>
<li><a href="#orgfe558e7">28.8. Backend Engineer</a></li>
<li><a href="#org182e237">28.9. Project manager (web3)</a></li>
<li><a href="#org5145987">28.10. Manager of ML team</a></li>
<li><a href="#org831cd3f">28.11. MLOps</a></li>
<li><a href="#orgea67ddb">28.12. Admin Linux/DevOps</a></li>
<li><a href="#org64d0f46">28.13. AI High Performance Computing Engineer</a>
<ul>
<li><a href="#org475e7fe">28.13.1. terms</a></li>
<li><a href="#orgc836d88">28.13.2. workloads</a></li>
<li><a href="#org6eca746">28.13.3. artcles</a></li>
<li><a href="#org64b9c28">28.13.4. NVIDIA</a></li>
<li><a href="#org06e3141">28.13.5. cooling</a></li>
<li><a href="#org22b9551">28.13.6. blogs</a></li>
<li><a href="#orgc9a8309">28.13.7. network</a></li>
<li><a href="#org4f2c909">28.13.8. ways to apply AI in HPC</a></li>
</ul>
</li>
<li><a href="#orga393f58">28.14. ML infrastructure engineer, ML platform engineer</a></li>
<li><a href="#org629d2cf">28.15. ML accelerator/hardware engineer</a></li>
<li><a href="#orgda1238f">28.16. Product analytic</a></li>
<li><a href="#org4bae08a">28.17. <span class="todo TODO">TODO</span> Operations research ?</a></li>
<li><a href="#orgd53ccbb">28.18. Optimization Modeling Specialist</a></li>
<li><a href="#orgb0e680c">28.19. links</a></li>
</ul>
</li>
<li><a href="#org01ca6c2">29. ML Scientists</a></li>
<li><a href="#org91a8441">30. pyannote - audio</a>
<ul>
<li><a href="#orgd4e3cdf">30.1. comparizion nvidia and pyannote</a></li>
</ul>
</li>
<li><a href="#orgb8f50b1">31. AI Coding Assistants</a>
<ul>
<li><a href="#org295476e">31.1. tasks</a></li>
<li><a href="#orgbc70b7e">31.2. products</a></li>
<li><a href="#org656f5b7">31.3. wide abilities</a></li>
<li><a href="#org291274b">31.4. narrow abilities</a></li>
<li><a href="#org9c6b7a1">31.5. heavy abilities</a></li>
<li><a href="#org4ca455f">31.6. Approaches: skillsets vs traditional agent</a></li>
</ul>
</li>
<li><a href="#orgca7a43d">32. Generative AI articles</a></li>
<li><a href="#org79beed5">33. Miracle webinars</a>
<ul>
<li><a href="#orgc7056df">33.1. Leveraging Explainable AI and GCP for predicting Loan Risk on Vimeo</a>
<ul>
<li><a href="#org422d253">33.1.1. link</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgdc08060">34. semi-supervised learning or week supervision</a>
<ul>
<li><a href="#org464b9e9">34.1. may refer to</a></li>
</ul>
</li>
<li><a href="#orgd55bf63">35. Mojo - language</a></li>
<li><a href="#orge45980c">36. интересные AI проекты</a></li>
<li><a href="#orgbeea570">37. nuancesprog.ru</a>
<ul>
<li><a href="#orgc41f3d2">37.1. общепринятая базовая оценка</a></li>
<li><a href="#org33e0f06">37.2. remove constant columns with VarianceThreshold</a></li>
<li><a href="#org791cecc">37.3. sklearn pitfalls</a></li>
</ul>
</li>
<li><a href="#org78bedde">38. NEXT LEVEL</a>
<ul>
<li><a href="#org6855d9a">38.1. learn</a></li>
</ul>
</li>
<li><a href="#org9e02bc4">39. sobes, собеседование</a>
<ul>
<li><a href="#org5966497">39.1. SQL</a></li>
<li><a href="#orgf2f205c">39.2. statistic</a></li>
<li><a href="#org1bbab5d">39.3. DS</a></li>
<li><a href="#org7d12157">39.4. ML</a></li>
<li><a href="#org0c7c743">39.5. DL</a></li>
<li><a href="#org7042c4f">39.6. Python</a></li>
<li><a href="#orgca5246c">39.7. NLP</a>
<ul>
<li><a href="#org0796dc0">39.7.1. https://github.com/masmahbubalom/InterviewQuestions/blob/main/NLP%20Interview%20Questions/README.md</a></li>
</ul>
</li>
<li><a href="#org0d4c428">39.8. CV:</a></li>
<li><a href="#org86b8c56">39.9. СберМаркет</a></li>
<li><a href="#org6f32947">39.10. MLOps:</a></li>
<li><a href="#org24b7497">39.11. DevOps</a></li>
<li><a href="#orgf43c695">39.12. Docker</a></li>
<li><a href="#org0a790db">39.13. prompt engineer</a></li>
<li><a href="#orge65e0cd">39.14. Общие вопросы:</a></li>
<li><a href="#org4b0c049">39.15. Поведение</a></li>
<li><a href="#orgf97632c">39.16. Секция Linux:</a></li>
<li><a href="#org6ea4e9a">39.17. Секция Network:</a></li>
</ul>
</li>
<li><a href="#orgc33ddac">40. articles</a>
<ul>
<li><a href="#org8077a36">40.1. 2019 A Survey of Optimization Methods from a Machine Learning Perspective</a>
<ul>
<li><a href="#org1f523e1">40.1.1. applications</a></li>
<li><a href="#org8d1c054">40.1.2. categories of methods:</a></li>
<li><a href="#org61b984e">40.1.3. problems</a></li>
<li><a href="#orgfa4a18b">40.1.4. 1)</a></li>
<li><a href="#orgec62c84">40.1.5. Summary of First-Order Optimization Methods </a></li>
<li><a href="#org58546d3">40.1.6. Summary of High-Order Optimization Methods</a></li>
<li><a href="#orgb56e56b">40.1.7. Available Toolkits for Optimization</a></li>
</ul>
</li>
<li><a href="#orge5573d4">40.2. 2023 A Survey on Machine Learning from Few Samples</a></li>
<li><a href="#org95b6809">40.3. <span class="todo TODO">TODO</span> DPO Direct Performance Optimization - training on pairs</a></li>
</ul>
</li>
<li><a href="#orgbff88b1">41. hardware</a>
<ul>
<li><a href="#orgf782cac">41.1. embedded networks</a></li>
</ul>
</li>
<li><a href="#org419e325">42. formats</a></li>
<li><a href="#org557b012">43. Free Courses</a>
<ul>
<li><a href="#org6629b3e">43.1. Beginer</a></li>
<li><a href="#org35fa6cd">43.2. Intermediate Level:</a></li>
<li><a href="#org19222bd">43.3. Advanced Level:</a></li>
</ul>
</li>
<li><a href="#org68cb82c">44. <span class="todo TODO">TODO</span> Model compression - smaller</a></li>
<li><a href="#org805fa0b">45. <span class="todo TODO">TODO</span> fusion operator optimization</a></li>
<li><a href="#org450512d">46. SAS (Statistical analysis system)</a></li>
</ul>
</div>
</div>
<p>
-<b>- mode: Org; fill-column: 110; coding: utf-8; -</b>-
</p>


<p>
Overwhelming topics  <a href="https://en.wikipedia.org/wiki/List_of_numerical_analysis_topics">https://en.wikipedia.org/wiki/List_of_numerical_analysis_topics</a>
</p>

<p>
Similar text categorization problems (word vectors, sentence vectors) <a href="https://stackoverflow.com/questions/64739194/similar-text-categorization-problems-word-vectors-sentence-vectors">https://stackoverflow.com/questions/64739194/similar-text-categorization-problems-word-vectors-sentence-vectors</a>
</p>

<p>
blog of one bustard <a href="https://github.com/senarvi/senarvi.github.io/tree/master/_posts">https://github.com/senarvi/senarvi.github.io/tree/master/_posts</a>
</p>

<p>
<a href="#org2601ee1">12</a>
<a href="#org6f1939c">12.15.2</a>
</p>
<div id="outline-container-org44984eb" class="outline-2">
<h2 id="org44984eb"><span class="section-number-2">1.</span> conferences</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="https://www.ijcai.org/">https://www.ijcai.org/</a>
</p>
</div>
</div>
<div id="outline-container-org4087d7e" class="outline-2">
<h2 id="org4087d7e"><span class="section-number-2">2.</span> best links</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li><a href="https://scholar.google.com">https://scholar.google.com</a></li>
<li>Sachin Date Master of Science, research direcotor, India  <a href="https://timeseriesreasoning.com">https://timeseriesreasoning.com</a></li>
<li>www.yuan-meng.com</li>
<li><a href="https://paperswithcode.com/methods/category/autoregressive-transformers">https://paperswithcode.com/methods/category/autoregressive-transformers</a></li>
</ul>

<p>
news:
</p>
<ul class="org-ul">
<li><a href="https://research.yandex.com/blog">https://research.yandex.com/blog</a></li>
</ul>

<p>
hackatons, news:
</p>
<ul class="org-ul">
<li><a href="https://ods.ai/">https://ods.ai/</a></li>
</ul>

<p>
97 Things Every Data Engineer Should Know
<a href="https://books.google.ru/books?id=ZTQzEAAAQBAJ&amp;pg=PT19&amp;hl=ru&amp;source=gbs_selected_pages&amp;cad=2#v=onepage&amp;q&amp;f=false">https://books.google.ru/books?id=ZTQzEAAAQBAJ&amp;pg=PT19&amp;hl=ru&amp;source=gbs_selected_pages&amp;cad=2#v=onepage&amp;q&amp;f=false</a>
</p>

<p>
best statistic blog <a href="https://www.youtube.com/@statisticsninja">https://www.youtube.com/@statisticsninja</a>
</p>

<p>
CV Neural networks in sports <a href="https://www.youtube.com/channel/UCHuEgvSdCWXBLAUvR516P1w">https://www.youtube.com/channel/UCHuEgvSdCWXBLAUvR516P1w</a>
</p>

<p>
<a href="https://machinelearningmastery.com/">https://machinelearningmastery.com/</a>
</p>

<p>
BibTeX <a href="https://aclanthology.org/">https://aclanthology.org/</a> - hosts 93419 papers on the study of computational linguistics and natural language processing.
</p>
<ul class="org-ul">
<li>a digital library of research papers</li>
</ul>

<p>
ML cases - system designs <a href="https://www.evidentlyai.com/ml-system-design">https://www.evidentlyai.com/ml-system-design</a>
</p>

<p>
Deep Learning Tutorials: University of Amsterdam <a href="https://uvadlc-notebooks.readthedocs.io">https://uvadlc-notebooks.readthedocs.io</a>
</p>
<ul class="org-ul">
<li>Jax, GNN, Self-Supervised Contrastive Learning</li>
<li>Vision Transformers</li>
<li>Meta Learning</li>
<li>Autoregressive Image Modeling</li>
<li>Deep Energy Models</li>
</ul>

<p>
<a href="https://www.freecodecamp.org/news/tag/data-science/">https://www.freecodecamp.org/news/tag/data-science/</a>
</p>

<p>
<a href="https://github.com/andresvourakis/data-scientist-handbook">https://github.com/andresvourakis/data-scientist-handbook</a>
</p>

<p>
ITMO University github.com/aimclub/open-source-ops/tree/master/meetups
</p>

<p>
Autoencoders, GAN, VAE, diffusion <a href="https://github.com/HSE-LAMBDA/DeepGenerativeModels/tree/spring-2024-short/seminars">https://github.com/HSE-LAMBDA/DeepGenerativeModels/tree/spring-2024-short/seminars</a>
</p>

<p>
Course: Embeddings, CV, multimodal transformers, RAG <a href="https://www.marqo.ai/courses/fine-tuning-embedding-models">https://www.marqo.ai/courses/fine-tuning-embedding-models</a>
</p>

<p>
books
</p>
<ul class="org-ul">
<li><a href="https://github.com/lovingers/ML_Books/tree/master">https://github.com/lovingers/ML_Books/tree/master</a></li>
<li><a href="https://paulvanderlaken.com/2019/03/12/best-free-programming-books-i-still-need-to-read/">https://paulvanderlaken.com/2019/03/12/best-free-programming-books-i-still-need-to-read/</a></li>
</ul>


<p>
yandex prepare
</p>
<ul class="org-ul">
<li><a href="http://web.stanford.edu/class/cs224n/?fbclid=IwAR0Ykb8VIX7UZwgmht3vnta1Ec3zb-CQMijr715WkF8YJ8MJRW0_gFM5hpA">http://web.stanford.edu/class/cs224n/?fbclid=IwAR0Ykb8VIX7UZwgmht3vnta1Ec3zb-CQMijr715WkF8YJ8MJRW0_gFM5hpA</a></li>
<li><a href="https://github.com/yandexdataschool/nlp_course">https://github.com/yandexdataschool/nlp_course</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
<li>RecSys/ClassicML</li>
<li><a href="https://arxiv.org/pdf/2009.10311.pdf">https://arxiv.org/pdf/2009.10311.pdf</a></li>
<li><a href="https://arxiv.org/pdf/1607.01759.pdf">https://arxiv.org/pdf/1607.01759.pdf</a></li>
<li><a href="https://arxiv.org/pdf/1606.07792.pdf">https://arxiv.org/pdf/1606.07792.pdf</a></li>
<li>algo</li>
<li><a href="https://habr.com/ru/articles/188010/">https://habr.com/ru/articles/188010/</a></li>
<li><a href="https://m.habrahabr.ru/company/yandex/blog/337690/">https://m.habrahabr.ru/company/yandex/blog/337690/</a></li>
</ul>

<p>
NLP:
</p>
<ul class="org-ul">
<li>2009 Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition
<a href="https://home.cs.colorado.edu/~martin/slp.html">https://home.cs.colorado.edu/~martin/slp.html</a></li>
</ul>
</div>
<div id="outline-container-org88023d8" class="outline-3">
<h3 id="org88023d8"><span class="section-number-3">2.1.</span> blogs</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li><a href="https://research.google/blog/">https://research.google/blog/</a></li>
<li><a href="https://research.yandex.com/">https://research.yandex.com/</a></li>
</ul>
</div>
</div>
<div id="outline-container-org44f2785" class="outline-3">
<h3 id="org44f2785"><span class="section-number-3">2.2.</span> papers</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li><a href="https://arxiv.org">https://arxiv.org</a></li>
<li><a href="https://www.semanticscholar.org/">https://www.semanticscholar.org/</a></li>
<li><a href="https://github.com/Anoncheg1/llm-selected-articles">https://github.com/Anoncheg1/llm-selected-articles</a></li>
<li>Papers without pay <a href="https://sci-hub.st/">https://sci-hub.st/</a></li>
</ul>
</div>
</div>
<div id="outline-container-orge50e86e" class="outline-3">
<h3 id="orge50e86e"><span class="section-number-3">2.3.</span> AI/ML Russian repositories <a id="org9ad01af"></a></h3>
<div class="outline-text-3" id="text-2-3">
<p>
github.com/aimclub
</p>

<p>
habr.com/ru/companies/spbifmo/articles/805455
</p>

<ul class="org-ul">
<li><b>ITMO University AIM.CLUB</b> <a href="https://github.com/aimclub/">https://github.com/aimclub/</a>
<ul class="org-ul">
<li>FEDOT - Automated modeling and machine learning framework
<ul class="org-ul">
<li>core: алгоритмы структурной и параметрической оптимизации направленных графов — выделилось в отдельный
фреймворк GOLEM, а специализированные инструменты для работы с промышленными временными рядами — в
FEDOT.Industrial.</li>
<li>unique: automation of solving the problem of time series classification.</li>
<li>build upon: catboost, lightgbm, xgboost, statsmodels, ete3 (trees), scikit-learn, NetworkX, sktime (time-serieses)</li>
</ul></li>
<li>BAMT - Bayesian networks.</li>
<li>GOLEM - Graph Optimiser</li>
<li>GEFEST - (Generative Evolution For Encoded STructures) is a toolbox for the generative design of physical
objects.</li>
</ul></li>
</ul>


<ul class="org-ul">
<li><b>HSE University</b>
<ul class="org-ul">
<li>hsemotion, - face emotion recognition in photos and videos
<ul class="org-ul">
<li><a href="https://github.com/av-savchenko/face-emotion-recognition">https://github.com/av-savchenko/face-emotion-recognition</a></li>
</ul></li>
<li>roerich, - change point detection for time series analysis, signal processing, and segmentation
<ul class="org-ul">
<li><a href="https://github.com/HSE-LAMBDA/roerich">https://github.com/HSE-LAMBDA/roerich</a></li>
</ul></li>
<li>probaforms - generative models for tabular data:  conditional GAN, Normalizing Flows, Var. Autoencoders
<ul class="org-ul">
<li><a href="https://github.com/HSE-LAMBDA/probaforms">https://github.com/HSE-LAMBDA/probaforms</a></li>
</ul></li>
</ul></li>
<li><b>МФТИ, SPC, Moscow Institute of Physics and Technology (MIPT)</b>
<ul class="org-ul">
<li>DeepPavlov, - dialog systems and chatbots. NLP framework built on PyTorch and transformers.
<ul class="org-ul">
<li><a href="https://github.com/deeppavlov/DeepPavlov">https://github.com/deeppavlov/DeepPavlov</a></li>
</ul></li>
<li>kmath - Kotlin-based analog to Python's NumPy library.
<ul class="org-ul">
<li><a href="https://github.com/SciProgCentre/kmath">https://github.com/SciProgCentre/kmath</a></li>
</ul></li>
</ul></li>
<li><b>Skoltech</b>
<ul class="org-ul">
<li>ttpy, <a href="https://github.com/oseledets/ttpy">https://github.com/oseledets/ttpy</a></li>
<li>h2tools - H<sup>2</sup> -matrices, on numpy. efficient for integral equations or particle-to-particle interactions.
<ul class="org-ul">
<li><a href="https://bitbucket.org/muxas/h2tools/">https://bitbucket.org/muxas/h2tools/</a> <a href="https://pythonhosted.org/h2tools/">https://pythonhosted.org/h2tools/</a></li>
</ul></li>
</ul></li>
<li><b>Yandex</b> <a href="https://github.com/yandex/">https://github.com/yandex/</a>
<ul class="org-ul">
<li>catboost - Gradient Boosting on Decision Trees  <a href="https://github.com/catboost/catboost">https://github.com/catboost/catboost</a></li>
<li>YaLM-100B is a GPT-like neural network for generating and processing text.</li>
<li>YaFSDP - Sharded Data Parallelism framework, designed to work well with transformer-like neural network
architectures. Competitor to FSDP of PyTorch for distributed learning.</li>
<li>rep - wrapper for popular ML libraries. try to extends scikit-learn.</li>
<li>ch-tools, ch-backup - administration and diagnostics and Backup tools for ClickHouse.</li>
<li>database ???????</li>
</ul></li>
<li><b>ETNA-team, corl-team</b> (old Tinkoff team)
<ul class="org-ul">
<li>etna, <a href="https://github.com/etna-team/etna">https://github.com/etna-team/etna</a></li>
<li>corl, <a href="https://github.com/corl-team/CORL">https://github.com/corl-team/CORL</a></li>
<li>reBRAC <a href="https://github.com/DT6A/ReBRAC">https://github.com/DT6A/ReBRAC</a></li>
</ul></li>
<li><b>sb-ai-lab “СБЕР”</b> <a href="https://github.com/sb-ai-lab/">https://github.com/sb-ai-lab/</a>
<ul class="org-ul">
<li>LightAutoML - Fast and customizable framework for automatic ML model creation (AutoML)</li>
<li>RePlay - Framework for Building End-to-End Recommendation Systems with State-of-the-Art Models</li>
<li>eco2ai - accumulates statistics about power consumption and CO2 emission during running code.</li>
<li>Py-Boost - Python based GBDT implementation on GPU. multiclass/multilabel/multitask training</li>
<li>HypEx - framework for automatic Causal Inference.</li>
<li>Sim4Rec - Simulator for training and evaluation of Recommender Systems</li>
<li>AutoMLWhitebox - or AutoWoE - automatic creation of interpretable ML model based on feature binning, WoE
features transformation, feature selection and Logistic Regression.</li>
<li>SLAMA - LightAutoML on Spark</li>
<li>ESGify - NLP model for multilabel news classification with respect to 47 ESG risks (company environmental,
social, and governance factors that could cause reputation or financial harm.)</li>
<li>sb-obp - Open Bandit Pipeline for Open Bandit Dataset: a python library for bandit algorithms and
off-policy evaluation</li>
</ul></li>
<li><b>AIRI Artificial Intelligence Research Institute</b> <a href="https://github.com/AIRI-Institute/">https://github.com/AIRI-Institute/</a>
<ul class="org-ul">
<li>pogema - Partially-Observable Grid Environment for Multiple Agents. grid-based, can generate maps, can be
tailored to a variety of PO-MAPF settings</li>
<li>GENA<sub>LM</sub> - a framework for active learning annotation in NLP: text classification and sequence
tagging. instead of annotating random samples, you annotate a portion of the examples that are most useful
to improving the model.</li>
<li>AriGraph - memory model for LLM agents interacting with environment and multi-hop question answering tasks.
<ul class="org-ul">
<li><a href="https://arxiv.org/abs/2407.04363">https://arxiv.org/abs/2407.04363</a></li>
</ul></li>
<li>ai<sub>toolbox</sub> - framework for active learning in NLP</li>
<li>eco4cast - reduce carbon footprint of machine learning models</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org126e6a8" class="outline-3">
<h3 id="org126e6a8"><span class="section-number-3">2.4.</span> youtube</h3>
<div class="outline-text-3" id="text-2-4">
<p>
2021 Deep Learning <a href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A">https://www.youtube.com/playlist?list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A</a>
</p>

<p>
Tinkoff <a href="https://www.youtube.com/channel/UCrzOqlmsQ_QF1Oi455sGfzA">https://www.youtube.com/channel/UCrzOqlmsQ_QF1Oi455sGfzA</a>
</p>
<ul class="org-ul">
<li>Tinkoff.AI — Infinity RecSys <a href="https://www.youtube.com/watch?v=I_iGZ_LshWA&amp;list=PLLrf_044z4Jp1OoWEox1VZRNc6QnUElQC">https://www.youtube.com/watch?v=I_iGZ_LshWA&amp;list=PLLrf_044z4Jp1OoWEox1VZRNc6QnUElQC</a></li>
<li>Ahead-of-Time P-Tuning <a href="https://www.youtube.com/watch?v=PgLL5XQSIi4&amp;list=PLLrf_044z4JrVk-BMqt5mkzVDxkHLL2ez">https://www.youtube.com/watch?v=PgLL5XQSIi4&amp;list=PLLrf_044z4JrVk-BMqt5mkzVDxkHLL2ez</a></li>
<li>NLP Research vs Abstract Deadlines <a href="https://www.youtube.com/watch?v=Hp625Q8t9ZI&amp;list=PLLrf_044z4Jq-in0z_fqU2HQHe0JI4cq6">https://www.youtube.com/watch?v=Hp625Q8t9ZI&amp;list=PLLrf_044z4Jq-in0z_fqU2HQHe0JI4cq6</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge5aca5f" class="outline-2">
<h2 id="orge5aca5f"><span class="section-number-2">3.</span> most frequent math methods</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>3/2 = math.exp(-math.log(2/3))</li>
<li>to log: log(value+1)</li>
<li>from log: exp(value) - 1</li>
<li>oldrange:0-240, new:0-100 =&gt; MinMaxScaling = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin =&gt; x*100 // 240</li>
<li>Percentage = (Part / Total) * 100</li>
</ul>
</div>

<div id="outline-container-org3920eff" class="outline-3">
<h3 id="org3920eff"><span class="section-number-3">3.1.</span> layout resolution</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>x/y = 2</li>
<li>x*y = 440</li>
<li>y = sqrt(440 / 2)</li>
<li>x = 440 / x</li>
</ul>
</div>
</div>

<div id="outline-container-org2cc9442" class="outline-3">
<h3 id="org2cc9442"><span class="section-number-3">3.2.</span> model size in memory</h3>
<div class="outline-text-3" id="text-3-2">
<p>
in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam
 optimizer <a href="https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer">https://huggingface.co/docs/transformers/perf_train_gpu_one#optimizer</a>
</p>

<ul class="org-ul">
<li>7B parameter model would use (2+8)*7B=70GB</li>
<li>(2+8)*7*10**9/1024/1024/1024</li>
</ul>
</div>
</div>

<div id="outline-container-org39e4e08" class="outline-3">
<h3 id="org39e4e08"><span class="section-number-3">3.3.</span> compare two objects by features</h3>
<div class="outline-text-3" id="text-3-3">
<p>
We cannot if we don't know max and min values of features.
But if we know, that min value is 0 and all max of features in the same distance from max:
</p>
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #cae682;">row1</span> = {<span style="color: #95e454;">'SPEAKER_00'</span>: 21.667442, <span style="color: #95e454;">'SPEAKER_00_fuzz'</span>: 100}
<span style="color: #cae682;">row2</span> = {<span style="color: #95e454;">'SPEAKER_01'</span>: 7.7048755, <span style="color: #95e454;">'SPEAKER_01_fuzz'</span>: 741}

<span style="color: #cae682;">a</span> = np.array([[row1[<span style="color: #95e454;">'SPEAKER_00'</span>], row1[<span style="color: #95e454;">'SPEAKER_00_fuzz'</span>]],
          [row2[<span style="color: #95e454;">'SPEAKER_01'</span>], row2[<span style="color: #95e454;">'SPEAKER_01_fuzz'</span>]]
          ]
         )
<span style="color: #e5786d;">print</span>((a.<span style="color: #e5786d;">max</span>(axis=0) - 0))
<span style="color: #cae682;">a</span> = a/ (a.<span style="color: #e5786d;">max</span>(axis=0) - 0)
<span style="color: #e5786d;">print</span>(a)
<span style="color: #8ac6f2; font-weight: bold;">if</span> np.<span style="color: #e5786d;">sum</span>(a[0] - a[1]) &gt; 0:
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">'SPEAKER_00 has greater value'</span>)
<span style="color: #8ac6f2; font-weight: bold;">else</span>:
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">'SPEAKER_01 has greater value'</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org41a1d3f" class="outline-3">
<h3 id="org41a1d3f"><span class="section-number-3">3.4.</span> distance matrix</h3>
<div class="outline-text-3" id="text-3-4">
</div>
<div id="outline-container-org515fc07" class="outline-4">
<h4 id="org515fc07"><span class="section-number-4">3.4.1.</span> calc</h4>
<div class="outline-text-4" id="text-3-4-1">
<p>
two forms:
</p>
<dl class="org-dl">
<dt>distance array</dt><dd>(distvec = pdist(x))</dd>
<dt>square form</dt><dd>(squareform(distvec))</dd>
</dl>

<div class="org-src-container">
<pre class="src src-python" id="org963376a"><span style="color: #8ac6f2; font-weight: bold;">from</span> scipy.spatial.distance <span style="color: #8ac6f2; font-weight: bold;">import</span> pdist
<span style="color: #8ac6f2; font-weight: bold;">from</span> scipy.spatial.distance <span style="color: #8ac6f2; font-weight: bold;">import</span> squareform
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np

<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">" --------- distance array:"</span>)
<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">cal</span>(x, y):
    <span style="color: #e5786d;">print</span>((x- y)[0])
    <span style="color: #8ac6f2; font-weight: bold;">return</span>(x- y)[0]

<span style="color: #cae682;">ar</span> = np.array([[2, 0, 2], [2, 2, 3], [-2, 4, 5], [0, 1, 9], [2, 2, 4]])

<span style="color: #cae682;">distvec</span> = pdist(ar, metric = cal)
<span style="color: #e5786d;">print</span>()
<span style="color: #e5786d;">print</span>(distvec)
<span style="color: #e5786d;">print</span>()
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">" --------- square form:"</span>)
<span style="color: #cae682;">sqf</span> = squareform(distvec)
<span style="color: #e5786d;">print</span>(sqf)
<span style="color: #e5786d;">print</span>()
</pre>
</div>

<pre class="example" id="org15c722d">
 --------- distance array:
0
4
2
0
4
2
0
-2
-4
-2

[ 0.  4.  2.  0.  4.  2.  0. -2. -4. -2.]

 --------- square form:
[[ 0.  0.  4.  2.  0.]
 [ 0.  0.  4.  2.  0.]
 [ 4.  4.  0. -2. -4.]
 [ 2.  2. -2.  0. -2.]
 [ 0.  0. -4. -2.  0.]]
</pre>

<pre class="example" id="org3a20fe0">
 --------- distance array:
[2 0 2] [2 2 3]
[2 0 2] [-2  4  5]
[2 0 2] [0 1 9]
[2 0 2] [2 2 4]
[2 2 3] [-2  4  5]
[2 2 3] [0 1 9]
[2 2 3] [2 2 4]
[-2  4  5] [0 1 9]
[-2  4  5] [2 2 4]
[0 1 9] [2 2 4]

[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]

 --------- square form:
[[0. 1. 1. 1. 1.]
 [1. 0. 1. 1. 1.]
 [1. 1. 0. 1. 1.]
 [1. 1. 1. 0. 1.]
 [1. 1. 1. 1. 0.]]
</pre>
</div>
</div>

<div id="outline-container-org280b95c" class="outline-4">
<h4 id="org280b95c"><span class="section-number-4">3.4.2.</span> find lowest/max</h4>
<div class="outline-text-4" id="text-3-4-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np

np.fill_diagonal(sqf, np.inf)
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"sqf</span><span style="color: #e5786d; font-weight: bold;">\n</span><span style="color: #95e454;">"</span>, sqf)
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">closest_points = sqf.argmin(keepdims=False) # indexes along axis=0</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print(closest_points)</span>
<span style="color: #cae682;">i</span>, <span style="color: #cae682;">j</span> = np.where(sqf==sqf.<span style="color: #e5786d;">min</span>())
<span style="color: #cae682;">i</span>, <span style="color: #cae682;">j</span> = i[0], j[0]
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"result indexes:"</span>, i, j)
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"result:</span><span style="color: #e5786d; font-weight: bold;">\n\t</span><span style="color: #95e454;">"</span>, ar[i], <span style="color: #95e454;">"</span><span style="color: #e5786d; font-weight: bold;">\n\t</span><span style="color: #95e454;">"</span>, ar[j])
</pre>
</div>

<pre class="example" id="org12af597">
sqf
 [[inf  0.  4.  2.  0.]
 [ 0. inf  4.  2.  0.]
 [ 4.  4. inf -2. -4.]
 [ 2.  2. -2. inf -2.]
 [ 0.  0. -4. -2. inf]]
result indexes: 2 4
result:
	 [-2  4  5]
	 [2 2 4]
</pre>
</div>
</div>

<div id="outline-container-org0df514e" class="outline-4">
<h4 id="org0df514e"><span class="section-number-4">3.4.3.</span> faster</h4>
<div class="outline-text-4" id="text-3-4-3">
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">matrix_rand_score</span>(a, b):
    <span style="color: #cae682;">correl</span> = np.zeros((<span style="color: #e5786d;">len</span>(a), <span style="color: #e5786d;">len</span>(b)), dtype=<span style="color: #e5786d;">float</span>)
    <span style="color: #8ac6f2; font-weight: bold;">for</span> i, ac <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">enumerate</span>(a):
        <span style="color: #8ac6f2; font-weight: bold;">for</span> j, bc <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">enumerate</span>(b):
            <span style="color: #8ac6f2; font-weight: bold;">if</span> i &gt; j:
                <span style="color: #8ac6f2; font-weight: bold;">continue</span>
            <span style="color: #cae682;">c</span> = ac+bc
            <span style="color: #e5786d;">print</span>(i,j, c)
            <span style="color: #cae682;">correl</span>[<span style="color: #cae682;">i</span>, <span style="color: #cae682;">j</span>] = c
    <span style="color: #8ac6f2; font-weight: bold;">return</span> correl

<span style="color: #cae682;">v</span> = matrix_rand_score([1,2,3,4], [6,7,8,9])
<span style="color: #e5786d;">print</span>(v)
</pre>
</div>

<pre class="example" id="org7a3aa25">
0 0 7
0 1 8
0 2 9
0 3 10
1 1 9
1 2 10
1 3 11
2 2 11
2 3 12
3 3 13
[[ 7.  8.  9. 10.]
 [ 0.  9. 10. 11.]
 [ 0.  0. 11. 12.]
 [ 0.  0.  0. 13.]]
</pre>
</div>
</div>
</div>

<div id="outline-container-org25fcba0" class="outline-3">
<h3 id="org25fcba0"><span class="section-number-3">3.5.</span> interpolation <a id="orgaa144d7"></a></h3>
<div class="outline-text-3" id="text-3-5">
<p>
<b>PolynomialFeatures - polynomial regression</b>
</p>

<ol class="org-ol">
<li>create Vandermonde matrix</li>
</ol>
<pre class="example">
[[1, x_0, x_0 ** 2, x_0 ** 3, ..., x_0 ** degree]
</pre>

<ol class="org-ol">
<li>in: y = ß0 + ß1*x + ß2*x2 + … + ßn*xn we trying to find B0, B1, B2 &#x2026; Bn with linear regression</li>
</ol>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt
<span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.preprocessing <span style="color: #8ac6f2; font-weight: bold;">import</span> PolynomialFeatures
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.linear_model <span style="color: #8ac6f2; font-weight: bold;">import</span> Ridge

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">interpol</span>(x,y, xn):
    <span style="color: #cae682;">poly</span> = PolynomialFeatures(degree=4, include_bias=<span style="color: #e5786d; font-weight: bold;">False</span>)
    <span style="color: #cae682;">ridge</span> = Ridge(alpha=0.006)

    <span style="color: #cae682;">x_appr</span> = np.linspace(x[0], xn, num=15)
    <span style="color: #cae682;">x</span> = np.array(x).reshape(-1,1)

    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">-- train</span>
    <span style="color: #cae682;">x_poly</span> = poly.fit_transform(x)
    ridge.fit(np.array(x_poly), y) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">train</span>

    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">-- test</span>
    <span style="color: #cae682;">x_appr_poly</span> = poly.fit_transform(x_appr.reshape(-1,1))
    <span style="color: #cae682;">y_pred</span> = ridge.predict(x_appr_poly) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">test</span>

    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">-- plot train</span>
    plt.scatter(x, y)

    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">-- plot test</span>
    plt.plot(x_appr, y_pred)
    plt.scatter(x_appr[-1], y_pred[-1])
    plt.ylabel(<span style="color: #95e454;">"time in minutes"</span>)
    plt.title(<span style="color: #95e454;">"interpolation of result for 25 max: "</span>+ <span style="color: #e5786d;">str</span>(<span style="color: #e5786d;">round</span>(y[-1], 2)))
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">plt.savefig('./autoimgs/result_appr.png')</span>
    plt.show()
    plt.close()
    <span style="color: #8ac6f2; font-weight: bold;">return</span> y_pred[-1]


<span style="color: #cae682;">x</span> = [5,15,20]
<span style="color: #cae682;">y</span> = [10,1260, 12175] <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">result</span>
<span style="color: #cae682;">yn</span> = interpol(x,y,xn)
<span style="color: #e5786d;">print</span>(yn)
</pre>
</div>

<pre class="example">
42166.34032715159
</pre>


<p>
<a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html">https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html</a>
</p>
</div>
</div>
<div id="outline-container-orgfc373a5" class="outline-3">
<h3 id="orgfc373a5"><span class="section-number-3">3.6.</span> softmax</h3>
<div class="outline-text-3" id="text-3-6">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #cae682;">z</span> = np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0])
<span style="color: #cae682;">softmax</span> = np.exp(z)/<span style="color: #e5786d;">sum</span>(np.exp(z))
<span style="color: #e5786d;">print</span>(softmax)
<span style="color: #e5786d;">print</span>(- np.log(softmax))
</pre>
</div>

<pre class="example">
[0.02364054 0.06426166 0.1746813  0.474833   0.02364054 0.06426166
 0.1746813 ]
[3.74479212 2.74479212 1.74479212 0.74479212 3.74479212 2.74479212
 1.74479212]
</pre>
</div>
</div>

<div id="outline-container-orgfb0908b" class="outline-3">
<h3 id="orgfb0908b"><span class="section-number-3">3.7.</span> minimize the negative log likelihood instead of maximizing the likelihood.</h3>
<div class="outline-text-3" id="text-3-7">
<p>
minimizing the negative log likelihood is mathematically equivalent to maximizing the likelihood.  The
 negative log likelihood formulation helps in simplifying the optimization process and aligns with the
 convention of minimizing a cost function.
</p>

<p>
We take the natural logarithm of the likelihood function, which transforms the product into a sum.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #8ac6f2; font-weight: bold;">from</span> scipy.optimize <span style="color: #8ac6f2; font-weight: bold;">import</span> minimize

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Generate some sample data from a normal distribution</span>
np.random.seed(0)
<span style="color: #cae682;">data</span> = np.random.normal(loc=100, scale=15, size=1000)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Define the log likelihood function</span>
<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">log_likelihood</span>(params):
    <span style="color: #cae682;">mu</span>, <span style="color: #cae682;">sigma</span> = params
    <span style="color: #8ac6f2; font-weight: bold;">return</span> -np.<span style="color: #e5786d;">sum</span>(np.log(sigma * np.sqrt(2 * np.pi)) + (data - mu)**2 / (2 * sigma**2))

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Define the negative log likelihood for minimization</span>
<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">neg_log_likelihood</span>(params):
    <span style="color: #8ac6f2; font-weight: bold;">return</span> -log_likelihood(params)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Initial guess for parameters</span>
<span style="color: #cae682;">initial_guess</span> = [50, 10]

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Minimize the negative log likelihood</span>
<span style="color: #cae682;">result</span> = minimize(neg_log_likelihood, initial_guess, method=<span style="color: #95e454;">'SLSQP'</span>)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Print the maximum likelihood estimates</span>
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"Maximum Likelihood Estimates: mu = {:.2f}, sigma = {:.2f}"</span>.<span style="color: #e5786d;">format</span>(result.x, result.x))
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">The output will give you the maximum likelihood estimates for &#956; and &#963;, which</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">should be close to the true values used to generate the data (100 and 15,</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">respectively).</span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgc6bbe62" class="outline-2">
<h2 id="orgc6bbe62"><span class="section-number-2">4.</span> common terms</h2>
<div class="outline-text-2" id="text-4">
<dl class="org-dl">
<dt>feature [ˈfiːʧə]</dt><dd>explanatory variable in statistic or property of observation or juct column</dd>
<dt>(no term)</dt><dd>observation</dd>
<dt>feature map</dt><dd>is the output activations for a given filter after sliding the filter over all the
locations. number of feature maps equal to number of output channels and filters.</dd>
<dt>sample</dt><dd>selected observations</dd>
<dt>sampling</dt><dd>is a selection of a subset to estimate charactersitics of the whole</dd>
<dt>variance [ˈve(ə)rɪəns]</dt><dd>дисперсия, разброс, результат переобучения</dd>
<dt>bias [ˈbaɪəs]</dt><dd>смещение, результат недообучения</dd>
<dt>pipeline [ˈpaɪplaɪn]</dt><dd>поэтапный процесс МЛ, используется для параметризации всего процесса</dd>
<dt>layer [ˈleɪə]</dt><dd>structure has input and output, part of NN</dd>
<dt>(no term)</dt><dd>weight [weɪt]</dd>
<dt>(no term)</dt><dd>end-to-end Deep Learning process -</dd>
<dt>(no term)</dt><dd>State-of-the-Art (SOTA) models</dd>
<dt>data ingesion</dt><dd>[ɪn'hiːʒən] - more broader term than ETL, is the process of connecting a wide variety of
data structures into where it needs to be in a given required format and quality. to get data into any
systems (storage and/or applications) that require data in a particular structure or format for operational
use of the data downstream.</dd>
<dt>Stochastic</dt><dd>the property of being well described by a random probability distribution</dd>
<dt>latent space or latent feature space or embedding space</dt><dd><p>
abstract multi-dimensional space containing
feature values that we cannot interpret directly, but which encodes a meaningful internal representation of
externally observed events.
</p>
<ul class="org-ul">
<li>in math: is an embedding of a set of items within a manifold in which items resembling each other are</li>
</ul>
<p>
positioned closer to one another in the latent space
</p></dd>
<dt>model selection</dt><dd>task of choosing the best algorithm and settings of it's parameters</dd>
<dt>stratification</dt><dd>class percentage maintained for both training and validation sets</dd>
<dt>Degrees of freedom (df)</dt><dd>is the number of values in the final calculation of a statistic that are free to
vary. количество «свободных» величин, необходимых для того, чтобы полностью определить вектор. может быть не
только натуральным, но и любым действительным числом.</dd>
<dt>Среднеквадратическое отклонение, Standard deviation</dt><dd>square root of the variance
<ul class="org-ul">
<li>:: √( ∑(deviations of each data point from the mean) / n)</li>
</ul></dd>
<dt>Statistical inference</dt><dd>is a collection of methods that deal with drawing conclusions from data that are
prone to random variation.</dd>
<dt>derivative test</dt><dd>if function is differentiable, for finding maxima.</dd>
<dt>Probability distribution</dt><dd>probabilities of occurrence</dd>
<dt>independent and identically distributed i.i.d., iid, or IID</dt><dd>criteria that features tell something new
every and was collected together that is why telling about same object y.</dd>
<dt>receptive field</dt><dd>is defined as the size of the region in the input that produces the feature</dd>
<dt>convolutional operation</dt><dd>is a linear application of a smaller filter/kernel to a larger input (sliding)
that results in an output feature map.</dd>
<dt>convolutional kernel or filter</dt><dd>apply to input image and result a single number.</dd>
<dt>head</dt><dd>top of a network - just output; or prediction head - output with loss function</dd>
<dt>data labeling or labels</dt><dd>target in dataset, usually produced by hired people.</dd>
<dt>Sparsity</dt><dd>is a measure of how many elements in a tensor are exact zeros, relative to the tensor size. A
tensor is considered sparse if "most" of its elements are zero.</dd>
<dt>convention of minimizing a cost function</dt><dd>optimization - finding the values of variables to reduce cost function.</dd>
<dt>inductive bias</dt><dd>the set of assumptions that a learning algorithm uses to make predictions or
generalizations about unseen data based on the observed training data.
<dl class="org-dl">
<dt>Relational Biases</dt><dd>define the structure of the relationships between different entities or parts in our
model.</dd>
<dt>Dynamic</dt><dd>learning model designed to shift their bias as they acquire more data. However, even the
process of shifting bias itself must be guided by some form of bias[1].</dd>
</dl></dd>
<dt>Discriminative models</dt><dd>conditional probability distribution of the output label given the input features,
denoted as P(Y∣X), concentrate on the direct mapping between inputs and outputs. learn to find the decision
boundary that separates different classes in the input space</dd>
</dl>
</div>
</div>
<div id="outline-container-org7fea063" class="outline-2">
<h2 id="org7fea063"><span class="section-number-2">5.</span> rare terms</h2>
<div class="outline-text-2" id="text-5">
<dl class="org-dl">
<dt>residual [rɪˈzɪdjʊəl]</dt><dd>differences between observed and predicted values of data</dd>
<dt>error term</dt><dd>statistical error or disturbance [dɪsˈtɜːbəns] + e</dd>
<dt>Type I error</dt><dd>(false positive) более критична чем 2-го рода</dd>
<dt>Type II error</dt><dd>(false negative) понятия задач проверки статистических гипотез</dd>
<dt>fold</dt><dd>equal sized subsamples in cross-validation</dd>
<dt>terms of reference</dt><dd>техническое задание</dd>
<dt>neuron's <b>receptive field</b></dt><dd>each neuron receives input from only a restricted area of the previous layer</dd>
<dt>Adversarial machine learning</dt><dd>where an attacker inputs data into a machine learning model with the aim to cause mistakes.</dd>
<dt>Coefficient of determination R<sup>2</sup></dt><dd>Его рассматривают как универсальную меру зависимости одной случайной
величины от множества других. Это доля дисперсии зависимой переменной, объясняемая рассматриваемой моделью
зависимости, то есть объясняющими переменными.  is the proportion of the variation in the dependent variable
that is predictable from the independent variable(s). Con: есть свойство, что чем больше количество
независимых переменных, тем большим он становится, вносят ли дополнительные «объясняющие переменные» вклад в
«объяснительную силу».</dd>
<dt>Adjusted coefficient of determination</dt><dd>fix con.</dd>
<dt>shrinkage [ˈSHriNGkij]</dt><dd>method of reduction in the effects of sampling variation.</dd>
<dt>skewness [ˈskjuːnɪs]</dt><dd>a measure of the asymmetry of the probability distribution of a real-valued random
variable about its mean. positive - left, negative - right. 0 - no skew</dd>
<dt>Kurtosis [kəˈtəʊsɪs]</dt><dd>measure of the "tailedness" of the probability distribution (like skewness, but for
peak). 0 -</dd>
<dt>Information content, self-information, surprisal, Shannon information</dt><dd>alternative way of expressing
probability, quantifying the level of "surprise" of a particular outcome. odds or log-odds</dd>
</dl>
</div>
</div>
<div id="outline-container-orgd08f9b1" class="outline-2">
<h2 id="orgd08f9b1"><span class="section-number-2">6.</span> number of parameters calculations</h2>
<div class="outline-text-2" id="text-6">
<p>
Keras Conv2D
</p>
<ul class="org-ul">
<li>out<sub>channels</sub> * (in<sub>channels</sub> * kernel<sub>h</sub> * kernel<sub>w</sub> + 1)  # 1 for bias - count of channels</li>
<li>independent of input image size, because kernel is slides across the input</li>
</ul>
</div>
</div>
<div id="outline-container-orgae4003d" class="outline-2">
<h2 id="orgae4003d"><span class="section-number-2">7.</span> Tasks, problems classification</h2>
<div class="outline-text-2" id="text-7">
<ul class="org-ul">
<li>ranking - ранжирование -  Information retrieval (IR) -
<ul class="org-ul">
<li>relevance score s = f(x), x=(q,d), q is a query, d is a document</li>
</ul></li>
</ul>

<p>
ML:
</p>
<ul class="org-ul">
<li>multi-armed bandit problem - a decision maker iteratively selects one of multiple fixed choices (i.e., arms
or actions) when the properties of each choice are only partially known at the time of allocation, and may
become better understood as time passes.</li>
<li>Boolean satisfiability problem (SAT or B-SAT) - Check werher given boolean expression can be satisfiable -
can be made TRUE by assigning appropriate logical values (i.e. TRUE, FALSE) to its variables.</li>
</ul>

<p>
Metric learning
</p>
<ul class="org-ul">
<li>clusterization</li>
<li>Dimensionality reduction снижение размерности</li>
</ul>

<p>
NLP: <a href="https://arxiv.org/pdf/2307.10652">https://arxiv.org/pdf/2307.10652</a>
</p>
<ul class="org-ul">
<li>Text classifiction</li>
<li>Word representation learning</li>
<li>Machine translation</li>
<li>NER (Named-Entity Recognizing) - classify named entities (also seeks to locate)</li>
<li>Information extraction</li>
<li>Knowledge Graph Question Answering (KGQA)</li>
<li>Nature Language generation</li>
<li>Dialogue system</li>
<li>Delation Learning &amp; Knowledge Graphs</li>

<li>Sentiment and Emotion Analysis (sarcasm, thwarting) - classifies of emotions (positive, negative and neutral)
<ul class="org-ul">
<li>speech emotion recognition (SER)</li>
</ul></li>
<li>speech recognition, automatic speech recognition (ASR)</li>
<li>Speaker verification - voices comarision</li>
<li>Named entity recognition</li>
<li>Topic modelling - descover the abstract "topic"</li>
<li>topic segmentation
<ul class="org-ul">
<li>speaker diarization - structuring an audio stream into speaker turns
<ul class="org-ul">
<li>speaker segmentation - finding speaker change points in an audio stream</li>
<li>speaker clustering - grouping together speech segments on the basis of speaker characteristics</li>
</ul></li>
<li>Voice activity detection (VAD) is the task of detecting speech regions in a given audio stream or recording.</li>
<li>Semantic Role Labeling (automatically identify actors and actions)</li>
<li>Word Sense Disambiguation - Identifies which sense of a word is used in a sentence</li>
<li>Keyword spotting (or word spotting) or Keyword Extraction - find instance in large data without fully recognition.</li>
<li>Speech-to-text</li>
<li>Text-to-speech (TTS)</li>
<li>relationship extraction</li>
<li>Question answering</li>
<li>Summarisation</li>
</ul></li>
</ul>


<p>
NLU - (subfield of NLP) - Natural language understanding
</p>
<ul class="org-ul">
<li>relation extraction</li>
<li>semantic parsing</li>
<li>paraprase &amp; natural language inference</li>
<li>semantic analysis</li>
<li>dialogue agents</li>
</ul>

<p>
Audio &amp; Speack
</p>
<ul class="org-ul">
<li>STT (speech-to-text)</li>
<li>TTS (text-to-speech)</li>
<li>Audio classification - классификация звука</li>
<li>Source Separation - разделение звуков по источникам</li>
<li>Diarization - разделение говорящих</li>
<li>Voice Activity Detection - определение наличия речи на участке аудио</li>
<li>Audio Enhancement</li>
<li>ASR automatic speech recognition or Audio recognition</li>
<li>Keyword Spotting</li>
<li>Sound Event Detection</li>
<li>Speech Generation</li>
<li>Text-to-text</li>
<li>Human-fall detection</li>
</ul>

<p>
Computer Vision:
</p>
<ul class="org-ul">
<li>Image classification</li>
<li>Image segmentation or Semantic Segmentation - to regions, class to every pixel.</li>
<li>Object detection - “Semantic Segmentation” + same class counting. Class Labeling and Instance Identification.</li>
<li>Image generation</li>
<li>Image retrival</li>
<li>Video classification</li>
<li>Scene graph prediction</li>
<li>localization</li>
<li>Gaze/Depth Estimation</li>
<li>Fine-grained recognition</li>
<li>person re-identification</li>
<li>Semantic indexing</li>
<li>Object Tracking</li>
<li>video generation</li>
<li>video prediction</li>
<li>video object segmentation</li>
<li>video detection</li>
<li>with NLP: Image captioning, Visual Qustion Answering</li>
</ul>

<p>
Data Analysis
</p>
<ul class="org-ul">
<li>Data Regression</li>
<li>Anomaly/Error</li>
<li>Detection&#x2026;</li>
</ul>

<p>
Reinforcement Learning &amp; Robotic - sequential decision making problems
</p>
<ul class="org-ul">
<li>imitation learning</li>
<li>Robot manipulation</li>
<li>Locomotion</li>
<li>Policy Learning</li>
<li>Tabular's MDPs</li>
<li>Visual Navigation</li>
</ul>

<p>
Other Fields
</p>
<ul class="org-ul">
<li>Drug discovery</li>
<li>Disease Prediciton</li>
<li>Biometrical recognition</li>
<li>Precision Agriculture</li>
<li>Internet Security</li>
</ul>
</div>
<div id="outline-container-orgf909ee9" class="outline-3">
<h3 id="orgf909ee9"><span class="section-number-3">7.1.</span> Classification problem and types</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>binary classification (two target classes)</li>
<li>multi-class classification
<ul class="org-ul">
<li>definition:
<ul class="org-ul">
<li>more than two exclusive targets</li>
<li>each sample can belong to only one class</li>
</ul></li>
<li>one softmax loss for all possible classes.</li>
</ul></li>
<li>multi-label classification
<ul class="org-ul">
<li>definition:
<ul class="org-ul">
<li>more than two non exclusive targets</li>
<li>inputs x to binary vectors y (assigning a
value of 0 or 1 for each element (label) in y)</li>
</ul></li>
</ul></li>
<li>multi-class signle-label classification (more than two non exclusive targets) in which multiple target classes can be on
at the same time
<ul class="org-ul">
<li>One logistic regression loss for each possible class</li>
</ul></li>
</ul>


<ul class="org-ul">
<li>binary: [0], [1] &#x2026; n  -&gt; binary cross entropy</li>
<li>multi-class: [0100], [0001] &#x2026; n -&gt; categorical cross entropy</li>
<li>multi-label: [0101], [1110] &#x2026; n -&gt; binary cross entropy</li>
</ul>


<p>
multiclass problem is broken down into a series of binary problems using either
</p>
<ul class="org-ul">
<li>One-vs-One (OVO)</li>
<li>One-vs-Rest (OVR also called One-vs-All) OVO presents computational drawbacks, so professionals prefer the
OVR approach.</li>
</ul>

<p>
Averaging techniques for metrics:
</p>
<ul class="org-ul">
<li>macro - compute the metric independently for each class and then take the average - treating all classes
equally</li>
<li>weighted - weighted average for classes (score*num<sub>occur</sub><sub>per</sub><sub>class</sub>)/totalnum</li>
<li>micro - aggregate the contributions of all classes to compute the average metric - micro-average is
preferable if you suspect there might be class imbalance</li>
</ul>
</div>
</div>
<div id="outline-container-orge381e2d" class="outline-3">
<h3 id="orge381e2d"><span class="section-number-3">7.2.</span> discriminative model vs generative models</h3>
<div class="outline-text-3" id="text-7-2">
<p>
<b>Generative Models</b> - generate new data samples by sampling from the learned distribution. model the joint
 probability distribution p(x,y) of the input data x and the output labels y.  Generative models capture the
 decision boundary indirectly.
</p>
<ul class="org-ul">
<li>model the underlying distribution of the data</li>
<li>often trained using unsupervised learning techniques</li>
</ul>

<p>
for  data augmentation, image synthesis, and text generation.
</p>

<p>
<b>Discriminative Models</b> - model the conditional probability distribution p(y|x) of the outplut labels y given
 the input data x.
</p>
<ul class="org-ul">
<li>learn a decision boundary</li>
<li>trained using supervised</li>
</ul>

<p>
for image classification, speech recognition, and sentiment analysis.
</p>
</div>
<div id="outline-container-org2a1049e" class="outline-4">
<h4 id="org2a1049e"><span class="section-number-4">7.2.1.</span> Examples</h4>
<div class="outline-text-4" id="text-7-2-1">
<p>
Examples of generative models include:
</p>
<ul class="org-ul">
<li>Gaussian mixture models</li>
<li>Hidden Markov models</li>
<li>Probabilistic context-free grammars</li>
<li>Bayesian networks</li>
<li>Variational autoencoders</li>
<li>Generative adversarial networks</li>
</ul>

<p>
Examples of discriminative models include:
</p>
<ul class="org-ul">
<li>Logistic regression</li>
<li>Support vector machines</li>
<li>Decision trees</li>
<li>Random forests</li>
<li>Conditional random fields</li>
</ul>
</div>
</div>

<div id="outline-container-org92b4bbe" class="outline-4">
<h4 id="org92b4bbe"><span class="section-number-4">7.2.2.</span> applications</h4>
<div class="outline-text-4" id="text-7-2-2">
<p>
Generative applications in:
</p>
<ul class="org-ul">
<li>Data augmentation</li>
<li>Image synthesis</li>
<li>Text generation</li>
<li>Anomaly detection</li>
</ul>

<p>
Discriminative applications:
</p>
<ul class="org-ul">
<li>Image classification</li>
<li>Speech recognition</li>
<li>Sentiment analysis</li>
<li>Recommendation systems</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgc93406b" class="outline-3">
<h3 id="orgc93406b"><span class="section-number-3">7.3.</span> links</h3>
<div class="outline-text-3" id="text-7-3">
<p>
<a href="https://paperswithcode.com">https://paperswithcode.com</a>
</p>
<p>
*
</p>
</div>
</div>
</div>
<div id="outline-container-orge91e606" class="outline-2">
<h2 id="orge91e606"><span class="section-number-2">8.</span> Data Analysis [ə'nælɪsɪs]</h2>
<div class="outline-text-2" id="text-8">
<p>
not analises
</p>
<ul class="org-ul">
<li>Открытый курс <a href="https://habr.com/en/company/ods/blog/327250/">https://habr.com/en/company/ods/blog/327250/</a></li>
<li>Выявление скрытых зависимостей <a href="https://habr.com/en/post/339250/">https://habr.com/en/post/339250/</a></li>
<li>example <a href="https://www.kaggle.com/startupsci/titanic-data-science-solutions">https://www.kaggle.com/startupsci/titanic-data-science-solutions</a></li>
<li>USA National institute of standards and technology (old) <a href="https://www.itl.nist.gov/div898/handbook/index.htm">https://www.itl.nist.gov/div898/handbook/index.htm</a></li>
</ul>
<p>
Cпециалисты по анализу данных Обычно перед ними ставят задачи, которые нуждаются в уточнении формулировки,
выборе метрики качества и протокола тестирования итоговой модели. Cводить задачу заказчика к формальной
постановке задачи машинного обучения. Проверять качество построенной модели на исторических данных и в
онлайн-эксперименте.
</p>
<ul class="org-ul">
<li>анализ текста и информационный поиск</li>
<li>коллаборативная фильтрация и рекомендательные системы</li>
<li>бизнес-аналитика</li>
<li>прогнозирование временных рядов</li>
</ul>
</div>
<div id="outline-container-orgf399947" class="outline-3">
<h3 id="orgf399947"><span class="section-number-3">8.1.</span> <span class="todo TODO">TODO</span> open-source tools</h3>
<div class="outline-text-3" id="text-8-1">
<p>
FreeViz
Orange 3 - exploring for teaching
PSPP - free alternative for IBM SPSS Statistics - statistical analysis in social science
Weka - data analysis and predictive modeling
Massive Online Analysis (MOA) - large scale mining of data streams
</p>
</div>
</div>
<div id="outline-container-org778fa1c" class="outline-3">
<h3 id="org778fa1c"><span class="section-number-3">8.2.</span> dictionary</h3>
<div class="outline-text-3" id="text-8-2">
<ul class="org-ul">
<li><b>intrinsic dimension</b> - for a data set - the number of variables needed in a minimal representation of the data</li>
<li><b>density</b> -</li>
<li><b>variance</b> - мера разброса значений случайной величины относительно её математического ожидания
<a href="math#MissingReference">math#MissingReference</a></li>
</ul>
</div>
</div>
<div id="outline-container-org080a4a7" class="outline-3">
<h3 id="org080a4a7"><span class="section-number-3">8.3.</span> Steps</h3>
<div class="outline-text-3" id="text-8-3">
</div>
<div id="outline-container-org9c4153b" class="outline-4">
<h4 id="org9c4153b"><span class="section-number-4">8.3.1.</span> стандарт CRISP-DM или Cross-Industry Standard Process for Data Mining/Data Science</h4>
<div class="outline-text-4" id="text-8-3-1">
<p>
методология CRISP-DM <a href="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining">https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining</a>
</p>
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1709.09003.pdf">https://arxiv.org/pdf/1709.09003.pdf</a></li>
</ul>

<p>
2002, 2004, 2007, and 2014 show that it was the leading methodology used by industry data miners
</p>

<p>
steps:
</p>
<ul class="org-ul">
<li>Business Understanding</li>
<li>Data Understanding (EDA) - see steps in <a href="./math#MissingReference">./math#MissingReference</a></li>
<li>Data Preparation
<ul class="org-ul">
<li>select data</li>
<li>clean data: missing data, data errors, coding inconsistences, bad metadata</li>
<li>construct data: derived attrigutes, replaced missing values</li>
<li>integrate date: merge data</li>
<li>format data</li>
</ul></li>
<li>Modeling
<ul class="org-ul">
<li>select modeling technique</li>
<li>Generate Test desing: how we will test, select performance metrics</li>
<li>Build Model</li>
<li>Assess Model</li>
<li>Reframe Setting</li>
</ul></li>
<li>Evalution</li>
<li>Deployment</li>
</ul>
</div>
</div>
<div id="outline-container-org45129ae" class="outline-4">
<h4 id="org45129ae"><span class="section-number-4">8.3.2.</span> ASUM-DM Analytics Solutions Unified Method for Data Mining/Predictive Analytics 2015</h4>
<div class="outline-text-4" id="text-8-3-2">
<p>
<a href="https://developer.ibm.com/articles/architectural-thinking-in-the-wild-west-of-data-science/#asum-dm">https://developer.ibm.com/articles/architectural-thinking-in-the-wild-west-of-data-science/#asum-dm</a>
</p>
<ul class="org-ul">
<li>2019 Model development process <a href="https://arxiv.org/pdf/1907.04461.pdf">https://arxiv.org/pdf/1907.04461.pdf</a></li>
<li>IBM Data and Analytics Reference Architecture</li>
</ul>
</div>
</div>
<div id="outline-container-org9bae417" class="outline-4">
<h4 id="org9bae417"><span class="section-number-4">8.3.3.</span> Процесс разработки</h4>
<div class="outline-text-4" id="text-8-3-3">
<p>
методологией разработки (моделью процесса разработки) - четкие шаги
</p>
<ul class="org-ul">
<li>Водопадная методология (Waterfall model, «Водопад»)
<ul class="org-ul">
<li>Установлены чёткие сроки окончания каждого из этапов.</li>
<li>Готовый продукт передаётся заказчику только один раз в конце проекта</li>
<li>где
<ul class="org-ul">
<li>отсутствует неопределённость в требованиях заказчика</li>
<li>в проектах, которые сопровождаются высокими затратами в случае провала: тщательным отслеживанием каждого
из этапов и уменьшением риска допустить ошибку</li>
</ul></li>
<li>cons: слишком фиксирован, нельзя вернуться</li>
</ul></li>
<li>Гибкая методология (Agile)
<ul class="org-ul">
<li>cons:
<ul class="org-ul">
<li>не понятно как распределить шаги</li>
<li>циклы могут затягиваться - долго перебирают модели или подстраивают параметры</li>
<li>Документирование не регламентировано. В DS-проектах документация и история всех используемых моделей
очень важна, позволяет экономить время и облегчает возможность вернуться к изначальному решению.</li>
</ul></li>
</ul></li>
<li>CIRSP-DM
<ul class="org-ul">
<li>проект состоит из спринтов</li>
<li>Последовательность этапов строго не определена, некоторые этапы можно менять местами. Возможна
параллельность этапов (например, подготовка данных и их исследования могут вестись
одновременно). Предусмотрены возвраты на предыдущие этапы.</li>
<li>Фиксирование ключевых моментов проекта: графиков, найденных закономерностей, результатов проверки гипотез,
используемых моделей и полученных метрик на каждой итерации цикла разработки.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org29e72c8" class="outline-4">
<h4 id="org29e72c8"><span class="section-number-4">8.3.4.</span> Descriptive analytics</h4>
<div class="outline-text-4" id="text-8-3-4">
<ol class="org-ol">
<li>Проверка на нормальность - что гистограмма похожа на нормальное распределение(критерий стьюдента требует)</li>
</ol>
<pre class="example">
print(df.describe())
# Find correlations
print(applicants.corr()) # матрица корреляции
# scatter matrix Матрица рассеивания - гистограммы
from pandas.plotting import scatter_matrix
print(scatter_matrix(df))
</pre>
</div>
</div>
<div id="outline-container-orgbec4d63" class="outline-4">
<h4 id="orgbec4d63"><span class="section-number-4">8.3.5.</span> Анализ временных рядов -</h4>
<div class="outline-text-4" id="text-8-3-5">
<ul class="org-ul">
<li><a href="https://habr.com/en/post/207160/">https://habr.com/en/post/207160/</a></li>
<li><a href="https://machinelearningmastery.com/feature-selection-time-series-forecasting-python/">https://machinelearningmastery.com/feature-selection-time-series-forecasting-python/</a></li>
<li><a href="https://towardsdatascience.com/time-series-in-python-part-2-dealing-with-seasonal-data-397a65b74051">https://towardsdatascience.com/time-series-in-python-part-2-dealing-with-seasonal-data-397a65b74051</a></li>

<li>Количество записей в месяц</li>
</ul>
<pre class="example">
df['birthdate'].groupby([df.birthdate.dt.year, df.birthdate.dt.month]).agg('count')
</pre>











<ul class="org-ul">
<li>по x - yt, по у - yt+1
<ul class="org-ul">
<li>в соседние месяцы - если много на диагонали - значения продаж в соседние месяцы похожи</li>
</ul></li>
<li>по x - yt, по у - yt+2</li>
<li>x- yt одного месяца (сумма), y - yt другого года того же месяца</li>
</ul>

<p>
Auto regressive (AR) process - when yt = c+ a1*yt-1 + a2*yt-2 &#x2026;
</p>

<p>
Измерение Автокорреляция
</p>
<ul class="org-ul">
<li>ACF is an (complete) auto-correlation function which gives us values of auto-correlation of any series with its lagged values.</li>
<li>PACF is a partial auto-correlation function.</li>
</ul>

<p>
Make Stationary - remove seasonality and trend <a href="https://machinelearningmastery.com/feature-selection-time-series-forecasting-python/">https://machinelearningmastery.com/feature-selection-time-series-forecasting-python/</a>
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> statsmodels.graphics.tsaplots <span style="color: #8ac6f2; font-weight: bold;">import</span> plot_acf
<span style="color: #8ac6f2; font-weight: bold;">from</span> matplotlib <span style="color: #8ac6f2; font-weight: bold;">import</span> pyplot
<span style="color: #cae682;">series</span> = read_csv(<span style="color: #95e454;">'seasonally_adjusted.csv'</span>, header=<span style="color: #e5786d; font-weight: bold;">None</span>)
plot_acf(series, lags = 150) <span style="color: #fa8072;">#  </span><span style="color: #99968b; font-style: italic;">lag values along the x-axis and correlation on the y-axis between -1 and 1</span>
plot_pacf(series) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1085;&#1077; &#1087;&#1086;&#1085;&#1103;&#1090;&#1100;. &#1082;&#1086;&#1088;&#1086;&#1095;&#1077;, &#1090;&#1086; &#1078;&#1077; &#1089;&#1072;&#1084;&#1086;&#1077;, &#1090;&#1086;&#1083;&#1100;&#1082;&#1086; &#1073;&#1086;&#1083;&#1077;&#1077; &#1082;&#1086;&#1088;&#1086;&#1090;&#1082;&#1080;&#1077; &#1082;&#1086;&#1088;&#1088;&#1077;&#1083;&#1103;&#1094;&#1080;&#1080; &#1085;&#1077; &#1084;&#1077;&#1096;&#1072;&#1102;&#1090;</span>
pyplot.show()
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org48dc1f2" class="outline-3">
<h3 id="org48dc1f2"><span class="section-number-3">8.4.</span> 2019 pro <a href="https://habr.com/ru/company/JetBrains-education/blog/438058/">https://habr.com/ru/company/JetBrains-education/blog/438058/</a></h3>
<div class="outline-text-3" id="text-8-4">
<p>
<a href="https://compscicenter.ru/courses/data-mining-python/2018-spring/classes/">https://compscicenter.ru/courses/data-mining-python/2018-spring/classes/</a>
</p>
<ul class="org-ul">
<li>математическая статистика по орлу и решке определяет симметричность монетки</li>
<li>теория вероятности говорит, что у орла и решки одна вероятность и вероятность случайна</li>
</ul>

<p>
Регрессионный анализ:
</p>
<ul class="org-ul">
<li>линейный - обыкновенный</li>
<li>логистический</li>
</ul>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">ковариация cov</th>
<th scope="col" class="org-left">корреляция corr</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">линейной зависимости двух случайных величин</td>
<td class="org-left">ковариация посчитанная для стандартизованных данных</td>
</tr>

<tr>
<td class="org-left">не инвариантна относительно смены масштаба</td>
<td class="org-left">инварианта</td>
</tr>

<tr>
<td class="org-left">dot(de<sub>mean</sub>(x),de<sub>mean</sub>(y))/(n-1), de<sub>mean</sub> отклон от mean</td>
<td class="org-left">cov(X,Y)/σx*σy где σ - standard deviation</td>
</tr>

<tr>
<td class="org-left">Лежат между -∞ и + ∞</td>
<td class="org-left">Лежат между -1 и +1</td>
</tr>
</tbody>
</table>

<p>
Оба измеряют только линейные отношения между двумя переменными, то есть когда коэффициент корреляции равен
нулю, ковариация также равна нулю
</p>
</div>
<div id="outline-container-orga0c1334" class="outline-4">
<h4 id="orga0c1334"><span class="section-number-4">8.4.1.</span> Часть 1</h4>
<div class="outline-text-4" id="text-8-4-1">
</div>
<ol class="org-ol">
<li><a id="org5b53424"></a>1 Гистограмма<br />
<div class="outline-text-5" id="text-8-4-1-1">
<ul class="org-ul">
<li>Синонимы - строчка, объект, наблюдение</li>
<li>Синонимы - стоблец, переменная, характеристика объекта, feature</li>
</ul>
<p>
Столбцы могут быть:
</p>
<ul class="org-ul">
<li>количественной шкале - килограммы, секунды, доллары</li>
<li>порядковой - результат бега спортсменов - 1 местов, второе, 10</li>
<li>в номинальной шкале - коды или индексы чего-то</li>
</ul>

<p>
Вариационный ряд (упорядоченная выборка[1]) - полученная в результате расположения в порядке неубывания
исходной последовательности независимых одинаково распределённых случайных величин. Вариационный ряд и его
члены являются порядковыми статистиками.
</p>

<p>
Поря́дковые стати́стики - это упорядоченная по неубыванию выборка одинаково распределённых независимых случайных
величин и её элементы, занимающие строго определенное место в ранжированной совокупности.
</p>

<p>
Квантиль Quantile - значение, которое заданная случайная величина не превышает с фиксированной вероятностью. В
процентах - процентиль. «90-й процентиль массы тела у новорожденных мальчиков составляет 4 кг» - 90 % мальчиков рождаются с весом меньше, либо равным 4 кг
</p>
<ul class="org-ul">
<li>First quartile - 1/4 25% -  10×(1/4) = 2.5 round up to 3 - где 10 - количество эллементов, берем 3 по возрастанию</li>
<li>Second quartile 2/4 - 50%</li>
</ul>

<p>
квартиль это квантиль выраженная не в процентах а в 1/4=25 2/4=50 3/4=75
</p>

<p>
Гистограмма - количество попаданий в интервалы значений
</p>
<ul class="org-ul">
<li>n<sub>p</sub> попавших</li>
<li>n<sub>p</sub>/ (n * длинну<sub>интервала</sub>) # площадь равна 1 - это нормирует несколько гистограм для сопоставления #
приближается к плотностьи распределения при увеличении числа испытаний - которая позволяет вычислить
вероятность</li>
</ul>

<p>
Kernel density estimation Ядерная оценка плотности распределения - can be ‘scott’, ‘silverman’ - задачей сглаживания данных
</p>
</div>
</li>

<li><a id="orgb68d56e"></a>2<br />
<div class="outline-text-5" id="text-8-4-1-2">
<p>
Ящиковые диаграммы (Ящики с усами (whiskers)) - min&#x2013;Q1-&#x2013;&#x2014;Q3&#x2014;max &#x2013;&gt;(толстая красная линия - медиана) -
это упрощенная Гистограмма
</p>
<ul class="org-ul">
<li>недостаток - скрывает горбы гистограммы</li>
<li>непонятно сколько налюдений в выборках</li>
</ul>

<p>
<b>Типичный</b> город, чек, день на сервере
</p>
<ul class="org-ul">
<li>убираем дни - которые выбросы</li>
<li>если mean превышает Q3 75% - то это не очень естественно</li>
<li>получается среднее арефметическое очень не устойчиво к выбросам, а медиана устойчива</li>
</ul>

<p>
Лог нормальное распределение - это распределение которое после логарифмирования становится нормальным
</p>

<p>
Медиана - число посередине выборки если ее упорядочить
</p>

<p>
Усеченное среднее - сортируем, удаляем по краям 5 или 25 и вычисл среднее арифметическое
</p>

<p>
<b>Измерение отклонения данных</b>
</p>
<ul class="org-ul">
<li>выборочная дисперсия, на практике используют стандартное отклонение std - корень из дисперссии - корень
возвращает размерность как и у исходных данных</li>
<li>межквартильный размах</li>
</ul>

<p>
<b>Доверительные интервалы</b> - в каком интервале с точностью ~0.95 будет прогноз?
</p>
<ul class="org-ul">
<li>ширина интервала будет опираться на стандартное отклонение std - больше std - шире интервал</li>
</ul>

<p>
Диаграммы рассеивания
</p>


<p>
feature - новые данные позволяющие решить задачу
</p>

<p>
кружек vs стобики -
</p>
<ul class="org-ul">
<li>длины лучше</li>
<li>углы норм
<ul class="org-ul">
<li>площади хуже всего</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="orge27ea2e"></a>Кластеризация и иерархический класерный анализ<br />
<div class="outline-text-5" id="text-8-4-1-3">
<p>
<b>Кластеризация</b>, она же
</p>
<ul class="org-ul">
<li>распознавание образов без учителя</li>
<li>стратификация</li>
<li>таксономия</li>
<li>автоматическая классификация</li>
</ul>

<p>
Инструменты
</p>
<ul class="org-ul">
<li>иерархический класерный анализ</li>
<li>метод к-средних - хорошо работает для больших наборов данных</li>
<li>самоорганизующиеся карты Кохонена (SOM)</li>
<li>Смесь (нормальных) распределений</li>
</ul>

<p>
Примеры
</p>
<ul class="org-ul">
<li>разделить пользователей на группу</li>
<li>выделить сегменты рынка</li>
</ul>

<p>
Классификация - два смысла
</p>
<ul class="org-ul">
<li>распознавание - по известным классам</li>
<li>кластеризация - по неизвестным классам</li>
</ul>

<p>
какой метод лучше - который удалось проинтерпритировать и проверить.
</p>

<p>
Типы кластеров
</p>
<ul class="org-ul">
<li>плотные шаровые</li>
<li>шаровые парообразные</li>
<li>ленточные</li>
<li>закручивающиеся</li>
<li>один внутри другого</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org8a6da2b"></a>иерархический класерный анализ<br />
<div class="outline-text-6" id="text-8-4-1-3-1">
<ol class="org-ol">
<li>Сведение задачи к геометрической - каждый объект точка</li>
<li>Определение меры сходства - расстояния
<ul class="org-ul">
<li>Евклидово расстояние d = sqrt((x1-y1)<sup>2</sup> + (x2-y2)<sup>2</sup>)
<ul class="org-ul">
<li>недостаток - различие по одной координате может определять расстояние</li>
</ul></li>
<li>Квадрат Евклидова расстояния d = (x1-y1)<sup>2</sup> + (x2-y2)<sup>2</sup>
<ul class="org-ul">
<li>can be used to strengthen the effect of longer distances</li>
<li>does not form a metric space, as it does not satisfy the triangle inequality.</li>
</ul></li>
<li>Блок Manhettand = |x1-y1| + |x2-y2|
<ul class="org-ul">
<li>достоинство - одной переменной тяжелее перевесить другие</li>
</ul></li>
</ul></li>
</ol>

<p>
Определяется ответом на вопрос - что значит объекты похожи. Начинающим: Варда, ближайшего и среднее невзвеш.
</p>

<ol class="org-ol">
<li>Расстояния между кластерами <a href="https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering">https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering</a>
<ul class="org-ul">
<li>Average linkage clustering (Среднее невзвешенное расстояние) - 3 и 4 точки - 12 расстрояний и усредняется
<ul class="org-ul">
<li>плотные паровые скопления</li>
</ul></li>
<li>Cetroid Method (Центроидный метод) - растояние между центрами - не показывает если одно в другом, объем не вляет</li>
<li>Complete linkage clustering (Метод дальнего соседа) - две самые далекие точки</li>
<li>Single linkage clustering (Метод ближнего соседа) - две самые близкие
<ul class="org-ul">
<li>ленточные</li>
</ul></li>
<li>Ward's method (Метод Варда) - хорош для k-средних
<ul class="org-ul">
<li>плотные шаровые скопления</li>
<li>он стремится создавать кластеры малого размера</li>
</ul></li>
</ul></li>
</ol>

<p>
Для расстояния могут быть использованы собственные формулы - мера сходства сайтов по посетителям
</p>

<ol class="org-ol">
<li>Все точки кластеры</li>
<li>Выбираем два ближайших кластера и объединяем</li>
<li>Остался 1 кластер</li>
</ol>

<p>
<b>Дендрограмма</b> где остановиться - Дерево (5-100 записей)
</p>
<ol class="org-ol">
<li>пронумерованные кластеры на одном расстоянии на прямой горизонтальной</li>
<li>вертикальные линии - расстояние между кластерами в момент объединения</li>
<li>горизонтальная - момент объединения</li>
</ol>

<p>
Scree plot <b>каменистая осыпь / локоть</b> - определить число кластеров - остановиться на изломе
</p>
<ul class="org-ul">
<li>вертикаль - distance</li>
<li><p>
горизонталь - номер слияния на равных расстояниях
</p>

<p>
Участие аналитика (насколько субъективна)
</p></li>
<li>отбор переменных</li>
<li>метод стандартизации
<ul class="org-ul">
<li>в основном два варианта - 0-1 или mean=0 std = 1</li>
</ul></li>
<li>расстояние между кластерами</li>
<li>расстояние между объектами</li>
<li>Если кластеров нет, поцедура их все равно найдет</li>
</ul>

<p>
Проблема ленточных кластеров
</p>
<ul class="org-ul">
<li>решение - Метод ближайшего соседа</li>
</ul>

<p>
Недостаток иерархического анализа - хранить в оперативной памяти матрицу попарных расстояний
</p>
<ul class="org-ul">
<li>невозможность работы с гиганскими наборами данных</li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org57122cb"></a>Метод k-means (k-средних)<br />
<div class="outline-text-5" id="text-8-4-1-4">
<p>
Используется только евклидова метрика, другие метрики в k-медоиды
</p>

<ol class="org-ol">
<li>Задается К число кластеров и k-точек начальных кластеров</li>
<li></li>
</ol>
</div>
</li>
<li><a id="org2b57990"></a><span class="todo TODO">TODO</span> 9 Прогнозирование линейно регрессией<br />
<div class="outline-text-5" id="text-8-4-1-5">
<p>
<b>Прогнозирование</b>
</p>
<ol class="org-ol">
<li>есть ли тренд?</li>
<li>есть ли сезонность?
<ul class="org-ul">
<li>аддитивная - поправки не меняются от величины f = f+ g(t)</li>
<li>мультипликативные - величина добавки зависит - выступают как множители f = f*g(x)</li>
</ul></li>
<li>Меняет ли ряд свой характер.</li>
<li>выбросы -резкие отклонения
<ul class="org-ul">
<li>отбросить</li>
<li>заменять на разумные значения</li>
</ul></li>
</ol>

<p>
Эмпирические правила
</p>
<ul class="org-ul">
<li>Если у вас меньше данных чем за 3 периода сезонных отклонений.</li>
<li>Если у вас больше чем за 5 сезонных отклонений, то самые ранние данные скорее всего устарели.</li>
</ul>

<p>
Сезонная декомпозиция - ???
</p>

<p>
Пример аддитивной модели
yt = a + bt + ct<sup>2</sup> + g(t) + εt
</p>
<ul class="org-ul">
<li>a + bt + ct<sup>2</sup> - тренд</li>
<li>εt - ошибка для каждого момента времени</li>
<li>не подходит для мультипликативной сезонности</li>
</ul>

<p>
Логирифм - произведение превращает в сумму
</p>
<ul class="org-ul">
<li>трюк: данные предварительно логарифмировать log(yt) = bxi+c(xi) + ε</li>
<li>потенциировать - взять экспоненту и получим прогнозы для исходного ряда</li>
</ul>

<p>
Лучше не брать базой сезонов пиковый месяц
</p>
</div>
</li>
<li><a id="orge9d53c6"></a>10<br />
<div class="outline-text-5" id="text-8-4-1-6">
<p>
<b>линейная регрессия</b> - плохая
</p>
<ul class="org-ul">
<li>3 сезонности может</li>
<li>в случае коротких временных рядов</li>
<li>когда сезонности не меняются</li>
</ul>

<p>
у - номинальная шкала
</p>
<ul class="org-ul">
<li>количестванная шкала (метры рубли)- регрессия</li>
<li>порядковая</li>
</ul>

<p>
У - количественная
</p>
<ul class="org-ul">
<li>Безопасный путь - считать что У номинальная, опасный но экономный количественный - регрессия</li>
</ul>

<p>
регрессия - weak learner
</p>

<p>
<b>sklearn.tree.DecisionTreeClassificator</b> - когда Y номинальной шкале
</p>

<p>
CART (Classification And Regression Tree)  - и задачу распознавания и задачу регрессии решать
</p>
<ul class="org-ul">
<li>используется в комбинации деревьев</li>
<li>можем понять как она устроена и чему-то у них научиться</li>
<li>быстро работает</li>
</ul>

<p>
Impurity Загрязнение - чтобы если толко крестики = 1 только 0 =1, а если 1/2 крестиков и 1/2 ноликов =
1/2. Варианты:
</p>
<ul class="org-ul">
<li>entropy H1 = -∑pj*log2(pj)</li>
<li>Gini index H2 = 1-∑pj<sup>2</sup>=∑pj*(1-pj)</li>
<li>classification error H3 = 1 - max(pj), где pj - вероятность принадлежать к классу j. на практике - доля
объектов класса j в узле</li>
</ul>

<p>
Для каждой колонки перебираем пороговые значения и выбираем тот столбец с которым стало чище
</p>

<p>
Увеличение частоты узлов (насколько лучше стало после расщепления) (информативность переменных):
</p>
<ul class="org-ul">
<li>ΔH = H<sub>родителя</sub> - ( (n<sub>левый</sub>/ n<sub>родителя</sub>)*H<sub>левый</sub> + (n<sub>правый</sub>/ n<sub>родителя</sub>)*H<sub>правый</sub>)</li>
<li>n<sub>левый</sub> - кол-во наблюдений в левом узле</li>
<li>n<sub>родителя</sub> - кол-во наблюдений в родителе</li>
<li>H<sub>левый</sub> - загрязнение в левом потомке</li>
<li>H<sub>родителя</sub> - загрязнение которое было в родительском узле</li>
</ul>

<p>
accuracy на обучающем 90% на тестовом 72% - <b>переобучение</b>
</p>
</div>
</li>

<li><a id="org968f5cf"></a><span class="todo TODO">TODO</span> 11 Random Forеst, Feature selection<br />
<div class="outline-text-5" id="text-8-4-1-7">
<p>
<b>sklearn.tree.DecisionTreeRegressor</b> - когда Y в количественной шкале
</p>
<ul class="org-ul">
<li>лучше линейной регрессии когда у вас нелинейная зависимость ( изогнутая линия)</li>
</ul>

<p>
prune - обрезание деревьев
</p>

<p>
Деревья годятся как кирпичек
</p>

<p>
From weak to strong alg:
</p>
<ul class="org-ul">
<li>stacking(5%) - X -&gt; [Y] -&gt; Y  предсказывает основываясь на предсказаниях (предикторы)</li>
<li>bagging (bootstrap aggregation) - average</li>
<li><a href="#org4eee0fb">8.19.5</a></li>
</ul>

<p>
<b>Random forest</b> - конечное решение
</p>
<ul class="org-ul">
<li>2d array, N - число строк, M - число столбцов</li>
<li>случайным образом выбираем подмножество строк и столбцов - каждое дерево обучается на своем подмножестве -
решает проблему декорреляции</li>
<li>могут переобучаться - регулируя максимальную глубину</li>
</ul>

<p>
Параметры:
</p>
<ul class="org-ul">
<li>число деревьев - сделай много, потом сокращай!</li>
</ul>

<p>
Проблемы
</p>
<ul class="org-ul">
<li>декареляции - сли две выборки оказались похожи друг на друга и на выходе одно и то же - а внешне</li>
</ul>
<p>
модель сложная
</p>
<ul class="org-ul">
<li>несбалансированная выборки - классы в разных пропорциях</li>
</ul>

<p>
<b>Информативность столбцов c помощью случайных лесов</b>:
</p>
<ul class="org-ul">
<li>сложением информативностей по каждому дереву</li>
<li>сравнивая out-of-bag error - берем столбце shuffle и пропускаем через дерево</li>
</ul>

<p>
Несбалансированность классов - когда 1-единичек меньше 0-ей
</p>
<ul class="org-ul">
<li>решение - повторить единички</li>
<li>лучшее решение - учеличить цену ошибки для 1 . class<sub>weight</sub> = {0:.1, 1:.9} - If the class<sub>weight</sub> doesn't sum
to 1, it will basically change the regularization parameter.</li>
</ul>


<p>
For new data points, each decision tree in the ensemble makes a prediction.
</p>
<ul class="org-ul">
<li>Classification: The final prediction is based on the majority vote of the predictions from all the
trees. The class with the most votes is selected as the final prediction[1][2][4].</li>
<li>Regression: The final prediction is the average of the outputs from all the trees[2][3][5].</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgd44e194" class="outline-4">
<h4 id="orgd44e194"><span class="section-number-4">8.4.2.</span> Часть 2</h4>
<div class="outline-text-4" id="text-8-4-2">
</div>
<ol class="org-ol">
<li><a id="org28dc2f1"></a>4 Прогнозирование NN<br />
<div class="outline-text-5" id="text-8-4-2-1">
<p>
1 &#x2026; 12 -&gt; 13
2 &#x2026; 13 -&gt; 14
3 &#x2026; 14 -&gt; 15
</p>

<p>
после 8, 12 наблюдения - уже не достоверно - накапливается ошибка
</p>

<p>
Чтобы это побороть тренируется две сети предсказывающие:
</p>
<ul class="org-ul">
<li>одна на 1 месяц вперед</li>
<li>вторая на 2 месяца вперед</li>
</ul>

<p>
В тестовую выборку нужно выбирать последние наблюдения!
</p>

<ul class="org-ul">
<li>linear - регрессия</li>
<li>logistic - 2 класса</li>
<li>softmax - k классов</li>
</ul>

<p>
Как выделить мультипликативную сезонность? вариант
</p>
<ul class="org-ul">
<li>разбиваем на окна сезонов</li>
<li>скользящее среднее</li>
<li>сумма сезонных поправок / кол-во наблюдений в окне = присутствует в каждом наблюдении сглаженного ряда</li>
<li>исходный ряд - сглаженный = сезонные поправки</li>
</ul>
</div>
</li>
<li><a id="org46de19f"></a>8 Факторный анализ<br />
<div class="outline-text-5" id="text-8-4-2-2">
<p>
Факторный анализ реинкарнировался в SVD разложение - и стал полезным для рекомендательных систем
</p>

<p>
Задачи
</p>
<ul class="org-ul">
<li>Cокращение числа переменных
<ul class="org-ul">
<li>входных на новые искуственные - факторы</li>
</ul></li>
<li>Измерение неизмеримого. Построение новых обобщенных показателей.
<ul class="org-ul">
<li>может оказаться, что факты измеряют исследуемую характеристику</li>
<li>исходные переменные отбирались так, чтобы косвенно имерить неизмеряемую величину</li>
</ul></li>
<li>Наглядное представление многомерных наблюдений (проецирование данных)</li>
<li>Описание структуры взаимных связей между переменными, в частности выявление групп взаимозависимых переменных.</li>
<li>Преодоление мультиколинеарности переменных в регрессионном анализе. Будут все ортогональны-независимы.</li>
</ul>

<p>
Коллинеарность - Если переменные линейно зависимы - то регрессионный анализ сбоит - обратную матрицу не
найти - или она плохо обусловлена - маленькие изменения в обращаемой матрице приводят к большим изменениям в
обращенной - что не хорошо.
</p>

<p>
Коэффициент корреляции близок к 1
</p>
</div>
</li>
<li><a id="org7b2dd87"></a>7 XGBoost<br />
<div class="outline-text-5" id="text-8-4-2-3">
<p>
Tianqi Chen
</p>

<p>
Extreme Gradient Boosting
</p>
</div>
</li>


<li><a id="org7c99881"></a>9<br />
<div class="outline-text-5" id="text-8-4-2-4">
<p>
Выявление структуры зависимости в данных:
</p>
<ul class="org-ul">
<li>метод корреляционных плеяд - устарел</li>
<li>факторный анализ - представляет модель структуры зависимости между переменных - <b>матрица корреляции</b>
<ul class="org-ul">
<li>Метод главных компонент -  principal component analysis (PCA) (он фактически когда SVD)</li>
<li>Факторный анализ который был придуман познее - пытается воспроизвести с меьшим количеством факторов
матрицу корреляции</li>
</ul></li>
</ul>

<p>
Факторный анализ вписывается в целый подход - поиск наилучших проекций
</p>

<p>
Методы проецирования:
</p>
<ul class="org-ul">
<li>Projection pursuit</li>
<li>Многомерное шкалирование</li>
<li>Карты Sommer'a</li>
</ul>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">1</th>
<th scope="col" class="org-right">0.8</th>
<th scope="col" class="org-right">0.001</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0.8</td>
<td class="org-right">1</td>
<td class="org-right">0.001</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-right">0.01</td>
<td class="org-right">0.01</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>

<p>
Способы:
</p>
<ul class="org-ul">
<li>Если проекция целевой переменной бимодальна - то это хорошо</li>
<li>В многомерном пространстве прокладываем ось в направлении максимального расброса данных - это дает
сокращение размерности данных</li>
</ul>

<p>
<b>Анализ главных компонент</b>
</p>
<ul class="org-ul">
<li>Пусть X1,X2,X3..  - cслучайный вектор</li>
<li>Задача1 Найти Y=a11*X1 + a12 * X2 + &#x2026; такую что D(Y) дисперсия максимальна. Y - фактор</li>
<li>тогда если все axx умножить на ? то дисперсия умножиться на ? поэтому вводится дополнительное ограничение</li>
<li>a1 * a1T =1 or a1<sup>2</sup>+a1<sup>2</sup> + a1<sup>2</sup>&#x2026; = 1</li>
<li>следующие Y - то же самое, но с новым условие corr(Y1,Y2) = 0</li>
</ul>

<p>
R - матрица ковариаций(корреляций) случайного вектора X. Задача сводится к:
</p>
<ul class="org-ul">
<li>R*a = λ*a</li>
<li>D(Yi)= λ</li>
</ul>

<p>
Способы завершения :
</p>
<ol class="org-ol">
<li>∑ λ / колво первоначальных столбцов</li>
<li>отбрасываем λ у которых дисперсия меньше 1 или меньше 0.8</li>
<li>каменная осыль/ локоть</li>
</ol>

<p>
<b>Факторный анализ который факторный анализ</b>
</p>
<ul class="org-ul">
<li>X1,X2 &#x2026; - наблюдаемые переменные</li>
<li>F1,F2 &#x2026; - факторы ( factors, common factors) - кол-во меньше чем X</li>
<li>Xi = ai1*F+ai2*F2 &#x2026;.</li>
<li>X = A*F + U, U = U1, U2  - то что не удалось объяснить факторами</li>
<li>чем меньше дисперсия U тем лучше</li>
</ul>

<pre class="example">
from pandas.plotting import scatter_matrix
scatter_matrix(df)
</pre>


<p>
Факторый анализ хорошо работает когда многие переменные коррелируют
</p>

<p>
По умолчанию работает матрица ковариации поэтому - Нужно не забыть стандартизировать.
</p>
<pre class="example">
from sklearn import preprocessing
scaled = preprocessing.StandardScaler().fit_transform(df)
df_scaled = pd.DataFrame(scaled, columns = df.columns)
</pre>


<p>
sklearn.decomposition.PCA - Linear dimensionality reduction using Singular Value Decomposition of the data to
project it to a lower dimensional space. The input data is centered but not scaled for each feature before
applying the SVD.
</p>

<pre class="example">
pca = PCA(n_components = 3)
pca.fit(df_scaled)
# pca... analys here
res = pca.transfrom(df_scaled)
</pre>
</div>
</li>

<li><a id="orgf7a5dfb"></a>11 Калибровка классификаторов <a id="orge2bcaa5"></a><br />
<div class="outline-text-5" id="text-8-4-2-5">
<p>
Выход классификатора это не вероятность, а <b>ранжировка</b> - с какой вероятностью есть неизвестная вероятность этого класса
</p>

<p>
Калибровка это поиск вероятности для ранжировки - лучше всего на выборке валидации
</p>

<p>
calibration plot <a href="https://changhsinlee.com/python-calibration-plot/">https://changhsinlee.com/python-calibration-plot/</a>
</p>
<ol class="org-ol">
<li>Разбиваен на bins</li>
<li>x - bins, y - proportion of true outcomes</li>
</ol>

<p>
Чем больше волатильность - тем больше сомнений в качестве модели
</p>

<p>
Убрать волатильность
</p>
<ul class="org-ul">
<li>isotonic регрессия</li>
<li>platt метор - найти в классе логистических прямых ту, которая апроксимирует</li>
</ul>

<p>
Клссификация с нескольким количеством классов сводится к двум классам : первый против всех остальных, второй
против всех остальных и тд
</p>
</div>
</li>
<li><a id="org32ad6ce"></a>12 Логистическая регрессия logistic or logit regression (binary regression)<br />
<div class="outline-text-5" id="text-8-4-2-6">
<p>
Логистическая функция от линейной комбинации - она же найрон - сеть это зависимо обучаемые ЛР c нелинейными
функциями активации.
</p>


<p>
Для задачи распознавания (y 0 1)
</p>

<p>
В настоящий момент может быть лучше только в:
</p>

<p>
y = a0 + ∑a1*X ,  y - вероятность
</p>

<p>
конкуренты - отличаются активацией 1/(1+e<sup>-x</sup>)
</p>
<ul class="org-ul">
<li>линейная</li>
<li>пробит регрессия</li>
<li>логит регрессия</li>
<li>Poisson regression</li>
</ul>
</div>
</li>
<li><a id="org7eb7a14"></a>other<br />
<div class="outline-text-5" id="text-8-4-2-7">
<p>
<b>распознавание классификация</b> инструменты
</p>
<ul class="org-ul">
<li>наивный байесовский классификатор</li>
<li>дискриминантный анализ</li>
<li>деревья классификации</li>
<li>к-го ближайшего соседа</li>
<li>нейронная сеть прямого распространения</li>
<li>SVM</li>
<li>Случайные леса</li>
<li>Gradient boosting machine</li>
</ul>


<p>
<a href="https://www.youtube.com/watch?v=VRAn1f6cUJ8">https://www.youtube.com/watch?v=VRAn1f6cUJ8</a>
</p>

<p>
Каменистая осыпь/локоть
</p>
</div>
</li>

<li><a id="orgc17063a"></a>code<br />
<div class="outline-text-5" id="text-8-4-2-8">
<div class="org-src-container">
<pre class="src src-python"> <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">11111111111111111</span>
 <span style="color: #8ac6f2; font-weight: bold;">import</span> pandas <span style="color: #8ac6f2; font-weight: bold;">as</span> pd
 <span style="color: #cae682;">AH</span> = pd.read_csv(<span style="color: #95e454;">'a.csv'</span>, header=0, index_col = <span style="color: #e5786d; font-weight: bold;">False</span>)
 <span style="color: #e5786d;">print</span>(AH.head()) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">header</span>
 <span style="color: #e5786d;">print</span>(df.columns) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1085;&#1072;&#1079;&#1074;&#1072;&#1085;&#1080;&#1103; &#1089;&#1090;&#1086;&#1083;&#1073;&#1094;&#1086;&#1074;</span>
 <span style="color: #e5786d;">print</span>(AH.shape())
 <span style="color: #e5786d;">print</span>(AH.dtypes) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1090;&#1080;&#1087;&#1099; &#1089;&#1090;&#1086;&#1083;&#1073;&#1094;&#1086;&#1074;</span>
 <span style="color: #e5786d;">print</span>(AH.describe(inclide=<span style="color: #95e454;">'all'</span>) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">pre column: unique, mean, std, min, &#1082;&#1074;&#1072;&#1085;&#1090;&#1080;&#1083;&#1100;</span>
 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1048;&#1097;&#1077;&#1084; &#1072;&#1085;&#1086;&#1084;&#1072;&#1083;&#1080;&#1080;!</span>
 AH[<span style="color: #95e454;">'SalePrice'</span>].hist(bins = 60, normed=1);
 <span style="color: #8ac6f2; font-weight: bold;">from</span> scipy.stats.kde <span style="color: #8ac6f2; font-weight: bold;">import</span> gaussian_kde
 <span style="color: #8ac6f2; font-weight: bold;">from</span> numpy <span style="color: #8ac6f2; font-weight: bold;">import</span> linespace
 my_density = gaussian_kde(AH[<span style="color: #95e454;">'SalePrice'</span>]) <span style="color: #fa8072;">#</span>
 x = linespace(<span style="color: #e5786d;">min</span>(AH[<span style="color: #95e454;">'SalePrice'</span>]), <span style="color: #e5786d;">max</span>(AH[<span style="color: #95e454;">'SalePrice'</span>]), 1)
 plot(x, my_density(x), <span style="color: #95e454;">'g'</span>) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">green line</span>
 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1089;&#1084;&#1086;&#1090;&#1088;&#1080;&#1084; &#1085;&#1072; &#1087;&#1083;&#1086;&#1097;&#1072;&#1076;&#1080;!&#1095;</span>
 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1087;&#1086;&#1079;&#1074;&#1086;&#1083;&#1103;&#1077;&#1090; &#1085;&#1072;&#1081;&#1090;&#1080; &#1074;&#1099;&#1073;&#1088;&#1086;&#1089;&#1099; - &#1086;&#1090;&#1089;&#1090;&#1072;&#1102;&#1097;&#1080;&#1077; &#1087;&#1080;&#1085;&#1077;&#1095;&#1082;&#1080;</span>
 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1084;&#1086;&#1078;&#1077;&#1090; &#1073;&#1099;&#1090;&#1100; &#1085;&#1086;&#1088;&#1084;&#1072;&#1083;&#1100;&#1085;&#1099;&#1084; &#1088;&#1072;&#1089;&#1087;&#1088;&#1077;&#1076;&#1077;&#1083;&#1077;&#1085;&#1080;&#1077;&#1084;</span>

 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">2222222222222222222222</span>
 AH.groupby(<span style="color: #95e454;">'MS Zoning'</span>)[<span style="color: #95e454;">'SalePrices'</span>].plot.hist(alpha=0.6) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1085;&#1077;&#1089;&#1082;&#1086;&#1083;&#1100;&#1082;&#1086; &#1075;&#1080;&#1089;&#1090;&#1086;&#1075;&#1088;&#1072;&#1084;&#1084; &#1085;&#1072; &#1086;&#1076;&#1085;&#1086;&#1081; - &#1053;&#1045;&#1042;&#1040;&#1056;&#1053;&#1054; - &#1053;&#1059;&#1046;&#1053;&#1054; &#1085;&#1086;&#1088;&#1084;&#1080;&#1088;&#1086;&#1074;&#1072;&#1090;&#1100;</span>
 plt.legend()
 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1048; &#1074;&#1089;&#1077; &#1088;&#1072;&#1074;&#1085;&#1086; &#1085;&#1077; &#1088;&#1072;&#1076;&#1091;&#1077;&#1090;!</span>
 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1080;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1091;&#1077;&#1084; &#1071;&#1097;&#1080;&#1082;&#1086;&#1074;&#1091;&#1102; &#1076;&#1080;&#1072;&#1075;&#1088;&#1072;&#1084;&#1084;&#1091;</span>
 ax = AH.boxplot(column=<span style="color: #95e454;">'SalePrice'</span>, by=<span style="color: #95e454;">'M&#918; Zoning'</span>)

 <span style="color: #e5786d;">print</span>(AH[<span style="color: #95e454;">'M&#918; Zoning'</span>].value_counts()) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1089;&#1082;&#1086;&#1083;&#1100;&#1082;&#1086; &#1085;&#1072;&#1083;&#1102;&#1076;&#1077;&#1085;&#1080;&#1081; &#1074; &#1082;&#1072;&#1078;&#1076;&#1086;&#1081; &#1080;&#1079; &#1074;&#1099;&#1073;&#1086;&#1088;&#1086;&#1082;</span>



 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1076;&#1080;&#1072;&#1075;&#1072;&#1085;&#1072;&#1083;&#1100; - &#1089;&#1075;&#1083;&#1072;&#1078;&#1077;&#1085;&#1085;&#1072;&#1103; &#1075;&#1080;&#1089;&#1090;&#1086;&#1075;&#1088;&#1072;&#1084;&#1084;&#1072;, x, y - Colone, Coltwo</span>
 <span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">&#1054;&#1087;&#1088;&#1077;&#1076;&#1077;&#1083;&#1080;&#1083;&#1080; &#1089;&#1072;&#1084;&#1099;&#1077; &#1088;&#1072;&#1079;&#1083;&#1080;&#1095;&#1072;&#1102;&#1097;&#1080;&#1077;&#1089;&#1103; &#1087;&#1077;&#1088;&#1077;&#1084;&#1077;&#1085;&#1085;&#1099;&#1077;</span>
 df = pandas.read_csv(...)
 <span style="color: #8ac6f2; font-weight: bold;">from</span> pandas.plotting <span style="color: #8ac6f2; font-weight: bold;">import</span> scatter_matrix
 colors=(<span style="color: #95e454;">'Colone'</span>: <span style="color: #95e454;">'green'</span>, <span style="color: #95e454;">'Coltwo'</span>: <span style="color: #95e454;">'red'</span>)
 scatter_matrix(df,
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1088;&#1072;&#1079;&#1084;&#1077;&#1088; &#1082;&#1072;&#1088;&#1090;&#1080;&#1085;&#1082;&#1080;</span>
    figsize(6,6),
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1087;&#1083;&#1086;&#1090;&#1085;&#1086;&#1089;&#1090;&#1100; &#1074;&#1084;&#1077;&#1089;&#1090;&#1086; &#1075;&#1080;&#1089;&#1090;&#1086;&#1075;&#1088;&#1072;&#1084;&#1084;&#1099; &#1085;&#1072; &#1076;&#1080;&#1072;&#1075;&#1086;&#1085;&#1072;&#1083;&#1080;</span>
    diagonal=<span style="color: #95e454;">'kde'</span>,
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1094;&#1074;&#1077;&#1090;&#1072; &#1082;&#1083;&#1072;&#1089;&#1089;&#1086;&#1074;</span>
    c = df[<span style="color: #95e454;">'Status'</span>].replace(colors),
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1089;&#1090;&#1077;&#1087;&#1077;&#1085;&#1100; &#1087;&#1088;&#1086;&#1079;&#1088;&#1072;&#1095;&#1085;&#1086;&#1089;&#1090;&#1080; &#1090;&#1086;&#1095;&#1077;&#1082;</span>
    alpha=0.2)

 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1089;&#1090;&#1088;&#1086;&#1080;&#1084; &#1087;&#1086; &#1086;&#1087;&#1088;&#1077;&#1076;&#1077;&#1083;&#1077;&#1085;&#1085;&#1086;&#1081; &#1087;&#1077;&#1088;&#1077;&#1084;&#1077;&#1085;&#1085;&#1086;&#1081; &#1089;&#1090;&#1086;&#1083;&#1073;&#1094;&#1091; Diagonal &#1076;&#1074;&#1077; &#1075;&#1080;&#1089;&#1090;&#1086;&#1075;&#1088;&#1072;&#1084;&#1084;&#1099;</span>
 df.groupby(<span style="color: #95e454;">'Status'</span>)[<span style="color: #95e454;">'Diagonal'</span>].plot.hist(alpha=0.6, bins=10, <span style="color: #e5786d;">range</span>=[0, 500000])
 plt.legend()

 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1076;&#1080;&#1072;&#1075;&#1088;&#1072;&#1084;&#1084;&#1099; &#1088;&#1072;&#1089;&#1089;&#1077;&#1080;&#1074;&#1072;&#1085;&#1080;&#1103; &#1076;&#1083;&#1103; &#1101;&#1090;&#1086;&#1075;&#1086; &#1078;&#1077; &#1089;&#1090;&#1086;&#1083;&#1073;&#1094;&#1072;</span>
 df.plot.scatter(x=<span style="color: #95e454;">'Top'</span>, y=<span style="color: #95e454;">'Bottom'</span>, c=df[<span style="color: #95e454;">'Status'</span>].replace(colors))

</pre>
</div>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgd5ad633" class="outline-3">
<h3 id="orgd5ad633"><span class="section-number-3">8.5.</span> EXAMPLES OF ANALYSIS</h3>
<div class="outline-text-3" id="text-8-5">
</div>
<div id="outline-container-org6b9b6a8" class="outline-4">
<h4 id="org6b9b6a8"><span class="section-number-4">8.5.1.</span> dobrinin links</h4>
<div class="outline-text-4" id="text-8-5-1">
<p>
<a href="https://habr.com/ru/post/204500/">https://habr.com/ru/post/204500/</a>
</p>

<p>
Просто сравниваются 4 разных классификатора на 280 тыс. данных, разделенных 2/3, 1/3. И у всех очень низкий результат.
</p>

<p>
<a href="https://ai-news.ru/2018/08/pishem_skoringovuu_model_na_python.html">https://ai-news.ru/2018/08/pishem_skoringovuu_model_na_python.html</a> <a href="https://sfeducation.ru/blog/quants/skoring_na_python">https://sfeducation.ru/blog/quants/skoring_na_python</a>
</p>

<p>
Обычный препроцессинг, классификатор случайный лес, кросс-валидация по AUC и Bagging ансамбль над лесом.
</p>

<p>
<a href="https://www.youtube.com/watch?v=q9I2ozvHOmQ">https://www.youtube.com/watch?v=q9I2ozvHOmQ</a>
</p>

<p>
Реклама mlbootcamp.ru клона kaggle. Приз часы и футболка. На сайте нет почти ничего полезного.
</p>

<p>
<a href="http://bb3x.ru/blog/primer-resheniya-zadachi-kreditnogo-skoringa-c-podderzhkoy-svyazki-python-pandas-scikit-learn/">http://bb3x.ru/blog/primer-resheniya-zadachi-kreditnogo-skoringa-c-podderzhkoy-svyazki-python-pandas-scikit-learn/</a>
</p>

<p>
Копия первой ссылки
<a href="https://habr.com/en/post/270201/">https://habr.com/en/post/270201/</a>
</p>

<p>
Очень интересная статья использующая конструирование признаков и бустинге деревьев в Microsoft Azure Machine Learning студии. Без стандартных средств pandas дело не обошлось.
</p>
</div>
</div>
<div id="outline-container-org0d6faa3" class="outline-4">
<h4 id="org0d6faa3"><span class="section-number-4">8.5.2.</span> <a href="https://github.com/firmai/industry-machine-learning">https://github.com/firmai/industry-machine-learning</a></h4>
<div class="outline-text-4" id="text-8-5-2">
<p>
Consumer Finance
</p>
<ul class="org-ul">
<li><a href="https://github.com/Paresh3189/Bankruptcy-Prediction-Growth-Modelling">Loan Acceptance</a> - Classification and time-series analysis for loan acceptance. ( Классический стат. анализ
на выявления критичных показателй компании: бин-классификатор банкротсва SVM, Предсказание котировок ARIMA,
предсказания складваются чтобы оценить рост или падение. Случайный лес бин-классификатор использется для
определения важнейших показателей)</li>
<li><a href="https://github.com/Featuretools/predict-loan-repayment">Predict Loan Repayment</a> - Predict whether a loan will be repaid using automated feature engineering.( реклама
библиотеки Featuretools для automatic feature engeering)</li>
<li><a href="https://github.com/RealRadOne/Gyani-The-Loan-Eligibility-Predictor">Loan Eligibility Ranking</a> - System to help the banks check if a customer is eligible for a given loan. (
Отличаем выплаченные кредиты от не выплаченных. Препроцессинг с заменой на средние. Перцептрон, Случайный
лес, дерево принятия решений для классификации.  Результаты не проверяются и возможно переобучаются.)</li>
<li><a href="http://www.firmai.org/documents/Aggregator/#each-time-step-takes-30-seconds">Home Credit Default (FirmAI)</a> - Predict home credit default. (Фиерические финты с Pandas, классификатор
LightGBM метрика AUC, сросс-валидация StratifiedKFold. Результат это средняя feature<sub>importance</sub> по фолдам)</li>
<li><a href="https://github.com/abuchowdhury/Mortgage_Bank_Loan_Analtsics/blob/master/Mortgage%20Bank%20Loan%20Analytics.ipynb">Mortgage Analytics</a> - Extensive mortgage loan analytics. (Анализ временных рядов ипотечных кредитов: проверка
нулевой гипотезы, что величина является случайным блужданием; автокорреляция. Статистики: суммы;
Вероятностные диаграммы; Важность по ExtraTreeClassifier; диаграммы рассеяния; матрица корреляции;
уменьшение размерности методом главних компонент. Предсказание: процентной ставки, количества займов с
помощью ARIMA, Linear Regression, Logistic Regression, SVM, SVR, Decision Tree, RF, k-NN. Лучшие k-NN и
RandomForest.)</li>
<li><a href="https://github.com/IBM-Cloud-DevFest-2018/Data-Science-for-Banking/blob/master/02-CreditCardApprovalModel/CreditCardApprovalModel.ipynb">Credit Approval</a> - A system for credit card approval. ( Логистическая регрессия, много анализа, 690 записей
2/3 обучающие 1/3 тестируемая. Accuracy: 0.84 gini:0.814, что довольно мало.)</li>
<li><a href="https://github.com/Brett777/Predict-Risk">Loan Risk</a> - Predictive model to help to reduce charge-offs and losses of loans. (Apache Spark, H2O
www.h2o.ai платформа для распределенного ML на Hadoop или Spark. Реализована AutoML)</li>
<li><a href="http://www.firmai.org/documents/Amortization%20Schedule/">Amortisation Schedule (FirmAI)</a> - Simple amortisation schedule in python for personal use. Расчет граффика
погашения. Линейная и столбчатая диаграмма.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orged6ffaa" class="outline-3">
<h3 id="orged6ffaa"><span class="section-number-3">8.6.</span> EDA Exploratory analysis</h3>
<div class="outline-text-3" id="text-8-6">
<p>
according to CRISP: distribution of key attributes, looking for errors in the data, relationships between
pairs or small numbers of attributes, results of simple aggregations, properties of significant
subpopulations, and simple statistical analyses
</p>
<ul class="org-ul">
<li>time period</li>
<li>boxplot</li>
<li>historgram</li>
<li>missing values</li>
<li>Bivariate Exploration - impact on target: sns.violinplot</li>
</ul>

<p>
TODO <a href="https://www.kaggle.com/pavansanagapati/a-simple-tutorial-on-exploratory-data-analysis">https://www.kaggle.com/pavansanagapati/a-simple-tutorial-on-exploratory-data-analysis</a>
</p>
</div>
<div id="outline-container-org835030b" class="outline-4">
<h4 id="org835030b"><span class="section-number-4">8.6.1.</span> median, mean value</h4>
<div class="outline-text-4" id="text-8-6-1">
<dl class="org-dl">
<dt>Median</dt><dd>good for outliers, skewed distribution or ordinal data, ordinal data (e.g., survey responses),
less sensitive to data errors or anomalies.</dd>
<dt>Mean</dt><dd>sensitive to outliers and may not be the best choice if the dataset is skewed. Good for normal
distribution, interval data (e.g., Temperatures, heights), Data with a clear central tendency.</dd>
<dt>Mode</dt><dd>categorical data or discrete data, data have cleark peak, When the data has multiple peaks
(multimodal distribution).</dd>
<dt>Interquartile Mean (IQM)</dt><dd>The IQM is a robust measure of the middle value, which is less affected by
outliers. It's calculated as the average of the values between the 25th and 75th percentiles.</dd>
<dt>Winsorized Mean</dt><dd>This method involves replacing a portion of the data (usually 10% to 20%) at the extremes
with the values at the 10th and 90th percentiles, and then calculating the mean of the modified dataset.</dd>
</dl>


<p>
Interval data is a type of quantitative data that has the following properties:
</p>
<ul class="org-ul">
<li>Equal intervals: The differences between consecutive values are equal.</li>
<li>No true zero point: There is no true zero point, meaning that the zero point is arbitrary and doesn't represent the absence of the quantity being measured.</li>
<li>Order and magnitude: The data has a natural order and magnitude, meaning that higher values represent more of the quantity being measured.</li>
</ul>

<p>
Examples: temperatures, Heights, Time, IQ score.
</p>

<p>
ratio data: Weight, Length, Count data( such as the number of items)
</p>

<p>
Ordinal data: is a type of categorical data that has a natural order or ranking, but the differences between
 consecutive values are not necessarily equal. each category has a specific meaning or value.
</p>
<ul class="org-ul">
<li>Order: The categories have a natural order or ranking.</li>
<li>No true zero point: There is no true zero point, meaning that the zero point is arbitrary and doesn't
represent the absence of the quantity being measured.</li>
<li>No equal intervals: The differences between consecutive categories are not necessarily equal.</li>
</ul>

<p>
ex.
</p>
<ul class="org-ul">
<li>Survey responses: Survey responses, such as "Strongly Agree", "Agree", "Neutral", "Disagree", and "Strongly
Disagree", are ordinal data.</li>
<li>Rankings: Rankings, such as 1st, 2nd, 3rd.</li>
<li>Education levels, Job titles</li>
</ul>
</div>
</div>
<div id="outline-container-orgbff9559" class="outline-4">
<h4 id="orgbff9559"><span class="section-number-4">8.6.2.</span> types of comparison</h4>
<div class="outline-text-4" id="text-8-6-2">
<ul class="org-ul">
<li>goodness of fit - whether an observed frequency distribution differs from a theoretical distribution.</li>
<li>homogeneity - compares the distribution of counts for two or more groups using the same categorical variable</li>
<li>independence -  expressed in a contingency table,</li>
</ul>

<p>
<b>degrees of freedom</b> (df) 1) is the number of values in the final calculation of a statistic that are free to
 vary. 2) number of values that are free to vary as you estimate parameters. количество «свободных» величин,
 необходимых для того, чтобы полностью определить вектор. может быть не только натуральным, но и любым
 действительным числом.
</p>
<ul class="org-ul">
<li>For Two Samples: df = (N1 + N2) - 2</li>
</ul>
<p>
ex: [2, 10, 11] - we estimate mean parameter, so we have: two degree
</p>
<ul class="org-ul">
<li>(2 + 10 + 11)/ 3 = 7.7</li>
<li>11 = 7.7*3 - 10 - 2</li>
</ul>
</div>





<ol class="org-ol">
<li><a id="org01738ae"></a>links<br />
<div class="outline-text-5" id="text-8-6-2-1">
<ul class="org-ul">
<li><a href="https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/degrees-of-freedom/">https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/degrees-of-freedom/</a></li>
<li><a href="https://statisticsbyjim.com/hypothesis-testing/degrees-freedom-statistics/">https://statisticsbyjim.com/hypothesis-testing/degrees-freedom-statistics/</a></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org21d343d" class="outline-4">
<h4 id="org21d343d"><span class="section-number-4">8.6.3.</span> skewness and kurtosis</h4>
<div class="outline-text-4" id="text-8-6-3">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #8ac6f2; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt
<span style="color: #8ac6f2; font-weight: bold;">from</span> scipy.stats <span style="color: #8ac6f2; font-weight: bold;">import</span> kurtosis, skew

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">-- toy normal distribution</span>
<span style="color: #cae682;">mu</span>, <span style="color: #cae682;">sigma</span> = 0, 1 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">mean and standard deviation</span>
<span style="color: #cae682;">x</span> = np.random.normal(mu, sigma, 1000)
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">-- calc skewness and kurtosis</span>
<span style="color: #e5786d;">print</span>( <span style="color: #95e454;">'excess kurtosis of normal distribution (should be 0): {}'</span>.<span style="color: #e5786d;">format</span>( kurtosis(x) ))
<span style="color: #e5786d;">print</span>( <span style="color: #95e454;">'skewness of normal distribution (should be 0): {}'</span>.<span style="color: #e5786d;">format</span>( skew(x) ))
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">--</span>
plt.hist(x, density=<span style="color: #e5786d; font-weight: bold;">True</span>, bins=40)  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">density=False would make counts</span>
plt.ylabel(<span style="color: #95e454;">'Probability'</span>)
plt.xlabel(<span style="color: #95e454;">'Data'</span>);
plt.show()
</pre>
</div>

<pre class="example">
excess kurtosis of normal distribution (should be 0): -0.05048549574403838
skewness of normal distribution (should be 0): 0.2162053890291638
</pre>
</div>
</div>

<div id="outline-container-orgd2baa13" class="outline-4">
<h4 id="orgd2baa13"><span class="section-number-4">8.6.4.</span> <span class="todo TODO">TODO</span> normal distribution test</h4>
<div class="outline-text-4" id="text-8-6-4">
<p>
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html</a>
</p>

<p>
D’Agostino and Pearson’s test - 0 - means it is normal distribution
</p>
<pre class="example">
scipy.stats.normaltest(df['trip_duration_log'])
</pre>

<ul class="org-ul">
<li>statistic - s<sup>2</sup> + k<sup>2</sup>, where s is the z-score returned by skewtest and k is the z-score returned by kurtosistest.</li>
<li>pvalue - (p-value) A 2-sided chi squared probability for the hypothesis test. if low - there is low
probability that big statistic value is realy describe not normal distribution.
<ul class="org-ul">
<li>inverse is not true, not used to provide evidence for the null hypothesis.</li>
<li></li>
</ul></li>
</ul>



<p>
normal distribution - symmetrical bell curve - может быть описано функцией Гауса (Gaussian distribution)
</p>
<ul class="org-ul">
<li>e<sup>((−(x − μ)<sup>2</sup>)/2*σ<sup>2</sup>)</sup>/(σ*√2π)
<ul class="org-ul">
<li>σ - standard devitation</li>
</ul></li>
</ul>

<p>
Null Hypothesis - The null hypothesis is that the observed difference is due to chance alone. Нулевая гипотеза
состоит в том, что наблюдаемая разница обусловлена только случайностью.
</p>

<p>
null distribution - when the null hypothesis is true. Here it is not normal distribution. for large number of
 samples equal to chi-squared distribution with two degrees of freedom.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #8ac6f2; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt
<span style="color: #8ac6f2; font-weight: bold;">from</span> scipy.stats <span style="color: #8ac6f2; font-weight: bold;">import</span> normaltest

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">-- toy normal distribution</span>
<span style="color: #cae682;">mu</span>, <span style="color: #cae682;">sigma</span> = 0, 1 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">mean and standard deviation</span>
<span style="color: #cae682;">x</span> = np.random.normal(mu, sigma, 100)
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">-- calc skewness and kurtosis</span>
<span style="color: #e5786d;">print</span>( <span style="color: #95e454;">'Test whether a sample differs from a normal distribution. (should be 0): {}'</span>.<span style="color: #e5786d;">format</span>( normaltest(x) ))

</pre>
</div>

<pre class="example">
Test whether a sample differs from a normal distribution. (should be 0): NormaltestResult(statistic=4.104513172099168, pvalue=0.12844472972455415)
</pre>
</div>
</div>

<div id="outline-container-org38855a7" class="outline-4">
<h4 id="org38855a7"><span class="section-number-4">8.6.5.</span> Analysis for regression model:</h4>
<div class="outline-text-4" id="text-8-6-5">
<ul class="org-ul">
<li>Linearity: assumes that the relationship between predictors and target variable is linear</li>
<li>No noise: eg. that there are no outliers in the data</li>
<li>No collinearity: if you have highly correlated predictors, it’s most likely your model will overfit</li>
<li>Normal distribution: more reliable predictions are made if the predictors and the target variable are
normally distributed</li>
<li>Scale: it’s a distance-based algorithm, so preditors should be scaled — like with standard scaler</li>
</ul>
</div>
</div>
<div id="outline-container-org5aef19a" class="outline-4">
<h4 id="org5aef19a"><span class="section-number-4">8.6.6.</span> quartile, quantile, percentile</h4>
<div class="outline-text-4" id="text-8-6-6">
<ul class="org-ul">
<li>Range from 0 to 100</li>
<li>Quartiles: Range from 0 to 4.</li>
<li>Quantiles: Range from any value to any other value.</li>
</ul>

<p>
percentiles and quartiles are simply types of quantiles
</p>
<ul class="org-ul">
<li>4-quantiles are called quartiles.</li>
<li>5-quantiles are called quintiles.</li>
<li>8-quantiles are called octiles.</li>
<li>10-quantiles are called deciles.</li>
<li>100-quantiles are called percentiles.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgea6ee7b" class="outline-3">
<h3 id="orgea6ee7b"><span class="section-number-3">8.7.</span> gradient boostings vs NN</h3>
<div class="outline-text-3" id="text-8-7">
<ul class="org-ul">
<li>NN are very efficient for dealing with high dimensional raw data</li>
<li>GBM can handle missing values</li>
<li>GBM do not need GPU</li>
<li>NN big data "the more the merrier" GBM - more - bigger error</li>
</ul>
</div>
</div>
<div id="outline-container-org967a057" class="outline-3">
<h3 id="org967a057"><span class="section-number-3">8.8.</span> theory</h3>
<div class="outline-text-3" id="text-8-8">
</div>
<div id="outline-container-org4bdacb6" class="outline-4">
<h4 id="org4bdacb6"><span class="section-number-4">8.8.1.</span> types of data, data types <a id="org2aa664e"></a>:</h4>
<div class="outline-text-4" id="text-8-8-1">
<ul class="org-ul">
<li>Categorical (Qualitative)
<ul class="org-ul">
<li>ordinal - order or ranking, difference between categories is not equeal/known. Arifmetic operations
prohibited.</li>
<li>normal - no oreder</li>
</ul></li>
<li>Numerical (Quantitative) - continuous, discrete</li>
</ul>

<p>
My
</p>
<ul class="org-ul">
<li>numerical - almost all values are unique</li>
<li>binary - only 2 values [red, blue, red, blue]</li>
<li>categorical - has frequent values [red, red, blue, yellow, black]</li>
</ul>

<p>
Good ML model should tread
</p>
</div>
</div>

<div id="outline-container-orgd84edcf" class="outline-4">
<h4 id="orgd84edcf"><span class="section-number-4">8.8.2.</span> terms</h4>
<div class="outline-text-4" id="text-8-8-2">
<ul class="org-ul">
<li>proportions - is a mathematical statement expressing equality of two ratios a/b = c/d</li>
</ul>
</div>
</div>
<div id="outline-container-orgca898b5" class="outline-4">
<h4 id="orgca898b5"><span class="section-number-4">8.8.3.</span> 1 column describe</h4>
<div class="outline-text-4" id="text-8-8-3">
<ul class="org-ul">
<li><b>count</b> - total count in each category of the categorical variables</li>
<li><b>среднее</b> - mean, median,</li>
<li><b>mode</b> - мультимодальность указывает на то, что набор данных не подчиняется нормальному распределению.
<ul class="org-ul">
<li>для категориальных - count (например: 6, 2, 6, 6, 8, 9, 9, 9, 0; мода — 6 и 9).</li>
<li>для числовых - пики гистограммы</li>
<li>.groupby(['Outlet<sub>Type</sub>']).agg(lambda x:x.value<sub>counts</sub>().index[0]))</li>
<li>.mode()</li>
</ul></li>
<li><b>Measures of Dispersion</b>
<ul class="org-ul">
<li>Range - max - min</li>
<li>Quartiles and  Interquartile (IQR) - difference between the 3rd and the 1st quartile</li>
<li>Standard Deviation - tells us how much all data points deviate from the mean value
<ul class="org-ul">
<li>.std()</li>
</ul></li>
<li>Skewness
<ul class="org-ul">
<li>skew() - data shapes are skewed or have asymmetry different from Gaussian. it is that measure.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgd961b4a" class="outline-4">
<h4 id="orgd961b4a"><span class="section-number-4">8.8.4.</span> categories of analysis</h4>
<div class="outline-text-4" id="text-8-8-4">
<ul class="org-ul">
<li>Descriptive analysis - What happened.
<ul class="org-ul">
<li>It does this by ordering, manipulating, and interpreting raw data from various sources to turn it into valuable insights to your business.</li>
<li>present our data in a meaningful way.</li>
</ul></li>
<li>Exploratory analysis - How to explore data relationships.
<ul class="org-ul">
<li>to find connections and generate hypotheses and solutions for specific problems</li>
</ul></li>
<li>Diagnostic analysis - Why it happened.</li>
<li>Predictive analysis - What will happen.</li>
<li>Prescriptive analysis - How will it happen.</li>
</ul>
</div>
</div>
<div id="outline-container-org97992f0" class="outline-4">
<h4 id="org97992f0"><span class="section-number-4">8.8.5.</span> methods</h4>
<div class="outline-text-4" id="text-8-8-5">
<ul class="org-ul">
<li>cluster analysis -  grouping a set of data elements in a way that said elements are more similar</li>
<li>Cohort analysis - behavioral analytics that breaks the data in a data set into related groups before analysis
<ul class="org-ul">
<li>to "see patterns clearly across the life-cycle of a customer (or user), rather than slicing across all
customers blindly without accounting for the natural cycle that a customer undergoes."</li>
</ul></li>
<li>Regression analysis - how a dependent variable's value is affected when one (linear regression) or more
independent variables (multiple regression) change or stay the same
<ul class="org-ul">
<li>you can anticipate possible outcomes and make better business decisions in the future</li>
</ul></li>
<li>Factor analysis - dimension reduction</li>
<li>Funnel analysis - analyzing a series of events that lead towards a defined goal - воронка</li>
</ul>
</div>
</div>
<div id="outline-container-org5d87823" class="outline-4">
<h4 id="org5d87823"><span class="section-number-4">8.8.6.</span> correlation</h4>
<div class="outline-text-4" id="text-8-8-6">
<p>
any statistical relationship between two random variables
</p>

<p>
<a href="https://github.com/8080labs/ppscore">https://github.com/8080labs/ppscore</a>
</p>
<ul class="org-ul">
<li><a href="https://machinelearningknowledge.ai/predictive-power-score-vs-correlation-with-python-implementation/">https://machinelearningknowledge.ai/predictive-power-score-vs-correlation-with-python-implementation/</a></li>
<li>Based on RandomForest</li>
<li>non-linear</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org47ff76f"></a>Pearson's product-moment coefficient<br />
<div class="outline-text-5" id="text-8-8-6-1">
<p>
sensitive only to a linear relationship between two variables
</p>

<p>
Corr(X,Y) = cov(X,Y) / σ(X)*σ(Y) = E[(X - μx)(Y-μx)]/σ(X)*σ(Y) , if σ(X)*σ(Y) &gt; 0, E is the expected value
 operator.
</p>
</div>
</li>
<li><a id="org390243b"></a>Spearman's rank correlation<br />
<div class="outline-text-5" id="text-8-8-6-2">
<p>
have been developed to be more robust than Pearson's, that is, more sensitive to nonlinear relationships
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org9ae155e" class="outline-4">
<h4 id="org9ae155e"><span class="section-number-4">8.8.7.</span> explanatory/inference vs prediction modeling</h4>
<div class="outline-text-4" id="text-8-8-7">
<p>
Prediction modeling: X information and Y are avaialable. We prepare model for new X without Y.
</p>

<p>
Explanatory/inference: testing validity of relationship between X and Y.
</p>
<ul class="org-ul">
<li>testing causal theories and it's hypotheses</li>
<li>usually association-based models</li>
</ul>

<p>
<b>Association-based models</b> - idenfity interestings, and find patterns or co-occurrences in data, which can be
 used to make predictions or recommendations. Association rule mining. Can uncover valuable insights from
 large datasets, enabling better decision-making across various domains.
</p>
<ul class="org-ul">
<li>Ex. Apriori, Eclat, and FP-Growth; WEKA, SQL Server Analysis Services, mlxtend</li>
</ul>

<p>
Interestings:
</p>
<ul class="org-ul">
<li>Support(X-&gt;Y) = (Number of transactions containing both X and Y) / Total number of transactions
<ul class="org-ul">
<li>Example: If a rule {onions, potatoes} → {burger} has a support of 0.05, it means that 5% of all
transactions in the database contain both onions, potatoes, and burgers.</li>
</ul></li>
<li>Confidence(X-&gt;Y) - (Number of transactions containing both X and Y) / (Number of transactions containing X)
<ul class="org-ul">
<li>Example: If a rule {onions, potatoes} → {burger} has a confidence of 0.8, it means that 80% of the transactions that contain onions and potatoes also contain burgers.</li>
</ul></li>
<li>Lift(X-&gt;Y) = Confidence(X-&gt;Y) / Support(Y) (devided by)
<ul class="org-ul">
<li>Example: If a rule {onions, potatoes} → {burger} has a lift greater than 1, it indicates that the presence
of onions and potatoes increases the likelihood of buying burgers more than the average likelihood of
buying burgers.</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org9ec6139" class="outline-4">
<h4 id="org9ec6139"><span class="section-number-4">8.8.8.</span> Independence of Irrelevant Alternatives(IIA)</h4>
<div class="outline-text-4" id="text-8-8-8">
<p>
relative probabilities of choosing between two classes are not affected by the presence or absence of other
 classes.
</p>

<p>
IIA stetes that relative likelihood of choosing between two alternatives (e.g., A and B) should not be affected by the
 presence or absence of a third, irrelevant alternative (e.g., C).
</p>

<p>
Violate the IIA assumption: introduction of a new alternative can change the relative probabilities of
 choosing the existing alternatives.
</p>
<ul class="org-ul">
<li>predictive modeling: violating IIA might not significantly impact the model's performance (primary goal is
prediction rather than understanding the underlying choice behavior)</li>
<li>for descriptive or explanatory models</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgcdf0cbc" class="outline-3">
<h3 id="orgcdf0cbc"><span class="section-number-3">8.9.</span> Feature Preparation <a id="org16ca2da"></a></h3>
<div class="outline-text-3" id="text-8-9">
<p>
Ideally data is i.i.d. Independent and identically distributed - simplify computations.
</p>

<ol class="org-ol">
<li>get information from string columns</li>
<li>encoding</li>
<li>scaling.
<ul class="org-ul">
<li>StandardScaling если нет skew.</li>
<li>Если есть skew, то clipping или log scaling или нормализация.</li>
<li>Если не знаем есть Skew или нет, то MinMaxScaler.
<ul class="org-ul">
<li>очень чувствителен к выбросам, поэтому их нужно обрезать</li>
</ul></li>
</ul></li>
<li>for categorical values get</li>
</ol>
</div>

<div id="outline-container-org5ee476b" class="outline-4">
<h4 id="org5ee476b"><span class="section-number-4">8.9.1.</span> terms</h4>
<div class="outline-text-4" id="text-8-9-1">
<ul class="org-ul">
<li><b>nominal features</b> are categoricals with values that have no order</li>
<li>binary symmetric and asymmetric attributes - man and woman, positive results in medical is more significant
than a negative</li>
<li>EDA - exploratory data analysis</li>
<li>OHE - one-hot-encoding</li>
<li><p>
<b>transformations</b> - preserve rank of the values along each feature
</p>
<ul class="org-ul">
<li>the log of the data or any other transformation of the data that preserves the order because what matters</li>
</ul>
<p>
is which ones have the smallest distance.
</p></li>
<li><b>normalization</b> - process of converting a variable's actual range of values into: -1 to +1, 0 to 1, the normal
distribution</li>
<li><p>
<b>scaling</b> - shifts the range of a label and/or feature value.
</p>
<ul class="org-ul">
<li>linear scaling - combination of subtraction and division to replace the original value with a number</li>
</ul>
<p>
between -1 and +1 or between 0 and 1.
</p>
<ul class="org-ul">
<li>logarithmic scaling</li>
<li>Z-score normalization or standard scaling</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org01216b0" class="outline-4">
<h4 id="org01216b0"><span class="section-number-4">8.9.2.</span> Выбросы Outliers</h4>
<div class="outline-text-4" id="text-8-9-2">
</div>
<ol class="org-ol">
<li><a id="org2fea24e"></a>quantile<br />
<div class="outline-text-5" id="text-8-9-2-1">
<ul class="org-ul">
<li><a href="https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/">https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/</a></li>
</ul>

<p>
в sklearn различные скалирования по разному чувствительны к выбросам
</p>

<p>
q<sub>low</sub> = df["col"].quantile(0.01)
q<sub>hi</sub>  = df["col"].quantile(0.99)
df<sub>filtered</sub> = df[(df["col"] &lt; q<sub>hi</sub>) &amp; (df["col"] &gt; q<sub>low</sub>)]
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">outliers</span>(p):
    <span style="color: #cae682;">df</span>: pd.DataFrame = pd.read_pickle(p)
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print(df.describe().to_string())</span>
    <span style="color: #8ac6f2; font-weight: bold;">for</span> c <span style="color: #8ac6f2; font-weight: bold;">in</span> df.columns:
        <span style="color: #cae682;">q_low</span> = df[c].quantile(0.001)
        <span style="color: #cae682;">q_hi</span> = df[c].quantile(0.999)

        <span style="color: #cae682;">df_filtered</span> = df[(df[c] &gt; q_hi) | (df[c] &lt; q_low)]
        df.drop(df_filtered.index, inplace=<span style="color: #e5786d; font-weight: bold;">True</span>)
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print(df.describe().to_string())</span>
    <span style="color: #cae682;">p</span> = <span style="color: #95e454;">'without_outliers.pickle'</span>
    pd.to_pickle(df, p)
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"ok"</span>)
    <span style="color: #8ac6f2; font-weight: bold;">return</span> p
</pre>
</div>
</div>
</li>

<li><a id="orgbb6a500"></a>TODO<br />
<div class="outline-text-5" id="text-8-9-2-2">
<ul class="org-ul">
<li><a href="file:///home/u2/Downloads/electronics-11-01213.pdf">file:///home/u2/Downloads/electronics-11-01213.pdf</a></li>
<li><a href="https://www.cse.wustl.edu/~jain/cse567-17/ftp/mttad/index.html">https://www.cse.wustl.edu/~jain/cse567-17/ftp/mttad/index.html</a></li>
<li><a href="https://medium.com/@katser/a-review-of-anomaly-detection-metrics-with-a-lot-of-related-information-736d88774712">https://medium.com/@katser/a-review-of-anomaly-detection-metrics-with-a-lot-of-related-information-736d88774712</a></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org236ea29" class="outline-4">
<h4 id="org236ea29"><span class="section-number-4">8.9.3.</span> IDs encoding with embaddings</h4>
</div>
<div id="outline-container-org2bafab5" class="outline-4">
<h4 id="org2bafab5"><span class="section-number-4">8.9.4.</span> Categorical encode</h4>
<div class="outline-text-4" id="text-8-9-4">
<ul class="org-ul">
<li>Replacing values</li>
<li>Encoding labels - to number 0&#x2026; n<sub>categories</sub>-1 - pandas: .get<sub>dummies</sub>(data, drop<sub>first</sub>=True)</li>
<li>One-Hot encoding - each category value into a new column and assign a 1 or 0</li>
<li>Binary encoding</li>
<li>Backward difference encoding</li>
<li>Miscellaneous features</li>
<li>MeanEncoding - A,B -&gt; 0.7, 0.3 - mean of binary target [1,0]</li>
</ul>

<p>
Pros of MeanEncoding:
</p>
<ul class="org-ul">
<li>Capture information within the label, therefore rendering more predictive features</li>
<li>Creates a monotonic relationship between the variable and the target</li>
</ul>
<p>
Cons of MeanEncodig:
</p>
<ul class="org-ul">
<li>It may cause over-fitting in the model.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org1906e25"></a>Label encoding<br />
<div class="outline-text-5" id="text-8-9-4-1">
<pre class="example">
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
le.fit([1, 2, 2, 6])
</pre>

<p>
LabelEncoder()
</p>
<pre class="example">
le.classes_
</pre>

<p>
array([1, 2, 6])
</p>
<pre class="example">
le.transform([1, 1, 2, 6])
</pre>

<p>
array([0, 0, 1, 2]&#x2026;)
</p>
<pre class="example">
le.inverse_transform([0, 0, 1, 2])
</pre>

<p>
array([1, 1, 2, 6])
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org334b8da" class="outline-4">
<h4 id="org334b8da"><span class="section-number-4">8.9.5.</span> imbalanced classes and sampling</h4>
<div class="outline-text-4" id="text-8-9-5">
<ul class="org-ul">
<li>very infrequent features are hard to learn</li>
</ul>
</div>
</div>
<div id="outline-container-org8e116c3" class="outline-4">
<h4 id="org8e116c3"><span class="section-number-4">8.9.6.</span> Skewed numerical feature</h4>
<div class="outline-text-4" id="text-8-9-6">
<ul class="org-ul">
<li><b>Linear Scaling</b> x'=(x - x<sub>min</sub>)/(x<sub>max</sub> - x<sub>min</sub>) - When the feature is more-or-less uniformly distributed
across a fixed range.</li>
<li><b>Clipping</b> if x &gt; max, then x' = max. if x &lt; min, then x' = min - When the feature contains some extreme
outliers.</li>
<li><b>Log Scaling</b> x' = log(x) - When the feature conforms to the <b>power law</b>.</li>
<li><b>Z-Score</b> or standard scaling - When the feature distribution does not contain extreme outliers. (as Google
say)</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org7c64ced"></a>power law<br />
<div class="outline-text-5" id="text-8-9-6-1">
<p>
is a functional relationship between two quantities
</p>
<div class="org-src-container">
<pre class="src src-text">
       |
     | |
     | |
     |  \
     |   \
     |    -----------------------
     |-------------------------------


</pre>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgbf06092" class="outline-4">
<h4 id="orgbf06092"><span class="section-number-4">8.9.7.</span> missing values: NaN, None</h4>
<div class="outline-text-4" id="text-8-9-7">
<p>
pands: data.info() - количество непустых значения для каждого столбца
</p>
</div>

<ol class="org-ol">
<li><a id="orgf8d6447"></a>missing flag<br />
<div class="outline-text-5" id="text-8-9-7-1">
<p>
for feature in df.columns:
  if df[feature].hasnans:
    df["is_" + feature + "<sub>missing</sub>"] = np.isnull(df[feature]) * 1
</p>
</div>
</li>
<li><a id="org9e08370"></a>Проблема выбора типичного значения<br />
<div class="outline-text-5" id="text-8-9-7-2">
<ul class="org-ul">
<li>заменить NaN на новый признак - если это отдельная группа .fillna(0)
<ul class="org-ul">
<li>Одна из хороших практик учета отсутствующих данных — генерация бинарных функций. Такие функции принимают
значение 0 или 1, указывающие на то, присутствует ли в записи значение признака или оно пропущено.</li>
</ul></li>
<li>усеченная средняя - сортируем и удаляем по краям</li>
<li>median - data['Age'] = data.Age.fillna(data.Age.median())</li>
<li>q3-q1</li>
<li>sd ?</li>
<li>предсказание - лучший метод</li>
<li>моды - значения которые встречаются наиболее часто</li>
</ul>

<p>
Другими распространенными практиками являются следующие подходы:
</p>
<ul class="org-ul">
<li>Удаление записей с отсутствующими значениями. Обычно так делается, если число недостающих значений очень
мало в сравнении со всей выборкой, при этом сам факт пропуска значения имеет случайный характер. Недостатком
такой стратегии является возникновение ошибок в случаях идентичных пропусков в тестовых данных.</li>
<li>Подстановка среднего, медианного или наиболее распространенного значения данного признака.</li>
<li>Использование различных предсказательных моделей для прогнозирования пропущенного значения при помощи остальных данных
датасета.</li>
</ul>
</div>
</li>
<li><a id="org7740cfd"></a>scikit-learn<br />
<div class="outline-text-5" id="text-8-9-7-3">
<ul class="org-ul">
<li><a href="https://scikit-learn.org/stable/modules/impute.html">https://scikit-learn.org/stable/modules/impute.html</a></li>
</ul>
<p>
IterativeImputer
</p>
</div>
<ol class="org-ol">
<li><a id="org3000410"></a>terms<br />
<div class="outline-text-6" id="text-8-9-7-3-1">
<ul class="org-ul">
<li>impute [ɪmˈpjuːt] - приписывать
<ul class="org-ul">
<li>is to impute the missing values, i.e., to infer them from the known part of the data</li>
</ul></li>
<li>imputation [ɪmpjʊˈteɪʃn]</li>
<li>infer [ɪnˈfɜː] - делать вывод, заключать</li>
</ul>

<p>
Types:
</p>
<ul class="org-ul">
<li>univariate - из того столбца в котором нет</li>
<li>Multivariate - из всего набора данных</li>
</ul>
</div>
</li>
</ol>
</li>

<li><a id="orga26a338"></a>autoimpute<br />
<div class="outline-text-5" id="text-8-9-7-4">
<p>
<a href="https://autoimpute.readthedocs.io/en/latest/">https://autoimpute.readthedocs.io/en/latest/</a>
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org6289db1" class="outline-4">
<h4 id="org6289db1"><span class="section-number-4">8.9.8.</span> numerical data to bins</h4>
<div class="outline-text-4" id="text-8-9-8">
<p>
there might be fluctuations in those numbers that don't reflect patterns in the data, which might be noise
</p>

<p>
Новый столбец с 4 бинами возростов [0, 1, 2, 3]:
</p>
<pre class="example">
data['CatAge'] = pd.qcut(data.Age, q=4, labels=False )
data = data.drop(['Age', 'Fare'], axis=1) # удаляем оригинальыне столбцы
</pre>


<p>
simple map
</p>
<pre class="example">
df['KIDSDRIV'] = df['KIDSDRIV'].map({0:0,1:1,2:2,3:2,4:2})
</pre>


<p>
разложить в бины:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #cae682;">df</span>[<span style="color: #95e454;">'HOMEKIDS'</span>]= pd.cut(df[<span style="color: #95e454;">'HOMEKIDS'</span>],
                       bins=[0,1,2,3,4,10],
                       labels=[0,1,2,3,4],
                       include_lowest=<span style="color: #e5786d; font-weight: bold;">True</span>,
                       right=<span style="color: #e5786d; font-weight: bold;">True</span>).astype(<span style="color: #e5786d;">float</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org0f6b9fe" class="outline-4">
<h4 id="org0f6b9fe"><span class="section-number-4">8.9.9.</span> Sparse Classes</h4>
<div class="outline-text-4" id="text-8-9-9">
<p>
categorical features) are those that have very few total observations.
</p>
<ul class="org-ul">
<li>переобучение модели</li>
</ul>

<p>
1 большой класс и тыща супер маленьких - объединяем маленькие в большие или просто в "Others"
</p>
</div>
</div>
<div id="outline-container-org838d163" class="outline-4">
<h4 id="org838d163"><span class="section-number-4">8.9.10.</span> Feature engeering or Feature Creation</h4>
<div class="outline-text-4" id="text-8-9-10">
<p>
Сильно зависит от модели - разные модели могут синтезировать разные операции
</p>
<ul class="org-ul">
<li>линейные модели - суммы столбцов создают мультиколлинеарность что мешает</li>
<li>neural network легко синтезирует +,-,*,counts, diff, power, rational polynominal ( bad ratio and</li>
</ul>

<p>
clusterization as a source of new features
</p>
</div>
<ol class="org-ol">
<li><a id="orga927fcb"></a>Why?<br />
<div class="outline-text-5" id="text-8-9-10-1">
<p>
Например два вида точек в полярных координатах и в прямоугольной системе координат
</p>
<ul class="org-ul">
<li>если получается круг - то тяжелее</li>
</ul>

<p>
Когда граница пролигает по операции которую модели тяжело синтезировать
</p>
</div>
</li>
<li><a id="org6729b0a"></a><a href="https://arxiv.org/pdf/1701.07852.pdf">https://arxiv.org/pdf/1701.07852.pdf</a><br />
<div class="outline-text-5" id="text-8-9-10-2">
<ul class="org-ul">
<li>Counts ?</li>
<li>Differences (diff) = x1-x2</li>
<li>Logarithns (log) = log(x)</li>
<li>Polynomials (poly) = 1 + 5x + 8x<sup>2</sup></li>
<li>Powers (pow) = x<sup>2</sup></li>
<li>Ratios = y = x1/x2</li>
<li>Rational Differences (ratio<sub>diff</sub>) y = (x1-x2)/(x3-x4)</li>
<li>Rational Polynomials y = 1/(5x + 8x<sup>2</sup>)</li>
<li>Root Distance ?</li>
<li>sqiare roots = sqrt(x)</li>
<li>quadratic equation (quad) = y = |((-b + sqrt(b<sup>2</sup>-4ac))/2a - (-b - sqrt(b<sup>2</sup>-4ac))/2a)</li>
</ul>
</div>
</li>

<li><a id="org99088fb"></a>Heaton <a href="https://towardsdatascience.com/importance-of-feature-engineering-methods-73e4c41ae5a3">https://towardsdatascience.com/importance-of-feature-engineering-methods-73e4c41ae5a3</a><br />
<div class="outline-text-5" id="text-8-9-10-3">
<p>
<b>NN</b> fail at synthesizing
</p>
<ol class="org-ol">
<li>ratio<sub>diff</sub></li>
<li>ratio</li>
<li>quad - ?</li>
<li>log - ?</li>
</ol>

<p>
<b>Random Forest</b>
</p>
<ol class="org-ol">
<li>ratio<sub>diff</sub></li>
<li>quad</li>
<li>count</li>
</ol>

<p>
<b>BDT</b> Gradient Boosted Decision Trees
</p>
<ol class="org-ol">
<li>ratio<sub>diff</sub></li>
<li>ratio</li>
<li>counts</li>
<li>quad</li>
</ol>
</div>
</li>

<li><a id="org7fcd283"></a>Time Series <a id="org1db2b2b"></a><br />
<div class="outline-text-5" id="text-8-9-10-4">
<ul class="org-ul">
<li><a href="https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/">https://machinelearningmastery.com/basic-feature-engineering-time-series-data-python/</a></li>
<li><a href="https://www.analyticsvidhya.com/blog/2019/12/6-powerful-feature-engineering-techniques-time-series/">https://www.analyticsvidhya.com/blog/2019/12/6-powerful-feature-engineering-techniques-time-series/</a></li>
<li>TODO <a href="https://codenrock.com/contests/kurs-po-poisku-anomalii-vremennyh-ryadov#/">https://codenrock.com/contests/kurs-po-poisku-anomalii-vremennyh-ryadov#/</a>
<ul class="org-ul">
<li><a href="https://youtu.be/92EF4vqaBSE?roistat_visit=1425857">https://youtu.be/92EF4vqaBSE?roistat_visit=1425857</a></li>
<li><a href="https://youtu.be/XQCAkcH--Kg?roistat_visit=1425857">https://youtu.be/XQCAkcH--Kg?roistat_visit=1425857</a></li>
<li><a href="https://youtu.be/i3ZC_Q5FSgs?roistat_visit=1425857">https://youtu.be/i3ZC_Q5FSgs?roistat_visit=1425857</a></li>
</ul></li>
</ul>

<p>
v
</p>
<ul class="org-ul">
<li>parts of date</li>
<li>quarter, type of year</li>
<li>logical indicator - first/last day of &#x2026;</li>
<li><b>Lag features</b>. t-1 target value = <b>lag</b> . lag<sub>1</sub> = NaN, 1,2,3, 8&#x2026;</li>
<li><b>Rolling window</b> - statistic based on past values - with static window size</li>
<li><b>Expanding window feature</b> - all past values into account</li>
<li>external dataset - holidays, weather</li>
</ul>

<p>
lag correlations:
</p>
<pre class="example">
from statsmodels.graphics.tsaplots import plot_acf
plot_acf(data['Count'], lags=10)
plot_pacf(data['Count'], lags=10)
</pre>
</div>
</li>

<li><a id="org8994a8a"></a>tools<br />
<ol class="org-ol">
<li><a id="org5b9f989"></a>featuretools<br />
<div class="outline-text-6" id="text-8-9-10-5-1">
<ul class="org-ul">
<li>jyputer <a href="https://github.com/brynmwangy/Beginner-Guide-to-Automated-Feature-Engineering-With-Deep-Feature-Synthesis./blob/master/Automated_Feature_Engineering.ipynb">https://github.com/brynmwangy/Beginner-Guide-to-Automated-Feature-Engineering-With-Deep-Feature-Synthesis./blob/master/Automated_Feature_Engineering.ipynb</a></li>
<li>article <a href="https://medium.com/@rrfd/simple-automatic-feature-engineering-using-featuretools-in-python-for-classification-b1308040e183">https://medium.com/@rrfd/simple-automatic-feature-engineering-using-featuretools-in-python-for-classification-b1308040e183</a></li>
<li><a href="https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics/notebook">https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics/notebook</a></li>
<li>doc <a href="https://docs.featuretools.com/en/stable/generated/featuretools.dfs.html#featuretools.dfs">https://docs.featuretools.com/en/stable/generated/featuretools.dfs.html#featuretools.dfs</a></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org63f2e54"></a>synthetic features<br />
<div class="outline-text-7" id="text-8-9-10-5-1-1">
<div class="org-src-container">
<pre class="src src-python"> <span style="color: #cae682;">prmt</span>=ft.list_primitives()
 pd.options.display.<span style="color: #cae682;">max_colwidth</span>=150
 <span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">aggregations</span>
 prmt[prmt[<span style="color: #95e454;">"type"</span>]==<span style="color: #95e454;">"aggregation"</span>].head(10)
 <span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">transformations</span>
 prmt[prmt[<span style="color: #95e454;">"type"</span>]==<span style="color: #95e454;">"transform"</span>].head(10)
</pre>
</div>
</div>
</li>
</ol>
</li>
<li><a id="orgf4a5926"></a><span class="todo TODO">TODO</span> Informationsfabrik<br />
<div class="outline-text-6" id="text-8-9-10-5-2">
<ul class="org-ul">
<li><a href="https://github.com/Informationsfabrik/feature-engineering-slides/blob/master/pydata2019_feature-engineering.pdf">https://github.com/Informationsfabrik/feature-engineering-slides/blob/master/pydata2019_feature-engineering.pdf</a></li>
<li><a href="https://github.com/Informationsfabrik/feature-engineering">https://github.com/Informationsfabrik/feature-engineering</a></li>
</ul>
</div>
</li>

<li><a id="org12b016f"></a><span class="todo TODO">TODO</span> TPOT<br />
<div class="outline-text-6" id="text-8-9-10-5-3">
<p>
<a href="https://epistasislab.github.io/tpot/">https://epistasislab.github.io/tpot/</a>
</p>
</div>
</li>
<li><a id="orgb15fdac"></a>tsfresh - time sequence<br />
<div class="outline-text-6" id="text-8-9-10-5-4">
<ul class="org-ul">
<li><a href="https://tsfresh.readthedocs.io/en/latest/">https://tsfresh.readthedocs.io/en/latest/</a></li>
</ul>
</div>
</li>
<li><a id="org248590a"></a>ATgfe<br />
<div class="outline-text-6" id="text-8-9-10-5-5">
<ul class="org-ul">
<li><a href="https://github.com/ahmed-mohamed-sn/ATgfe">https://github.com/ahmed-mohamed-sn/ATgfe</a></li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="orgba364cb"></a>on featuretools<br />
<div class="outline-text-5" id="text-8-9-10-6">
<p>
<a href="https://github.com/ramco-labs/automated-feature-engineering">https://github.com/ramco-labs/automated-feature-engineering</a>
</p>
</div>
</li>

<li><a id="org60b8c48"></a>by hands<br />
<div class="outline-text-5" id="text-8-9-10-7">
<ul class="org-ul">
<li><a href="https://github.com/guifeliper/automated-feature-engineering/blob/master/Python/feature_engineering.py">https://github.com/guifeliper/automated-feature-engineering/blob/master/Python/feature_engineering.py</a></li>
</ul>
</div>
</li>
<li><a id="org633f6a8"></a>ratio<br />
<div class="outline-text-5" id="text-8-9-10-8">
<ul class="org-ul">
<li>(A*c)/B = (A/B)*c</li>
<li>(A +/- c)/B = A/B +/- c/B - the lerge c, B will have more value in ratio</li>
<li>if A and B has + and - values: then A/B will sort by values with same sign and they with different.</li>
<li>if A has + and - but B has only - or +, then ratio will be clearly separated for + and - of A</li>
<li>if A has + and - but B has only - or +, then you can not use (-A)/B</li>
</ul>
</div>
</li>

<li><a id="org269b7f1"></a>links<br />
<div class="outline-text-5" id="text-8-9-10-9">
<p>
<a href="http://www.feat.engineering/">http://www.feat.engineering/</a>
applied-predictive-modeling-max-kuhn-kjell-johnson
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org19b4b86" class="outline-4">
<h4 id="org19b4b86"><span class="section-number-4">8.9.11.</span> Standardization, Rescale, Normalization</h4>
<div class="outline-text-4" id="text-8-9-11">
<ul class="org-ul">
<li><a href="https://scikit-learn.org/0.22/modules/preprocessing.html">https://scikit-learn.org/0.22/modules/preprocessing.html</a></li>
<li><a href="https://scikit-learn.org/0.22/auto_examples/preprocessing/plot_all_scaling.html">https://scikit-learn.org/0.22/auto_examples/preprocessing/plot_all_scaling.html</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_scaling">https://en.wikipedia.org/wiki/Feature_scaling</a></li>
<li>comparizion <a href="https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py">https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py</a></li>
</ul>
</div>


<ol class="org-ol">
<li><a id="orgdb2b04a"></a>terms<br />
<div class="outline-text-5" id="text-8-9-11-1">
<dl class="org-dl">
<dt>Scale</dt><dd>generally means to change the range of the values</dd>
<dt>Standardize</dt><dd>generally means changing the values so that the distribution’s standard deviation equals
one. Scaling is often implied.</dd>
<dt>Normalize (Google)</dt><dd>working with skew -scaling to a range, clipping, log scaling, z-score</dd>
<dt>Bucketing</dt><dd>reduce rare categorical</dd>
<dt>Out of Vocab (OOV)</dt><dd>new category for aglomerate rare categories</dd>
</dl>
</div>
</li>

<li><a id="org8aa9022"></a>StandardScaler - Standardize features<br />
<div class="outline-text-5" id="text-8-9-11-2">
<p>
Centering and scaling.
</p>

<ul class="org-ul">
<li>(x-mean(x))/std(x), where x is a column</li>
</ul>

<p>
If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective
 function and make the estimator unable to learn from other features correctly as expected.
</p>

<p>
very sensitive to the presence of outliers.
</p>

<p>
/std - it change feature importance a + b = v, do not change distribution of data
-mean - do not change distribution of data. Important for PCA.
</p>

<p>
Standardization and Its Effects on K-Means Clustering Algorithm <a href="https://www.semanticscholar.org/paper/Standardization-and-Its-Effects-on-K-Means-Mohamad-Usman/1d352dd5f030589ecfe8910ab1cc0dd320bf600d?p2df">https://www.semanticscholar.org/paper/Standardization-and-Its-Effects-on-K-Means-Mohamad-Usman/1d352dd5f030589ecfe8910ab1cc0dd320bf600d?p2df</a>
</p>
</div>
<ol class="org-ol">
<li><a id="orgb95c5a1"></a>required by:<br />
<div class="outline-text-6" id="text-8-9-11-2-1">
<ul class="org-ul">
<li>Gaussian with 0 mean and unit variance</li>
<li>objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and</li>
</ul>
<p>
L2 regularizers of linear models)
</p>
<ul class="org-ul">
<li>Deep learning algorithms often call for zero mean and unit variance.</li>
<li>Regression-type algorithms also benefit
from normally distributed data with small sample sizes.</li>
</ul>
</div>
</li>
</ol>
</li>


<li><a id="org4ff4da5"></a>MinMaxScaler<br />
<div class="outline-text-5" id="text-8-9-11-3">
<ul class="org-ul">
<li>range [0, 1]</li>
</ul>

<p>
transformation:
</p>
<ul class="org-ul">
<li>X<sub>std</sub> = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))</li>
<li>X<sub>scaled</sub> = X<sub>std</sub> * (max - min) + min</li>
</ul>

<p>
very sensitive to the presence of outliers.
</p>
</div>
</li>
<li><a id="orgd093e99"></a>MaxAbsScaler<br />
<div class="outline-text-5" id="text-8-9-11-4">
<p>
If only positive values are present, the range is [0, 1]. If only negative values are present, the range is
 [-1, 0]. If both negative and positive values are present, the range is [-1, 1]
</p>

<p>
also suffers from the presence of large outliers.
</p>
</div>
</li>
<li><a id="orgea17325"></a>RobustScaler<br />
<div class="outline-text-5" id="text-8-9-11-5">
<ul class="org-ul">
<li>[-1, 1] + outliers</li>
</ul>

<p>
transforms the feature vector by subtracting the median and then dividing by the interquartile range (75%
 value — 25% value).
</p>

<p>
centering and scaling statistics are based on percentiles and are therefore not influenced by a small number
of very large marginal outliers.
</p>
</div>
</li>
<li><a id="org5ad9aab"></a><span class="todo TODO">TODO</span> PowerTransformer, QuantileTransformer (uniform output)<br /></li>
<li><a id="org1bbc916"></a>Normalization<br />
<div class="outline-text-5" id="text-8-9-11-7">
<p>
norm - функция расстояния
</p>
</div>
<ol class="org-ol">
<li><a id="org7051d53"></a><b>Mean normalization</b> ( mean removal) - (-1;1)<br />
<div class="outline-text-6" id="text-8-9-11-7-1">
<ul class="org-ul">
<li>data = (np.array(data) - np.mean(data)) / (max(data) - min(data))</li>
</ul>
</div>
</li>
<li><a id="org0418a53"></a>Normaliztion l1 l2 (sklearn)<br />
<div class="outline-text-6" id="text-8-9-11-7-2">
<p>
works on the rows, not the columns!
</p>

<p>
By default, L2 normalization is applied to each observation so the that the values in a row have a unit
 norm. Unit norm with L2 means that if each element were squared and summed, the total would equal 1.
</p>

<p>
sklearn.preprocessing.normalize()
</p>
<ul class="org-ul">
<li>l1 - each element - ∑|x|, sum = 1</li>

<li>used with - latent semantic analysis (LSA)</li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org0c88270"></a><b>Standardization (Z-score Normalization)</b> mean removal and variance scaling (0:1) <a id="org597c81d"></a><br />
<div class="outline-text-5" id="text-8-9-11-8">
<p>
transform the data to center and scale it by dividing non-constant features - получить нулевое
матожидание(mean) и единичную дисперсию(np.std)
</p>
<ul class="org-ul">
<li>mean = 0 print(np.nanmean(data, axis=0))</li>
<li>std = 1 print(np.nanstd(data, axis=0))</li>
<li>for line XNormed = (X - X.mean())/(X.std())</li>
<li>for table XNormed = (X - X.mean(axis=0))/(X.std(axis=0))</li>
<li>for table rest =  (data - np.nanmean(data, axis=0))/ np.nanstd(data, axis=0)</li>
<li>maintains useful information about outliers - less sensitive to them</li>
<li>отнять среднне сначала или разделить - нет разницы</li>

<li>numpy array with nan</li>
</ul>
<p>
from sklearn import preprocessing
df = preprocessing.StandardScaler().fit<sub>transform</sub>(df)
</p>

<ol class="org-ol">
<li>DataFrame saved with float</li>
</ol>
<p>
df /= np.nanstd(df, axis=0)
df -= np.nanmean(df, axis=0)
</p>

<p>
print(df)
print(df.describe())
print(df.dtypes)
print(df.isna().sum().sum())
</p>

<p>
if the dataset does not have a normal or more or less normal distribution for some feature, the z-score may
not be the most suitable method.
</p>
</div>
</li>
<li><a id="org63e98dd"></a><b>Scaling features to a range</b> or <b>min-max scaling</b> or min-max normalization<br />
<div class="outline-text-5" id="text-8-9-11-9">
<ul class="org-ul">
<li>x<sub>norm</sub> = (x - x<sub>min</sub>)/(x<sub>max</sub> - x<sub>min</sub>)</li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgd6f699e" class="outline-4">
<h4 id="orgd6f699e"><span class="section-number-4">8.9.12.</span> feature selection (correlation) - Filter Methods</h4>
<div class="outline-text-4" id="text-8-9-12">
<p>
<b>Multicollinearity</b> - one predictor variable in a multiple regression model can be perfectly predicted from the others
</p>

<p>
tech for structural risk minimization to remove redundant or irrelevant data from input
</p>
<ul class="org-ul">
<li><a href="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/">https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/</a></li>
</ul>
</div>

<ol class="org-ol">
<li><a id="orgd524cdb"></a>detection<br />
<div class="outline-text-5" id="text-8-9-12-1">
<p>
detecting multicollinearity:
</p>
<ul class="org-ul">
<li>The analysis exhibits the signs of multicollinearity — such as, estimates of the coefficients vary
excessively from model to model.</li>
<li>The t-tests for each of the individual slopes are non-significant (P &gt; 0.05), but the overall F-test for
testing all of the slopes are simultaneously 0 is significant (P &lt; 0.05).</li>
<li>The correlations among pairs of predictor variables are large.</li>
</ul>

<p>
It is possible that the pairwise correlations are small, and yet a linear dependence exists among three or
 even more variables.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">continuous</th>
<th scope="col" class="org-left">categorical</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">continuous</td>
<td class="org-left">Pearson</td>
<td class="org-left">LDA</td>
</tr>

<tr>
<td class="org-left">categorical</td>
<td class="org-left">Anova</td>
<td class="org-left">Chi-Square</td>
</tr>
</tbody>
</table>

<ul class="org-ul">
<li>Pearson's correlation (feature selection) is very popular for determining the relevance of all independent
variables, relative to the target variable (dependent variable).</li>
<li>LDA: Linear discriminant analysis is used to find a linear combination of features that characterizes or
separates two or more classes (or levels) of a categorical variable.</li>
<li>ANOVA: ANOVA stands for Analysis of variance. It is similar to LDA except for the fact that it is operated
using one or more categorical independent features and one continuous dependent feature. It provides a
statistical test of whether the means of several groups are equal or not.</li>
<li>Predictive Power Score (PPS) <a href="https://github.com/8080labs/ppscore">https://github.com/8080labs/ppscore</a>
<ul class="org-ul">
<li><a href="https://machinelearningknowledge.ai/predictive-power-score-vs-correlation-with-python-implementation/">https://machinelearningknowledge.ai/predictive-power-score-vs-correlation-with-python-implementation/</a></li>
<li>Based on RandomForest, non-linear</li>
<li><a href="https://www.kaggle.com/code/frtgnn/predictive-power-score-vs-correlation">https://www.kaggle.com/code/frtgnn/predictive-power-score-vs-correlation</a></li>
</ul></li>
<li>Chi-Square: It is a is a statistical test applied to the groups of categorical features to evaluate the
likelihood of correlation or association between them using their frequency distribution.</li>
</ul>
</div>
</li>

<li><a id="orga80e9d5"></a>questionable cause / causal fallacy / false cause<br />
<div class="outline-text-5" id="text-8-9-12-2">
<p>
non causa pro causa ("non-cause for cause" in Latin)
</p>

<p>
<b>correlation does not imply causation</b>
</p>

<p>
example: "Every time I go to sleep, the sun goes down. Therefore, my going to sleep causes the sun to set."
</p>
</div>
</li>
<li><a id="orgc3b58cb"></a>handle correlated features<br />
<div class="outline-text-5" id="text-8-9-12-3">
<p>
high collinearity indicates that it is exceptionally important to include all variables, as excluding any
 variable will cause strong <b>confounding</b>.
</p>

<ol class="org-ol">
<li>One way to handle multicollinear features is by performing hierarchical clustering on the Spearman
rank-order correlations, picking a threshold, and <b>keeping a single feature from each cluster</b>
<ul class="org-ul">
<li>кластеризация для корреляций <a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html">https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html</a></li>
</ul></li>
<li>Detecting Multicollinearity Using Variance Inflation Factors.
<ul class="org-ul">
<li><a href="https://online.stat.psu.edu/stat462/node/180/">https://online.stat.psu.edu/stat462/node/180/</a></li>
<li><a href="https://www.datasklr.com/ols-least-squares-regression/multicollinearity">https://www.datasklr.com/ols-least-squares-regression/multicollinearity</a></li>
</ul></li>
</ol>
</div>
<ol class="org-ol">
<li><a id="org8374aaa"></a>s<br />
<div class="outline-text-6" id="text-8-9-12-3-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> statsmodels.stats.outliers_influence <span style="color: #8ac6f2; font-weight: bold;">import</span> variance_inflation_factor
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">from statsmodels.tools.tools import add_constant</span>
<span style="color: #8ac6f2; font-weight: bold;">import</span> pandas <span style="color: #8ac6f2; font-weight: bold;">as</span> pd

<span style="color: #cae682;">df</span> = pd.DataFrame(
    {<span style="color: #95e454;">'a'</span>: [1, 1, 2, 3, 4],
     <span style="color: #95e454;">'b'</span>: [2, 2, 3, 2, 1],
     <span style="color: #95e454;">'c'</span>: [4, 6, 7, 8, 9],
     <span style="color: #95e454;">'d'</span>: [4, 3, 4, 5, 4]}
)

<span style="color: #e5786d;">print</span>(pd.Series([variance_inflation_factor(df.values, i) <span style="color: #8ac6f2; font-weight: bold;">for</span> i <span style="color: #8ac6f2; font-weight: bold;">in</span>
                 <span style="color: #e5786d;">range</span>(df.shape[1])], index=df.columns))
</pre>
</div>

<pre class="example">
a    47.136986
b    28.931507
c    80.315068
d    40.438356
dtype: float64
</pre>
</div>
</li>
</ol>
</li>

<li><a id="orge73f06e"></a>correlation matrix<br />
<div class="outline-text-5" id="text-8-9-12-4">
<p>
boston<sub>pd.corr</sub>()
import seaborn as sn
import matplotlib.pyplot as plt
corrMatrix = boston<sub>pd.corr</sub>()
sn.heatmap(corrMatrix, annot=True)
plt.show()
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orge260aa2" class="outline-4">
<h4 id="orge260aa2"><span class="section-number-4">8.9.13.</span> отбор признаков feature filtrating or feature selection</h4>
<div class="outline-text-4" id="text-8-9-13">
<p>
<b>feature selection</b> - eliminating candidate predictors that have little predictive power for the target of
 interest.
</p>
<ul class="org-ul">
<li>For: simpler model, clearer interpretation, shorter training times, reduced risk of overfitting.</li>
</ul>



<p>
Types:
</p>
<ul class="org-ul">
<li>intrinsic (or implicit) methods - pro: provide a direct connection between selecting features and the
<i>objective function</i> feature selection procedure occurs naturally course of the model
fitting process, methods that are embedded within the machine learning algorithm itself.
<ul class="org-ul">
<li>Tree- and rule-based models. (ex. random forest). Con: model-dependent.</li>
<li>Multivariate adaptive regression spline (MARS) models.</li>
<li>Regularization models (ex. lasso)</li>
<li>neural network based</li>
</ul></li>
<li>Filter Methods - BEFORE any training (supervised feature selection technique) - by correlation between
independent features or F-test.</li>
<li>Wrapper Methods - we try to use a subset of features and train a model using them. separates the feature
search process from the model fitting process
<ul class="org-ul">
<li>ex. feature selection, backward feature elimination, recursive feature elimination.</li>
<li>may be gready and non-gredy</li>
<li>RMSE or ROC criterion for classification. Akaike Information Criterion for linear regression (logistic)</li>
<li>Lasso regression performs L1 regularization, Ridge regression performs L2 regularization</li>
</ul></li>
</ul>

<p>
Types
</p>
<ol class="org-ol">
<li>supervised feature selection:
<ul class="org-ul">
<li>Correlation analysis: Measures the correlation between each feature and the target variable.</li>
<li>Mutual information: Measures the mutual information between each feature and the target variable.</li>
<li>F-statistic: Measures the F-statistic between each feature and the target variable.</li>
<li>Recursive feature elimination: Uses a machine learning algorithm to recursively eliminate features that are not important for predicting the target variable.</li>
<li>ex: Recursive feature elimination (RFE), Forward feature selection, Backward feature elimination, Lasso
regression</li>
</ul></li>
<li>Non-Supervised (Unsupervised) Feature Selection
<ul class="org-ul">
<li>Clustering: Groups similar features together based on their values.</li>
<li>Dimensionality reduction: Reduces the number of features by selecting a subset of features that capture the most important information in the data.</li>
<li>Feature correlation analysis: Measures the correlation between each pair of features.</li>
<li>Mutual information: Measures the mutual information between each pair of features.</li>
<li>ex. Principal component analysis (PCA), t-SNE (t-distributed Stochastic Neighbor Embedding),
Autoencoders, Independent component analysis (ICA)</li>
</ul></li>
</ol>

<p>
objective function - the statistic that the model attempts to optimize (e.g., the likelihood in generalized
 linear models or the impurity in tree-based models).
</p>

<p>
Forward Feature selection involves evaluating each individual feature, starting from the feature with a higher
 score and then adding one feature at a time so that the extended subset improves on the selected metric. We
 can keep adding features as long as the selected set of features reaches a threshold for the value of the
 metric, which is selected according to the context of the problem or using the random feature technique to
 obtain a cutoff value. May be performance based (create models for each feature before add it) or static
 criteria based.
</p>
<ul class="org-ul">
<li>greedy algorithm</li>
</ul>

<p>
Backward Feature selection or recursive feature elimination (RFE), on the other hand, involves starting from
 the full set and evaluates the metric for the set without each feature. At each stage, the set is shrunk by
 the feature that produces the smallest reduction to the target metric. We can keep dropping features as long
 as the performance improves or stays the same. That is, stop when it gets worse.
</p>
<ul class="org-ul">
<li>greedy wrapper method</li>
<li>criterion: feature importance or correlation or mutual information.</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="orga553ae9"></a>Stepwise Selection modification of Forward Feature selection or “Stepwise Forward Selection” or “Stepwise Regression”.<br />
<div class="outline-text-5" id="text-8-9-13-1">
<p>
May be forward and backward based. Less greedy
</p>
<ul class="org-ul">
<li>Initialize the model: Start with an empty set of features and a baseline model (e.g., a simple linear model).</li>
<li>Evaluate feature candidates: Evaluate each feature in the dataset to determine its relevance to the target variable.</li>
<li>Select the best feature: Choose the feature that has the highest correlation with the target variable or the
feature that improves the model's performance the most.</li>
<li>Add the selected feature to the model: Add the selected feature to the model and retrain the model with the
updated feature set.</li>
<li>Evaluate the model's performance: Evaluate the model's performance with the updated feature set.</li>
<li>Remove non-contributing features: If a previously selected feature no longer contributes to the model's
performance, remove it from the model.</li>
<li>Repeat steps 2-6: Repeat the process of evaluating feature candidates, selecting the best feature, adding it
to the model, evaluating the model's performance, and removing non-contributing features until a stopping
criterion is met.</li>
</ul>

<p>
May lead to overfitting: Stepwise Selection may lead to overfitting, as it can select features that are highly
 correlated with the target variable, but not necessarily relevant to the underlying relationship.
</p>
</div>
</li>
<li><a id="org80d2879"></a>Simulated Annealing (SA)<br />
<div class="outline-text-5" id="text-8-9-13-2">
<ol class="org-ol">
<li>Initial subset of predictors is selected and is used to estimate performance.</li>
<li>subset is changed. If new pefromance is better subset is accepted if worse: with some probability may be accepted. This probability configured to decrase over time.</li>
<li>iterate for some number of iterations.</li>
</ol>
</div>
</li>
<li><a id="org12948e0"></a>Genetic Algorithms<br />
<div class="outline-text-5" id="text-8-9-13-3">
<p>
principle: current population of solutions reproduce children which compete to survive. Next survived
 childrens are the next population of solutions. Converge to a fitness plateau. Finally optimal solution can
 be selected.
</p>

<p>
probability of mutation (pm &lt; 0.05)
</p>
</div>
</li>

<li><a id="orgf5f3783"></a><span class="todo TODO">TODO</span> F-test to select features<br />
<div class="outline-text-5" id="text-8-9-13-4">
<p>
Filter Method
</p>

<p>
F-Test is a statistical test used to compare models and check if the difference is significant between them. The hypothesis testing uses a model X and Y, where:
</p>
<ul class="org-ul">
<li>X is a model created by just a constant.</li>
<li>Y is the model created by a constant and a feature.</li>
</ul>

<p>
We calculate the least square errors in both models and compare and check if the difference in errors between model X and Y are significant or introduced by chance. This significance returns the importance of each feature, so we select the features that return high F-values and use those for further analysis.
</p>

<p>
Handles multicollinearity: The F-test can handle multicollinearity between features.
</p>

<p>
Cons:
</p>
<ul class="org-ul">
<li>Assumes normality: The F-test assumes that the data follows a normal distribution, which may not always be
the case.</li>
<li>Sensitive to sample size: The F-test is sensitive to sample size, and may not be reliable for small sample
sizes.</li>
<li>May not capture non-linear relationships: The F-test may not capture non-linear relationships between
features and the target variable.</li>
</ul>
</div>
</li>

<li><a id="org081af79"></a>me exp<br />
<div class="outline-text-5" id="text-8-9-13-5">
<p>
Удалять:
</p>
<ul class="org-ul">
<li>коррелирующие переменные с целевой - только руками</li>
<li>значение неизменно</li>
<li>неважные признаки - принимают шум за сигнал, переобучаясь. Вычислительная сложность</li>
<li>низковариативные признаки скорее хуже, чем высоковариативные - отсекать признаки, дисперсия которых ниже
определенной границы</li>
<li>если признаки явно бесполезны в простой модели, то не надо тянуть их и в более сложную.</li>
<li>Exhaustive Feature Selector</li>
</ul>



<p>
Из моего опыта - для конкретной модели - лучше всего удалить:
</p>
<ul class="org-ul">
<li>с низкой значимостью и коррелирующие c коррелирующие (с низкой значимостью).</li>
</ul>
</div>
</li>

<li><a id="org21a5402"></a><a href="http://www.feat.engineering/feature-selection">http://www.feat.engineering/feature-selection</a><br />
<div class="outline-text-5" id="text-8-9-13-6">
<ul class="org-ul">
<li><b>Wrapper methods</b> use an external search procedure to choose different subsets of the whole predictor set to
evaluate in a model. separates the feature search process from the model fitting process
<ul class="org-ul">
<li>Examples of this approach would be backwards or stepwise selection as well as genetic algorithms.</li>
</ul></li>
<li><b>Embedded methods</b> are models where the feature selection procedure occurs naturally course of the model
fitting process</li>
</ul>
</div>
</li>
<li><a id="orga7ab85e"></a>links<br />
<div class="outline-text-5" id="text-8-9-13-7">
<ul class="org-ul">
<li><a href="http://www.feat.engineering/feature-selection">http://www.feat.engineering/feature-selection</a></li>
<li><a href="https://aiml.com/what-is-the-purpose-of-feature-selection-and-what-are-some-common-approaches/">https://aiml.com/what-is-the-purpose-of-feature-selection-and-what-are-some-common-approaches/</a></li>
<li><a href="https://bookdown.org/max/FES/classes-of-feature-selection-methodologies.html">https://bookdown.org/max/FES/classes-of-feature-selection-methodologies.html</a></li>
<li>Best: <a href="file:///home/u/docsmy_books/ml-machine-learning/Forecasting/applied-predictive-modeling-max-kuhn-kjell-johnson_1518.pdf">file:///home/u/docsmy_books/ml-machine-learning/Forecasting/applied-predictive-modeling-max-kuhn-kjell-johnson_1518.pdf</a></li>
<li><a href="https://paulvanderlaken.com/2019/03/12/best-free-programming-books-i-still-need-to-read/">https://paulvanderlaken.com/2019/03/12/best-free-programming-books-i-still-need-to-read/</a></li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-org8efbe6c" class="outline-4">
<h4 id="org8efbe6c"><span class="section-number-4">8.9.14.</span> links</h4>
<div class="outline-text-4" id="text-8-9-14">
<ul class="org-ul">
<li>how to select encoding <img src="https://innovation.alteryx.com/content/images/2019/08/categorical-encoding-01-01.png" alt="categorical-encoding-01-01.png" /></li>
<li><a href="https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/">https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/</a></li>
<li><a href="https://scikit-learn.org/stable/modules/feature_selection.html">https://scikit-learn.org/stable/modules/feature_selection.html</a></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org9a7e62e" class="outline-3">
<h3 id="org9a7e62e"><span class="section-number-3">8.10.</span> поиск зависимостей между признаками (Finding relationships among variables) или data mining или Интеллектуальный анализ данных</h3>
<div class="outline-text-3" id="text-8-10">
<p>
<a href="http://elib.sfu-kras.ru/bitstream/handle/2311/29014/potehin.pdf?sequence=2">http://elib.sfu-kras.ru/bitstream/handle/2311/29014/potehin.pdf?sequence=2</a>
<a href="https://murraylax.org/bus230/notes/relationships_print.pdf">https://murraylax.org/bus230/notes/relationships_print.pdf</a>
</p>
<ul class="org-ul">
<li>Корреляционный анализ</li>
<li>Регрессинвый анализ
<ul class="org-ul">
<li>Определение вклада отдельных независимых переменных</li>
</ul></li>
<li>Метод  последовательного  сокращенияи  и метод  последовательного добавления параметров</li>
<li>NEAT for neural networks - интерпритация</li>
<li>кластерный анализ - если нет главного признака</li>
<li>Decision Tree интерпретация модели</li>
<li>Pattern recognition - автоматически, без привязки к бизнес логике</li>
</ul>

<p>
data mining is analysis step in "knowledge discovery in databases" KDD
</p>
</div>
<div id="outline-container-orgbe86692" class="outline-4">
<h4 id="orgbe86692"><span class="section-number-4">8.10.1.</span> <span class="todo TODO">TODO</span> нелинейная коррелцяи - поиск через регрессию</h4>
</div>
<div id="outline-container-orgd02e4da" class="outline-4">
<h4 id="orgd02e4da"><span class="section-number-4">8.10.2.</span> simple</h4>
<div class="outline-text-4" id="text-8-10-2">
<p>
df.values<sub>count</sub>(subset=['CLIENT<sub>AGE</sub>', 'ander'], dropna=False)
</p>
</div>
</div>
</div>
<div id="outline-container-orgc56949b" class="outline-3">
<h3 id="orgc56949b"><span class="section-number-3">8.11.</span> Корреляционный анализ</h3>
<div class="outline-text-3" id="text-8-11">
<ol class="org-ol">
<li>pearson [ˈpɪsən]: standard correlation coefficient (корреляция моментов произведений)
<ul class="org-ul">
<li>linear correlation between two sets of data</li>
</ul></li>
<li>rank correlation (Non-parametric correlations )
<ol class="org-ol">
<li>spearman [ˈspɪəmən]: Spearman rank correlation</li>
<li>kendall [kændl]: Kendall Tau correlation coefficient</li>
</ol></li>
</ol>

<p>
Если по меньшей мере одна из двух переменных имеет порядковую шкалу, либо не является нормально
распределённой, необходимо использовать ранговую корреляцию Спирмена или τ (тау) Кендалла.
</p>
<ul class="org-ul">
<li>Номинальная шкала - категориальный столбец</li>
<li>Переменные с интервальной и с номинальной шкалой: коэффициент корреляции Пирсона (корреляция моментов произведений).</li>
<li>Порядковая, или ранговая, шкала - целые числа, их не имеет смысла складывать и вычитать умножать делить.</li>
</ul>
</div>
<div id="outline-container-org90106b5" class="outline-4">
<h4 id="org90106b5"><span class="section-number-4">8.11.1.</span> корреляция Пуассона</h4>
<div class="outline-text-4" id="text-8-11-1">
<p>
df.corr()
</p>

<p>
Свойства
</p>
<ul class="org-ul">
<li>r изменяется в интервале от —1 до +1.</li>
<li>Знак r означает, увеличивается ли одна переменная по мере того, как увеличивается другая (положительный r),
или уменьшается ли одна переменная по мере того, как увеличивается другая (отрицательный r).</li>
<li>Величина r указывает, как близко расположены точки к прямой линии. В частности, если r = +1 или r= —1, то
имеется абсолютная (функциональная) корреляция по всем точкам, лежащим на линии (практически это
маловероятно); если r~0, то линейной корреляции нет (хотя может быть нелинейное соотношение). Чем ближе r к
крайним точкам (±1), тем больше степень линейной связи.</li>
<li>Коэффициент корреляции r безразмерен, т. е. не имеет единиц измерения.</li>
<li>Величина r обоснована только в диапазоне значений x и y в выборке. Нельзя заключить, что он будет иметь ту
же величину при рассмотрении значений x или y, которые значительно больше, чем их значения в выборке.</li>
<li>x и y могут взаимозаменяться, не влияя на величину r (rxy=ryx).</li>
</ul>

<p>
Расчет r может ввести в заблуждение, если:
</p>
<ul class="org-ul">
<li>соотношение между двумя переменными нелинейное, например квадратичное;</li>
<li>данные включают более одного наблюдения по каждому случаю;</li>
<li>есть аномальные значения (выбросы);</li>
<li>данные содержат ярко выраженные подгруппы наблюдений.</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org817a428"></a>требования к переменным<br />
<div class="outline-text-5" id="text-8-11-1-1">
<ul class="org-ul">
<li>Обе переменные являются количественными и непрерывными</li>
<li>Как минимум один из признаков (а лучше оба) имеет нормальное распределение (поэтому расчет этого
коэффициента является параметрическим методом оценки взаимосвязи признаков)</li>
<li>Зависимость между переменными носит линейный характер</li>
<li>Гомоскедастичность  (вариабельность  одной  переменной  не  зависит  от значений другой переменной)</li>
<li>Независимость участников исследования друг от друга (признаки Х и Y у одного участника исследования
независимы от признаков Х и Y у другого)</li>
<li>Парность наблюдений (признак Х и признак Y изучаются у одних и тех же участников исследования)</li>
<li>Достаточно большой объем выборки</li>
<li>Для адекватной проекции расчетов на генеральную совокупность выборка должна быть репрезентативной.</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org87c7617" class="outline-4">
<h4 id="org87c7617"><span class="section-number-4">8.11.2.</span> pearson vs spearman vs kendall</h4>
<div class="outline-text-4" id="text-8-11-2">
<p>
<b>pearson</b>
</p>
<ul class="org-ul">
<li>Each observation should have a pair of values.</li>
<li>Each variable should be continuous.</li>
<li>It should be the absence of outliers.</li>
<li>It assumes linearity and homoscedasticity (дисперсии одинаковы во все моменты измерения)(не рассеиваются при
увеличении значений).</li>
<li>Corr(x,y) = ∑( (xi - mean(x))*(yi - mean(y)) ) / sqrt(∑ (xi - mean(x))<sup>2</sup>)*sqrt(∑ (yi - mean(y))<sup>2</sup>)</li>
</ul>


<p>
<b>spearman</b> and <b>kendall</b>
</p>
<ul class="org-ul">
<li>Pairs of observations are independent.</li>
<li>Two variables should be measured on an ordinal, interval or ratio scale.</li>
<li>It assumes that there is a monotonic relationship between the two variables.</li>
</ul>

<p>
<b>Pearson correlation vs Spearman and Kendall correlation</b>
</p>
<ul class="org-ul">
<li>Correlation coefficients only measure linear (Pearson) or monotonic (Spearman and Kendall) relationships.</li>
<li>Non-parametric correlations are less powerful because they use less information in their calculations. In
the case of Pearson's correlation uses information about the mean and deviation from the mean, while
non-parametric correlations use only the ordinal information and scores of pairs.</li>
</ul>

<p>
<b>Spearman correlation vs Kendall correlation</b>
</p>
<ul class="org-ul">
<li>In the normal case, Kendall correlation is more robust and efficient than Spearman correlation. It means
that Kendall correlation is preferred when there are small samples or some outliers.</li>
<li>Kendall correlation has a O(n<sup>2</sup>) computation complexity comparing with O(n logn) of Spearman correlation,
where n is the sample size.</li>
<li>Spearman’s rho usually is larger than Kendall’s tau.</li>
<li>The interpretation of Kendall’s tau in terms of the probabilities of observing the agreeable (concordant)
and non-agreeable (discordant) pairs is very direct.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org93a60f8" class="outline-3">
<h3 id="org93a60f8"><span class="section-number-3">8.12.</span> Кластерный анализ</h3>
<div class="outline-text-3" id="text-8-12">
<p>
однородность и полнота
</p>
<ul class="org-ul">
<li>все кластеризуемые сущности были одной природы, описывались сходным набором характеристик</li>
<li>полнота видимо без пропусков?</li>
</ul>

<p>
<b>иерархическая кластеризация</b>, когда крупные кластеры дробятся на более мелкие, те в свою очередь дробятся ещё
  мельче, и т. д. Такие задачи называются задачами <b>таксономии</b> - результат дерево
</p>
</div>
<div id="outline-container-org8a71c92" class="outline-4">
<h4 id="org8a71c92"><span class="section-number-4">8.12.1.</span> terms</h4>
<div class="outline-text-4" id="text-8-12-1">
<dl class="org-dl">
<dt>flat clusters</dt><dd>cluster labels [3, 3, 3, 4, 4, 4, 2, 2, 2, 1, 1, 1]</dd>
<dt>singleton clusters</dt><dd>with one or several point</dd>
<dt>inconsistency coefficient</dt><dd>the greater the difference between the objects connected by the link.  for
each link of linkage</dd>
</dl>
</div>
</div>
<div id="outline-container-org316c7c6" class="outline-4">
<h4 id="org316c7c6"><span class="section-number-4">8.12.2.</span> steps</h4>
<div class="outline-text-4" id="text-8-12-2">
<p>
Этапы
</p>
<ol class="org-ol">
<li>Отбор количественных данных</li>
<li>Определение множества переменных, по которым будут оцениваться объекты в выборке, то есть признакового пространства.</li>
<li>Вычисление значений той или иной меры сходства (или различия) между объектами.</li>
<li>Применение метода кластерного анализа для создания групп сходных объектов.</li>
<li>Проверка достоверности результатов кластерного решения.</li>
</ol>

<p>
<a href="https://developers.google.com/machine-learning/clustering">https://developers.google.com/machine-learning/clustering</a>
</p>
<ol class="org-ol">
<li>Prepare data.</li>
<li>Create similarity metric.</li>
<li>Run clustering algorithm.</li>
<li>Interpret results and adjust your clustering.</li>
</ol>

<p>
Process for Supervised Similarity Measure:
</p>
<ol class="org-ol">
<li>Preprocess data: remove features that could distort the similarity calculation. Here, the features
review<sub>date</sub> and reference<sub>number</sub> are not correlated with similarity.</li>
<li>Choose DNN: autoencoder or predictor</li>
<li>Extract embeddings</li>
<li>Choose Measurement: Dot product, cosine, ecudlidiean distance</li>
</ol>
</div>
</div>
<div id="outline-container-org4eb146c" class="outline-4">
<h4 id="org4eb146c"><span class="section-number-4">8.12.3.</span> preparation</h4>
<div class="outline-text-4" id="text-8-12-3">
<p>
see <a href="#org16ca2da">8.9</a>
</p>
</div>
<ol class="org-ol">
<li><a id="org259f486"></a>problems<br />
<div class="outline-text-5" id="text-8-12-3-1">
<ul class="org-ul">
<li>how to equaly treat all features
<ul class="org-ul">
<li>normalize all data - what about outsidders?</li>
<li>calc importance per feature</li>
</ul></li>
<li>how to choose right distance</li>
<li>how to measure perfomance of clusterization</li>
<li>correlation  PCA with whiten=True to further remove the linear correlation across features.</li>
</ul>
</div>
</li>
<li><a id="orgbf28dab"></a>weight dilema (feature weighting) (Clustering on Mixed Data Types)<br />
<ol class="org-ol">
<li><a id="org17d8b69"></a>the-ultimate-guide-for-clustering-mixed-data<br />
<div class="outline-text-6" id="text-8-12-3-2-1">
<p>
<a href="https://medium.com/analytics-vidhya/the-ultimate-guide-for-clustering-mixed-data-1eefa0b4743b">https://medium.com/analytics-vidhya/the-ultimate-guide-for-clustering-mixed-data-1eefa0b4743b</a>
<a href="#org2aa664e">8.8.1</a>
</p>

<p>
<b>scale each feature by dividing by standard deviation</b>
</p>
<ul class="org-ul">
<li>cons: change importance of categorical features to not equal values</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org2a10e5f"></a>1. Gower dissimilarity (pip gower)<br />
<div class="outline-text-7" id="text-8-12-3-2-1-1">
<p>
Allow to calc weight for columns.
</p>

<p>
0 (identical) and 1 (maximally dissimilar)
</p>

<p>
3 approaches:
</p>
<ul class="org-ul">
<li>quantitative (interval): range-normalized Manhattan distance</li>
<li>ordinal: variable is first ranked, then Manhattan distance is used with a special adjustment for ties</li>
<li>nominal: variables of k categories are first converted into k binary columns and then the Dice coefficient is used</li>
</ul>

<p>
If the data feature are categorical, then a DICE coefficient is applied. DICE is explained here. However, If
 you are familiar with Jaccard coefficient and or binary classification (e.g. True Positives TP and False
 Positives FP etc) and confusion matrices then DICE is going to be familiar as
</p>
</div>

<ol class="org-ol">
<li><a id="orgd2428e5"></a><a href="https://github.com/Sreemanto/Gower-s-Distance/blob/master/Gower's%20Measure.ipynb">https://github.com/Sreemanto/Gower-s-Distance/blob/master/Gower's%20Measure.ipynb</a><br />
<div class="outline-text-8" id="text-8-12-3-2-1-1-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.neighbors <span style="color: #8ac6f2; font-weight: bold;">import</span> DistanceMetric
<span style="color: #8ac6f2; font-weight: bold;">import</span> pandas <span style="color: #8ac6f2; font-weight: bold;">as</span> pd
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">gower_distance</span>(df:pd.DataFrame):
    <span style="color: #cae682;">individual_variable_distances</span> = []
    <span style="color: #8ac6f2; font-weight: bold;">for</span> c <span style="color: #8ac6f2; font-weight: bold;">in</span> df.columns:
        <span style="color: #8ac6f2; font-weight: bold;">if</span> df[c].dtype.name == <span style="color: #95e454;">'object'</span>:
            <span style="color: #cae682;">feature_dist</span> = DistanceMetric.get_metric(<span style="color: #95e454;">'dice'</span>).pairwise(pd.get_dummies(df[c]))
        <span style="color: #8ac6f2; font-weight: bold;">else</span>:
            <span style="color: #cae682;">feature_dist</span> = DistanceMetric.get_metric(<span style="color: #95e454;">'manhattan'</span>).pairwise(df[[c]]) / <span style="color: #e5786d;">max</span>(np.ptp(df[c].values),1)

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">individual_variable_distances.append(feature_dist) # -- per observation (old)</span>
        individual_variable_distances.append(np.mean(feature_dist)) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">per column (new)</span>
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">return np.array(individual_variable_distances).mean(0) # -- per observation (old)</span>
    <span style="color: #8ac6f2; font-weight: bold;">return</span> np.array(individual_variable_distances) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">per column (new)</span>

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">------ main ----</span>
<span style="color: #cae682;">df</span> = pd.DataFrame([[1,2.6,<span style="color: #95e454;">'A'</span>],[12,5,<span style="color: #95e454;">'X'</span>],[4,7,<span style="color: #95e454;">'A'</span>],[4,7,<span style="color: #95e454;">'A'</span>]])
df.<span style="color: #cae682;">columns</span> = [<span style="color: #95e454;">'Num_1'</span>,<span style="color: #95e454;">'Num_2'</span>,<span style="color: #95e454;">'Cat_1'</span>]
<span style="color: #e5786d;">print</span>(df)
<span style="color: #e5786d;">print</span>([df[c].dtype.name <span style="color: #8ac6f2; font-weight: bold;">for</span> c <span style="color: #8ac6f2; font-weight: bold;">in</span> df.columns])
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"gower_distance"</span>, gower_distance(df))

<span style="color: #cae682;">v1</span>=<span style="color: #e5786d;">list</span>(<span style="color: #95e454;">"0101010101010101"</span>) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">2</span>
<span style="color: #cae682;">v2</span>=<span style="color: #e5786d;">list</span>(<span style="color: #95e454;">"0202020202010101"</span>) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">3</span>
<span style="color: #cae682;">v3</span>=<span style="color: #e5786d;">list</span>(<span style="color: #95e454;">"0202020212121212"</span>) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">3</span>
<span style="color: #cae682;">df</span> = pd.DataFrame({<span style="color: #95e454;">"v1"</span>:v1, <span style="color: #95e454;">"v2"</span>:v2, <span style="color: #95e454;">"v3"</span>:v3}) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">.astype(str)</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">df.v1 = df.v1.astype(int)</span>
<span style="color: #e5786d;">print</span>(df)
<span style="color: #e5786d;">print</span>([df[c].dtype.name <span style="color: #8ac6f2; font-weight: bold;">for</span> c <span style="color: #8ac6f2; font-weight: bold;">in</span> df.columns])
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">----------- scale  -----------</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">from scipy.cluster.vq import whiten</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">numbers_prepared = whiten( obs = df )</span>
<span style="color: #cae682;">gd</span> = gower_distance(df)
<span style="color: #e5786d;">print</span>(gd)
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"this is weight"</span>)

</pre>
</div>
</div>
</li>

<li><a id="org5cac5fc"></a>links<br />
<div class="outline-text-8" id="text-8-12-3-2-1-1-2">
<ul class="org-ul">
<li><a href="https://jamesmccaffrey.wordpress.com/2020/04/21/example-of-calculating-the-gower-distance/">https://jamesmccaffrey.wordpress.com/2020/04/21/example-of-calculating-the-gower-distance/</a></li>
<li><a href="https://www.thinkdatascience.com/post/2019-12-16-introducing-python-package-gower/">https://www.thinkdatascience.com/post/2019-12-16-introducing-python-package-gower/</a></li>
<li><a href="https://bpostance.github.io/posts/clustering-mixed-data/">https://bpostance.github.io/posts/clustering-mixed-data/</a></li>
<li><a href="https://github.com/wwwjk366/gower/blob/master/gower/gower_dist.py">https://github.com/wwwjk366/gower/blob/master/gower/gower_dist.py</a></li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org0ea2f5c"></a>2. Dimensionality Reduction<br />
<ol class="org-ol">
<li><a id="org48c33cc"></a>Factorial Analysis of Mixed Data (FAMD) (pip prince)<br />
<div class="outline-text-8" id="text-8-12-3-2-1-2-1">
<p>
preparation:
</p>
<ul class="org-ul">
<li><p>
categorical variables:
</p>
<ul class="org-ul">
<li>one hot encoding</li>
<li>divided by the squared root of the proportion of objects in the column (the number of 1s over the number</li>
</ul>
<p>
of observations in the column)
</p>
<ul class="org-ul">
<li>subtract the mean</li>
</ul></li>
<li>standard scaling for numerical.</li>
</ul>

<p>
Finally the PCA algorithm is executed on the resulting matrix to obtain the final output.
</p>
</div>

<ol class="org-ol">
<li><a id="org7c37cae"></a>code (drop first or not? median or mean for categorical?)<br />
<div class="outline-text-9" id="text-8-12-3-2-1-2-1-1">
<div class="org-src-container">
<pre class="src src-python" id="org3eb9d66"><span style="color: #8ac6f2; font-weight: bold;">import</span> pandas <span style="color: #8ac6f2; font-weight: bold;">as</span> pd
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #8ac6f2; font-weight: bold;">import</span> math
<span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.decomposition <span style="color: #8ac6f2; font-weight: bold;">import</span> PCA

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">calculate_zscore</span>(df, columns):
  <span style="color: #f08080; font-style: italic;">'''</span>
<span style="color: #f08080; font-style: italic;">  scales columns in dataframe using z-score</span>
<span style="color: #f08080; font-style: italic;">  '''</span>
  <span style="color: #cae682;">df</span> = df.copy()
  <span style="color: #8ac6f2; font-weight: bold;">for</span> col <span style="color: #8ac6f2; font-weight: bold;">in</span> columns:
      <span style="color: #cae682;">df</span>[col] = (df[col] - df[col].mean())/df[col].std(ddof=0)

  <span style="color: #8ac6f2; font-weight: bold;">return</span> df


<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">one_hot_encode</span>(df, columns):
  <span style="color: #f08080; font-style: italic;">'''</span>
<span style="color: #f08080; font-style: italic;">  one hot encodes list of columns and</span>
<span style="color: #f08080; font-style: italic;">  concatenates them to the original df</span>
<span style="color: #f08080; font-style: italic;">  '''</span>

  <span style="color: #cae682;">concat_df</span> = pd.concat([pd.get_dummies(df[col], drop_first=<span style="color: #e5786d; font-weight: bold;">False</span>, prefix=col) <span style="color: #8ac6f2; font-weight: bold;">for</span> col <span style="color: #8ac6f2; font-weight: bold;">in</span> columns], axis=1)
  <span style="color: #cae682;">one_hot_cols</span> = concat_df.columns

  <span style="color: #8ac6f2; font-weight: bold;">return</span> concat_df, one_hot_cols


<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">normalize_column_modality</span>(df, columns):
  <span style="color: #f08080; font-style: italic;">'''</span>
<span style="color: #f08080; font-style: italic;">  divides each column by the probability &#956;&#8344; of the modality</span>
<span style="color: #f08080; font-style: italic;">  (number of ones in the column divided by N) only for one hot columns</span>
<span style="color: #f08080; font-style: italic;">  '''</span>

  <span style="color: #cae682;">length</span> = <span style="color: #e5786d;">len</span>(df)
  <span style="color: #8ac6f2; font-weight: bold;">for</span> col <span style="color: #8ac6f2; font-weight: bold;">in</span> columns:

    <span style="color: #cae682;">weight</span> = math.sqrt(<span style="color: #e5786d;">sum</span>(df[col])/length)
    <span style="color: #e5786d;">print</span>(col, weight)
    <span style="color: #cae682;">df</span>[col] = df[col]/weight

  <span style="color: #8ac6f2; font-weight: bold;">return</span> df


<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">center_columns</span>(df, columns):
  <span style="color: #f08080; font-style: italic;">'''</span>
<span style="color: #f08080; font-style: italic;">  center columns by subtracting the mean value</span>
<span style="color: #f08080; font-style: italic;">  '''</span>
  <span style="color: #8ac6f2; font-weight: bold;">for</span> col <span style="color: #8ac6f2; font-weight: bold;">in</span> columns:
      <span style="color: #cae682;">df</span>[col] = (df[col] - df[col].median())

  <span style="color: #8ac6f2; font-weight: bold;">return</span> df


<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">FAMD_prep</span>(df):
  <span style="color: #f08080; font-style: italic;">'''</span>
<span style="color: #f08080; font-style: italic;">  Factorial Analysis of Mixed Data (FAMD),</span>
<span style="color: #f08080; font-style: italic;">  which generalizes the Principal Component Analysis (PCA)</span>
<span style="color: #f08080; font-style: italic;">  algorithm to datasets containing numerical and categorical variables</span>
<span style="color: #f08080; font-style: italic;">  a) For the numerical variables</span>
<span style="color: #f08080; font-style: italic;">    - Standard scale (= get the z-score)</span>

<span style="color: #f08080; font-style: italic;">  b) For the categorical variables:</span>
<span style="color: #f08080; font-style: italic;">    - Get the one-hot encoded columns</span>
<span style="color: #f08080; font-style: italic;">    - Divide each column by the square root of its probability sqrt(&#956;&#8344;)</span>
<span style="color: #f08080; font-style: italic;">    - Center the columns</span>
<span style="color: #f08080; font-style: italic;">  c) Apply a PCA algorithm over the table obtained!</span>
<span style="color: #f08080; font-style: italic;">  '''</span>

  <span style="color: #cae682;">variable_distances</span> = []

  <span style="color: #cae682;">numeric_cols</span> = df.select_dtypes(include=np.number)
  <span style="color: #cae682;">cat_cols</span> = df.select_dtypes(include=<span style="color: #95e454;">'object'</span>)

  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">numeric process</span>
  <span style="color: #cae682;">normalized_df</span> = calculate_zscore(df, numeric_cols)
  <span style="color: #cae682;">normalized_df</span> = normalized_df[numeric_cols.columns]

  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">categorical process</span>
  <span style="color: #cae682;">cat_one_hot_df</span>, <span style="color: #cae682;">one_hot_cols</span> = one_hot_encode(df, cat_cols)
  <span style="color: #cae682;">cat_one_hot_norm_df</span> = normalize_column_modality(cat_one_hot_df, one_hot_cols)
  <span style="color: #cae682;">cat_one_hot_norm_center_df</span> = center_columns(cat_one_hot_norm_df, one_hot_cols)

  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Merge DataFrames</span>
  <span style="color: #cae682;">processed_df</span> = pd.concat([normalized_df, cat_one_hot_norm_center_df], axis=1)
  <span style="color: #8ac6f2; font-weight: bold;">return</span> processed_df


<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">FAMD_pca</span>(df, n_components=2):
  <span style="color: #f08080; font-style: italic;">'''</span>
<span style="color: #f08080; font-style: italic;">  c) Apply a PCA algorithm over the table obtained!</span>
<span style="color: #f08080; font-style: italic;">  '''</span>
  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Perform (PCA)</span>
  <span style="color: #cae682;">pca</span> = PCA(n_components=n_components)
  <span style="color: #cae682;">principalComponents</span> = pca.fit_transform(df)

  <span style="color: #8ac6f2; font-weight: bold;">return</span> principalComponents


<span style="color: #cae682;">v1</span>=<span style="color: #e5786d;">list</span>(<span style="color: #95e454;">"0101010101010101"</span>) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">2</span>
<span style="color: #cae682;">v2</span>=<span style="color: #e5786d;">list</span>(<span style="color: #95e454;">"0202020202010101"</span>) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">3</span>
<span style="color: #cae682;">v3</span>=<span style="color: #e5786d;">list</span>(<span style="color: #95e454;">"0202020212121212"</span>) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">3</span>
<span style="color: #cae682;">df</span> = pd.DataFrame({<span style="color: #95e454;">"v1"</span>:v1, <span style="color: #95e454;">"v2"</span>:v2, <span style="color: #95e454;">"v3"</span>:v3}) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">.astype(str)</span>

<span style="color: #cae682;">FAMD_processed</span> = FAMD_prep(df)
<span style="color: #cae682;">FAMD_components</span> = FAMD_pca(FAMD_processed, n_components=2)

<span style="color: #e5786d;">print</span>(pd.DataFrame(np.<span style="color: #e5786d;">round</span>(FAMD_components,0)))
</pre>
</div>

<pre class="example">
output :session famd
</pre>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> matplotlib <span style="color: #8ac6f2; font-weight: bold;">import</span> pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print(FAMD_components)</span>
<span style="color: #e5786d;">print</span>(pd.DataFrame(np.<span style="color: #e5786d;">round</span>(FAMD_components,0)))
plt.scatter(FAMD_components[:,0], FAMD_components[:,1])
plt.savefig(<span style="color: #95e454;">'./autoimgs/ds-famd.png'</span>)
plt.close()
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python" id="org75db7f9"><span style="color: #8ac6f2; font-weight: bold;">from</span> matplotlib <span style="color: #8ac6f2; font-weight: bold;">import</span> pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt
<span style="color: #8ac6f2; font-weight: bold;">from</span> scipy.cluster.hierarchy <span style="color: #8ac6f2; font-weight: bold;">import</span> linkage, dendrogram
<span style="color: #cae682;">l</span> = linkage(y=FAMD_processed, method=<span style="color: #95e454;">'complete'</span>, metric=<span style="color: #95e454;">'matching'</span>, optimal_ordering=<span style="color: #e5786d; font-weight: bold;">False</span>)
dendrogram(Z=l, p=1.1, truncate_mode=<span style="color: #95e454;">'level'</span>, labels=df.index, count_sort=<span style="color: #e5786d; font-weight: bold;">False</span>, distance_sort=<span style="color: #e5786d; font-weight: bold;">False</span>, orientation=<span style="color: #95e454;">'right'</span>, leaf_font_size=15)
plt.savefig(<span style="color: #95e454;">'/tmp/tmp2.png'</span>)
plt.close()
</pre>
</div>
</div>
</li>
</ol>
</li>

<li><a id="org345213c"></a>Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP).<br />
<div class="outline-text-8" id="text-8-12-3-2-1-2-2">
<p>
manifold learning &amp; ideas from topological data analysis
</p>
</div>
</li>
</ol>
</li>
</ol>
</li>

<li><a id="orgdb65ebc"></a>old<br />
<div class="outline-text-6" id="text-8-12-3-2-2">
<ul class="org-ul">
<li><a href="https://stats.stackexchange.com/questions/77850/assign-weights-to-variables-in-cluster-analysis">https://stats.stackexchange.com/questions/77850/assign-weights-to-variables-in-cluster-analysis</a></li>
<li><a href="https://stackoverflow.com/questions/6700897/how-can-i-weight-features-for-better-clustering-with-a-very-small-data-set">https://stackoverflow.com/questions/6700897/how-can-i-weight-features-for-better-clustering-with-a-very-small-data-set</a></li>
<li><a href="https://scikit-learn.org/stable/modules/preprocessing.html">https://scikit-learn.org/stable/modules/preprocessing.html</a></li>
<li>Feature-weighted clustering with inner product induced norm based dissimilarity measures: an optimization
perspective <a href="https://link.springer.com/article/10.1007/s10994-016-5623-3">https://link.springer.com/article/10.1007/s10994-016-5623-3</a></li>
<li>An Accurate Method of Determining Attribute Weights in Distance-Based Classification Algorithms
<a href="https://www.hindawi.com/journals/mpe/2022/6936335/">https://www.hindawi.com/journals/mpe/2022/6936335/</a></li>
<li>TODO: at bottom <a href="https://en.wikipedia.org/wiki/Mode_(statistics)">https://en.wikipedia.org/wiki/Mode_(statistics)</a></li>
</ul>
<p>
feature weight learning algorithm
</p>


<p>
feature weighting scheme
</p>
<ul class="org-ul">
<li>distance-based clustering algorithms -  limited to Euclidean, Mahalanobis, and exponential distances
<ul class="org-ul">
<li>standardize before is important</li>
</ul></li>
<li>inner product induced norm based dissimilarity measures</li>
</ul>

<p>
Dissimilarity measures are a generalized version of the distance functions
</p>

<p>
Standard deviation σ -  indicates that the values tend to be close to the mean
</p>
<ul class="org-ul">
<li>2, 4, 4, 4, 5, 5, 7, 9</li>
<li>mean average = 40/8 = 5</li>
<li>std = sqrt(((2-5)<sup>2</sup> + (4-5)<sup>2</sup> + (4-5)<sup>2</sup> + (4-5)<sup>2</sup> &#x2026;)/8) = 2</li>
</ul>

<p>
Coefficient of variation -  relative standard deviation (RSD)
</p>
<ul class="org-ul">
<li>ratio of the standard deviation σ to the mean μ (or its absolute value, | μ |)</li>
<li>cv = σ/μ</li>
</ul>

<p>
Least absolute deviations - optimization technique for L1 norm or sum of absolute errors
</p>

<p>
least squares technique - optimization technique for minimizing the sum of the squares of the residuals
</p>

<p>
Mathematical optimization (discrete optimization) - is the selection of a best element, with regard to some criterion
</p>
<ul class="org-ul">
<li>min (x<sup>2</sup>+1) , where x ∈ R. =1, occurring at x=0</li>
<li>argmax/argmin f(x) - elements of the domain of some function at which the function values are maximized/minimized.</li>
</ul>
</div>
</li>
</ol>
</li>

<li><a id="org637068d"></a>standartization and regression<br />
<div class="outline-text-5" id="text-8-12-3-3">
<ul class="org-ul">
<li><a href="https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca">https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca</a></li>
<li><a href="https://stats.stackexchange.com/questions/19523/need-for-centering-and-standardizing-data-in-regression">https://stats.stackexchange.com/questions/19523/need-for-centering-and-standardizing-data-in-regression</a></li>
</ul>

<p>
PCA is a regressional model without intercept. If you forget to center your data, the 1st principal component
may pierce the cloud not along the main direction of the cloud, and will be (for statistics purposes)
misleading.
</p>

<ul class="org-ul">
<li>Centering dont play role for clusterization but for PCA.</li>
<li>unit norm required for clusterization</li>
</ul>
</div>
</li>
<li><a id="org0b07625"></a>dimensionaly reduction, multidimensional scaling<br />
<div class="outline-text-5" id="text-8-12-3-4">
<p>
PCA - main linear technique for dimensionality reduction. The covariance (and sometimes the correlation)
  matrix of the data is constructed and the <b>eigenvectors</b> on this matrix are computed.
</p>

<p>
Kernel PCA - nonlinear way of PCA. kernel trick.
</p>

<p>
TruncatedSVD (aka LSA) - Contrary to PCA, this estimator does not center the data before computing the
 singular value decomposition. This means it can work with sparse matrices efficiently.
</p>
<ul class="org-ul">
<li>works on term count/tf-idf matrices (latent semantic analysis (LSA))</li>
</ul>

<p>
PCA, MCA, or t-SNE to obtain a 2 or 3 dimensional vectors for plotting.
</p>
<ul class="org-ul">
<li>use t-SNE alters the scale and magnitude of the feature spaces and some methods, such as plotting centroids,
will not work as shown below.</li>
</ul>

<p>
linear:
</p>
<ul class="org-ul">
<li>Independent Component Analysis</li>
<li>Linear Discriminant Analysis</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org31c24c0"></a>Manifold learning<br />
<div class="outline-text-6" id="text-8-12-3-4-1">
<p>
approach to non-linear dimensionality reduction.
</p>

<p>
Multidimensional scaling (MDS) - seeks a low-dimensional representation of the data in which the distances
 respect well the distances in the original high-dimensional space.
</p>
<ul class="org-ul">
<li>metric</li>
<li>non metric - preserve the order of the distances, seek for a monotonic relationship between the distances in
the embedded space and the similarities/dissimilarities.</li>
</ul>
</div>
</li>

<li><a id="org421f2fd"></a>PCA<br />
<div class="outline-text-6" id="text-8-12-3-4-2">
<p>
recommended standard scaling
</p>

<p>
step
</p>
<ol class="org-ol">
<li>compute the covariance matrix ( Pearson correlations)</li>
<li>Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components</li>
<li>Recast the Data Along the Principal Components Axes</li>
</ol>

<p>
notes
</p>
<ul class="org-ul">
<li>Geometrically speaking, principal components represent the directions of the data that explain a maximal amount of variance</li>
<li>If the measures of correlation used are product-moment coefficients, the correlation matrix is the same as
the <b>covariance matrix</b> of the standardized random variables X/σ(X)</li>
<li>Time complexity O(nmax<sup>2</sup>*nmin), nmax = max(n<sub>samples</sub>, n<sub>features</sub>), nmin=(n<sub>samples</sub>, n<sub>features</sub>).</li>
<li>Memory footprint = nmax<sup>2</sup>*nmin</li>
</ul>
</div>
</li>
<li><a id="org6c9ca0a"></a>links<br />
<div class="outline-text-6" id="text-8-12-3-4-3">
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Dimensionality_reduction">https://en.wikipedia.org/wiki/Dimensionality_reduction</a></li>
<li><a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix</a></li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org5eba7d7"></a>normalization vs standardisation<br />
<div class="outline-text-5" id="text-8-12-3-5">
<p>
<a href="https://www.datanovia.com/en/lessons/clustering-distance-measures/">https://www.datanovia.com/en/lessons/clustering-distance-measures/</a>
<a href="https://iq.opengenus.org/standardization-regularization-vs-normalization/">https://iq.opengenus.org/standardization-regularization-vs-normalization/</a>
</p>

<p>
Нужно только стандартизировать, чтобы стандартное отклонение было 1, так как это важность признака.
</p>

<p>
see <a href="#org9fb3736">12.8.6</a> <a href="#org597c81d">8.9.11.8</a>
</p>

<p>
Normalization is useful when your data has varying scales and the algorithm you are using does not make
assumptions about the distribution of your data
</p>

<p>
The goal is to make the variables comparable.
Generally variables are scaled to have i) standard deviation one and ii) mean zero.
</p>

<p>
(xi - center(x))/scale(x)
Where center(x) can be the mean or the median of x values, and scale(x) can be the standard deviation (SD)
</p>

<p>
<a href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/">https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/</a>
</p>

<p>
<a href="https://www.geeksforgeeks.org/normalization-vs-standardization/">https://www.geeksforgeeks.org/normalization-vs-standardization/</a>
</p>


<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Normalisation</th>
<th scope="col" class="org-left">Standardisation</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">min max</td>
<td class="org-left">Mean and standard deviation is used for scaling.</td>
</tr>

<tr>
<td class="org-left">Scales values between [0, 1] or [-1, 1].</td>
<td class="org-left">It is not bounded to a certain range. (But lay in -1 1 mostly)</td>
</tr>

<tr>
<td class="org-left">It is really affected by outliers.</td>
<td class="org-left">It is much less affected by outliers.</td>
</tr>

<tr>
<td class="org-left">MinMaxScaler</td>
<td class="org-left">StandardScaler</td>
</tr>

<tr>
<td class="org-left">It is useful when we don’t know about the distribution</td>
<td class="org-left">It is useful when the feature distribution is Normal or Gaussian.</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</li>

<li><a id="orgdcb3de4"></a>one-hot encoding<br />
<div class="outline-text-5" id="text-8-12-3-6">
<p>
Если не сделать кодирование для категориальных столбцов, то важность будет определяться в каком значения порядке в столце
</p>

<p>
Лучше всего сделать one-hot и разделить на количесто основных значений.
</p>
</div>
</li>
<li><a id="orgaf52e2c"></a>как нормализация влияет на важность<br />
<div class="outline-text-5" id="text-8-12-3-7">
<p>
Чем больше стандартное отклонение, тем тем больше значение расстояния для разных векторов и потому больше
важность.
</p>

<p>
При вычислении растояния
(x1 y1) (x2,y2)
e = sqrt( (x1-x2)<sup>2</sup> + (y1-y2)<sup>2</sup> )
</p>

<p>
все переменные должны лежать в одном диапазоне -1 1
</p>
</div>
</li>
<li><a id="orgd1a005b"></a>standardization and Euclidian distance<br />
<div class="outline-text-5" id="text-8-12-3-8">
<p>
<a href="https://www.stat.pitt.edu/sungkyu/course/2221Fall13/lec8_mds_combined.pdf">https://www.stat.pitt.edu/sungkyu/course/2221Fall13/lec8_mds_combined.pdf</a>
</p>

<p>
Multidimensional scaling (MDS)
</p>

<p>
Distance, dissimilarity and similarity (or proximity)
</p>

<p>
<b>metric</b> - In mathematics, a distancefunction (that gives a distance between two objects)
</p>

<p>
standardized Euclidian distance - distance after standardization
</p>
</div>
</li>

<li><a id="org1fb2e9a"></a>overdispersion<br />
<div class="outline-text-5" id="text-8-12-3-9">
<p>
when variance increases faster than the mean
</p>
</div>
</li>

<li><a id="org3f67930"></a>embeddings or embedding matrix<br />
<div class="outline-text-5" id="text-8-12-3-10">
<p>
<a href="https://colab.research.google.com/github/google/eng-edu/blob/main/ml/clustering/clustering-supervised-similarity.ipynb">https://colab.research.google.com/github/google/eng-edu/blob/main/ml/clustering/clustering-supervised-similarity.ipynb</a>
</p>

<p>
Empirical rule-of-thumb:
</p>
<ul class="org-ul">
<li>dimensions = ∜(possible values)</li>
</ul>
</div>
</li>



<li><a id="org019228e"></a>distance<br />
<div class="outline-text-5" id="text-8-12-3-11">
<ul class="org-ul">
<li>Euclidean distance is a common measure to continuous attributes</li>
<li>For multivariate data instances, distance or similarity is usually computed for each attributes and then combined.</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org526e02c" class="outline-4">
<h4 id="org526e02c"><span class="section-number-4">8.12.4.</span> Цели кластеризации</h4>
<div class="outline-text-4" id="text-8-12-4">
<ul class="org-ul">
<li>Понимание данных
<ul class="org-ul">
<li>кластеров стараются сделать поменьше.</li>
</ul></li>
<li>Сжатие данных. Если исходная выборка избыточно большая, то можно сократить её, оставив по одному наиболее
типичному представителю от каждого кластера.
<ul class="org-ul">
<li>важнее обеспечить высокую степень сходства объектов внутри каждого кластера, а кластеров может быть сколько угодно.</li>
</ul></li>
<li>Обнаружение новизны (англ. novelty detection). Выделяются нетипичные объекты, которые не удаётся
присоединить ни к одному из кластеров.</li>
</ul>
</div>
</div>
<div id="outline-container-org5e7adab" class="outline-4">
<h4 id="org5e7adab"><span class="section-number-4">8.12.5.</span> Методы кластеризации</h4>
<div class="outline-text-4" id="text-8-12-5">
<p>
data clustering algorithms can be of two types:
</p>
<ul class="org-ul">
<li>hierarchical - seeks to build a hierarchy of clusters (using a tree-like structure, called the dendrogram)
following the agglomerative or the divisive approach</li>
<li>Partitional attempt to partition the dataset  directly into a given number of clusters.</li>
</ul>

<p>
Partitional algorithms:
</p>
<ul class="org-ul">
<li>hard clustering, where we assign each pattern to a single cluster only</li>
<li>fuzzy clustering, where each pattern can belong to all the clusters with a certain membership degree (in [0,
1]) for each of them.</li>
</ul>


<p>
hierarchical, density, and similarity based
</p>



<p>
Временная сложность
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Иерархический</td>
<td class="org-left">O(n2)</td>
</tr>

<tr>
<td class="org-left">k-средних, c-средних</td>
<td class="org-left">O(nkl), где k – число кластеров, l – число итераций</td>
</tr>

<tr>
<td class="org-left">Выделение связных компонент</td>
<td class="org-left">зависит от алгоритма</td>
</tr>

<tr>
<td class="org-left">Минимальное покрывающее дерево</td>
<td class="org-left">O(n2 log n)</td>
</tr>

<tr>
<td class="org-left">Послойная кластеризация</td>
<td class="org-left">O(max(n, m)), где m &lt; n(n-1)/2</td>
</tr>
</tbody>
</table>


<ol class="org-ol">
<li>Вероятностный подход
<ul class="org-ul">
<li>K-средних и К-медиан -
<ul class="org-ul">
<li>Результат зависит от выбора исходных центров кластеров</li>
<li>Число кластеров надо знать заранее.</li>
</ul></li>
<li>Expectation–maximization algorithm
<ul class="org-ul">
<li>It is possible that it can be arbitrarily poor in high dimensions</li>
</ul></li>
<li>Алгоритмы семейства FOREL
<ul class="org-ul">
<li>Сходимость алгоритма</li>
<li>Плохая применимость алгоритма при плохой разделимости выборки на кластеры</li>
<li>зависимость от выбора начального объекта</li>
<li>Произвольное по количеству разбиение на кластеры</li>
<li>Необходимость априорных знаний о ширине (диаметре) кластеров</li>
</ul></li>
<li>Дискриминантный анализ</li>
</ul></li>
<li>Neural Nenwork
<ul class="org-ul">
<li>Fuzzy clustering Метод нечеткой кластеризации C-средних (C-means)</li>
<li>Нейронная сеть Кохонена</li>
<li>Генетический алгоритм</li>
</ul></li>
<li>Логический подход. Построение дендрограммы осуществляется с помощью дерева решений.</li>
<li>Теоретико-графовый подход.
<ul class="org-ul">
<li>Графовые алгоритмы кластеризации
<ul class="org-ul">
<li>Под дендрограммой обычно понимается дерево, построенное по матрице мер близости.</li>
<li>тер&lt;ет наглядность при увеличении числа кластеров</li>
</ul></li>
</ul></li>
<li>Иерархический подход. - по расстоянию объединияя близкие, остановиться по Дендрограмме</li>
<li>DBSCAN
<ul class="org-ul">
<li>does not require one to specify the number of clusters in the data a priori, as opposed to k-means.</li>
<li>arbitrarily-shaped clusters. It can even find a cluster completely surrounded by (but not connected to) a different cluster</li>
<li>has a notion of noise, and is robust to outliers.</li>
</ul></li>
</ol>

<p>
?
</p>
<ul class="org-ul">
<li>moment-based approaches</li>
<li>spectral techniques</li>
<li>Elbow plots - метод локтя для определения количества кластеров в иерархическом анализе</li>
<li>Silhouette Scores, plot -sklearn  silhouette<sub>score</sub>()  - very simular to Elbow plot and tree
<ul class="org-ul">
<li>silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation)</li>
<li>Silhouette Samples - ?</li>
<li><a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py">https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py</a></li>
</ul></li>
</ul>

<p>
Метод нечёткой кластеризации C-средних ( fuzzy clustering, soft k-means, c-means)
</p>
<ul class="org-ul">
<li>each data point can belong to more than one cluster.</li>
</ul>
</div>
</div>

<div id="outline-container-orgcef0923" class="outline-4">
<h4 id="orgcef0923"><span class="section-number-4">8.12.6.</span> Create similarity metric</h4>
<div class="outline-text-4" id="text-8-12-6">
<ul class="org-ul">
<li>manual similarity measure - manually select: euclidian or RMSE</li>
<li>supervised similarity measure - model calculates the similarity
<ul class="org-ul">
<li>autoencoder or predictor</li>
</ul></li>
</ul>

<p>
<b>Manual</b> - <i>Supervised</i>
</p>
<ul class="org-ul">
<li>Eliminate redundant information in correlated features.
<ul class="org-ul">
<li><b>Yes</b>
<ul class="org-ul">
<li><i>No</i></li>
</ul></li>
</ul></li>
<li>Suitable for small datasets with few features.
<ul class="org-ul">
<li><b>Yes</b>
<ul class="org-ul">
<li><i>No</i></li>
</ul></li>
</ul></li>
<li>Suitable for large datasets with many features.
<ul class="org-ul">
<li><b>No</b>
<ul class="org-ul">
<li><i>Yes</i></li>
</ul></li>
</ul></li>
</ul>

<p>
Autoencode - DNN that uses the same feature data both as input and as the labels.
</p>
<ul class="org-ul">
<li>Once the DNN is trained, you extract the embeddings from the last hidden layer to calculate similarity.</li>
</ul>

<p>
Predictor - input one field, output all.
</p>

<p>
guidelines to choose a feature as the label:
</p>
<ul class="org-ul">
<li>Prefer numeric features to categorical features as labels because loss is easier to calculate and interpret
for numeric features.</li>
<li>Do not use categorical features with cardinality 100 as labels. If you do, the DNN will not be forced to
reduce your input data to embeddings because a DNN can easily predict low-cardinality categorical labels.</li>
<li>Remove the feature that you use as the label from the input to the DNN; otherwise, the DNN will perfectly predict the output.</li>
</ul>

<p>
Loss Function for DNN:
</p>
<ul class="org-ul">
<li>Numeric, use mean square error (MSE).</li>
<li>Univalent categorical, use log loss</li>
<li>Multivalent categorical, use softmax cross entropy loss.</li>
</ul>
</div>
</div>
<div id="outline-container-orgaf244b3" class="outline-4">
<h4 id="orgaf244b3"><span class="section-number-4">8.12.7.</span> Measuring Similarity from Embeddings</h4>
<div class="outline-text-4" id="text-8-12-7">
<ul class="org-ul">
<li>Euclidean distance</li>
<li>Cosine</li>
<li>Dot Product</li>
<li>normalized dot-product</li>
</ul>




<p>
links
</p>
<ul class="org-ul">
<li><a href="https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity">https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity</a></li>
<li><a href="https://colab.research.google.com/github/google/eng-edu/blob/main/ml/clustering/clustering-supervised-similarity.ipynb?utm_source=ss-clustering&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=clustering-supervised-similarity">https://colab.research.google.com/github/google/eng-edu/blob/main/ml/clustering/clustering-supervised-similarity.ipynb?utm_source=ss-clustering&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=clustering-supervised-similarity</a></li>
</ul>
</div>
</div>
<div id="outline-container-org79c65c8" class="outline-4">
<h4 id="org79c65c8"><span class="section-number-4">8.12.8.</span> cosine-similarity</h4>
<div class="outline-text-4" id="text-8-12-8">
<p>
If normalization is applied only after the embeddings have been learned, this can noticeably reduce the
 resulting (semantic) similarities compared to applying some normalization, or reduction of popularity-bias,
 before or during learning.
</p>

<p>
in High-Dimensional Spaces - two documents with the same words but different frequencies might be considered
 highly similar by cosine similarity.
</p>
</div>
</div>
<div id="outline-container-orgb45fac8" class="outline-4">
<h4 id="orgb45fac8"><span class="section-number-4">8.12.9.</span> Hierarchical clustering</h4>
<div class="outline-text-4" id="text-8-12-9">
<p>
<a href="https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering">https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering</a>
</p>
</div>
<ol class="org-ol">
<li><a id="org6c3ef5e"></a>theory<br />
<div class="outline-text-5" id="text-8-12-9-1">
<p>
<a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">https://en.wikipedia.org/wiki/Hierarchical_clustering</a>
</p>
<ul class="org-ul">
<li>linkages <a href="https://darrenho.github.io/AMA/clusteringHierarchical.pdf">https://darrenho.github.io/AMA/clusteringHierarchical.pdf</a></li>
</ul>

<p>
hierarchical clustering [haɪərˈɑːkɪkəl] [ˈklʌstərɪŋ]
</p>

<ul class="org-ul">
<li>Agglomerative: This is a "bottom-up" approach: each observation starts in its own cluster, and pairs of
clusters are merged as one moves up the hierarchy.</li>
<li>Divisive: This is a "top-down" approach: all observations start in one cluster, and splits are performed
recursively as one moves down the hierarchy.</li>
</ul>

<p>
elbow method [ˈelbəʊ] - метод локтя
affinity [əˈfɪnɪtɪ] - сходство
</p>
<ul class="org-ul">
<li>euclidean [juːˈklɪdɪən] - for ward mainly</li>
<li>manhattan or cityblock</li>
<li>cosine</li>
<li>precomputed</li>
</ul>

<p>
Linkages [ˈlɪŋkɪʤ] - связи
</p>
<ul class="org-ul">
<li>Single linkage = min dij - плотные ленточные - suffer from chaining</li>
<li>Complete = max dij - suffor from crowding - скученность - apoint can be closer to points in other cluster than to points in its own</li>
<li>Average = sum dij / count - парообразные</li>
<li><p>
ward - minimize the within-cluster sum of squares - like k-means
</p>

<p>
S C A - produces a dendrogram with <b>no inversions</b> - linkage distance between mergedclusters only increases as
we run the algorithm
</p></li>
</ul>

<p>
Taxonomy - close term, is a practice of categorization and classification
</p>
</div>
</li>

<li><a id="org181c228"></a>choosing linkage<br />
<div class="outline-text-5" id="text-8-12-9-2">
<p>
Single and complete linkage give the same dendrogram whether you use the raw data, the log of the data or any
 other transformation of the data that preserves the order because what matters is which ones have the
 smallest distance. The other methods are sensitive to the measurement scale.
</p>
</div>
</li>
<li><a id="orgfa26ddf"></a>Ward distance matrix<br />
<div class="outline-text-5" id="text-8-12-9-3">
<p>
d(u,v) = \sqrt{\frac{|v|+|s|}{T}d(v,s)<sup>2</sup>+ \frac{|v|+|t|}{T}d(v,t)<sup>2</sup>- \frac{|v|}{T}d(s,t)<sup>2</sup>}
</p>

<p>
where u is the newly joined cluster consisting of clusters s and t, v is an unused cluster in the forest,
 T=|v|+|s|+|t|, and |*| is the cardinality of its argument. This is also known as the incremental algorithm.
</p>
</div>
</li>
<li><a id="orgc1a8ad9"></a>choosing distance/simularity/affinity <a id="org8439b40"></a><br />
<div class="outline-text-5" id="text-8-12-9-4">
<p>
<a href="https://www.datanovia.com/en/lessons/clustering-distance-measures/">https://www.datanovia.com/en/lessons/clustering-distance-measures/</a>
<a href="https://en.wikipedia.org/wiki/Similarity_measure">https://en.wikipedia.org/wiki/Similarity_measure</a>
</p>
<ul class="org-ul">
<li>Евклидово расстояние d = sqrt((x1-y1)<sup>2</sup> + (x2-y2)<sup>2</sup>)
<ul class="org-ul">
<li>недостаток - различие по одной координате может определять расстояние из-за возведения в квадрат</li>
</ul></li>
<li>Квадрат Евклидова расстояния d = (x1-y1)<sup>2</sup> + (x2-y2)<sup>2</sup>
<ul class="org-ul">
<li>can be used to strengthen the effect of longer distances</li>
<li>does not form a metric space, as it does not satisfy the triangle inequality.</li>
</ul></li>
<li>Блок Manhettand = |x1-y1| + |x2-y2|
<ul class="org-ul">
<li>достоинство - одной переменной тяжелее перевесить другие</li>
<li>good for sparse features, or sparse noise: i.e. many of the features are zero, as in text mining using
occurrences of rare words.</li>
</ul></li>
<li>Cosine simularity - −1 meaning exactly opposite, to 1 meaning exactly the same, with 0 indicating
orthogonality or decorrelation
<ul class="org-ul">
<li>interesting because it is invariant to global scalings of the signal</li>
</ul></li>
<li>squared Euclidean distance - can be used to strengthen the effect of longer distances</li>

<li>minkowski - d = (∑(|x1-y1|<sup>p</sup> + |x2-y2|<sup>p</sup>))<sup>(1/p)</sup>
<ul class="org-ul">
<li>for p=2 equal to euclidean<sub>distance</sub> (l2)</li>
<li>for p=1, this is equivalent to using manhattan<sub>distance</sub> (l1)</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="orgb6f42e5"></a>performance<br />
<div class="outline-text-5" id="text-8-12-9-5">
<p>
<a href="https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation">https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation</a>
</p>

<ol class="org-ol">
<li>Rand index - measures the similarity of the two assignments, ignoring permutations 0-bad 1-good
<ul class="org-ul">
<li>metrics.rand<sub>score</sub>(labels<sub>true</sub>, labels<sub>pred</sub>)  -does not ensure to obtain a value close to 0.0 for a random labelling</li>
<li>metrics.adjusted<sub>rand</sub><sub>score</sub>(labels<sub>true</sub>, labels<sub>pred</sub>)</li>
</ul></li>

<li>Mutual Information based scores -
<ul class="org-ul">
<li>metrics.adjusted<sub>mutual</sub><sub>info</sub><sub>score</sub>(labels<sub>true</sub>, labels<sub>pred</sub>)</li>
</ul></li>

<li>Homogeneity, completeness and V-measure
<ul class="org-ul">
<li>metrics.homogeneity<sub>score</sub>(labels<sub>true</sub>, labels<sub>pred</sub>)</li>
<li>metrics.completeness<sub>score</sub>(labels<sub>true</sub>, labels<sub>pred</sub>)</li>
<li>metrics.v<sub>measure</sub><sub>score</sub>(labels<sub>true</sub>, labels<sub>pred</sub>)</li>
</ul></li>
<li>Fowlkes-Mallows scores
<ul class="org-ul">
<li>metrics.fowlkes<sub>mallows</sub><sub>score</sub>(labels<sub>true</sub>, labels<sub>pred</sub>)</li>
</ul></li>
<li>Silhouette Coefficient [-1,1]
<ul class="org-ul">
<li>metrics.silhouette<sub>score</sub>(X, labels, metric='euclidean')</li>
</ul></li>
<li>Calinski-Harabasz Index
<ul class="org-ul">
<li>metrics.calinski<sub>harabasz</sub><sub>score</sub>(X, labels)</li>
<li>is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN.</li>
<li>The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster.</li>
</ul></li>
<li>Davies-Bouldin Index
<ul class="org-ul">
<li>davies<sub>bouldin</sub><sub>score</sub>(X, labels)</li>
</ul></li>
<li>Contingency Matrix
<ul class="org-ul">
<li>from sklearn.metrics.cluster import contingency<sub>matrix</sub></li>
<li>contingency<sub>matrix</sub>(x, y)</li>
</ul></li>
</ol>
</div>
</li>

<li><a id="orge443085"></a>Cophenetic correlation<br />
<div class="outline-text-5" id="text-8-12-9-6">
<p>
uses Linkage and distances
</p>

<p>
Linkage: observations or clusters (0,1), distance(2), count of collected observations in new cluster(3)
</p>

<p>
Distances:
</p>
<pre class="example">
[[0. 0. 2.] (1)
 [0. 0. 2.]
 [2. 2. 0.]]
</pre>

<p>
here:  [0. 0. 2.] (1) - distances between first observation and first, second, third observation
</p>

<p>
dendrogram (y - observation, x - distances) - show distance at which clusters merged
</p>

<p>
<b>Cophenetic matrix</b> - minimum merging distance betwen observations.
</p>

<p>
<b>Cophenetic correlation coefficien</b> - correlation between distance matrix and cophenetic matrix.
</p>

<p>
Measures the correlation between the distances between observations and the lowest height on the dendrogram
 where the points are in the same cluster.
</p>

<p>
Suppose p and q are original observations in disjoint clusters s an t, respectively and s and t are joined by
 a direct parent cluster u. The cophenetic distance between observations i and j is simply the distance
 between clusters s and t.
</p>

<p>
The correlation between the distance matrix and the cophenetic distance is one metric to help assess which
 clustering linkage to select.
</p>

<p>
How to use:
</p>
<ul class="org-ul">
<li>It can be argued that a dendrogram is an appropriate summary of some data if the correlation between the
original distances and the cophenetic distances is high.</li>
<li>as the value of the <b>Cophenetic Correlation Coefficient</b> is quite close to 100%, we can say that the clustering is quite fit.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org331fbb5"></a>lins<br />
<div class="outline-text-6" id="text-8-12-9-6-1">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=Oeon9f-Xx78">https://www.youtube.com/watch?v=Oeon9f-Xx78</a></li>
<li><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.cophenet.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.cophenet.html</a></li>
<li><a href="https://people.revoledu.com/kardi/tutorial/Clustering/Cophenetic.htm">https://people.revoledu.com/kardi/tutorial/Clustering/Cophenetic.htm</a></li>
</ul>
</div>
</li>
<li><a id="org000d2d7"></a>ex<br />
<div class="outline-text-6" id="text-8-12-9-6-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Data</span>
<span style="color: #cae682;">d0</span>=dist(USArrests)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Hierarchical Agglomerative Clustering</span>
<span style="color: #cae682;">h1</span>=hclust(d0,method=<span style="color: #95e454;">'average'</span>)
<span style="color: #cae682;">h2</span>=hclust(d0,method=<span style="color: #95e454;">'complete'</span>)
<span style="color: #cae682;">h3</span>=hclust(d0,method=<span style="color: #95e454;">'ward.D'</span>)
<span style="color: #cae682;">h4</span>=hclust(d0,method=<span style="color: #95e454;">'single'</span>)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Cophenetic Distances, for each linkage</span>
<span style="color: #cae682;">c1</span>=cophenetic(h1)
<span style="color: #cae682;">c2</span>=cophenetic(h2)
<span style="color: #cae682;">c3</span>=cophenetic(h3)
<span style="color: #cae682;">c4</span>=cophenetic(h4)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Correlations</span>
cor(d0,c1) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">0.7658983</span>
cor(d0,c2) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">0.7636926</span>
cor(d0,c3) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">0.7553367</span>
cor(d0,c4) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">0.5702505</span>

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Dendograms</span>
par(mfrow=c(2,2))
plot(h1,main=<span style="color: #95e454;">'Average Linkage'</span>)
plot(h2,main=<span style="color: #95e454;">'Complete Linkage'</span>)
plot(h3,main=<span style="color: #95e454;">'Ward Linkage'</span>)
plot(h4,main=<span style="color: #95e454;">'Single Linkage'</span>)
par(mfrow=c(1,1))

</pre>
</div>
<p>
We see that the correlations for average and complete are extremely similar, and their dendograms appear very
 similar. The correlation for ward is similar to average and complete but the dendogram looks fairly
 different. single linkage is doing its own thing. Best professional judgement from a subject matter expert,
 or precedence toward a certain link in the field of interest should probably override numeric output from
 cor().
</p>
</div>
</li>
</ol>
</li>

<li><a id="org97d5adc"></a>sklearn<br />
<div class="outline-text-5" id="text-8-12-9-7">
<p>
cons:
</p>
<ul class="org-ul">
<li>only euclidean with Warp</li>

<li>kmean and scree plot <a href="https://towardsdatascience.com/analyzing-credit-cards-kmeans-581565208cdb">https://towardsdatascience.com/analyzing-credit-cards-kmeans-581565208cdb</a></li>
<li>AgglomerativeClustering <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering">https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering</a></li>
<li>childrens traverse <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py">https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py</a></li>
</ul>

<p>
sklearn.cluster.AgglomerativeClustering
</p>
<ul class="org-ul">
<li>labels_ - Result, each object marked with label, two clasters = [0,0,0,1,1,1]</li>
<li>n<sub>clusters</sub>_ - n cluster found</li>
<li>n<sub>leaves</sub>_ - ?</li>
<li>n<sub>connected</sub><sub>components</sub>_ - ?</li>
<li>children_ - list of [child1, child2] for each step</li>
<li>distances - list of distances from smallest, from the begining</li>
</ul>


<ul class="org-ul">
<li>n<sub>clusters</sub> - should be None</li>
<li>affinity
<ul class="org-ul">
<li>"euclidean" or "l2",</li>
<li>"manhattan" or "l1" (insite affinity = 'cityblock')</li>
<li>"cosine" <a href="https://en.wikipedia.org/wiki/Cosine_similarity">https://en.wikipedia.org/wiki/Cosine_similarity</a></li>
<li>'precomputed'
<ul class="org-ul">
<li>sklearn.metrics.pairwise<sub>distances</sub>
<ul class="org-ul">
<li>'cityblock'     metrics.pairwise.manhattan<sub>distances</sub></li>
<li>'cosine'        metrics.pairwise.cosine<sub>distances</sub></li>
<li>'euclidean'     metrics.pairwise.euclidean<sub>distances</sub></li>
<li>'haversine'     metrics.pairwise.haversine<sub>distances</sub></li>
<li>'l1'            metrics.pairwise.manhattan<sub>distances</sub></li>
<li>'l2'            metrics.pairwise.euclidean<sub>distances</sub></li>
<li>'manhattan'     metrics.pairwise.manhattan<sub>distances</sub></li>
<li>'nan<sub>euclidean</sub>' metrics.pairwise.nan<sub>euclidean</sub><sub>distances</sub></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
</li>
<li><a id="orga485a67"></a>scipy<br />
<div class="outline-text-5" id="text-8-12-9-8">
<ul class="org-ul">
<li>pdist defaults: metric='euclidean'</li>
<li>linkage defaults: method='single', metric='euclidean'</li>
</ul>

<p>
<a href="https://www.youtube.com/watch?v=l4vTwXL_5Cc">https://www.youtube.com/watch?v=l4vTwXL_5Cc</a>
</p>
</div>
<ol class="org-ol">
<li><a id="org23c2d92"></a>ex<br />
<div class="outline-text-6" id="text-8-12-9-8-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn <span style="color: #8ac6f2; font-weight: bold;">import</span> cluster, datasets
<span style="color: #cae682;">n_samples</span> = 1500
<span style="color: #cae682;">noisy_circles</span> = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)
<span style="color: #cae682;">X</span> = noisy_circles

<span style="color: #8ac6f2; font-weight: bold;">from</span> scipy.spatial.distance <span style="color: #8ac6f2; font-weight: bold;">import</span> pdist
<span style="color: #cae682;">distances</span> = pdist(X, <span style="color: #95e454;">'euclidean'</span>)
<span style="color: #e5786d;">print</span>(Y)
<span style="color: #cae682;">Y</span> = linkage(distances)
<span style="color: #e5786d;">print</span>(Y)
</pre>
</div>
</div>
</li>
</ol>
</li>
</ol>
</div>

<div id="outline-container-org4872e03" class="outline-4">
<h4 id="org4872e03"><span class="section-number-4">8.12.10.</span> Automatic clustering</h4>
<div class="outline-text-4" id="text-8-12-10">
<ul class="org-ul">
<li>article <a href="https://www.toptal.com/machine-learning/clustering-algorithms">https://www.toptal.com/machine-learning/clustering-algorithms</a></li>
<li>sklearn all <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py">https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py</a></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org36d34e4"></a>k-means<br />
<div class="outline-text-5" id="text-8-12-10-1">
<p>
def
</p>
<ul class="org-ul">
<li>стремится минимизировать саммарное квадратичное отклонение точек кластеров от центров этих кластеров</li>
<li>observations to those clusters so that the means across clusters (for all variables) are as different from
each other as possible.</li>
<li>assigning examples to clusters to maximize the differences in means for continuous variables</li>
</ul>

<p>
cons
</p>
<ul class="org-ul">
<li>только евклидово расстрояние</li>
<li>решение зависит от начальных центров</li>
<li>надо определять число кластеров</li>
<li>слишком много вычислений расстояний</li>
<li>на поздних итерациях мало точек меняют кластер</li>
<li>Не гарантируется достижение глобального минимума суммарного квадратичного отклонения V, а только одного из локальных минимумов.</li>
<li>ищет только шаровые скопления</li>
</ul>

<p>
Альтернативы
</p>
<ul class="org-ul">
<li>Gaussian mixture model</li>
</ul>
</div>
</li>
<li><a id="org9cbd152"></a>EM clustering - expectation maximization<br />
<div class="outline-text-5" id="text-8-12-10-2">
<ul class="org-ul">
<li><a href="https://docs.rapidminer.com/latest/studio/operators/modeling/segmentation/expectation_maximization_clustering.html">https://docs.rapidminer.com/latest/studio/operators/modeling/segmentation/expectation_maximization_clustering.html</a></li>
<li><a href="https://ru.wikipedia.org/wiki/EM-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC">https://ru.wikipedia.org/wiki/EM-%D0%B0%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC</a></li>
<li><a href="http://espressocode.top/gaussian-mixture-model/">http://espressocode.top/gaussian-mixture-model/</a></li>
</ul>

<p>
Предполагается что исходные данные можно представить в виде гауссовского распределения.
</p>

<p>
EM algorithm attempts to approximate the observed distributions of values based on mixtures of different
distributions in different clusters
</p>

<p>
EM для:
</p>
<ul class="org-ul">
<li>для разделения смеси гауссиан.</li>
<li>используется для оценки максимального правдоподобия при вычислении параметров статистической модели со скрытыми переменными.
<ul class="org-ul">
<li>распределение помогает понять, сколько человек, сдающих экзамен, получат ту или иную оценку.</li>
<li>правдоподобие - это вероятность того, что кривая нормального распределения с оцененными значениями
среднего арифметического и дисперсии будет достаточно точно описывать (?)
<ul class="org-ul">
<li>На основании этих оцененных параметров модели считается гипотетическая вероятность появления того или
иного исхода, называемая правдоподобием</li>
</ul></li>
<li>вероятность - Шанс, что мы пронаблюдаем определенные оценки с определенной частотой</li>
</ul></li>
</ul>


<p>
How
</p>
<ul class="org-ul">
<li>Describe each cluster by its centroid (mean), covariance (so that we can have elliptical clusters), and weight</li>
</ul>
<p>
(the size of the cluster).
</p>
<ul class="org-ul">
<li>The probability that a point belongs to a cluster is now given by a multivariate Gaussian probability
distribution (multivariate - depending on multiple variables).</li>
</ul>

<p>
pros:
</p>
<ul class="org-ul">
<li>clusters that are overlapping, or ones that are not of circular shape</li>
<li>“soft clustering” - one point have distribution of probabilities over clusters</li>
</ul>
<p>
cons:
</p>
<ul class="org-ul">
<li>maximum may be local, so we can run the algorithm several times to get better clusters.</li>
</ul>

<p>
two steps:
</p>
<ol class="org-ol">
<li>E-step - calculating, for each point, the probabilities of it belonging to each of the current clusters (which, again, may be randomly created at the beginning)</li>
<li>M-step - recalculates the parameters of each cluster, using the assignments of points to the previous set of clusters.</li>
<li>Предыдущие два шага повторяются до тех пор, пока параметры модели и кластерное распределение не уравняются.</li>
</ol>

<p>
недостатки:
</p>
<ul class="org-ul">
<li>С ростом количества итераций падает производительность алгоритма.</li>
<li>EM не всегда находит оптимальные параметры и может застрять в локальном оптимуме, так и не найдя глобальный.</li>
</ul>


<p>
Mixture model - Гауссова Смесь Распределений
</p>
</div>

<ol class="org-ol">
<li><a id="orgb36d259"></a>sklearn: GaussianMixture<br />
<div class="outline-text-6" id="text-8-12-10-2-1">
<p>
<a href="https://cmdlinetips.com/2021/03/gaussian-mixture-models-with-scikit-learn-in-python/">https://cmdlinetips.com/2021/03/gaussian-mixture-models-with-scikit-learn-in-python/</a>
</p>


<p>
Информационный критерий Акаике (AIC) Akaike information criterion - Чем меньше тем лучше
AIC = 2k-2ln(L)
</p>
<ul class="org-ul">
<li>k - число параметров в статистической модели</li>
<li>L — максимизированное значение функции правдоподобия модели.</li>
</ul>

<p>
Bayesian information criterion (BIC) - налагает больший штраф на увеличение количества параметров по сравнению с AIC
BIC = kln(n)-2l   n  - обхем выборки
</p>
</div>
</li>
</ol>
</li>

<li><a id="org8ff0d88"></a>AffinityPropagation<br /></li>
<li><a id="orgc2d66e6"></a><span class="todo TODO">TODO</span> NN  Semantic Clustering by Adopting Nearest Neighbors (SCAN)<br />
<div class="outline-text-5" id="text-8-12-10-4">
<p>
<a href="https://arxiv.org/abs/2005.12320">https://arxiv.org/abs/2005.12320</a>
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgf40d40e" class="outline-4">
<h4 id="orgf40d40e"><span class="section-number-4">8.12.11.</span> mistakes</h4>
<div class="outline-text-4" id="text-8-12-11">
<ol class="org-ol">
<li>Lack of an exhaustive Exploratory Data Analysis (EDA) and digestible Data Cleaning. how they correlate with
each other are essential. WHY you decided to choose the respective approach.</li>
<li></li>
</ol>
</div>
</div>
<div id="outline-container-org023c616" class="outline-4">
<h4 id="org023c616"><span class="section-number-4">8.12.12.</span> quality, validation, evalutaion</h4>
<div class="outline-text-4" id="text-8-12-12">
</div>
<ol class="org-ol">
<li><a id="orgea514b1"></a>google<br />
<div class="outline-text-5" id="text-8-12-12-1">
<ul class="org-ul">
<li>Cluster cardinality - number of examples per cluster</li>
<li>Cluster magnitude - sum of distances from all examples to the centroid of the cluster</li>
<li>Performance of downstream system - output is often used in downstream ML systems</li>
</ul>

<p>
Clusters are anomalous when cardinality doesn't correlate with magnitude relative to the other clusters.
</p>
<ul class="org-ul">
<li>plotting magnitude against cardinality.</li>
</ul>

<p>
k-means - k selection -  the sum of cluster magnitudes
</p>

<p>
<a href="https://developers.google.com/machine-learning/clustering/interpret">https://developers.google.com/machine-learning/clustering/interpret</a>
</p>
</div>
</li>
<li><a id="org0af9327"></a>arror rate, accuracy<br />
<div class="outline-text-5" id="text-8-12-12-2">
<p>
confusion matrix:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">actual P(1)</th>
<th scope="col" class="org-left">actua N(0)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">out P(1)</td>
<td class="org-left">TP</td>
<td class="org-left">FP</td>
</tr>

<tr>
<td class="org-left">out N(0)</td>
<td class="org-left">FN</td>
<td class="org-left">TN</td>
</tr>
</tbody>
</table>

<dl class="org-dl">
<dt>error rate</dt><dd>what fraction of the rows in your testing data is misclassified:</dd>
</dl>
<pre class="example">
TPR = TP/P, P = TP + FN
TNR = TN/N, N = TN + FP
</pre>

<dl class="org-dl">
<dt>accuracy</dt><dd>the fraction of rows that are properly classified</dd>
</dl>
<pre class="example">
acc = sum([x==y for x, y in zip(labels_true, labels_pred)])/len(labels_true)
errate = len(labels_true) - acc
</pre>

<dl class="org-dl">
<dt>balanced accuracy</dt><dd>(TPR + TNR)/2 - good for inbalanced classification</dd>
</dl>
</div>
</li>
<li><a id="orgf2af8e7"></a>Rand Index (RI)<br />
<div class="outline-text-5" id="text-8-12-12-3">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">TP:</td>
<td class="org-left">FN:</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>

<!-- This HTML table template is generated by emacs 29.4 -->
<table border="1">
  <tr>
    <td align="left" valign="top">
      &nbsp;TP:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;Same&nbsp;class&nbsp;+&nbsp;same&nbsp;cluster&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
    <td align="left" valign="top">
      &nbsp;FN:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;Same&nbsp;class&nbsp;+&nbsp;different&nbsp;clusters&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    </td>
  </tr>
  <tr>
    <td align="left" valign="top">
      &nbsp;FP:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;different&nbsp;class&nbsp;+&nbsp;same&nbsp;cluster&nbsp;
    </td>
    <td align="left" valign="top">
      &nbsp;TN:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />
      &nbsp;different&nbsp;class&nbsp;+&nbsp;different&nbsp;clusters&nbsp;
    </td>
  </tr>
</table>
</div>
</li>
</ol>
</div>

<div id="outline-container-org9cf93de" class="outline-4">
<h4 id="org9cf93de"><span class="section-number-4">8.12.13.</span> links</h4>
<div class="outline-text-4" id="text-8-12-13">
<ul class="org-ul">
<li><a href="https://bpostance.github.io/posts/clustering-mixed-data/">https://bpostance.github.io/posts/clustering-mixed-data/</a></li>
<li><a href="https://towardsdatascience.com/common-mistakes-in-cluster-analysis-and-how-to-avoid-them-eb960116d773">https://towardsdatascience.com/common-mistakes-in-cluster-analysis-and-how-to-avoid-them-eb960116d773</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org11388b5" class="outline-3">
<h3 id="org11388b5"><span class="section-number-3">8.13.</span> Регрессивный линейный анализ - linear regression</h3>
<div class="outline-text-3" id="text-8-13">
</div>
<div id="outline-container-orgbfa9b86" class="outline-4">
<h4 id="orgbfa9b86"><span class="section-number-4">8.13.1.</span> types</h4>
<div class="outline-text-4" id="text-8-13-1">
<p>
y= ∑wi*f(x)
</p>
<ul class="org-ul">
<li>Одномерная регрессия f = w1 +w2*xi</li>
<li>Полиномиальная регрессия f = (1, x, x<sup>2</sup> &#x2026;)</li>
<li>Криволинейная регрессия f = (g1, g2, g3), where g1,g2,g3 - нелинейные функции</li>
</ul>


<p>
<b>multiple linear regression</b> - more than one independent variable
</p>
<ul class="org-ul">
<li>Polynomial regression see <a href="#orgaa144d7">3.5</a></li>
<li><b>logistic regression</b> as the equivalent of linear regression for a classification problem - Any input to the
model yields a number lying between 0 and 1.</li>
</ul>

<p>
<b>general linear model</b> (multivariate linear regression) - just a compact way of simultaneously writing several
 multiple linear regression models. assumes that the residuals will follow a conditionally normal
 distribution. general linear model is a special case of the GLM
</p>

<p>
<b>generalized linear model</b> (GLM) - как способ объединения различных других статистических моделей, включая
 линейную регрессию, логистическую регрессию и регрессию Пуассона
</p>
</div>
</div>
<div id="outline-container-org15b8f4b" class="outline-4">
<h4 id="org15b8f4b"><span class="section-number-4">8.13.2.</span> parameters estimation methods</h4>
<div class="outline-text-4" id="text-8-13-2">
<ul class="org-ul">
<li>maximum likelihood estimation (MLE) - a method that determines values for the parameters of a model. model
should produce data with maximum likelihood.</li>
<li>Bayes estimators</li>
<li><p>
Least squares Метод наименьших квадратов
</p>
<ul class="org-ul">
<li>linear or ordinary least squares (по англ. OLS) — линейная регрессия c SSE(a,b) в качестве функции</li>
</ul>
<p>
потерь - Sum of Squared Errors (SSE) =  ∑(f(xi) - yi)<sup>s</sup>
</p>
<ul class="org-ul">
<li>nonlinear least squares</li>
</ul></li>
<li>Least Absolute Distance (LAD) = ∑|f(xi) - yi|</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org6b871ec"></a>maximum likelihood estimation (MLE)<br />
<div class="outline-text-5" id="text-8-13-2-1">
<ol class="org-ol">
<li></li>

<li><a href="https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1">https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1</a></li>
</ol>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgdc178e4" class="outline-4">
<h4 id="orgdc178e4"><span class="section-number-4">8.13.3.</span> цели регрессивного анализа</h4>
<div class="outline-text-4" id="text-8-13-3">
<ul class="org-ul">
<li>Определение степени детерминированности вариации критериальной (зависимой) переменной предикторами
(независимыми переменными)</li>
<li>Предсказание значения зависимой переменной с помощью независимой(-ых)</li>
<li>Определение вклада отдельных независимых переменных в вариацию зависимой</li>
</ul>
</div>
</div>
<div id="outline-container-orgfa9ffa4" class="outline-4">
<h4 id="orgfa9ffa4"><span class="section-number-4">8.13.4.</span> требования для регрессивного анализа</h4>
<div class="outline-text-4" id="text-8-13-4">
<p>
The correlation between the two independent variables is called multicollinearity. Multicollinearity is fine,
but the excess of multicollinearity can be a problem.
</p>
</div>
</div>
<div id="outline-container-orgd7bec83" class="outline-4">
<h4 id="orgd7bec83"><span class="section-number-4">8.13.5.</span> Linear least squares (LLS) - most simple</h4>
<div class="outline-text-4" id="text-8-13-5">
<p>
is the least squares approximation of linear functions.
</p>
<ul class="org-ul">
<li>y = mx + b</li>
<li>m = (n∑xy - ∑y∑x)/n∑x<sup>2</sup> - (∑x)<sup>2</sup></li>
<li>b = (∑y - m∑x)/n ,where n is the number of data points.</li>
</ul>

<p>
Steps:
</p>
<ul class="org-ul">
<li>yi = a + b*xi + ei, where ei - error</li>
<li>ei = yi - a - b*xi</li>
<li>(a,b) = argmin(Q(a,b)) #  minimization problem:  - armin Returns the indices of the minimum values along an axis</li>
<li>Q(a,b) = ∑e<sup>2</sup> = ∑(yi-a-b*xi)<sup>2</sup> # if we calc best as least-squares.</li>
<li></li>
</ul>

<p>
Ax = b
</p>
</div>
<ol class="org-ol">
<li><a id="org452126f"></a>cons<br />
<div class="outline-text-5" id="text-8-13-5-1">
<ul class="org-ul">
<li>Only for two variables x,y</li>
<li>This method is unreliable when data is not evenly distributed.</li>
<li>This method is very sensitive to outliers. In fact, this can skew the results of the least-squares analysis.</li>
</ul>
</div>
</li>
<li><a id="org64002e2"></a>links<br />
<div class="outline-text-5" id="text-8-13-5-2">
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Simple_linear_regression">https://en.wikipedia.org/wiki/Simple_linear_regression</a></li>
<li><a href="https://www.cuemath.com/data/least-squares/">https://www.cuemath.com/data/least-squares/</a></li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgc742ef3" class="outline-4">
<h4 id="orgc742ef3"><span class="section-number-4">8.13.6.</span> regularization methods</h4>
<div class="outline-text-4" id="text-8-13-6">
<p>
regularization method (reduce overfitting using less complicated functions):
</p>
<ul class="org-ul">
<li><b>LASSO</b> (Least Absolute Shrinkage and Selection Operator), a powerful feature selection technique that is very
useful for regression problems</li>
</ul>
</div>
</div>

<div id="outline-container-org464af04" class="outline-4">
<h4 id="org464af04"><span class="section-number-4">8.13.7.</span> logistic regression (or logit regression)</h4>
<div class="outline-text-4" id="text-8-13-7">
<p>
a logistic model in form of linear combination of binary (0,1) or a continuous variables (any real value).
</p>
<ul class="org-ul">
<li>p = 1/(1 + e<sup>ß0 + ß1*x + ß2*x2 + … + ßn*xn</sup>)</li>
</ul>

<p>
st. logistic function (-∞,+∞) - &gt; (0,1)
</p>
<ul class="org-ul">
<li>σ(x)=1/(1+e<sup>-x</sup>)</li>
<li>converts log-odds (-∞,+∞) to probability (0,1)</li>
</ul>

<p>
<b>the logit</b> is the inverse of the standard logistic function: (0,1) -&gt; (-∞,+∞)
</p>
<ul class="org-ul">
<li>f(p)= σ<sup>-1</sup>(p) = ln ( p/(1-p) ),  for p ∈ (0,1)</li>
</ul>

<p>
Types of Logistic Regression
</p>
<ul class="org-ul">
<li><b>binary logistic regression</b> - probability of the value labeled "1" can vary between 0 and 1.</li>
<li>Multinomial Logistic Regression: The target variable has three or more nominal categories such as predicting
the type of Wine.</li>
<li>Ordinal Logistic Regression: the target variable has three or more ordinal categories such as restaurant or
product rating from 1 to 5.</li>
</ul>

<p>
<b>goodness of fit</b> for a logistic regression uses:
</p>
<ul class="org-ul">
<li>logistic loss, log loss, binary cross-entropy loss</li>
<li>the negative log-likelihood.</li>
</ul>

<p>
<b>logistic loss</b> and <b>binary cross-entropy loss</b> (Log loss) are in fact the same
</p>
<ul class="org-ul">
<li>for y in {0,1}: L{log(y, p)} = -(y * log (p) + (1 - y) * log (1 - p))</li>
</ul>


<div id="orge3b98a2" class="figure">
<p><img src="https://images.datacamp.com/image/upload/v1660054820/Regression_charts_b9de7355cf.png" alt="Regression_charts_b9de7355cf.png" />
</p>
</div>

<p>
<a href="https://web.stanford.edu/~jurafsky/slp3/5.pdf">https://web.stanford.edu/~jurafsky/slp3/5.pdf</a>
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.linear_model <span style="color: #8ac6f2; font-weight: bold;">import</span> LogisticRegression
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #cae682;">y</span> = [0]*5 + [1]*5
<span style="color: #cae682;">X</span> = np.array(<span style="color: #e5786d;">list</span>(<span style="color: #e5786d;">range</span>(10))).reshape(-1, 1)
<span style="color: #e5786d;">print</span>(X)
<span style="color: #cae682;">clf</span> = LogisticRegression(random_state=0).fit(X, y)
<span style="color: #e5786d;">print</span>(clf.predict(X/1.6))
</pre>
</div>

<pre class="example" id="org04ea486">
[[0]
 [1]
 [2]
 [3]
 [4]
 [5]
 [6]
 [7]
 [8]
 [9]]
[0 0 0 0 0 0 0 0 1 1]
</pre>
</div>
</div>

<div id="outline-container-orgb1478df" class="outline-4">
<h4 id="orgb1478df"><span class="section-number-4">8.13.8.</span> Linear Regression Vs. Logistic Regression</h4>
<div class="outline-text-4" id="text-8-13-8">
<p>
Linear regression is frequently estimated using Ordinary Least Squares (OLS) while logistic regression is
 estimated using Maximum Likelihood Estimation (MLE) approach.
</p>
</div>
</div>
<div id="outline-container-org82853df" class="outline-4">
<h4 id="org82853df"><span class="section-number-4">8.13.9.</span> example1</h4>
<div class="outline-text-4" id="text-8-13-9">
<p>
<a href="https://youtu.be/g335THJxkto">https://youtu.be/g335THJxkto</a>
</p>

<p>
берешь подмножество признаков - строишь линейную регрессию предсказывая какой-то другой признак - если ошибка
стремится к нулю - есть зависимость
</p>

<p>
бывает что какие-то значения признаков хорошо группируют строки - решение средние значения таргета для разных групп
</p>
<ul class="org-ul">
<li>создаем новую переменную - среднее значение таргета для данной переменной</li>
</ul>

<p>
Подсчет статистик по таргету хорошо работает где есть категориальные признаки
</p>
</div>
</div>
<div id="outline-container-org9da5fbc" class="outline-4">
<h4 id="org9da5fbc"><span class="section-number-4">8.13.10.</span> example2</h4>
<div class="outline-text-4" id="text-8-13-10">
<p>
<a href="https://habr.com/ru/post/339250/">https://habr.com/ru/post/339250/</a>
</p>
<ul class="org-ul">
<li>Скрытые зависимости между признаками могут описываться разными функциями, и в разных случаях разные функции
могут проявить себя лучше остальных.</li>
<li>стоит изначально выбрать набор функций, адекватность применения которых зависит от специфики задачи.</li>
<li>Число производных столбцов для анализа равно k*(n² — n) / 2, где k — число выбранных функций F(Xi,Xj), n —
число исходных признаков.</li>
<li>Для не очень большого числа признаков можно позволить себе полный перебор всех пар с полноценной проверкой
полезности для каждого полученного признака.</li>
<li>Или быстрое отбрасывание самых неинформативных производных признаков и последующий более качественный разбор
оставшихся.</li>
<li>Гипотетически есть возможность вычисления производных признаков F(Xi, Xj) от множества признаков M', которые
даст нам применение метода главных компонент на исходное множество признаков M, но встаёт вопрос о том, все
ли скрытые зависимости в этом случае могут быть проявлены.</li>
</ul>
</div>
</div>

<div id="outline-container-org41e6c42" class="outline-4">
<h4 id="org41e6c42"><span class="section-number-4">8.13.11.</span> links</h4>
<div class="outline-text-4" id="text-8-13-11">
<ul class="org-ul">
<li><a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F_(%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80)">http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F_(%D0%BF%D1%80%D0%B8%D0%BC%D0%B5%D1%80)</a></li>
<li><a href="https://habr.com/ru/articles/514818/">https://habr.com/ru/articles/514818/</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgfec0f29" class="outline-3">
<h3 id="orgfec0f29"><span class="section-number-3">8.14.</span> Факторный анализ</h3>
<div class="outline-text-3" id="text-8-14">
<p>
Узучает variability одних переменных(видимых) с точки зрения других переменных(невидимых) меньшего количества.
</p>

<p>
Использует корреляционный анализ
</p>
</div>
</div>
<div id="outline-container-org817e1e9" class="outline-3">
<h3 id="org817e1e9"><span class="section-number-3">8.15.</span> Time Series Analysis</h3>
<div class="outline-text-3" id="text-8-15">
<ul class="org-ul">
<li><a href="https://github.com/Yorko/mlcourse.ai/tree/main/jupyter_english/topic09_time_series">https://github.com/Yorko/mlcourse.ai/tree/main/jupyter_english/topic09_time_series</a></li>
<li><a href="https://github.com/stepanovD/ts_anomaly_detection_course">https://github.com/stepanovD/ts_anomaly_detection_course</a></li>
<li><a href="https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/">https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/</a></li>
</ul>

<p>
Univariate and Multivariate time series - y or (x,y,z).
</p>
</div>
<div id="outline-container-orgd4869b9" class="outline-4">
<h4 id="orgd4869b9"><span class="section-number-4">8.15.1.</span> terms</h4>
<div class="outline-text-4" id="text-8-15-1">
<dl class="org-dl">
<dt>Structural break</dt><dd>unexpected change over time in the parameters of regression models, which can lead to
huge forecasting errors</dd>
</dl>
</div>
</div>
<div id="outline-container-orgee55f8f" class="outline-4">
<h4 id="orgee55f8f"><span class="section-number-4">8.15.2.</span> forecasting methods</h4>
<div class="outline-text-4" id="text-8-15-2">
<ul class="org-ul">
<li>Autoregression (AR)</li>
<li>Moving Average (MA)</li>
<li>Autoregressive Moving Average (ARMA)</li>
<li>Autoregressive Integrated Moving Average (ARIMA)</li>
<li>Seasonal Autoregressive Integrated Moving-Average (SARIMA)</li>
<li>Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)</li>
<li>Vector Autoregression (VAR)</li>
<li>Vector Autoregression Moving-Average (VARMA)</li>
<li>Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX)</li>
<li>Simple Exponential Smoothing (SES)</li>
<li>Holt Winter’s Exponential Smoothing (HWES)</li>
</ul>

<p>
<a href="https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/">https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/</a>
</p>
</div>
</div>
<div id="outline-container-orgbfcb412" class="outline-4">
<h4 id="orgbfcb412"><span class="section-number-4">8.15.3.</span> forecasting loss metrics</h4>
<div class="outline-text-4" id="text-8-15-3">
<ul class="org-ul">
<li>MAE Mean Absolute Error</li>
<li>RMSE - Root Mean Squared Error</li>
<li>MAPE - Mean Absolute Percentage Error</li>
<li>SMAPE - Symmetric Mean Absolute Percentage Error</li>
<li>коэффициент детерминации или R<sup>2</sup> = 1 - RSS/TSS'</li>
</ul>
<p>
Сравнение моделей прогнозирования с точки зрения баланса между точностью предсказания и сложностью (кол-вом
 параметров модели) применяется критерий Акаике (AIC)
</p>
<ul class="org-ul">
<li>AIC = 2 lnL + 2k</li>
<li>k = число параметров модели</li>
<li>L - соответствующее значение функции правдоподобия модели.</li>
</ul>
</div>
</div>
<div id="outline-container-org9692cf2" class="outline-4">
<h4 id="org9692cf2"><span class="section-number-4">8.15.4.</span> features</h4>
<div class="outline-text-4" id="text-8-15-4">
<p>
see <a href="#org1db2b2b">8.9.10.4</a>
</p>
<ul class="org-ul">
<li>временные интервалы между измерениями постоянны или меняются?</li>
<li>тренд - плавное долгосрочное изменение уровня ряда</li>
<li>цикл - изменение уровня ряда с переменным перидом</li>
<li>шум - прогнозируемая случайная компонента ряда.</li>
<li>стационарность - ряд сгенерирован стационарным процессом</li>
</ul>
</div>
</div>
<div id="outline-container-orgbd73554" class="outline-4">
<h4 id="orgbd73554"><span class="section-number-4">8.15.5.</span> определение стационарности</h4>
<div class="outline-text-4" id="text-8-15-5">
<p>
автокорреляция  ACF - является корреляцией сигнала с задержанной копией - или задержкой - самого себя как функция задержки.
</p>
<ul class="org-ul">
<li>коррелограмма), значения имеют тенденцию быстро уменьшаться до нуля для стационарных временных рядов</li>
</ul>

<p>
<a href="https://www.jstor.org/stable/3879300?seq=1#metadata_info_tab_contents">https://www.jstor.org/stable/3879300?seq=1#metadata_info_tab_contents</a>
</p>
<ul class="org-ul">
<li>, [ Нильсен, 2006 ] предполагает, что построение коррелограмм на основе как автокорреляций, так и
масштабированных автоковариаций и сравнение их обеспечивает лучший способ различения стационарных и
нестационарных данных.</li>
</ul>

<p>
Параметрические испытания - статистические тесты, разработанные для обнаружения
</p>

<p>
Модульные корневые тесты
</p>
<ul class="org-ul">
<li>Тест Дики-Фуллера - в statsmodels а также ARCH пакеты.</li>
<li>Тест КПСС  KPSS тест, [Kwiatkowski et al, 1992]</li>
</ul>

<p>
Тест Зивота и Эндрюса - допускает возможность структурный разрыв
<a href="https://machinelearningmastery.ru/detecting-stationarity-in-time-series-data-d29e0a21e638/">https://machinelearningmastery.ru/detecting-stationarity-in-time-series-data-d29e0a21e638/</a>
</p>
</div>
</div>
<div id="outline-container-orgc505f61" class="outline-4">
<h4 id="orgc505f61"><span class="section-number-4">8.15.6.</span> rate of change</h4>
<div class="outline-text-4" id="text-8-15-6">
<ul class="org-ul">
<li>forward	= (f(t2) - f(t1)) / △t</li>
<li>backward	= (f(t1) - f(t2)) / △t</li>
<li>center	= (f(t3) - f(t1)) / 2△t</li>
</ul>

<p>
np.diff - a[i+1] - a[i]
</p>


<div class="org-src-container">
<pre class="src src-python"><span style="color: #cae682;">measurements</span> = [2,3,4,4,3] <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">5</span>
<span style="color: #cae682;">dt</span> = [1,1,2,3] <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">4</span>
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #e5786d;">print</span>( np.diff(measurements))
<span style="color: #e5786d;">print</span>( np.diff(measurements) / dt)
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print(list(reversed(measurements)))</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print("backward" np.diff(list(reversed(measurements))) / dt)</span>
<span style="color: #e5786d;">print</span>( np.diff(measurements) / (np.array(dt)*2))
</pre>
</div>

<pre class="example">
[ 1  1  0 -1]
[ 1.          1.          0.         -0.33333333]
[ 0.5         0.5         0.         -0.16666667]
</pre>


<p>
<a href="https://e2eml.school/rate_of_change">https://e2eml.school/rate_of_change</a>
</p>
</div>
</div>
<div id="outline-container-orgc456d52" class="outline-4">
<h4 id="orgc456d52"><span class="section-number-4">8.15.7.</span> one dimension convolution</h4>
<div class="outline-text-4" id="text-8-15-7">
<p>
Convolution vs. cross-correlation
</p>
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Cross-correlation">https://en.wikipedia.org/wiki/Cross-correlation</a></li>
</ul>

<p>
autocorrelation - cross-correlate a signal with itself
</p>

<p>
<a href="https://e2eml.school/convolution_one_d">https://e2eml.school/convolution_one_d</a>
</p>
</div>
</div>
<div id="outline-container-orga2d6261" class="outline-4">
<h4 id="orga2d6261"><span class="section-number-4">8.15.8.</span> graphs</h4>
<div class="outline-text-4" id="text-8-15-8">
<ul class="org-ul">
<li>simple plot plt.plot - x - date, y - value</li>
<li>two sides simple plot</li>
<li>each year as a separate line in the same plot - Seasonal Plot of a Time Series</li>
<li>Boxplot of Month-wise (Seasonal) and Year-wise (trend) Distribution</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="orgf966fc9"></a>two sides simple plot<br />
<div class="outline-text-5" id="text-8-15-8-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #cae682;">fig</span>, <span style="color: #cae682;">ax</span> = plt.subplots(1, 1, figsize=(16,5), dpi= 120)
plt.fill_between(x, y1=y1, y2=-y1, alpha=0.5, linewidth=2, color=<span style="color: #95e454;">'seagreen'</span>)
plt.ylim(-800, 800)
plt.title(<span style="color: #95e454;">'Air Passengers (Two Side View)'</span>, fontsize=16)
plt.hlines(y=0, xmin=np.<span style="color: #e5786d;">min</span>(df.date), xmax=np.<span style="color: #e5786d;">max</span>(df.date), linewidth=.5)
plt.show()

</pre>
</div>
</div>
</li>
<li><a id="orgc5f4936"></a><span class="todo TODO">TODO</span> Boxplot of Month-wise (Seasonal) and Year-wise (trend) Distribution<br /></li>
</ol>
</div>
<div id="outline-container-org0c115a8" class="outline-4">
<h4 id="org0c115a8"><span class="section-number-4">8.15.9.</span> datasets</h4>
<div class="outline-text-4" id="text-8-15-9">
<ul class="org-ul">
<li>Panel data    df = pd.read<sub>csv</sub>('https://raw.githubusercontent.com/selva86/datasets/master/MarketArrivals.csv')</li>
<li>Monthly anti-diabetic drug sales in Australia from 1992 to 2008. df = pd.read<sub>csv</sub>('https://raw.githubusercontent.com/selva86/datasets/master/a10.csv', parse<sub>dates</sub>=['date'], index<sub>col</sub>='date')</li>
</ul>
</div>
</div>
<div id="outline-container-org71ef7e5" class="outline-4">
<h4 id="org71ef7e5"><span class="section-number-4">8.15.10.</span> <span class="todo TODO">TODO</span> forecasting</h4>
<div class="outline-text-4" id="text-8-15-10">
<p>
<a href="https://facebook.github.io/prophet/">https://facebook.github.io/prophet/</a>
</p>
</div>
</div>
<div id="outline-container-orgc168633" class="outline-4">
<h4 id="orgc168633"><span class="section-number-4">8.15.11.</span> links</h4>
<div class="outline-text-4" id="text-8-15-11">
<ul class="org-ul">
<li><a href="https://www.machinelearningplus.com/time-series/time-series-analysis-python/">https://www.machinelearningplus.com/time-series/time-series-analysis-python/</a></li>
<li>Time Series Analysis, Regression, and Forecasting <a href="https://timeseriesreasoning.com/">https://timeseriesreasoning.com/</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org7c14de4" class="outline-3">
<h3 id="org7c14de4"><span class="section-number-3">8.16.</span> Feature Importance</h3>
<div class="outline-text-3" id="text-8-16">
<ul class="org-ul">
<li>2020 book <a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a></li>
<li><a href="https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined">https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined</a></li>
</ul>

<p>
Нет однозначного ответа.
</p>
<ul class="org-ul">
<li>корреляция с таргетом</li>
<li>Random forest feature importance</li>
<li>NN - impotance путем перестановки значений поочереди в столбцах</li>
</ul>

<p>
Permutation feature importance - для любых моделей, путем перемешивании каждого столбца по очереди.
</p>
</div>
<div id="outline-container-org14643f1" class="outline-4">
<h4 id="org14643f1"><span class="section-number-4">8.16.1.</span> классификационные модели показывающие важность признаков</h4>
<div class="outline-text-4" id="text-8-16-1">
<ul class="org-ul">
<li>Random Forest, DesigionTreeClassification, DesigionTreeRegression</li>
<li><p>
линейная модель с Lasso регуляризацией, склонной обнулять веса слабых признаков
</p>

<p>
p-values, bootstrap scores, various "discriminative indices"
</p></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org949f069" class="outline-3">
<h3 id="org949f069"><span class="section-number-3">8.17.</span> Малое количество данных</h3>
<div class="outline-text-3" id="text-8-17">
<ul class="org-ul">
<li><a href="https://habr.com/en/post/436668/">https://habr.com/en/post/436668/</a></li>
<li><a href="https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89">https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89</a></li>
<li>сглаженные средние значения от целевой переменной <a href="https://www.youtube.com/watch?v=NVKDSNM702k">https://www.youtube.com/watch?v=NVKDSNM702k</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgcc5c8c2" class="outline-3">
<h3 id="orgcc5c8c2"><span class="section-number-3">8.18.</span> Probability Callibration</h3>
<div class="outline-text-3" id="text-8-18">
<ul class="org-ul">
<li><a href="https://scikit-learn.org/stable/modules/calibration.html">https://scikit-learn.org/stable/modules/calibration.html</a></li>
<li>Лекция <a href="#orge2bcaa5">8.4.2.5</a></li>
</ul>
</div>

<div id="outline-container-org868fb0d" class="outline-4">
<h4 id="org868fb0d"><span class="section-number-4">8.18.1.</span> prediction intervals</h4>
<div class="outline-text-4" id="text-8-18-1">
<ul class="org-ul">
<li>confidence and credible intervals <a href="https://www.kaggle.com/shawlu/understanding-credible-interval">https://www.kaggle.com/shawlu/understanding-credible-interval</a></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgbe46cd4"></a>Вычисление credible interval (частотный)<br />
<div class="outline-text-5" id="text-8-18-1-1">
<ul class="org-ul">
<li><a href="https://www.kaggle.com/shawlu/understanding-credible-interval">https://www.kaggle.com/shawlu/understanding-credible-interval</a></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">1 ----------------</span>
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #8ac6f2; font-weight: bold;">import</span> scipy.stats

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">mean_confidence_interval</span>(data, confidence=0.95):
    <span style="color: #cae682;">a</span> = 1.0 * np.array(data)
    <span style="color: #cae682;">n</span> = <span style="color: #e5786d;">len</span>(a)
    <span style="color: #cae682;">m</span>, <span style="color: #cae682;">se</span> = np.mean(a), scipy.stats.sem(a)
    <span style="color: #cae682;">h</span> = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)
    <span style="color: #8ac6f2; font-weight: bold;">return</span> m, m-h, m+h
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">2 ----------------</span>
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np, scipy.stats <span style="color: #8ac6f2; font-weight: bold;">as</span> st
st.t.interval(0.95, <span style="color: #e5786d;">len</span>(a)-1, loc=np.mean(a), scale=st.sem(a))

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">3 ----------------</span>
<span style="color: #8ac6f2; font-weight: bold;">import</span> statsmodels.stats.api <span style="color: #8ac6f2; font-weight: bold;">as</span> sms
sms.DescrStatsW(a).tconfint_mean()

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">4 ----------------</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1052;&#1086;&#1085;&#1077;&#1090;&#1082;&#1072;</span>

</pre>
</div>
</div>
</li>

<li><a id="org443539a"></a><span class="todo TODO">TODO</span> Вычисление confidence interval (баесовый)<br />
<div class="outline-text-5" id="text-8-18-1-2">
<ul class="org-ul">
<li><a href="https://www.kaggle.com/shawlu/understanding-credible-interval">https://www.kaggle.com/shawlu/understanding-credible-interval</a></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">1 ----------------</span>
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #8ac6f2; font-weight: bold;">import</span> scipy.stats

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">mean_confidence_interval</span>(data, confidence=0.95):
    <span style="color: #cae682;">a</span> = 1.0 * np.array(data)
    <span style="color: #cae682;">n</span> = <span style="color: #e5786d;">len</span>(a)
    <span style="color: #cae682;">m</span>, <span style="color: #cae682;">se</span> = np.mean(a), scipy.stats.sem(a)
    <span style="color: #cae682;">h</span> = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)
    <span style="color: #8ac6f2; font-weight: bold;">return</span> m, m-h, m+h
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">2 ----------------</span>
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np, scipy.stats <span style="color: #8ac6f2; font-weight: bold;">as</span> st
st.t.interval(0.95, <span style="color: #e5786d;">len</span>(a)-1, loc=np.mean(a), scale=st.sem(a))

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">3 ----------------</span>
<span style="color: #8ac6f2; font-weight: bold;">import</span> statsmodels.stats.api <span style="color: #8ac6f2; font-weight: bold;">as</span> sms
sms.DescrStatsW(a).tconfint_mean()

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">4 ----------------</span>
</pre>
</div>
</div>
</li>

<li><a id="org85fd56b"></a>quantile loss method<br />
<div class="outline-text-5" id="text-8-18-1-3">
<ul class="org-ul">
<li>0 <a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html">https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html</a></li>
<li>1 <a href="https://towardsdatascience.com/how-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed">https://towardsdatascience.com/how-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed</a>
<ul class="org-ul">
<li><a href="https://github.com/WillKoehrsen/Data-Analysis/tree/master/prediction-intervals">https://github.com/WillKoehrsen/Data-Analysis/tree/master/prediction-intervals</a></li>
</ul></li>
<li>2 <a href="https://medium.com/@qucit/a-simple-technique-to-estimate-prediction-intervals-for-any-regression-model-2dd73f630bcb">https://medium.com/@qucit/a-simple-technique-to-estimate-prediction-intervals-for-any-regression-model-2dd73f630bcb</a></li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-orgf5f391a" class="outline-3">
<h3 id="orgf5f391a"><span class="section-number-3">8.19.</span> Ensembles</h3>
<div class="outline-text-3" id="text-8-19">
<ul class="org-ul">
<li><a href="https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier">https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier</a></li>
<li><a href="https://mlwave.com/kaggle-ensembling-guide/">https://mlwave.com/kaggle-ensembling-guide/</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ensemble_learning">https://en.wikipedia.org/wiki/Ensemble_learning</a></li>
<li>rus article <a href="https://dyakonov.org/2017/03/10/c%D1%82%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3-stacking-%D0%B8-%D0%B1%D0%BB%D0%B5%D0%BD%D0%B4%D0%B8%D0%BD%D0%B3-blending/">https://dyakonov.org/2017/03/10/c%D1%82%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3-stacking-%D0%B8-%D0%B1%D0%BB%D0%B5%D0%BD%D0%B4%D0%B8%D0%BD%D0%B3-blending/</a></li>
<li><img src="https://www.helenkapatsa.ru/content/images/2021/12/image-11.png" alt="image-11.png" /></li>
<li>Performance stop growing when I add more than 4 good models into the ensemble.</li>
<li>it helps to add some mediocre models</li>
</ul>

<p>
decrease the variance of a single estimate
</p>

<p>
Для регрессии ансамблирование происходит посредством уследнения результата каждой модели (Averaging)
</p>

<p>
метапризнаки - предсказания базовых моделей
</p>

<p>
метамодель - предиктор вход которого использует метапризнаки
</p>
</div>


<div id="outline-container-org2c48803" class="outline-4">
<h4 id="org2c48803"><span class="section-number-4">8.19.1.</span> stacking vs bagging vs boosting (old):</h4>
<div class="outline-text-4" id="text-8-19-1">
<ul class="org-ul">
<li>Бэггинг (баггинг, bagging, bootstrap aggregating): параллельное независимое построение моделей на различных
наборах данных с последующим выбором предсказания по результатам голосования моделей(например мажоритарное
голосование majority vote).
<ul class="org-ul">
<li><p>
Стекинг (stacking): построение k моделей базовых учеников (не обязательно одной природы) с дальнейшей
подгонкой модели под метаклассификатор, обучение на одних и тех же данных
</p>
<ul class="org-ul">
<li>Смешивание (blending, блендинг): усреднение прогнозов группы моделей.  multiple different algorithms are</li>
</ul>
<p>
prepared on the training data. uses the held out validation set for that, typically 10% of instances are
used for this purpose. Упрощенная модель стекинга.
</p></li>
</ul></li>
<li>Бустинг (boosting): последовательное построение моделей, при котором каждая модель учится с учетом
результатов предыдущей модели. Чтобы избежать ошибок переобучения, каждая новая модель учится на результатах
всех предыдущих моделей.
<ul class="org-ul">
<li>AdaBoost</li>
</ul></li>
</ul>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">technique</th>
<th scope="col" class="org-left">pros</th>
<th scope="col" class="org-left">cons</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">bagging</td>
<td class="org-left">parallel, lower variance</td>
<td class="org-left">одинаковые модели, глубокие деревья</td>
</tr>

<tr>
<td class="org-left">stacking</td>
<td class="org-left">parallel</td>
<td class="org-left">качество стльно зависит от базовых моделей</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">boosting</td>
<td class="org-left">lower bias смещение</td>
<td class="org-left">плохо параллелится</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">модели уточняют друг-друга, простые базовые</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-org1fd08f1" class="outline-4">
<h4 id="org1fd08f1"><span class="section-number-4">8.19.2.</span> stacking vs bagging vs boosting</h4>
<div class="outline-text-4" id="text-8-19-2">
<ul class="org-ul">
<li>Bagging: Simple voting or averaging of predictions.
<ul class="org-ul">
<li>Bagged Decision Trees (canonical bagging)</li>
<li>Random Forest</li>
<li>Extra Trees</li>
</ul></li>

<li>Stacking: 1. Different machine learning algorithms for each ensemble member. 2. Machine learning model to
learn how to best combine predictions.
<ul class="org-ul">
<li>Stacked Models (canonical stacking)</li>
<li>Blending</li>
<li>Super Ensemble</li>
</ul></li>

<li>Boosting: 1. Bias training data toward those examples that are hard to predict. 2. Combine predictions using
a weighted average of models.
<ul class="org-ul">
<li>AdaBoost (canonical boosting)</li>
<li>Boosting Machines</li>
<li>Gradient Boosting (XGBoost and similar)</li>
</ul></li>
</ul>

<div class="org-src-container">
<pre class="src src-artist">Bagging:
           +----------+
           | Input(X) |
           +----+++---+
              -/ | \-
            -/   |   \-
          -/     |     \-
        -/      /        \
      -/        |         \-
+----V---+ +----V---+ +-----V--+
| Sample1| | Sample2| | Sample3|
+----+---+ +----+---+ +----+---+
     |          |          |
+----V---+ +----V---+ +----V---+
| Tree1  | | Tree2  | | Tree3  |  --- model
+-----+--+ +----+---+ +--+-----+
       \--      |      -/
          \-    |   --/
            \-- | -/
               \+/
           +----V----+
           | Combine |            --- model
           +---------+
                |
           +----V----+
           | Output  |
           +---------+


Stacking:
           +----------+
           | Input(X) |
           +----+++---+
              -/ | \-
            -/   |   \-
          -/     |     \-
        -/      /        \
      -/        |         \-
+----V---+ +----V---+ +-----V--+
| Model1 | | Model2 | | Model3 |
+----+---+ +--------+ +--------+
       \--      |      -/
          \-    |   --/
            \-- | -/
               \+/
           +----V----+
           |  Model  |
           +---------+
                |
           +----V----+
           | Output  |
           +---------+

Boosting:

 +----------+
 | Input(X) |
 +----+-----+
      |
      +-------------+--------------+--------------+
      |             |              |              |
      |        +----v-----+        |              |
 +----v-----+  | Weighted |        |              |
 | Model1   +--&gt; Sample1  |        |              |
 +----+-----+  +----+-----+        |              |
       \            |              |              |
       |            |         +----v-----+        |
        \      +----v-----+   | Weighted |        |
         \     | Model2   +---&gt; Sample2  |        |
         |     +----+-----+   +----+-----+        |
          \         |              |         +----v-----+
           \        |         +----v-----+   | Weighted |
           |        |         | Model3   +---&gt; Sample3  |
            \       |         +----+-----+   +----+-----+
             \      |            -/               |
             |      |          -/                 |
              \    /          /              +----v-----+
               \   |        -/               |   ...    |
               |   |      -/                 +--+-------+
                \  |     /             -------/
                 \ |   -/       ------/
                 | | -/ -------/
                  \|/--/
              +----v-----+
              | Combine  |
              +----+-----+
                   |
              +----v-----+
              |  Output  |
              +----------+


</pre>
</div>
<p>
<a href="https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/">https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/</a>
</p>
</div>
</div>
<div id="outline-container-orgc7f53ee" class="outline-4">
<h4 id="orgc7f53ee"><span class="section-number-4">8.19.3.</span> Stacking</h4>
<div class="outline-text-4" id="text-8-19-3">
<p>
Linear Stacking and Bayes optimal classifier or Stacked Generalization или Stacking - в задаче регрессии их
среднее, а в задаче классификации — голосование по большинству, часто превосходят по качеству все эти
алгоритмы.
</p>

<p>
stacking(5%) - X -&gt; [Y] -&gt; Y  предсказывает основываясь на предсказаниях (предикторы)
</p>
<ol class="org-ol">
<li>тренируются алгоритмы</li>
<li>тренируется обобщающий алгоритм</li>
</ol>

<p>
Обучаем базовые модели на одних фолдах, проверяя на других мы уменьшаем риск переобучения
</p>

<p>
недостатки:
</p>
<ul class="org-ul">
<li>использование разных моделей требует подбирание гиперпараметров под каждый</li>
</ul>

<p>
<b>Blending</b>
</p>
</div>
</div>

<div id="outline-container-orgf005130" class="outline-4">
<h4 id="orgf005130"><span class="section-number-4">8.19.4.</span> bagging (bootstrap aggregation)</h4>
<div class="outline-text-4" id="text-8-19-4">
<p>
bagging trains each model in the ensemble using a randomly drawn subset of the training set.
</p>

<p>
The trick is that each sample of the training dataset is different, giving each classifier that is trained, a
  subtly different focus and perspective on the problem.
</p>

<p>
модели обучаются паралельно!
</p>

<p>
пример:
</p>
<ul class="org-ul">
<li>случайный лес</li>
</ul>
</div>
</div>
<div id="outline-container-org4eee0fb" class="outline-4">
<h4 id="org4eee0fb"><span class="section-number-4">8.19.5.</span> boosting</h4>
<div class="outline-text-4" id="text-8-19-5">
<p>
исходные данные модифицируются каждым алгоритмов в ансамбле
</p>
<ul class="org-ul">
<li>чаще выбираются входные данные показавшие ошибку</li>
<li>добавляются веса</li>
</ul>

<p>
недостатки
</p>
<ul class="org-ul">
<li>модели обучаются последовательно, поэтому используются слабые модели для скорости</li>
</ul>

<p>
пример:
</p>
<ul class="org-ul">
<li>градиентный бустинг над деревьями</li>
</ul>
</div>
</div>
<div id="outline-container-orgf1f0648" class="outline-4">
<h4 id="orgf1f0648"><span class="section-number-4">8.19.6.</span> skillfactory apporach</h4>
<div class="outline-text-4" id="text-8-19-6">
<ol class="org-ol">
<li>bootstarp + bagging</li>
<li>L1, L2, L3, L4 of random features</li>
<li>decision tree 1,2,3,4</li>
<li>мажоритарное голосование</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org26095c8" class="outline-3">
<h3 id="org26095c8"><span class="section-number-3">8.20.</span> Проверка гипотез <a id="orgb310370"></a></h3>
<div class="outline-text-3" id="text-8-20">
<p>
величину (значение) переменной называют <b>статисти́чески зна́чимой</b>, если мала вероятность ее случайного
возникновения или ещё более крайних величин.
</p>

<ol class="org-ol">
<li>Null hypothesis (H0) -  предположение о том, что не существует связи между двумя наблюдаемыми событиями, феноменами
<ul class="org-ul">
<li>augmented Dickey–Fuller test (ADF)</li>
</ul></li>
<li>Альтернатива (H1)</li>
</ol>
</div>
</div>
<div id="outline-container-org5315607" class="outline-3">
<h3 id="org5315607"><span class="section-number-3">8.21.</span> Автокорреляция ACF</h3>
<div class="outline-text-3" id="text-8-21">
<ul class="org-ul">
<li><a href="https://www.coursera.org/lecture/data-analysis-applications/avtokorrieliatsiia-4PEHZ">https://www.coursera.org/lecture/data-analysis-applications/avtokorrieliatsiia-4PEHZ</a></li>
<li><a href="https://yashuseth.blog/2018/01/19/time-series-analysis-forecasting-modelling-arima/">https://yashuseth.blog/2018/01/19/time-series-analysis-forecasting-modelling-arima/</a></li>
</ul>
<p>
Изучаются в:
</p>
<ul class="org-ul">
<li>анализ временных рядов</li>
<li>пространственная эконометрика</li>
</ul>
<p>
Автокорреляция - обычная корреляция Pearson между рядом и его версией сдвинутой на t+лаг
</p>
<ul class="org-ul">
<li>lag 0 - corr = +1</li>
<li>lag 1 - corr = 0.8</li>
<li>автокорреляция шума - слабо коррелированного процесса:
<ul class="org-ul">
<li>имеет один пик lag 0</li>
<li>при малейшем сдвиге corr сразу падает до нуля</li>
</ul></li>
<li>uncorrelated does not necessarily mean random.</li>
</ul>



<p>
Выборочная автокорреляция -
</p>

<p>
<b>Коррелограмма</b> - диаграмма автокорреляционной функции
</p>
</div>
<div id="outline-container-orga3b42bf" class="outline-4">
<h4 id="orga3b42bf"><span class="section-number-4">8.21.1.</span> plotting</h4>
<div class="outline-text-4" id="text-8-21-1">
<p>
<a href="https://stackoverflow.com/questions/36038927/whats-the-difference-between-pandas-acf-and-statsmodel-acf">https://stackoverflow.com/questions/36038927/whats-the-difference-between-pandas-acf-and-statsmodel-acf</a>
</p>
<ul class="org-ul">
<li>pandas.plotting.autocorrelation<sub>plot</sub>(loan<sub>amt.tail</sub>(1000)[::7]) - get every 7 record</li>
<li>statsmodels.graphics.tsaplots.plot<sub>acf</sub></li>
<li>matplotlib.pyplot.acorr(data.astype(float),maxlags=10) # -10, +10
<ul class="org-ul">
<li>detrend: optional parameter.    Default value: mlab.detrend<sub>none</sub>.</li>
<li>normed: True</li>
<li>usevlines: Default value: True.</li>
<li>maxlags:  Default value: 10</li>
<li>linestyle: optional parameter used to plot the data points when usevlines is False.</li>
<li>marker: optional parameter having string value. Default value: ‘o’</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orga678c57" class="outline-4">
<h4 id="orga678c57"><span class="section-number-4">8.21.2.</span> calc</h4>
<div class="outline-text-4" id="text-8-21-2">
<ul class="org-ul">
<li>df['cost<sub>requested</sub>'].autocorr() # lag=1 - Pearson correlation series and shifted self</li>
<li>np.corelate(a,v,mode=) modes:
<ul class="org-ul">
<li>valid -</li>
<li>same -</li>
<li>full - от -len до +len</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org03efe35" class="outline-4">
<h4 id="org03efe35"><span class="section-number-4">8.21.3.</span> похожие понятия</h4>
<div class="outline-text-4" id="text-8-21-3">
<ul class="org-ul">
<li>взаимно-корреляционная функция</li>
<li>cross-correlation - measure of similarity of two series as a function of the displacement of one relative to the other</li>
<li>convolution - mathematical operation on two functions (f and g) that produces a third function (f*g) that
expresses how the shape of one is modified by the other.</li>
<li>Partial Autocorrelation Function (PACF)</li>
<li><b>partial correlation</b> measures the degree of association between two random variables, with the effect of a set of controlling random variables removed</li>
</ul>
</div>
</div>
<div id="outline-container-org027ddd5" class="outline-4">
<h4 id="org027ddd5"><span class="section-number-4">8.21.4.</span> &#x2013; СРАВНЕНИЕ СПОСОБОВ &#x2013; <a href="https://stackoverflow.com/questions/643699/how-can-i-use-numpy-correlate-to-do-autocorrelation">https://stackoverflow.com/questions/643699/how-can-i-use-numpy-correlate-to-do-autocorrelation</a></h4>
<div class="outline-text-4" id="text-8-21-4">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy
<span style="color: #8ac6f2; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">autocorr1</span>(x,lags):
    <span style="color: #f08080; font-style: italic;">'''numpy.corrcoef, partial'''</span>

    <span style="color: #cae682;">corr</span>=[1. <span style="color: #8ac6f2; font-weight: bold;">if</span> l==0 <span style="color: #8ac6f2; font-weight: bold;">else</span> numpy.corrcoef(x[l:],x[:-l])[0][1] <span style="color: #8ac6f2; font-weight: bold;">for</span> l <span style="color: #8ac6f2; font-weight: bold;">in</span> lags]
    <span style="color: #8ac6f2; font-weight: bold;">return</span> numpy.array(corr)

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">autocorr2</span>(x,lags):
    <span style="color: #f08080; font-style: italic;">'''manualy compute, non partial'''</span>

    <span style="color: #cae682;">mean</span>=numpy.mean(x)
    <span style="color: #cae682;">var</span>=numpy.var(x)
    <span style="color: #cae682;">xp</span>=x-mean
    <span style="color: #cae682;">corr</span>=[1. <span style="color: #8ac6f2; font-weight: bold;">if</span> l==0 <span style="color: #8ac6f2; font-weight: bold;">else</span> numpy.<span style="color: #e5786d;">sum</span>(xp[l:]*xp[:-l])/<span style="color: #e5786d;">len</span>(x)/var <span style="color: #8ac6f2; font-weight: bold;">for</span> l <span style="color: #8ac6f2; font-weight: bold;">in</span> lags]

    <span style="color: #8ac6f2; font-weight: bold;">return</span> numpy.array(corr)

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">autocorr3</span>(x,lags):
    <span style="color: #f08080; font-style: italic;">'''fft, pad 0s, non partial'''</span>

    <span style="color: #cae682;">n</span>=<span style="color: #e5786d;">len</span>(x)
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">pad 0s to 2n-1</span>
    <span style="color: #cae682;">ext_size</span>=2*n-1
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">nearest power of 2</span>
    <span style="color: #cae682;">fsize</span>=2**numpy.ceil(numpy.log2(ext_size)).astype(<span style="color: #95e454;">'int'</span>)

    <span style="color: #cae682;">xp</span>=x-numpy.mean(x)
    <span style="color: #cae682;">var</span>=numpy.var(x)

    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">do fft and ifft</span>
    <span style="color: #cae682;">cf</span>=numpy.fft.fft(xp,fsize)
    <span style="color: #cae682;">sf</span>=cf.conjugate()*cf
    <span style="color: #cae682;">corr</span>=numpy.fft.ifft(sf).real
    <span style="color: #cae682;">corr</span>=corr/var/n

    <span style="color: #8ac6f2; font-weight: bold;">return</span> corr[:<span style="color: #e5786d;">len</span>(lags)]

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">autocorr4</span>(x,lags):
    <span style="color: #f08080; font-style: italic;">'''fft, don't pad 0s, non partial'''</span>
    <span style="color: #cae682;">mean</span>=x.mean()
    <span style="color: #cae682;">var</span>=numpy.var(x)
    <span style="color: #cae682;">xp</span>=x-mean

    <span style="color: #cae682;">cf</span>=numpy.fft.fft(xp)
    <span style="color: #cae682;">sf</span>=cf.conjugate()*cf
    <span style="color: #cae682;">corr</span>=numpy.fft.ifft(sf).real/var/<span style="color: #e5786d;">len</span>(x)

    <span style="color: #8ac6f2; font-weight: bold;">return</span> corr[:<span style="color: #e5786d;">len</span>(lags)]

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">autocorr5</span>(x,lags):
    <span style="color: #f08080; font-style: italic;">'''numpy.correlate, non partial'''</span>
    <span style="color: #cae682;">mean</span>=x.mean()
    <span style="color: #cae682;">var</span>=numpy.var(x)
    <span style="color: #cae682;">xp</span>=x-mean
    <span style="color: #cae682;">corr</span>=numpy.correlate(xp,xp,<span style="color: #95e454;">'full'</span>)[<span style="color: #e5786d;">len</span>(x)-1:]/var/<span style="color: #e5786d;">len</span>(x)

    <span style="color: #8ac6f2; font-weight: bold;">return</span> corr[:<span style="color: #e5786d;">len</span>(lags)]


<span style="color: #8ac6f2; font-weight: bold;">if</span> <span style="color: #e5786d;">__name__</span>==<span style="color: #95e454;">'__main__'</span>:

    <span style="color: #cae682;">y</span>=[28,28,26,19,16,24,26,24,24,29,29,27,31,26,38,23,13,14,28,19,19,\
            17,22,2,4,5,7,8,14,14,23]
    <span style="color: #cae682;">y</span>=numpy.array(y).astype(<span style="color: #95e454;">'float'</span>)

    <span style="color: #cae682;">lags</span>=<span style="color: #e5786d;">range</span>(15)
    <span style="color: #cae682;">fig</span>,<span style="color: #cae682;">ax</span>=plt.subplots()

    <span style="color: #8ac6f2; font-weight: bold;">for</span> funcii, labelii <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">zip</span>([autocorr1, autocorr2, autocorr3, autocorr4,
        autocorr5], [<span style="color: #95e454;">'np.corrcoef, partial'</span>, <span style="color: #95e454;">'manual, non-partial'</span>,
            <span style="color: #95e454;">'fft, pad 0s, non-partial'</span>, <span style="color: #95e454;">'fft, no padding, non-partial'</span>,
            <span style="color: #95e454;">'np.correlate, non-partial'</span>]):

        <span style="color: #cae682;">cii</span>=funcii(y,lags)
        <span style="color: #e5786d;">print</span>(labelii)
        <span style="color: #e5786d;">print</span>(cii)
        ax.plot(lags,cii,label=labelii)

    ax.set_xlabel(<span style="color: #95e454;">'lag'</span>)
    ax.set_ylabel(<span style="color: #95e454;">'correlation coefficient'</span>)
    ax.legend()
    plt.show()

</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org5b08aa5" class="outline-3">
<h3 id="org5b08aa5"><span class="section-number-3">8.22.</span> Оптимизацинные задачи Mathematical Optimization Математическое программирование</h3>
<div class="outline-text-3" id="text-8-22">
</div>
<div id="outline-container-orgda96a74" class="outline-4">
<h4 id="orgda96a74"><span class="section-number-4">8.22.1.</span> definition</h4>
<div class="outline-text-4" id="text-8-22-1">
<p>
задача оптимизации сводится к нахождению экстремума целевой функции
</p>

<p>
The constraints of the problem can be used directly in producing the optimal solutions. There are algorithms
 that can solve any problem in this category, such as the popular simplex algorithm.
</p>

<p>
If a problem additionally requires that one or more of the unknowns must be an integer then it is classified
 in <b>integer programming</b> or <b>integer linear programs</b>.
</p>

<p>
A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer
 values are superficial, i.e., the solutions satisfy these restrictions anyway.
</p>

<p>
In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used,
 depending on the difficulty of the problem.
</p>

<p>
решается:
</p>
<ul class="org-ul">
<li>эвристический алгоритм - heuristic (from Greek εὑρίσκω "I find, discover") is a technique designed for
solving a problem more quickly when classic methods are too slow, or for finding an approximate solution
when classic methods fail to find any exact solution
<ul class="org-ul">
<li>Градиентный спуск gradient descent</li>
<li>имитации отжига Simulated annealing [əˈnēl] -</li>
<li>genetic algorithm - maintain a pool of solutions rather than just one. New candidate solutions are
generated not only by "mutation" (as in SA), but also by "recombination" of two solutions from the pool.</li>
<li>Simulated annealing [əˈnēl] - better than gradient descent, but more time consuming</li>
<li>Quantum annealing - will usually give better results, it will have problems finding global minimum
surrounded by large area of high values, because if it does not hit the small low area early, it won't get
there after the parameter decreases.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org20c0270" class="outline-4">
<h4 id="org20c0270"><span class="section-number-4">8.22.2.</span> terms</h4>
<div class="outline-text-4" id="text-8-22-2">
<ul class="org-ul">
<li>y - Критерием оптимальности, на основании его составляется целевая функция</li>
<li>целевая цункция <b>objective function</b> f(x) which output you are trying to min or max</li>
<li>variables x1,x2&#x2026;</li>
<li>constaints - how big and small some variables may be</li>
<li>the <b>feasible region</b> defined by all values of x such that A x ≤ b and ∀ i , x i ≥ 0 is a (possibly
unbounded) convex polytope.</li>
<li>basic feasible solution (BFS) - An extreme point or vertex of this polytope.</li>
</ul>
</div>
</div>

<div id="outline-container-orgd24fe3d" class="outline-4">
<h4 id="orgd24fe3d"><span class="section-number-4">8.22.3.</span> problem forms</h4>
<div class="outline-text-4" id="text-8-22-3">
</div>
<ol class="org-ol">
<li><a id="orgd495497"></a>problem - canonical form<br />
<div class="outline-text-5" id="text-8-22-3-1">
<p>
Find a vector x that maximizes cT*x
</p>

<p>
subject to A*x &lt;= b and x &gt;= 0
</p>
</div>
</li>
<li><a id="orgc40929e"></a>problem - standard form<br />
<div class="outline-text-5" id="text-8-22-3-2">
<p>
Linear function to be maximized:
</p>
<ul class="org-ul">
<li>f(x1, x2) = c1*x1 + c2*x2</li>
</ul>
<p>
Problem constraints:
</p>
<ul class="org-ul">
<li>a11*x1 + a12*x2 &lt;= b1</li>
<li>a21*x1 + a22*x2 &lt;= b2</li>
<li>a31*x1 + a32*x2 &lt;= b3</li>
</ul>
<p>
Non-negative variables:
</p>
<ul class="org-ul">
<li>x1 &gt;= 0</li>
<li>x2 &gt;= 0</li>
</ul>
<p>
Problem:
</p>
<ul class="org-ul">
<li>max{ cTx | x ∈ Rn ^ A*x&lt;=b ^ x&gt;=0 }</li>
</ul>
</div>
</li>

<li><a id="org561f0b5"></a>constrains inequalities to equalities and "standrad maximum form"<br />
<div class="outline-text-5" id="text-8-22-3-3">
<p>
lets:
</p>
<pre class="example">
f = x1 + 2*x2
15*x1 + 10*x2 &lt;= 1200
1*x1 + 2*x2 &lt;= 120
x1, x2 &gt;=0
</pre>


<pre class="example">
15*x1 + 10*x2 &lt;= 1200
</pre>

<p>
difference bettween 15*x1 + 10*x2 and 1200 will be "slack variable" x3
</p>
<pre class="example">
15*x1 + 10*x2 + x3 = 1200
1*x1 + 2*x2 + x4 = 120
x1, x2 &gt;=0  - not changed
-x1 - 2*x + f = 0
</pre>

<p>
it is <b>standrad maximum form</b>:
</p>
<ul class="org-ul">
<li>the objective fuction is to be maximized, so the leading coefficients are
negative in the matrix</li>
<li>the constraints are all &lt;=, resulting in positive coefficients for slack variables</li>
</ul>
</div>
</li>
<li><a id="org1f8de6c"></a>problem - tableau ['tæbləu] form (живая картина)<br />
<div class="outline-text-5" id="text-8-22-3-4">
<pre class="example">
[ 1 -cT 0 ]
[ 0  A  b ]
</pre>

<p>
for problem above in simplex tableu:
</p>
<pre class="example">
  x1 x2 x3 x4 f   ans
[ 15 10 1  0  0  1200 ]
[  1  2 0  1  0   120 ]
[ -1 -2 0  0  1     0 ]
</pre>


<p>
basic variables: x3 and x4, objective fuction is f
</p>
</div>
</li>
<li><a id="org5089006"></a>linear constraint standard format<br />
<div class="outline-text-5" id="text-8-22-3-5">
<ul class="org-ul">
<li>x0   + 2*x1 &lt;= 1</li>
<li>2*x0 +   x1  = 1</li>
</ul>
<pre class="example">
-∞ &lt;= 1 2 &lt;= 1
1     2 1    1
</pre>
</div>
</li>
</ol>
</div>
<div id="outline-container-org40a47db" class="outline-4">
<h4 id="org40a47db"><span class="section-number-4">8.22.4.</span> <span class="todo TODO">TODO</span> simplex algorithm</h4>
<div class="outline-text-4" id="text-8-22-4">
<pre class="example">
Z = -2*x - 3*y - 4*z minimize
</pre>

<p>
subject to:
</p>
<pre class="example">
3*x + 2*y + z &lt;= 10
2*x + 5*y + 3*z &lt;= 15
x,y,z &gt;= 0
</pre>


<p>
canonical tableau:
</p>
<pre class="example">
[ 1 2 3 4 0 0 0  ]
[ 0 3 2 1 1 0 10 ]
[ 0 2 5 3 0 1 15 ]
</pre>


<p>
slack variables s and t, column 5 and 6, basic feasible solution:
</p>
<pre class="example">
x = y = z = 0, s = 10, t = 15
</pre>


<p>
Simplex method:
</p>
<ol class="org-ol">
<li>Convert a word problem into inequality constraints and an objective fuction.</li>
<li>Add slack variables, convert the objective function and build an initial tableau.</li>
<li>Choose a pivot.</li>
<li>Pivot operation. (поворотная операция)</li>
<li>Repeat steps 3 and 4 until done.</li>
</ol>
</div>
</div>

<div id="outline-container-org4b90148" class="outline-4">
<h4 id="org4b90148"><span class="section-number-4">8.22.5.</span> <span class="todo TODO">TODO</span> branch and bound</h4>
<div class="outline-text-4" id="text-8-22-5">
<p>
solving discrete, combinatorial and math op problems by braking them into smaller and using bounded function.
</p>
</div>
</div>
<div id="outline-container-org30d0c3e" class="outline-4">
<h4 id="org30d0c3e"><span class="section-number-4">8.22.6.</span> good known problems</h4>
<div class="outline-text-4" id="text-8-22-6">
</div>
<ol class="org-ol">
<li><a id="org49aea69"></a>combinatorial optimization<br />
<div class="outline-text-5" id="text-8-22-6-1">
<p>
In many such problems, such as the ones previously mentioned, <b>exhaustive search is not tractable</b>, and so
 specialized algorithms that quickly rule out large parts of the search space or approximation algorithms must
 be resorted to instead.
</p>
<ul class="org-ul">
<li>exhaustive search is not tractable - исчерпывающий поиск невозможен</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgb3aa0d9"></a>Knapsack problem ['næpsæk] рюкзак<br />
<div class="outline-text-6" id="text-8-22-6-1-1">
<p>
combinatorial optimization
</p>
</div>
<ol class="org-ol">
<li><a id="orgb04d0a6"></a>0-1 knapsack problem<br />
<div class="outline-text-7" id="text-8-22-6-1-1-1">
<p>
Which restricts the number xi of copies of each kind of item to zero or one.
</p>
<ul class="org-ul">
<li>W - maximum weight capacity</li>
<li>n - items numbered from 1 up to n. each with weight wi and a value 𝞾i.</li>
</ul>

<p>
maximize: (i=1..n)∑n𝞾i*xi
</p>

<p>
subject to: ∑wi*xi &lt;= W and xi ∈ Z, xi &gt;= 0
</p>

<p>
types:
</p>
<ul class="org-ul">
<li><b>weakly NP-complete</b> -  If the weights and profits are given as integers</li>
<li><b>strongly NP-complete</b> - if the weights and profits are given as rational numbers.</li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org7df56b2"></a>Change-making problem<br />
<div class="outline-text-6" id="text-8-22-6-1-2">
<p>
finding the minimum number of coins (of certain denominations) that add up to a given amount of money.
</p>

<p>
It is a special case of the integer <b>knapsack problem</b>.
</p>
</div>
</li>
<li><a id="orga1fafe4"></a>Partition problem or number partitioning<br />
<div class="outline-text-6" id="text-8-22-6-1-3">
<p>
Special case of change-making problem.
</p>

<p>
Deciding whether a given multiset S of positive integers can be partitioned into two subsets S1 and S2 such
 that the sum of the numbers in S1 equals the sum of the numbers in S2 (sum(S1) == sum(S2)).
</p>

<p>
multiset - allows for multiple instances for each of its elements.
</p>
</div>
</li>
<li><a id="orgc11793a"></a>travelling salesman problem ("TSP")<br /></li>
<li><a id="org86f348f"></a>minimum spanning tree problem ("MST")<br /></li>
</ol>
</li>
<li><a id="orga6ca386"></a>Cutting stock problem<br /></li>

<li><a id="org0a9ca14"></a>Packing problems<br />
<div class="outline-text-5" id="text-8-22-6-3">
<p>
Bin packing problem: items of different sizes must be packed into a finite number of bins or containers, each
 of a fixed given capacity.
</p>

<p>
Subclass or form of Cutting stock problem.
</p>
</div>
</li>
<li><a id="orgb3c6591"></a>Covering problems<br />
<div class="outline-text-5" id="text-8-22-6-4">
<p>
ask whether a certain combinatorial structure 'covers' another, or how large the structure has to be to do
 that
</p>
</div>
</li>
<li><a id="org1518771"></a>Combinatorial auction (multi-lot auction)<br />
<div class="outline-text-5" id="text-8-22-6-5">
<p>
special case of <b>Smart market</b>
</p>
</div>
</li>

<li><a id="orgc75e5f1"></a><span class="todo TODO">TODO</span> suffix trees<br /></li>
<li><a id="org2ec02af"></a>Generalized assignment problem<br /></li>
<li><a id="org94f3e6d"></a>classic assignment problem<br />
<div class="outline-text-5" id="text-8-22-6-8">
<p>
subclass of Generalized assignment problem
</p>
</div>
</li>
<li><a id="org8077637"></a>Weapon target assignment problem<br />
<div class="outline-text-5" id="text-8-22-6-9">
<p>
finding an optimal assignment of a set of weapons of various types to a set of targets in order to maximize
 the total expected damage done to the opponent.
</p>

<p>
There are a number of weapons and a number of targets. The weapons Wi are of type i = 1 , … , m. Targets Vj
 are j = 1 , … , n. Any of the weapons can be assigned to any target. Each weapon type has a certain
 probability of destroying each target, given by p<sub>ij</sub>.
</p>

<p>
Notice that as opposed to the classic assignment problem or the generalized assignment problem, more than one
 agent (i.e., weapon) can be assigned to each task (i.e., target) and not all targets are required to have
 weapons assigned.
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orge543d94" class="outline-4">
<h4 id="orge543d94"><span class="section-number-4">8.22.7.</span> Optimization with Calculus</h4>
<div class="outline-text-4" id="text-8-22-7">
</div>
<ol class="org-ol">
<li><a id="org0428d3a"></a><span class="todo TODO">TODO</span> finding function zeroes(root, x-intercept or solution). Newton's method.<br /></li>
<li><a id="org97f62dd"></a><span class="todo TODO">TODO</span> guessing at the limiting slope. finding it with derivatives<br /></li>
<li><a id="org4223d03"></a><span class="todo TODO">TODO</span> finding maximum and minimum values (without referencing or second derivatives)<br /></li>
</ol>
</div>
<div id="outline-container-org676b709" class="outline-4">
<h4 id="org676b709"><span class="section-number-4">8.22.8.</span> имитация отжига</h4>
<div class="outline-text-4" id="text-8-22-8">
<p>
<a href="https://habr.com/ru/post/209610/">https://habr.com/ru/post/209610/</a>
</p>

<p>
Нужно определить функции
</p>
<ul class="org-ul">
<li>E:S -&gt; R   S - состояния</li>
<li>T:N -&gt; R   N - номер итарации - убывающая функция изменения температуры</li>
<li>F:S -&gt; S   - порождающая новое состояние-кандидат</li>
</ul>

<p>
алгоритм
</p>
<ol class="org-ol">
<li>На входе: минимальная температура tmin, начальная температура tmax</li>
<li>Задаём произвольное первое состояние s1</li>
<li>Пока ti&gt;tmin
<ol class="org-ol">
<li>S = F(s)</li>
<li>diff E = E(s) - E(s-1)</li>
<li>Если diff E&lt;=0 , тогда состояние остается</li>
<li>Иначе переходим в новое состояние с вероятностью P(diff E, ti)</li>
<li>Понижаем температуру ti=T(i)</li>
</ol></li>
<li>Возвращаем последнее состояние s</li>
</ol>
</div>
</div>

<div id="outline-container-org0fb5efe" class="outline-4">
<h4 id="org0fb5efe"><span class="section-number-4">8.22.9.</span> Linerization</h4>
<div class="outline-text-4" id="text-8-22-9">
<p>
<b>Linearize the relationships: Transforming non-linear optimization tasks to linear one.</b>
</p>
<ul class="org-ul">
<li>Logarithmic Transformations - used For certain types of non-linear models, such as exponential or power models.
<ul class="org-ul">
<li>Exponential Models: ex: y=a*b<sup>cx</sup>, logarithm of both sides, =&gt; ln(y) = ln(a) + c*x</li>
<li>Power Models: ex: y=a*b<sup>cx</sup> =&gt; ln(y) = ln(a) + b*ln(x)</li>
</ul></li>
<li>decomposition: breaking down complex non-linear problems into simpler, more manageable parts that can be handled using linear techniques.
<ul class="org-ul">
<li>piecewise linear approximation: Divide the domain of the non-linear function into several intervals and
approximate the function within each interval using a linear segment.</li>
<li>Dynamic Mode Decomposition (DMD)</li>
<li>Decomposition can also involve creating new features that are linear combinations of the original
variables.</li>
<li>big-M method: for products, when y(j) = G*Z(j)</li>
</ul></li>
<li>Deep Learning and Koopman Operator(advanced): find linear embeddings of non-linear dynamics.</li>
<li>Linear Programming Relaxations: L1 or L<sub>inf</sub> norms can be transformed into linear programming problems by
introducing additional variables and constraints. For example, minimizing the L1 norm can be formulated as a
linear program by introducing slack variables.</li>
<li>simplex method for linear programming</li>
<li>applying transformations to the data to make non-linear relationships more linear</li>
</ul>

<p>
Dynamic Mode Decomposition (DMD) - identifies the dominant modes (eigenvalues and eigenvectors) of the
 system's dynamics, allowing for a reduced-order model that captures the essential behavior of the
 system. This can be combined with Koopman operator theory to linearize the system.
</p>
</div>
</div>
<div id="outline-container-org867a05b" class="outline-4">
<h4 id="org867a05b"><span class="section-number-4">8.22.10.</span> course</h4>
<div class="outline-text-4" id="text-8-22-10">
<p>
x<sub>ij</sub> - сколько забирается со i склада клиенту j
f = ∑{i,j} cost<sub>ij</sub> * x<sub>ij</sub>
</p>

<p>
Для каждого склада количество взятых предметов должно быть меньше, чем на складе:
</p>

<p>
\[\forall i: \sum_j x_{ij} \leq stock_i\]
</p>

<p>
Для каждого клиента количество приобретаемых товаров должно быть больше на единицу, чем спрос:
</p>

<p>
\[\forall j: \sum_i x_{ij} \geq demand_j\]
</p>

<p>
Который также:
</p>

<p>
\[\forall j: - \sum_i x_{ij} \leq -demand_j\]
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> scipy.optimize <span style="color: #8ac6f2; font-weight: bold;">import</span> linprog
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #cae682;">cost</span> = np.array([ <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1094;&#1077;&#1085;&#1099;</span>
    [2, 5, 3], <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">1 &#1089;&#1082;&#1083;&#1072;&#1076; - 1 2 3 &#1082;&#1083;&#1080;&#1077;&#1085;&#1090;</span>
    [7, 7, 6] <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">2 &#1089;&#1082;&#1083;&#1072;&#1076; - 1 2 3 &#1082;&#1083;&#1080;&#1077;&#1085;&#1090;</span>
])
<span style="color: #cae682;">stock</span> = np.array([180,
                  220]) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1085;&#1072;&#1083;&#1080;&#1095;&#1080;&#1077; &#1088;&#1077;&#1089;&#1091;&#1088;&#1089;&#1086;&#1074; &#1085;&#1072; &#1089;&#1082;&#1083;&#1072;&#1076;&#1077; 1 &#1080; 2</span>
<span style="color: #cae682;">demand</span> = np.array([110, 150, 140]) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1082;&#1083;&#1080;&#1077;&#1085;&#1090;&#1072;&#1084; &#1090;&#1088;&#1077;&#1073;&#1091;&#1077;&#1090;&#1089;&#1103; &#1088;&#1077;&#1089;&#1091;&#1088;&#1089;&#1086;&#1074;</span>
<span style="color: #cae682;">num_warehouse</span> = 2
<span style="color: #cae682;">num_clients</span> = 3
</pre>
</div>


<div class="org-src-container">
<pre class="src src-python">
<span style="color: #cae682;">A</span> = []
<span style="color: #cae682;">b</span> = []
<span style="color: #8ac6f2; font-weight: bold;">for</span> i <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(0, num_warehouse):
    A.append([0] * (num_clients * i) + [1] * num_clients + [0] * (num_clients * (num_warehouse - i - 1)))
    b.append(stock[i])
<span style="color: #cae682;">A</span> = np.asarray(A)
<span style="color: #cae682;">b</span> = np.asarray(b)
<span style="color: #e5786d;">print</span>(A)
<span style="color: #e5786d;">print</span>(b)

<span style="color: #cae682;">A</span> = A.tolist()
<span style="color: #cae682;">b</span> = b.tolist()
<span style="color: #8ac6f2; font-weight: bold;">for</span> j <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(0, num_clients):
    A.append(([0] * j + [-1] + [0] * (num_clients - j - 1)) * num_warehouse)
    b.append(-demand[j])
<span style="color: #cae682;">A</span> = np.asarray(A)
<span style="color: #cae682;">b</span> = np.asarray(b)

<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"A"</span>, A)
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"b"</span>, b)
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"c"</span>, c)

<span style="color: #e5786d;">print</span>(linprog(c=c, A_ub=A, b_ub=b))

</pre>
</div>

<pre class="example" id="orgc3925e4">
[[1 1 1 0 0 0]
 [0 0 0 1 1 1]]
[180 220]
A [[ 1  1  1  0  0  0]
 [ 0  0  0  1  1  1]
 [-1  0  0 -1  0  0]
 [ 0 -1  0  0 -1  0]
 [ 0  0 -1  0  0 -1]]
b [ 180  220 -110 -150 -140]
c [2 5 3 7 7 6]
        message: Optimization terminated successfully. (HiGHS Status 7: Optimal)
        success: True
         status: 0
            fun: 1900.0
              x: [ 1.100e+02  0.000e+00  7.000e+01  0.000e+00  1.500e+02
                   7.000e+01]
            nit: 5
          lower:  residual: [ 1.100e+02  0.000e+00  7.000e+01  0.000e+00
                              1.500e+02  7.000e+01]
                 marginals: [ 0.000e+00  1.000e+00  0.000e+00  2.000e+00
                              0.000e+00  0.000e+00]
          upper:  residual: [       inf        inf        inf        inf
                                    inf        inf]
                 marginals: [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00
                              0.000e+00  0.000e+00]
          eqlin:  residual: []
                 marginals: []
        ineqlin:  residual: [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00
                              0.000e+00]
                 marginals: [-3.000e+00 -0.000e+00 -5.000e+00 -7.000e+00
                             -6.000e+00]
 mip_node_count: 0
 mip_dual_bound: 0.0
        mip_gap: 0.0
</pre>



<p>
```stdout
[[1 1 1 0 0 0]
 [0 0 0 1 1 1]]
[180 220]
</p>



<p>
Ответ: 110 единиц со склада 1 клиенту 1, 0 единиц со склада 1 клиенту 2, 70 единиц со склада 1 клиенту 3
0 единиц со склада 2 клиенту 1, 150 единиц со склада 2 клиенту 2, 70 единиц со склада 2 клиенту 3
</p>
</div>
</div>

<div id="outline-container-org12a8e4d" class="outline-4">
<h4 id="org12a8e4d"><span class="section-number-4">8.22.11.</span> scipy</h4>
<div class="outline-text-4" id="text-8-22-11">
</div>
<ol class="org-ol">
<li><a id="orgee8d237"></a>Unconstrained minimization of multivariate scalar functions (minimize)<br />
<div class="outline-text-5" id="text-8-22-11-1">
<p>
Objective functions in scipy.optimize expect a numpy array as their first parameter which is to be optimized
 and must return a float value.
</p>
<ul class="org-ul">
<li>f(x, *args) where x represents a numpy array and args a tuple of additional arguments supplied to the objective function.</li>
</ul>
</div>
</li>

<li><a id="org36f2637"></a>Constrained minimization of multivariate scalar functions (minimize)<br /></li>
<li><a id="orgfb99c2d"></a>Global optimization<br />
<div class="outline-text-5" id="text-8-22-11-3">
<p>
finding global minima or maxima of a function (usually described as a minimization problem) (f = (-1) * g)
</p>
</div>
</li>
<li><a id="org152ebdb"></a>Least-squares minimization (least<sub>squares</sub>)<br /></li>
<li><a id="org1e54fd2"></a>Univariate function minimizers (minimize<sub>scalar</sub>)<br /></li>
<li><a id="orgb97a48b"></a>Custom minimizers<br /></li>
<li><a id="org9b84a3c"></a>Root finding<br /></li>
<li><a id="org45aaf69"></a>Linear programming (linprog)<br /></li>
<li><a id="org40cf534"></a>Assignment problems<br /></li>
</ol>
</div>

<div id="outline-container-org7d4a266" class="outline-4">
<h4 id="org7d4a266"><span class="section-number-4">8.22.12.</span> links</h4>
<div class="outline-text-4" id="text-8-22-12">
<ul class="org-ul">
<li><a href="https://web.stanford.edu/group/sisl/k12/optimization/#!index.md">https://web.stanford.edu/group/sisl/k12/optimization/#!index.md</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9700710" class="outline-3">
<h3 id="org9700710"><span class="section-number-3">8.23.</span> Optimization algorithms</h3>
<div class="outline-text-3" id="text-8-23">
<p>
Optimization algorithms tend to be iterative procedures. Generate trial solutions that converge to a
 “solution”.
</p>
<ul class="org-ul">
<li>Deterministic Algorithm</li>
<li>Randomized Algorithm</li>
</ul>

<p>
types by complexity and speed:
</p>
<ul class="org-ul">
<li>Finite versus infinite convergence. For some classes of optimization problems there are algorithms that
obtain an exact solution—or detect the unboundedness–in a finite number of iterations</li>
<li>Polynomial-time versus exponential-time. The solution time grows, in the worst-case, as a function of
problem sizes (number of variables, constraints, accuracy, etc.)</li>
<li>Convergence order and rate: arithmetically, geometrically or linearly, quadratically.</li>
</ul>

<p>
<b>Algorithm Classes</b> depending on information of the problem being used to create a new iterate:
</p>
<dl class="org-dl">
<dt>Zero-order</dt><dd>when the gradient and Hessian information are difficult to obtain, e.g., no explicit function
forms are given, functions are not differentiable, etc.</dd>
<dt>First-order</dt><dd>large scale data optimization with low accuracy requirement. good for Machine Learning,
Statistical Predictions.</dd>
<dt>Second-order</dt><dd>Popular for optimization problems with high accuracy need, e.g., some</dd>
</dl>
<p>
scientific computing, etc.
</p>


<p>
<a href="https://web.stanford.edu/class/msande311/lecture09.pdf">https://web.stanford.edu/class/msande311/lecture09.pdf</a>
</p>
</div>
</div>
<div id="outline-container-org0dd4c9e" class="outline-3">
<h3 id="org0dd4c9e"><span class="section-number-3">8.24.</span> виды графиков</h3>
<div class="outline-text-3" id="text-8-24">
<ul class="org-ul">
<li><b>Line chart</b> [ʧɑːt]
<ul class="org-ul">
<li><b>Scree plot</b> (skriː) [plɒt]  - Улучшенная Дендрограмма для иерархической кластирезации</li>
<li>graph of a function</li>
</ul></li>
<li><b>Scatter plot</b> [ˈskætə] Диаграмма рассеяния - для демонстрации наличия или отсутствия корреляции между двумя
переменными.
<ul class="org-ul">
<li>2D Histogram - температура скопления</li>
</ul></li>
<li><b>pie chart</b> - кусочки</li>
<li><b>bar plot</b> or <b>chart</b> - Столбчатая диаграмма</li>
<li><b>гистограмма</b> x-зачения y - количество таких значений
<ul class="org-ul">
<li>по группам - данные разбиваются на группы и для каждой рисуется гистограмма</li>
<li>kdeplot - проксимация линией</li>
</ul></li>
<li><b>Box plot</b>, <b>Ящиковая диаграмма</b>, Ящики с усами - свеча от quantile 1 - quantile 3, median = quantile 2. Толщина
не имеет значения.</li>
<li>Q–Q plot or <b>Probability plot</b> - comparing two probability distributions - plotting their quantiles against
each other or agains normal distribution.</li>
<li><b>AUC</b> ROC Curve</li>
<li>Временные:
<ul class="org-ul">
<li><b>ACF</b> - x - лаг, y - корреляция</li>
<li>PACF statsmodels</li>
</ul></li>
<li><b>Correlation Matrix</b> with Heatmap</li>
<li><b>Scatter matrix</b></li>
<li><b>Partial Dependence Plots</b> PDP - shows the marginal effect one or two features have on the predicted outcome
of a machine learning model</li>
<li><b>individual conditional expectation (ICE)</b> plot - like PDP but visualizes the dependence of the prediction
on a feature for each sample separately with one line per sample</li>
</ul>
</div>

<div id="outline-container-org67e9391" class="outline-4">
<h4 id="org67e9391"><span class="section-number-4">8.24.1.</span> простые линейные графики с описанием</h4>
<div class="outline-text-4" id="text-8-24-1">
<p>
from matplotlib import pyplot as plt
plt.plot(list(n<sub>m</sub>), gmm<sub>model</sub><sub>comparision</sub>['AIC'], label='AIC')
plt.plot(list(n<sub>m</sub>), gmm<sub>model</sub><sub>comparision</sub>['BIC'], label='BIC')
plt.legend()
plt.gca().set(xlabel='число кластеров', ylabel='оценка модели')
plt.show()
</p>
</div>
</div>
<div id="outline-container-orgae0ee2b" class="outline-4">
<h4 id="orgae0ee2b"><span class="section-number-4">8.24.2.</span> форматирование axis</h4>
<div class="outline-text-4" id="text-8-24-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> matplotlib.ticker <span style="color: #8ac6f2; font-weight: bold;">import</span> FuncFormatter

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">millions</span>(x, pos):
    <span style="color: #8ac6f2; font-weight: bold;">return</span> <span style="color: #95e454;">'%1.1fM'</span> % (x * 1e-6) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">remove 6 digits</span>

<span style="color: #cae682;">formatter</span> = FuncFormatter(millions)
<span style="color: #cae682;">a</span> = df.groupby(<span style="color: #95e454;">'education'</span>)[<span style="color: #95e454;">'cost_requested'</span>].plot.hist()
a[0].xaxis.set_major_formatter(formatter)
</pre>
</div>
</div>
</div>
<div id="outline-container-org15756c8" class="outline-4">
<h4 id="org15756c8"><span class="section-number-4">8.24.3.</span> гистограмма</h4>
<div class="outline-text-4" id="text-8-24-3">
<div class="org-src-container">
<pre class="src src-python">df.groupby(<span style="color: #95e454;">'education'</span>)[<span style="color: #95e454;">'cost_requested'</span>].plot.hist()
plt.legend()
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org7d70c0d" class="outline-4">
<h4 id="org7d70c0d"><span class="section-number-4">8.24.4.</span> box plot</h4>
<div class="outline-text-4" id="text-8-24-4">
<p>
boxplot = df.boxplot(column=['Col1', 'Col2', 'Col3'])
</p>
</div>
</div>
<div id="outline-container-org39b317a" class="outline-4">
<h4 id="org39b317a"><span class="section-number-4">8.24.5.</span> bar plot, bar chart</h4>
<div class="outline-text-4" id="text-8-24-5">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Bar Chart Vertical</span>
<span style="color: #cae682;">dfg</span> = df.groupby(<span style="color: #95e454;">'address_actual'</span>)[<span style="color: #95e454;">'cost_requested'</span>].agg(<span style="color: #95e454;">'sum'</span>)
<span style="color: #cae682;">x</span> = <span style="color: #e5786d;">range</span>(<span style="color: #e5786d;">len</span>(dfg))
plt.bar(x, dfg)
<span style="color: #cae682;">x_labels</span> = df[<span style="color: #95e454;">'address_actual'</span>].unique()
plt.xticks(x, <span style="color: #e5786d;">sorted</span>(x_labels))
plt.xticks(rotation=60) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">much better</span>
plt.show()
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Horizontal Bar Chart</span>
<span style="color: #cae682;">x</span> = <span style="color: #e5786d;">range</span>(3)
plt.barh(x,[1,2,3])
plt.yticks(x, [<span style="color: #95e454;">'a'</span>,<span style="color: #95e454;">'b'</span>,<span style="color: #95e454;">'c'</span>])
plt.show()

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Horizontal Bar Chart with center</span>
<span style="color: #8ac6f2; font-weight: bold;">import</span> matplotlib
<span style="color: #8ac6f2; font-weight: bold;">from</span> pylab <span style="color: #8ac6f2; font-weight: bold;">import</span> *

<span style="color: #cae682;">val</span> = 3-6*rand(5)    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">the bar lengths        # changed your data slightly</span>
<span style="color: #cae682;">pos</span> = arange(5)+.5    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">the bar centers on the y axis</span>
<span style="color: #e5786d;">print</span> pos
figure(1)
barh(pos,val, align=<span style="color: #95e454;">'center'</span>,height=0.1)    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">notice the 'height' argument</span>
yticks(pos, (<span style="color: #95e454;">'Tom'</span>, <span style="color: #95e454;">'Dick'</span>, <span style="color: #95e454;">'Harry'</span>, <span style="color: #95e454;">'Slim'</span>, <span style="color: #95e454;">'Jim'</span>))

gca().axvline(0,color=<span style="color: #95e454;">'k'</span>,lw=3)   <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">poor man's zero level</span>

xlabel(<span style="color: #95e454;">'Performance'</span>)
title(<span style="color: #95e454;">'horizontal bar chart using matplotlib'</span>)
grid(<span style="color: #e5786d; font-weight: bold;">True</span>)
show()
</pre>
</div>
</div>
</div>
<div id="outline-container-orgdffeab5" class="outline-4">
<h4 id="orgdffeab5"><span class="section-number-4">8.24.6.</span> Q–Q plot</h4>
<div class="outline-text-4" id="text-8-24-6">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> pylab  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Plotting</span>
<span style="color: #8ac6f2; font-weight: bold;">import</span> scipy.stats <span style="color: #8ac6f2; font-weight: bold;">as</span> stats  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">scintific calculation</span>
stats.probplot(df[<span style="color: #95e454;">'cost_requested'</span>], dist=<span style="color: #95e454;">"norm"</span>, plot=pylab)
pylab.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org0244d11" class="outline-4">
<h4 id="org0244d11"><span class="section-number-4">8.24.7.</span> Scatter plot</h4>
<div class="outline-text-4" id="text-8-24-7">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">for two</span>
<span style="color: #cae682;">x</span> = df[<span style="color: #95e454;">'cost_requested'</span>]
<span style="color: #cae682;">y</span> = df[<span style="color: #95e454;">'income'</span>]
plt.scatter(x, y)
plt.title(<span style="color: #95e454;">'&#1044;&#1080;&#1072;&#1075;&#1088;&#1072;&#1084;&#1084;&#1072; &#1088;&#1072;&#1089;&#1089;&#1077;&#1103;&#1085;&#1080;&#1103;'</span>)
plt.xlabel(<span style="color: #95e454;">'cost_requested'</span>)
plt.ylabel(<span style="color: #95e454;">'income'</span>)
plt.show()

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">for three</span>
plt.plot(x,y, <span style="color: #95e454;">'b*'</span>, z, <span style="color: #95e454;">'g^'</span>) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">y -blue, z -green</span>
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf41f5a2" class="outline-4">
<h4 id="orgf41f5a2"><span class="section-number-4">8.24.8.</span> Scatter matrix</h4>
<div class="outline-text-4" id="text-8-24-8">
<p>
по диаганали ядерные оценки плотности или сглаженные гистограммы
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> pandas.plotting <span style="color: #8ac6f2; font-weight: bold;">import</span> scatter_matrix
<span style="color: #cae682;">colours</span> = {0:<span style="color: #95e454;">'red'</span>, 1:<span style="color: #95e454;">'green'</span>}
scatter_matrix(df[cols],
               diagonal=<span style="color: #95e454;">'kde'</span>,
               c =df[<span style="color: #95e454;">'result'</span>].replace(colours))
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-orgde0da15" class="outline-4">
<h4 id="orgde0da15"><span class="section-number-4">8.24.9.</span> Correlation Matrix with heatmap</h4>
<div class="outline-text-4" id="text-8-24-9">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #cae682;">cols</span> = [<span style="color: #95e454;">'cost_requested'</span>, <span style="color: #95e454;">'income'</span>, <span style="color: #95e454;">'loan'</span>, <span style="color: #95e454;">'charge'</span>]
<span style="color: #cae682;">corr</span> = df[cols].corr()
plt.matshow(corr,  cmap=plt.cm.Reds)
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">or</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">plt.imshow(corr, cmap='RdYlGn', interpolation='none', aspect='auto')</span>
<span style="color: #cae682;">tick_marks</span> = [i <span style="color: #8ac6f2; font-weight: bold;">for</span> i <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(<span style="color: #e5786d;">len</span>(cols))]
plt.xticks(tick_marks, cols, rotation=<span style="color: #95e454;">'vertical'</span>)
plt.yticks(tick_marks, cols)
plt.colorbar()
plt.title(<span style="color: #95e454;">"&#1052;&#1072;&#1090;&#1088;&#1080;&#1094;&#1072; &#1082;&#1086;&#1088;&#1088;&#1077;&#1083;&#1103;&#1094;&#1080;&#1080;"</span>)
plt.show()
</pre>
</div>
</div>
</div>

<div id="outline-container-orgae67901" class="outline-4">
<h4 id="orgae67901"><span class="section-number-4">8.24.10.</span> PDP</h4>
<div class="outline-text-4" id="text-8-24-10">
<p>
<a href="https://scikit-learn.org/stable/modules/partial_dependence.html#partial-dependence">https://scikit-learn.org/stable/modules/partial_dependence.html#partial-dependence</a>
</p>

<p>
Влияние анкетного скоринга на решение модели
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.inspection <span style="color: #8ac6f2; font-weight: bold;">import</span> partial_dependence
<span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.inspection <span style="color: #8ac6f2; font-weight: bold;">import</span> plot_partial_dependence
<span style="color: #8ac6f2; font-weight: bold;">from</span> xgboost <span style="color: #8ac6f2; font-weight: bold;">import</span> XGBClassifier

<span style="color: #cae682;">X</span> = df0.drop([<span style="color: #95e454;">'system'</span>], 1)
<span style="color: #cae682;">X</span> = X.drop([<span style="color: #95e454;">'under'</span>], 1)
<span style="color: #cae682;">Y</span> = df0[[<span style="color: #95e454;">'system'</span>, <span style="color: #95e454;">'under'</span>]]

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print(X.columns.values)</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">exit(0)</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">train model</span>
<span style="color: #cae682;">model</span> = XGBClassifier(booster=<span style="color: #95e454;">'gbtree'</span>, objective=<span style="color: #95e454;">'binary:logistic'</span>, scale_pos_weight=45, max_depth=3,
                      learning_rate=0.1, gamma=1, num_round=4)
<span style="color: #cae682;">est</span> = model.fit(X, Y[<span style="color: #95e454;">'under'</span>])

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">a = partial_dependence(est, features=[0], X=X, percentiles=(0, 1), grid_resolution=2)</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print(a)</span>
<span style="color: #cae682;">X_uses</span> = X[X[<span style="color: #95e454;">'`condition`_uses'</span>] == 1]
<span style="color: #cae682;">_</span> = plot_partial_dependence(est, X_uses, features=[<span style="color: #95e454;">'anket_score'</span>], n_jobs=4, grid_resolution=20)

</pre>
</div>
</div>
</div>
<div id="outline-container-org48c0a5a" class="outline-4">
<h4 id="org48c0a5a"><span class="section-number-4">8.24.11.</span> pie chart</h4>
<div class="outline-text-4" id="text-8-24-11">
<p>
Распределение чего-то между чем-то. Когда 100 процентов делится между кем-то
</p>
</div>
</div>
<div id="outline-container-org92dbf1b" class="outline-4">
<h4 id="org92dbf1b"><span class="section-number-4">8.24.12.</span> sns.lmplot для 2 столбцов (scatter + regression)</h4>
<div class="outline-text-4" id="text-8-24-12">
<p>
sns.lmplot(data = df, x = 'Age', y = 'SprintSpeed',lowess=True,scatter<sub>kws</sub>={'alpha':0.01, 's':5,'color':'green'},
           line<sub>kws</sub>={'color':'red'})
</p>
</div>
</div>
</div>
<div id="outline-container-org54b2c34" class="outline-3">
<h3 id="org54b2c34"><span class="section-number-3">8.25.</span> виды графиков по назначению</h3>
<div class="outline-text-3" id="text-8-25">
<p>
<a href="https://python-graph-gallery.com/">https://python-graph-gallery.com/</a>
<a href="https://foxhugh.com/visual-communication/visualization-2/list-of-visualization-methods-3/">https://foxhugh.com/visual-communication/visualization-2/list-of-visualization-methods-3/</a>
</p>
<ol class="org-ol">
<li>DISTRIBUTION
<ul class="org-ul">
<li>VIOLIN</li>
<li>DENSITY</li>
<li>BOXPLOT</li>
<li>HISTOGRAM</li>
</ul></li>
<li>CORRELATION
<ul class="org-ul">
<li>Scatterplot</li>
<li>Connected Scatter plot</li>
<li>Bubble plot</li>
<li>Heatmap</li>
<li>2D density plot</li>
<li>Correlogram</li>
</ul></li>
<li>RANKING
<ul class="org-ul">
<li>Barplot</li>
<li>Boxplot</li>
<li>parallel plot</li>
<li>Lollipop plot</li>
<li>Wordcloud</li>
<li>Radar chart or Spider plot or Polar chart or Web chart</li>
</ul></li>
<li>PART OF A WHOLE
<ul class="org-ul">
<li>Stacked barplot</li>
<li>Tree plot</li>
<li>Venn diagram</li>
<li>Doughnut plot</li>
<li>Pie plot</li>
<li>Tree diagram</li>
</ul></li>
<li>EVOLUTION
<ul class="org-ul">
<li>Line plot</li>
<li>Area plot</li>
<li>Stacked area plot</li>
<li>Parrallel plot</li>
<li>Streamchart</li>
</ul></li>
<li>MAPS
<ul class="org-ul">
<li>Map</li>
<li>Choropleth map</li>
<li>Connection map</li>
<li>Bubble map</li>
</ul></li>
<li>FLOW
<ul class="org-ul">
<li>Chord diagram</li>
<li>Network chart</li>
<li>Sankey diagram</li>
</ul></li>
<li>Other
<ul class="org-ul">
<li>Animation</li>
<li>Cheat sheet</li>
<li>Data Art</li>
<li>Color</li>
<li>3D</li>
<li>Bad chart</li>
</ul></li>
</ol>
</div>
</div>

<div id="outline-container-org74c6b00" class="outline-3">
<h3 id="org74c6b00"><span class="section-number-3">8.26.</span> библиотеки для графиков</h3>
<div class="outline-text-3" id="text-8-26">
<ul class="org-ul">
<li>Matplotlib</li>
<li>Plotly</li>
<li>Seaborn</li>
<li>Altair</li>
<li>Bokeh</li>
</ul>
</div>
</div>

<div id="outline-container-orgf8aecda" class="outline-3">
<h3 id="orgf8aecda"><span class="section-number-3">8.27.</span> тексты</h3>
<div class="outline-text-3" id="text-8-27">
<p>
Convert a collection of text documents to a matrix of token counts
</p>
<ul class="org-ul">
<li>from sklearn.feature<sub>extraction.text</sub> import CountVectorizer</li>
</ul>


<p>
TF-IDF - оценка важности слова. Вес слова равен частоте употреблений в документе и обратно пропорционален
частоте употреблений во всех докумнетах коллекции.
</p>
<ul class="org-ul">
<li>from sklearn.feature<sub>extraction.text</sub> import TfidfTransformer</li>
</ul>
</div>
</div>
<div id="outline-container-orga207164" class="outline-3">
<h3 id="orga207164"><span class="section-number-3">8.28.</span> типичное значение</h3>
<div class="outline-text-3" id="text-8-28">
<ul class="org-ul">
<li>mean - среднее арифметическое 1+2+3/3
<ul class="org-ul">
<li>если есть выброс  - среднее будет больше 75 квантили или меньше 25</li>
</ul></li>
<li>медиана - список сортируется и берется значение из середины 50/50, равна квартили 50%</li>
<li>усеченная средняя</li>
</ul>
</div>
</div>

<div id="outline-container-org896af25" class="outline-3">
<h3 id="org896af25"><span class="section-number-3">8.29.</span> simularity measure - Коэффициент сходства</h3>
<div class="outline-text-3" id="text-8-29">
<p>
безразмерный показатель сходства сравниваемых объектов.
</p>
<ol class="org-ol">
<li>унарные - меры разнообразия Diversity index и меры концентрации degree of concentration
<ul class="org-ul">
<li>Diversity index - quantify the entropy</li>
<li></li>
</ul></li>
<li>бинарные -</li>
<li>n-арные, многоместные</li>
</ol>


<p>
other terms:
</p>
<ul class="org-ul">
<li>Матрица мер конвергенции - similarity matrix ( recommender systems)</li>
<li><p>
Contingency table -  multivariate frequency distribution of the variables
</p>
<ul class="org-ul">
<li>measure significance of the difference between the two proportions: Pearson's chi-squared test, the</li>
</ul>
<p>
G-test, Fisher's exact test, Boschloo's test, and Barnard's test.
</p></li>
</ul>

<p>
Binary:
</p>
<ul class="org-ul">
<li>between sets, areas in object detection (CV):
<ul class="org-ul">
<li>Jaccard index J(A,B) = |A⋃B| / |A⋂B| = |A⋂B| / (|A| + |B| - |A⋂B| )  - intersection of two sets / union of two sets
<ul class="org-ul">
<li>good for binary data</li>
<li>0 &lt;= J(A,B) &lt;= 1</li>
<li>good for binary comparision = TP</li>
<li>Kj = c / a + b - c, where c is intersection of a and b</li>
</ul></li>
<li>Sorensen similarity index - the weight for the number of shared items is larger</li>
<li>Sørensen–Dice coefficient (F1 score) 2*|A⋃B| / |A⋂B| = 1 - 2* |A⋂B| / (|A| + |B|)</li>
</ul></li>
<li>between two data points: see <a href="#org8439b40">8.12.9.4</a>
<ul class="org-ul">
<li>Euclidean distance</li>
<li>Manhattan distance</li>
</ul></li>
<li>between vectors:
<ul class="org-ul">
<li>Cosine simularity = ∑(Ai*Bi) / sqrt(∑Ai<sup>2</sup> * ∑Bi<sup>2</sup>))
<ul class="org-ul">
<li>V and a*V are maximally similar.</li>
<li>Ko = c / sqrt(a*b)</li>
<li>good for embeddings, because embeddings is vectors and vectors close when sources is close.</li>
<li>not invariant to adding a constant to all elements</li>
</ul></li>
</ul></li>
<li>between strings
<ul class="org-ul">
<li>Levenshtein distance</li>
</ul></li>
</ul>

<p>
Cosine simularity = (| A - B | ^2) / 2 where  |A|<sup>2</sup> = |B|<sup>2</sup>
</p>


<p>
Correlation - linearly related x1*a+b = x2*c+d or x1*a1+x2*a2 + c = 0
</p>
<ul class="org-ul">
<li>partial correlation - measures the degree of association between two random variables, with the effect of a
set of controlling random variables removed.</li>
<li>Pearson product-moment correlation</li>
<li>Rank correlation: Kendall's, Spearman's ρ (for ordinal data: like 1, neutral 2, dislike 3</li>
</ul>

<p>
Pearson vs  cosine simularity.
</p>
<ul class="org-ul">
<li>Pearson invariant to adding any constant to all elements.</li>
<li>Pearson Correlation Coefficient and Cosine Similarity are equivalent when X and Y have means of 0.</li>
<li>Corr(x,y) = CosSim(x - mean(x), y - mean(x))</li>
</ul>
</div>
</div>


<div id="outline-container-org602af8e" class="outline-3">
<h3 id="org602af8e"><span class="section-number-3">8.30.</span> libs</h3>
<div class="outline-text-3" id="text-8-30">
<ul class="org-ul">
<li>ArviZ: Exploratory analysis of Bayesian models</li>
<li>statsmodels - provides classes and functions for the estimation of many different statistical models, as
well as for conducting statistical tests, and statistical data exploration.</li>
<li>seaborn: statistical data visualization</li>
</ul>
</div>
</div>

<div id="outline-container-org3f88075" class="outline-3">
<h3 id="org3f88075"><span class="section-number-3">8.31.</span> decision tree</h3>
<div class="outline-text-3" id="text-8-31">
<p>
pros
</p>
<ul class="org-ul">
<li>easy to interpret</li>
<li>Can handle data of different types, including continuous, categorical, ordinal, and binary. Transformations
of the data are not required.</li>
<li>Handle missing data by identifying surrogate splits in the modeling process. Surrogate splits are splits
highly associated with the primary split. In other models, records with missing values are omitted by
default.</li>
</ul>
<p>
cons
</p>
<ul class="org-ul">
<li>unstable</li>
<li>overfit</li>
</ul>
<p>
pro/cons:
</p>
<ul class="org-ul">
<li>Independence of Irrelevant Alternatives(IIA): relative probabilities of choosing between two classes are not
affected by the presence or absence of other classes.</li>
</ul>

<p>
<a href="https://webfocusinfocenter.informationbuilders.com/wfappent/TLs/TL_rstat/source/DecisionTree47.htm">https://webfocusinfocenter.informationbuilders.com/wfappent/TLs/TL_rstat/source/DecisionTree47.htm</a>
</p>


<ul class="org-ul">
<li>comparision of algorithms <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0210236">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0210236</a></li>
</ul>

<p>
Which is better Linear or tree-based models?
</p>
<ul class="org-ul">
<li>If you need to build a model that is easy to explain to people, a decision tree model will always do better
than a linear model.</li>
</ul>
</div>
<div id="outline-container-orgda1e0ff" class="outline-4">
<h4 id="orgda1e0ff"><span class="section-number-4">8.31.1.</span> how it works</h4>
<div class="outline-text-4" id="text-8-31-1">
<p>
features are always randomly permuted at each split,
</p>

<ol class="org-ol">
<li><p>
splits the nodes on all available variables and then selects the split which results in most homogeneous
sub-nodes.
</p>
<ul class="org-ul">
<li>function to measure the quality of a split: default=”squared<sub>error</sub>”</li>
<li>Different algorithms use different metrics for measuring "best": 1. calculates Entropy(H) and Information</li>
</ul>
<p>
gain(IG) of this attribute. 2. selects the attribute which has the smallest Entropy or Largest Information gain.
</p></li>
<li>algorithm continues to recur on each subset, considering only attributes never selected before.</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-orgd9d8cef" class="outline-3">
<h3 id="orgd9d8cef"><span class="section-number-3">8.32.</span> продуктовая аналитика</h3>
<div class="outline-text-3" id="text-8-32">
<p>
Продуктовый аналитик — это человек, который умеет:
</p>
<ul class="org-ul">
<li>оценить, какие действия и параметры пользователей в продукте нужно отслеживать;</li>
<li>настроить сбор этих данных;</li>
<li>создавать отчеты, графики для принятия продуктовых решений на основе собранных ранее данных.</li>
</ul>

<p>
Продуктовая аналитика помогает понять:
</p>
<ul class="org-ul">
<li>какие элементы продукта пользователи используют, а какие игнорируют;</li>
<li>какие сценарии внутри продукта приводят к покупке, а какие к отказам;</li>
<li>какие характеристики тех пользователей, кто становится клиентом, и тех кто уходит с продукта;</li>
<li>как меняется поведение пользователей в результате обновлений продукта.</li>
</ul>

<p>
встречу по уточнению бэклога (PBR-Product Backlog Refinement)
</p>
<ul class="org-ul">
<li>продуктовый аналитик – представитель владельца продукта на встрече с командой,</li>
</ul>

<p>
практику «3 амиго»,задачу с трех точек зрения:
</p>
<ul class="org-ul">
<li>контекст бизнес-задачи (что нужно бизнес-заказчику)</li>
<li>технический контекст (как это сделать)</li>
<li>контекст валидации решения (как мы узнаем, что сделали то, что нужно).</li>
</ul>



<p>
Продумывать дизайн A/B-тестов и интерпретировать их результаты;
Добавлять новые метрики в систему A/B-тестов, проверять метрики на статистическую корректность;
Развивать дашборды, позволяющие отвечать на вопросы о том, что происходит с продуктом;
Проводить adhoc анализ данных о поведении пользователей.
</p>

<p>
Имеет опыт проведения A/B-тестов и теоретическую базу для их проведения: знает математическую статистику и теорию вероятностей;
Имеет опыт создания дашбордов в Tableau или другой BI системе. Интересуется современными практиками визуализации данных;
</p>
</div>
</div>
<div id="outline-container-orgb085303" class="outline-3">
<h3 id="orgb085303"><span class="section-number-3">8.33.</span> links</h3>
<div class="outline-text-3" id="text-8-33">
<p>
<a href="https://www.kdnuggets.com/">https://www.kdnuggets.com/</a>
</p>
</div>
</div>
</div>
<div id="outline-container-org796d28f" class="outline-2">
<h2 id="org796d28f"><span class="section-number-2">9.</span> Information retrieval</h2>
<div class="outline-text-2" id="text-9">
</div>
<div id="outline-container-orgff5efa6" class="outline-3">
<h3 id="orgff5efa6"><span class="section-number-3">9.1.</span> measures</h3>
<div class="outline-text-3" id="text-9-1">
<p>
<b>Evaluation measures</b> for IR - how well an index, search engine or database returns results from a collection of
 resources that satisfy a user's query
</p>
</div>
</div>
</div>

<div id="outline-container-orgd16ba8c" class="outline-2">
<h2 id="orgd16ba8c"><span class="section-number-2">10.</span> Recommender system</h2>
<div class="outline-text-2" id="text-10">
<p>
subclass of information filtering system
</p>
</div>

<div id="outline-container-org40a2a63" class="outline-3">
<h3 id="org40a2a63"><span class="section-number-3">10.1.</span> basic</h3>
<div class="outline-text-3" id="text-10-1">
<p>
ways:
</p>
<ul class="org-ul">
<li>Content-based filtering (or personality-based approach) - compare pre-tagged characteristics of an item with
user profile.
<ul class="org-ul">
<li>best suited when there is known data on an item, but not on the user.</li>
</ul></li>
<li>collaborative filtering technique - user's past behavior
<ul class="org-ul">
<li>requires a large amount of information about a user</li>
<li>cold start problem is common in collaborative filtering systems</li>
<li>memory-based and model-based</li>
<li>advantage - does not rely on machine analyzable content and doesn't need to "understand" of the item itself.</li>
</ul></li>
</ul>

<p>
types
</p>
<ul class="org-ul">
<li>Multi-criteria recommender systems</li>
<li>Risk-aware recommender systems</li>
<li>Mobile recommender systems</li>
<li>Hybrid recommender systems</li>
<li>knowledge-based systems</li>
<li>opinion-based recommender systems</li>
<li>Session-based recommender systems - mainly based on generative sequential models such as Recurrent Neural
Networks, Transformers, and other deep learning based approaches.</li>
</ul>

<p>
recommender systems
</p>
<ul class="org-ul">
<li>Collaborative filtering (CF) - user's past behavior + similar decisions made by other users
<ul class="org-ul">
<li></li>

<li>Model-based
<ul class="org-ul">
<li>clustering</li>
</ul></li>
</ul></li>
<li>Content-based</li>
<li>Hybrid models (CF + Content-based)</li>
</ul>
</div>
</div>

<div id="outline-container-org9422b5b" class="outline-3">
<h3 id="org9422b5b"><span class="section-number-3">10.2.</span> algorithms all</h3>
<div class="outline-text-3" id="text-10-2">
<p>
collaborative
</p>
<ul class="org-ul">
<li>user-based algorithm - memory-based</li>
<li>Matrix factorization (recommender systems) - model-based approaches</li>
<li>k-nearest neighbor (k-NN)</li>
<li>the Pearson Correlation as first implemented by Allen.</li>
<li>item-to-item collaborative filtering (people who buy x also buy y), an algorithm popularized by Amazon.com's
recommender system</li>
</ul>

<p>
content based:
</p>
<ul class="org-ul">
<li>create user profile as a weighted vector of item features. The weights denote the importance of each feature.</li>
<li>Bayesian Classifiers</li>
<li>cluster analysis</li>
<li>decision trees</li>
<li>artificial neural networks in order to estimate the probability that the user is going to like the item.</li>
</ul>

<p>
hybridization techniques:
</p>
<ul class="org-ul">
<li>Weighted: Combining the score of different recommendation components numerically.</li>
<li>Switching: Choosing among recommendation components and applying the selected one.</li>
<li>Mixed: Recommendations from different recommenders are presented together to give the recommendation.</li>
<li>Feature Combination: Features derived from different knowledge sources are combined together and given to a single recommendation algorithm.[54]</li>
<li>Feature Augmentation: Computing a feature or set of features, which is then part of the input to the next technique.[54]</li>
<li>Cascade: Recommenders are given strict priority, with the lower priority ones breaking ties in the scoring of the higher ones.</li>
<li>Meta-level: One recommendation technique is applied and produces some sort of model, which is then the input used by the next technique.[55]</li>
</ul>

<p>
techs
</p>
<ul class="org-ul">
<li>Reinforcement learning</li>
<li>Multi-criteria recommender systems (MCRS) - multiple criteria of item that affect this overall preference value.</li>
<li>Risk-aware recommender systems - risk of disturbing the user with unwanted notifications - content-based
technique and a contextual bandit algorithm.</li>
</ul>

<p>
fast:
</p>
<ul class="org-ul">
<li>Near-neighbor search in high dimensions (LSH). Take an item to quickly find a set of neighbors. This can be done once every day or every few hours.</li>
<li>clustering to search only within clusters.</li>
</ul>
</div>
</div>
<div id="outline-container-org5dad62b" class="outline-3">
<h3 id="org5dad62b"><span class="section-number-3">10.3.</span> matrix factorization</h3>
<div class="outline-text-3" id="text-10-3">
<p>
factor rating matrix "all users by all items" to multiplication of matrixes “all items by some taste
 dimensions” and “all users by some taste dimensions”. These dimensions are called <b>latent</b> or <b>hidden features</b>
 and we learn them from our data.
</p>

<p>
express each user as a vector of their taste values, and at the same time express each item as a vector of
 what tastes they represent
</p>

<p>
ways to factor a matrix:
</p>
<ul class="org-ul">
<li>Singular Value Decomposition (SVD)</li>
<li>Probabilistic Latent Semantic Analysis (PLSA)</li>
</ul>


<p>
For explicit data we treat missing data as just unknown fields that we should assign some predicted rating
 to. But for implicit we can’t just assume the same since there is information in these unknown values as well
</p>

<p>
ALS is an iterative optimization process where we for every iteration try to arrive closer and closer to a
 factorized representation of our original data.
</p>

<p>
R = U * V
</p>
<ul class="org-ul">
<li>V - vector for each item</li>
<li>U - vector for each user</li>
</ul>

<p>
simularity items score  = V*VT
</p>

<p>
making recommendations score = Ui*VT, matrix transpose.
</p>
</div>
<div id="outline-container-org97b8b2d" class="outline-4">
<h4 id="org97b8b2d"><span class="section-number-4">10.3.1.</span> links</h4>
<div class="outline-text-4" id="text-10-3-1">
<ul class="org-ul">
<li>Collaborative filtering for Implicit Feedback Datasets <a href="http://yifanhu.net/PUB/cf.pdf">http://yifanhu.net/PUB/cf.pdf</a></li>
<li><a href="https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe">https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org1f63539" class="outline-3">
<h3 id="org1f63539"><span class="section-number-3">10.4.</span> algoriths</h3>
<div class="outline-text-3" id="text-10-4">
</div>
<div id="outline-container-org7ec260a" class="outline-4">
<h4 id="org7ec260a"><span class="section-number-4">10.4.1.</span> memory based <a id="orgfd1c6d8"></a></h4>
<div class="outline-text-4" id="text-10-4-1">
<p>
ratings user u gives to item i is calculated as an aggregation of some similar users' rating of the item:
</p>
<pre class="example">
r_ui = aggr(r_u'i)
</pre>

<p>
where u' is the set of N top users that most similar to user u, who rated item i.
</p>

<p>
aggr - may vary
</p>

<p>
disadvantages:
</p>
<ul class="org-ul">
<li>performance decreases when data gets sparse,</li>
<li>This hinders the scalability of this approach and creates problems with large datasets</li>
<li>Adding new items requires inclusion of the new item and the re-insertion of all the elements in the structure.</li>
</ul>
</div>
</div>
<div id="outline-container-org36e1d7b" class="outline-4">
<h4 id="org36e1d7b"><span class="section-number-4">10.4.2.</span> Model-based</h4>
<div class="outline-text-4" id="text-10-4-2">
<p>
dimensionality reduction methods are mostly being used as complementary technique to improve robustness and
 accuracy of memory-based approach, models often called "latent factor models". they compress user-item matrix
 into a low-dimensional representation in terms of latent factors.
</p>

<p>
models:
</p>
<ul class="org-ul">
<li>Bayesian networks, clustering models, latent semantic models such as singular value decomposition, probabilistic latent semantic analysis, multiple multiplicative factor, latent Dirichlet allocation and Markov decision process based models</li>
</ul>

<p>
low-dimensional representation utilied by user-based or item-based neighborhood algorithms see <a href="#orgfd1c6d8">10.4.1</a>
</p>
</div>
</div>
<div id="outline-container-org6114e40" class="outline-4">
<h4 id="org6114e40"><span class="section-number-4">10.4.3.</span> Deep learning</h4>
<div class="outline-text-4" id="text-10-4-3">
<ul class="org-ul">
<li>Autoencoders</li>
<li>WIde and deep learning - linear algorithm + deep componen of emvedding vectors as a liner combination of
output and trained together</li>
<li>Neural Graph Matching-Based CF (GMCF) - on graph neural network (GNN)</li>
</ul>
</div>
</div>
<div id="outline-container-org8d181d3" class="outline-4">
<h4 id="org8d181d3"><span class="section-number-4">10.4.4.</span> keras</h4>
<div class="outline-text-4" id="text-10-4-4">
<p>
<a href="https://keras.io/examples/structured_data/collaborative_filtering_movielens/">https://keras.io/examples/structured_data/collaborative_filtering_movielens/</a>
<a href="https://www.kaggle.com/code/faressayah/collaborative-filtering-for-movie-recommendations">https://www.kaggle.com/code/faressayah/collaborative-filtering-for-movie-recommendations</a>
</p>
<ul class="org-ul">
<li>Map user ID to a "user vector" via an embedding matrix</li>
<li>Map movie ID to a "movie vector" via an embedding matrix</li>
<li>Compute the dot product between the user vector and movie vector, to obtain the a match score between the user and the movie (predicted rating).</li>
<li>Train the embeddings via gradient descent using all known user-movie pairs.</li>
</ul>
</div>
</div>
<div id="outline-container-org70e9083" class="outline-4">
<h4 id="org70e9083"><span class="section-number-4">10.4.5.</span> pyTorch - TorchRec</h4>
<div class="outline-text-4" id="text-10-4-5">
<ul class="org-ul">
<li>platform <a href="https://github.com/pytorch/torchrec">https://github.com/pytorch/torchrec</a></li>
<li>Deep Learning Recommendation Model (DLRM)<a href="https://arxiv.org/abs/1906.00091">https://arxiv.org/abs/1906.00091</a></li>
<li>example (main in 502 line)  <a href="https://github.com/facebookresearch/dlrm/blob/main/torchrec_dlrm/dlrm_main.py">https://github.com/facebookresearch/dlrm/blob/main/torchrec_dlrm/dlrm_main.py</a>
<ul class="org-ul">
<li>Criteo Terabyte Dataset <a href="https://labs.criteo.com/2013/12/download-terabyte-click-logs/">https://labs.criteo.com/2013/12/download-terabyte-click-logs/</a></li>
</ul></li>
<li>article(800 GPU) <a href="https://www.adityaagrawal.net/blog/dnn/dlrm">https://www.adityaagrawal.net/blog/dnn/dlrm</a></li>
<li>article <a href="https://medium.com/swlh/deep-learning-recommendation-models-dlrm-a-deep-dive-f38a95f47c2c">https://medium.com/swlh/deep-learning-recommendation-models-dlrm-a-deep-dive-f38a95f47c2c</a></li>
<li>article (multi GPU) <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/resources/dlrm_for_pytorch">https://catalog.ngc.nvidia.com/orgs/nvidia/resources/dlrm_for_pytorch</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgfefbfb8" class="outline-4">
<h4 id="orgfefbfb8"><span class="section-number-4">10.4.6.</span> TensorFlow Recommenders</h4>
</div>
<div id="outline-container-org0e6ee66" class="outline-4">
<h4 id="org0e6ee66"><span class="section-number-4">10.4.7.</span> Neural Graph Matching based Collaborative Filtering (GMCF)</h4>
<div class="outline-text-4" id="text-10-4-7">
<p>
<a href="https://github.com/ruizhang-ai/GMCF_Neural_Graph_Matching_based_Collaborative_Filtering">https://github.com/ruizhang-ai/GMCF_Neural_Graph_Matching_based_Collaborative_Filtering</a>
<a href="https://arxiv.org/abs/2105.04067">https://arxiv.org/abs/2105.04067</a>
</p>
</div>
</div>
<div id="outline-container-org847903a" class="outline-4">
<h4 id="org847903a"><span class="section-number-4">10.4.8.</span> DLRM vs GMCF</h4>
<div class="outline-text-4" id="text-10-4-8">
<p>
Both models are highly scalable
DLRM 2019
</p>
<ul class="org-ul">
<li>ability to handle massive amounts of feature data</li>
<li>excels at capturing <b>complex</b> user-item relationships</li>
</ul>
<p>
GMCF 2021 pytorch
</p>
<ul class="org-ul">
<li>useful when there is limited user-item interaction data available</li>
<li>more adept at handling <b>sparse and incomplete data</b></li>
<li>capture graph structure of user-item interactions</li>
</ul>
</div>
</div>

<div id="outline-container-org7618d58" class="outline-4">
<h4 id="org7618d58"><span class="section-number-4">10.4.9.</span> surprise</h4>
<div class="outline-text-4" id="text-10-4-9">
<p>
<a href="https://pypi.org/project/scikit-surprise/">https://pypi.org/project/scikit-surprise/</a>
</p>
</div>
</div>
</div>
<div id="outline-container-orgb6db5e3" class="outline-3">
<h3 id="orgb6db5e3"><span class="section-number-3">10.5.</span> datasets</h3>
<div class="outline-text-3" id="text-10-5">
<p>
MovieLens dataset <a href="https://grouplens.org/datasets/movielens/">https://grouplens.org/datasets/movielens/</a>
</p>

<p>
ratings
</p>
<ul class="org-ul">
<li>userId</li>
<li>movieId</li>
<li>rating</li>
<li>timestamp</li>
</ul>

<p>
tags
</p>
<ul class="org-ul">
<li>userId</li>
<li>movieId</li>
<li>tag</li>
<li>timestamp</li>
</ul>

<p>
movies
</p>
<ul class="org-ul">
<li>movieId - key</li>
<li>title</li>
<li>genres</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> pandas <span style="color: #8ac6f2; font-weight: bold;">as</span> pd
<span style="color: #cae682;">movielens_dir</span> = <span style="color: #95e454;">'/home/u/proj_dolgoletie/movl/ml-latest-small/'</span>
<span style="color: #cae682;">ratings_file</span> = movielens_dir + <span style="color: #95e454;">"ratings.csv"</span>
<span style="color: #cae682;">tags_file</span> = movielens_dir + <span style="color: #95e454;">"tags.csv"</span>
<span style="color: #cae682;">movies_file</span> = movielens_dir + <span style="color: #95e454;">"movies.csv"</span>
<span style="color: #cae682;">df</span> = pd.read_csv(ratings_file)
<span style="color: #cae682;">tags</span> = pd.read_csv(tags_file)
<span style="color: #cae682;">movies</span> = pd.read_csv(movies_file)
<span style="color: #e5786d;">print</span>(df.movieId.unique().size, df.shape)
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"ratings</span><span style="color: #e5786d; font-weight: bold;">\n</span><span style="color: #95e454;">"</span>, df.sample(3))
<span style="color: #e5786d;">print</span>()
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"tags</span><span style="color: #e5786d; font-weight: bold;">\n</span><span style="color: #95e454;">"</span>, tags.sample(3))
<span style="color: #e5786d;">print</span>()
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"movies</span><span style="color: #e5786d; font-weight: bold;">\n</span><span style="color: #95e454;">"</span>, movies.sample(3))

<span style="color: #cae682;">user_ids</span> = df[<span style="color: #95e454;">"userId"</span>].unique().tolist()

<span style="color: #cae682;">movie_ids</span> = df[<span style="color: #95e454;">"movieId"</span>].unique().tolist()

Number of users: 610, Number of Movies: 9724, Min Rating: 0.5, Max Rating: 5.0
</pre>
</div>

<pre class="example" id="org8d37b07">
9724 (100836, 4)
ratings
        userId  movieId  rating   timestamp
62873     414     1639     4.0   961437358
37318     249   112556     5.0  1422171907
98771     608      527     4.0  1117415161

tags
      userId  movieId                 tag   timestamp
999     474       31         high school  1137375502
233      62    87430                  DC  1525555176
155      62    37729  visually appealing  1530310541

movies
       movieId                                              title                           genres
4613     6872                      House of the Dead, The (2003)                    Action|Horror
8669   121342                           Carry on Cruising (1962)                   Comedy|Romance
6982    66785  Good, the Bad, the Weird, The (Joheunnom nabbe...  Action|Adventure|Comedy|Western
</pre>
</div>
</div>

<div id="outline-container-orgb996072" class="outline-3">
<h3 id="orgb996072"><span class="section-number-3">10.6.</span> simularity</h3>
<div class="outline-text-3" id="text-10-6">
<ul class="org-ul">
<li>jaccard simularity - ignore rating values</li>
<li>centered cosine simularity - treats the unknown values as zeros. If we normalize by substractin mean - blank
fields will be neutral.</li>
</ul>

<p>
items to items outperforms user to user, items simpler.
</p>
</div>
</div>
<div id="outline-container-org94c8436" class="outline-3">
<h3 id="org94c8436"><span class="section-number-3">10.7.</span> terms</h3>
<div class="outline-text-3" id="text-10-7">
<ul class="org-ul">
<li><b>cold start</b> - the issue that the system cannot draw any inferences for users or items about which it has not
yet gathered sufficient information
<ul class="org-ul">
<li>New community</li>
<li>New item</li>
<li>New user</li>
</ul></li>
<li><b>explicit</b> and <b>implicit</b> forms of <b>data collection</b>. - explicit asking and implicit observing.</li>
<li><b>meta-data</b> of items</li>
<li>user-item (utility) matrix or Rating Matrix</li>
</ul>
</div>
</div>
<div id="outline-container-orgeeb3290" class="outline-3">
<h3 id="orgeeb3290"><span class="section-number-3">10.8.</span> problems</h3>
<div class="outline-text-3" id="text-10-8">
<ul class="org-ul">
<li><b>Cold start</b></li>
<li><b>Scalability</b></li>
<li><b>Sparsity</b> - most active users will only have rated a small subset of the overall database, most popular items
have very few ratings</li>
<li>the value from the recommendation system is significantly less than when other content types from other
services can be recommended - more for content based systems</li>
</ul>
</div>
</div>
<div id="outline-container-org4dd8ec3" class="outline-3">
<h3 id="org4dd8ec3"><span class="section-number-3">10.9.</span> scikit-surprise</h3>
<div class="outline-text-3" id="text-10-9">
<p>
<a href="https://rubikscode.net/2020/04/27/collaborative-filtering-with-machine-learning-and-python/">https://rubikscode.net/2020/04/27/collaborative-filtering-with-machine-learning-and-python/</a>
<a href="https://pypi.org/project/scikit-surprise/">https://pypi.org/project/scikit-surprise/</a>
</p>
</div>
</div>
<div id="outline-container-org932111f" class="outline-3">
<h3 id="org932111f"><span class="section-number-3">10.10.</span> <span class="todo TODO">TODO</span> <a href="https://github.com/sb-ai-lab/RePlay">https://github.com/sb-ai-lab/RePlay</a></h3>
</div>
<div id="outline-container-orgc7fe6e5" class="outline-3">
<h3 id="orgc7fe6e5"><span class="section-number-3">10.11.</span> TIGER: Transformer Index for GEnerative Recommenders</h3>
<div class="outline-text-3" id="text-10-11">
<p>
Recommender Systems with Generative Retrieval 2023 <a href="https://arxiv.org/pdf/2305.05065">https://arxiv.org/pdf/2305.05065</a>
</p>

<p>
Современные модели для генерации кандидатов обычно строят так: обучают энкодеры (матричные разложения,
 трансформеры, модели dssm-like) для получения ембеддингов запроса (пользователя) и кандидата в одном
 пространстве. Далее по кандидатам строится ANN-индекс, в котором по ембеддингу запроса ищутся ближайшие по
 выбранной метрике кандидаты. Авторы предлагают отойти от такой схемы и научиться генерировать ID айтемов
 напрямую моделью, которую они обучают. Для этого предлагают использовать энкодер-декодер трансформенную
 модель на основе фреймворка T5X.
</p>

<p>
Two main parts:
</p>
<ol class="org-ol">
<li>(a) Semantic ID generation for items using quantization of content embeddings
<ul class="org-ul">
<li>assigns Semantic IDs to each item, and trains a retrieval model to predict the Semantic ID of an item that</li>
</ul></li>
</ol>
<p>
a given user may engage with.
</p>
<ul class="org-ul">
<li>we use RQ-VAE to generate Semantic IDs, which leads to hierarchical representation of items</li>
<li>hierarchical Semantic IDs can be used to replace item IDs for ranking models in large scale recommender
systems improves model generalization.</li>
<li>RQ-VAE - is a multi-level vector quantizer that applies quantization on residuals to generate a tuple of
codewords (aka Semantic IDs). The Autoencoder is jointly trained by updating the quantization codebook and
the DNN encoder-decoder parameters. Fig. 3 illustrates the process of generating Semantic IDs through
residual quantization.</li>
<li>The number of items that the Semantic IDs can represent uniquely is thus equal to the product of the
codebook sizes.</li>
<li>For each level, a codebook of cardinality 256 is maintained, where each vector in the codebook has a
dimension of 32.</li>
</ul>
<ul class="org-ul">
<li>Generative Retrieval: Transformer based encoder-decoder setup for building the sequence-to-sequence model
<ul class="org-ul">
<li>Training a generative recommender system on Semantic IDs. A Transformer model is trained on</li>
</ul></li>
</ul>
<p>
the sequential recommendation task using sequences of Semantic IDs
</p>
<ul class="org-ul">
<li>Transformer [36 ] memory (parameters) as an end-to-end index for retrieval in recommendation systems,</li>
</ul>

<p>
Cold-Start Recommendation - TIGER framework can easily perform cold-start recommendations since it leverages
 item semantics when predicting the next item.
</p>

<p>
Semantic ID
</p>
<ul class="org-ul">
<li>иерархичность — ID в начале отвечают за общие характеристики, а в конце — за более детальные;</li>
<li>они позволяют описывать новые айтемы, решая проблему cold-start;</li>
<li>при генерации можно использовать сэмплинг с температурой, что позволяет контролировать разнообразие.</li>
</ul>

<p>
Semantic IDs строили следующим образом: каждый товар описывался строкой из названия, цены, бренда и
 категории. Полученное предложение кодировали предобученной моделью Sentence-T5, получая эмбеддинг
 размерности 768. На этих ембеддингах обучали RQ-VAE с размерностями слоев 512, 256, 128, активацией ReLU и
 внутренним ембеддингом 32. Использовали три кодовые книги (codebooks) размером 256 ембеддингов. Для
 стабильности обучения их инициализировали центроидами кластеров k-means на первом батче. В результате каждый
 айтем описывает три ID, каждый из словаря размера 256. Для предотвращения коллизий добавляли еще один ID с
 порядковым номером.
</p>

<p>
RQ-VAE
</p>
<ul class="org-ul">
<li>Autoregressive Image Generation using Residual Quantization <a href="https://arxiv.org/pdf/2203.01941">https://arxiv.org/pdf/2203.01941</a></li>
<li>SoundStream: An End-to-End Neural Audio Codec <a href="https://arxiv.org/pdf/2107.03312">https://arxiv.org/pdf/2107.03312</a></li>
<li>Vector quantization <a href="https://notesbylex.com/residual-vector-quantisation">https://notesbylex.com/residual-vector-quantisation</a></li>
<li>Vector Quantised-Variational AutoEncoder (VQ-VAE) <a href="https://arxiv.org/pdf/1711.00937">https://arxiv.org/pdf/1711.00937</a></li>
</ul>
</div>
</div>
<div id="outline-container-org1e8845a" class="outline-3">
<h3 id="org1e8845a"><span class="section-number-3">10.12.</span> links</h3>
<div class="outline-text-3" id="text-10-12">
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Recommender_system">https://en.wikipedia.org/wiki/Recommender_system</a></li>
<li><a href="http://snap.stanford.edu/class/cs246-2015/handouts.html">http://snap.stanford.edu/class/cs246-2015/handouts.html</a></li>
<li><a href="https://medium.com/@shengyuchen/recommender-systems-intro-notes-stanford-mining-massive-datasets-lecture-41-43-71188b5bedaf">https://medium.com/@shengyuchen/recommender-systems-intro-notes-stanford-mining-massive-datasets-lecture-41-43-71188b5bedaf</a></li>
<li><a href="https://chaitanyabelhekar.medium.com/recommendation-systems-a-walk-trough-33587fecc195">https://chaitanyabelhekar.medium.com/recommendation-systems-a-walk-trough-33587fecc195</a></li>
</ul>
</div>
<div id="outline-container-orgab9d677" class="outline-4">
<h4 id="orgab9d677"><span class="section-number-4">10.12.1.</span> Alternating Least Squares (ALS)</h4>
<div class="outline-text-4" id="text-10-12-1">
</div>
<ol class="org-ol">
<li><a id="org35d057d"></a>links<br />
<div class="outline-text-5" id="text-10-12-1-1">
<p>
<a href="https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe">https://medium.com/radon-dev/als-implicit-collaborative-filtering-5ed653ba39fe</a>
</p>
</div>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org4d0e414" class="outline-2">
<h2 id="org4d0e414"><span class="section-number-2">11.</span> Machine learning</h2>
<div class="outline-text-2" id="text-11">
<ul class="org-ul">
<li>национальная тех инициатива, хуйня какае-то <a href="http://www.nti2035.ru/">http://www.nti2035.ru/</a></li>
<li>офигенный блог End-to-End Machine Learning <a href="https://brohrer.github.io/blog.html">https://brohrer.github.io/blog.html</a></li>
<li><a href="https://samoa.incubator.apache.org/">https://samoa.incubator.apache.org/</a></li>
<li>tadviser.ru <a href="http://www.tadviser.ru/index.php/%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F:%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82_%D0%B2_%D0%B1%D0%B0%D0%BD%D0%BA%D0%B0%D1%85">http://www.tadviser.ru/index.php/%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F:%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82_%D0%B2_%D0%B1%D0%B0%D0%BD%D0%BA%D0%B0%D1%85</a></li>
<li>scholar.google.ru - поисковая система по текстам научных публикаций</li>
<li>канал по статистике и машинное обучение <a href="https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw">https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw</a></li>
<li>Cheatsheets  <a href="https://ml-cheatsheet.readthedocs.io">https://ml-cheatsheet.readthedocs.io</a></li>
<li>2) Машинное обучение, чему учатся дебилы яндекса <a href="https://yandexdataschool.ru/edu-process/program/ml-dev">https://yandexdataschool.ru/edu-process/program/ml-dev</a></li>
<li>1) лекции по машинному обучению на русском <a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2%29">http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2%29</a></li>
<li>3) <a href="http://sberbank.ai/">http://sberbank.ai/</a></li>
<li>4) Google Deep Learning course <a href="https://www.udacity.com/course/deep-learning--ud730">https://www.udacity.com/course/deep-learning--ud730</a></li>
<li>главный сайт дата майнеров <a href="https://www.kdnuggets.com/">https://www.kdnuggets.com/</a></li>
<li>еще дата майнеры  <a href="https://www.datasciencecentral.com">https://www.datasciencecentral.com</a></li>
<li>UCI ML Repository (349 datasets) <a href="https://archive.ics.uci.edu/ml">https://archive.ics.uci.edu/ml</a></li>
<li>Яндекс Академия канал <a href="https://www.youtube.com/channel/UCKFojzto0n4Ab3CRQRZ2zYA/videos">https://www.youtube.com/channel/UCKFojzto0n4Ab3CRQRZ2zYA/videos</a></li>
<li>блог сбера <a href="https://habr.com/en/company/sberbank/">https://habr.com/en/company/sberbank/</a></li>
<li>Введение в архитектуры нейронных сетей 2017  <a href="https://habr.com/ru/company/oleg-bunin/blog/340184/">https://habr.com/ru/company/oleg-bunin/blog/340184/</a></li>
<li>AI Journey 20.12.03 <a href="https://www.youtube.com/watch?v=mYvHDaQCRXc&amp;list=PLdtmzrRhJMFITdlt-MYV2Wq6I_W-Ki0ZW&amp;index=1">https://www.youtube.com/watch?v=mYvHDaQCRXc&amp;list=PLdtmzrRhJMFITdlt-MYV2Wq6I_W-Ki0ZW&amp;index=1</a></li>
</ul>

<p>
прикладной статистики, численных методов оптимизации, дискретного анализа
-&gt; интеллектуального анализа данных (data mining)
</p>
</div>
<div id="outline-container-org1bb128b" class="outline-3">
<h3 id="org1bb128b"><span class="section-number-3">11.1.</span> steps</h3>
<div class="outline-text-3" id="text-11-1">
<p>
ISO/IEC-23053 › Framework for Artificial Intelligence (AI) Systems Using Machine Learning (ML)
</p>

<p>
<b>yandex ml course</b>
</p>

<p>
бизнес задачи:
</p>
<ul class="org-ul">
<li>дашборды для метрик</li>
<li>бизнес запрос в задачу МЛ</li>
<li>готовит презентацию задачи заказчику</li>
</ul>

<p>
исследование
</p>
<ul class="org-ul">
<li>подбирает метод и силу регуляризации</li>
<li>исключает выбросы и ложные данные</li>
</ul>

<p>
инженерные
</p>
<ul class="org-ul">
<li>отбирает информативные признаки</li>
<li>разрабатывает пайплайн обучения модели</li>
<li>создает микросервис предсказаний</li>
<li>создает пайплайн трансформации данных</li>
</ul>
</div>
</div>

<div id="outline-container-org0a88ed9" class="outline-3">
<h3 id="org0a88ed9"><span class="section-number-3">11.2.</span> ensembles theory</h3>
<div class="outline-text-3" id="text-11-2">
</div>
<div id="outline-container-org16a243a" class="outline-4">
<h4 id="org16a243a"><span class="section-number-4">11.2.1.</span> terms</h4>
<div class="outline-text-4" id="text-11-2-1">
<dl class="org-dl">
<dt>base learners</dt><dd>most ensemble methods use a single base learning algorithm to produce homogeneous base
learners.</dd>
<dt>classification hyperplate</dt><dd>the boundary that separates the different classes in a classification problem.</dd>
<dt>merging or fusion</dt><dd>1) distance from x in f(x) to the classification hyperplate 2) the process of combining
the predictions or outputs generated by multiple individual models, in order to make a final prediction or
decision 3) margin refers to the distance between the hyperplane and the closest data points from each
class. A larger margin indicates a better separation between the classes.</dd>
</dl>
</div>
</div>
<div id="outline-container-org068ce1d" class="outline-4">
<h4 id="org068ce1d"><span class="section-number-4">11.2.2.</span> history</h4>
<div class="outline-text-4" id="text-11-2-2">
<p>
Epicurus (341-270 B.C.): principle of multiple explanations - are consistent with empirical observations.
</p>

<p>
areas
</p>
<ul class="org-ul">
<li>combining classifiers - strong classifiers (recognition community)</li>
<li>ensembles of weak learners - (ml community)</li>
<li>mixture of experts - divide-and-conqure strategy (nn community)</li>
</ul>

<p>
1990 Hansen and Salamon: it was found that predictions made by the combination of a set of classifiers are
 often more accurate than predictions made by the best single classifier.
</p>
<ul class="org-ul">
<li>combination is nice</li>
<li>best single is good</li>
<li>average is the best</li>
</ul>

<p>
1990 Schapire: weak learners can be boosted to strong learners
</p>
</div>
</div>
<div id="outline-container-org689b015" class="outline-4">
<h4 id="org689b015"><span class="section-number-4">11.2.3.</span> Может ли набор слабых обучающих алгоритмов создать сильный обучающий алгоритм</h4>
<div class="outline-text-4" id="text-11-2-3">
<p>
Вопрос поднятый Michael Kearns and Вэлиант Лусли "Может ли набор слабых обучающих алгоритмов создать сильный
 обучающий алгоритм?"
</p>
<ul class="org-ul">
<li>утвредительный ответ <a href="http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf">http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf</a></li>
</ul>

<p>
how base learners are generated:
</p>
<ul class="org-ul">
<li><b>sequential</b> ensemble methods (with adaboost for ex) - exploit the dependence between the base
learners. overall performace can be boosted in a residual-decreasing way.</li>
<li><b>parallel</b> ensemble methods - exploit the independence between the base learners.</li>
</ul>

<p>
steps
</p>
<ol class="org-ol">
<li>Generating the base learners - <b>accurate</b> as possible and <b>diverse</b> as possible.</li>
<li>combining them.</li>
</ol>

<p>
with a large ensemble, there are a lot of weights to learn, and this can easily lead to overfitting
</p>
</div>
</div>
<div id="outline-container-org3e9096a" class="outline-4">
<h4 id="org3e9096a"><span class="section-number-4">11.2.4.</span> AdaBoost</h4>
<div class="outline-text-4" id="text-11-2-4">
<ul class="org-ul">
<li>reduces the error exponentially fast</li>
<li>in order to achieve a good generalization, it is necessary to constrain the complexity of base learners and
number of learning rounds</li>
<li>often does not overfit - empirical.</li>
</ul>
</div>
</div>

<div id="outline-container-org588f21b" class="outline-4">
<h4 id="org588f21b"><span class="section-number-4">11.2.5.</span> Hoeffding's inequality</h4>
<div class="outline-text-4" id="text-11-2-5">
<p>
provides an upper bound on the probability that the sum of bounded independent random variables.
</p>

<p>
the sum of bounded independent random variables deviates from its <b>expected value</b> by more than a certain
 amount.
</p>

<ul class="org-ul">
<li>S = X1+  &#x2026; + Xn, where Xn - independent random variables</li>
<li></li>
</ul>
</div>
</div>
<div id="outline-container-org2802d04" class="outline-4">
<h4 id="org2802d04"><span class="section-number-4">11.2.6.</span> <span class="todo TODO">TODO</span> Bias-Variance Decompostion, Statistical Computational and Representational, Diversity</h4>
</div>
<div id="outline-container-orgb37f51d" class="outline-4">
<h4 id="orgb37f51d"><span class="section-number-4">11.2.7.</span> error rate</h4>
<div class="outline-text-4" id="text-11-2-7">
<p>
binary classification {-1, +1}, classificator hi, ground-truth function f:
</p>
<ul class="org-ul">
<li>independent generalization error: P(hi(x) != f(x)) = e</li>
</ul>
</div>
</div>

<div id="outline-container-org46158d9" class="outline-4">
<h4 id="org46158d9"><span class="section-number-4">11.2.8.</span> fusion strategy or combination methods</h4>
<div class="outline-text-4" id="text-11-2-8">
<ul class="org-ul">
<li>majority voting (hard voting) - 1) calc argmax per individual learner 2) select mode from all learners</li>
<li>Majority Voting</li>
<li>Bayes Optimal Classifier</li>
<li>Stacked Generalization</li>
<li>Super Learner</li>
<li>Consensus</li>
<li>Query-By-Committe</li>
</ul>
</div>


<ol class="org-ol">
<li><a id="org1408702"></a>Weighted Average Probabilities (Soft Voting) - returns the class label as argmax of the sum of predicted<br />
<div class="outline-text-5" id="text-11-2-8-1">
<p>
probabilities.
</p>
<ul class="org-ul">
<li>steps: 1) calc average per class, 2) select max NN</li>
<li>H(x) = sum(wi*hi(x)), i =1..T, wi&gt;=0, sum(wi) = 1</li>
<li>other combination methods are special cases of weighted averaging (Perrone and Cooper 1993)</li>
<li>there is no evidence that weighted average is better than simple averaging</li>
<li>good for combining learers with nonidentical strength</li>
</ul>
</div>
</li>
<li><a id="org8c7c7ec"></a>Averaging or Unweighted Model Averaging<br />
<div class="outline-text-5" id="text-11-2-8-2">
<ul class="org-ul">
<li>simple averaging: (1/T)*sum(hi(x))
<ul class="org-ul">
<li>err(H) &lt;= err(h)</li>
<li>able to get err(H) = (1/T)*err(h), where T - count of learners, H - f of all.</li>
<li>does not have to learn any weights (less parameters) , and so suffer little from overfitting</li>
<li>good for combining learners with similar performance</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="orgfe5d5ca"></a>Voting<br />
<div class="outline-text-5" id="text-11-2-8-3">
<ul class="org-ul">
<li>hi, i..T - classifiers</li>
<li>cj, j..l - classes</li>
</ul>
<p>
<b>majority</b> voting - if more that half of classifiers votes for same class, else <b>rejection option</b> used.
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org927b592" class="outline-4">
<h4 id="org927b592"><span class="section-number-4">11.2.9.</span> mixture-of-experts</h4>
<div class="outline-text-4" id="text-11-2-9">
<p>
<b>gating network</b>, also known as the "router" or "traffic director," is a crucial component that determines
 which expert(s) are most suitable for a given input. Typically outputs a probability distribution or weights
 indicating the relevance of each expert to the input.
</p>
<ul class="org-ul">
<li><b>Sparsely-Gated MoE</b> - selection is often sparse, meaning only a few experts are activated for each input,
which significantly reduces computational costs</li>
<li><b>Hard MoE</b> variant involves selecting only the highest-ranked expert for each input, rather than combining
the outputs of multiple experts. This approach can accelerate training and inference times.</li>
</ul>


<p>
outputs from the selected experts are combined using a function such as averaging or weighted averaging to
 produce the final prediction. The weights assigned by the gating network can be used to compute this weighted
 sum.
</p>

<p>
Training MoE Models:
</p>
<ul class="org-ul">
<li>Each expert is trained independently</li>
<li>optimal assignment of inputs to experts - expectation-maximization (EM) or gradient-based methods - optimal
assignment of inputs to experts</li>
</ul>
</div>
</div>
<div id="outline-container-orga18b386" class="outline-4">
<h4 id="orga18b386"><span class="section-number-4">11.2.10.</span> Sparse mixture-of-expert</h4>
<div class="outline-text-4" id="text-11-2-10">
<p>
such as the Switch
Transformer (Fedus et al., 2021), GLaM (Du et al., 2021) and/or GShard (Lepikhin et al., 2020)
</p>
</div>
</div>
<div id="outline-container-org81a0c8f" class="outline-4">
<h4 id="org81a0c8f"><span class="section-number-4">11.2.11.</span> Mixture-of-Denoisers (MoD)</h4>
<div class="outline-text-4" id="text-11-2-11">
<p>
UL2 (Unifying Language Learning) “google/ul2” Text2Text Generation - disentangling architectural archetypes
 with pre-training objectives. <a href="https://arxiv.org/pdf/2205.05131v1">https://arxiv.org/pdf/2205.05131v1</a>
</p>
</div>
</div>

<div id="outline-container-org9f2501f" class="outline-4">
<h4 id="org9f2501f"><span class="section-number-4">11.2.12.</span> links</h4>
<div class="outline-text-4" id="text-11-2-12">
<ul class="org-ul">
<li><a href="https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python">https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python</a></li>
<li>Ensemble Methods: Foundations and Algorithms -  Zhi-Hua Zhou - 2012</li>
<li>2022 [2104.02395] Ensemble deep learning: A review <a href="https://arxiv.org/abs/2104.02395">https://arxiv.org/abs/2104.02395</a></li>
<li><a href="https://scikit-learn.org/stable/modules/ensemble.html">https://scikit-learn.org/stable/modules/ensemble.html</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2a80553" class="outline-3">
<h3 id="org2a80553"><span class="section-number-3">11.3.</span> Энтропия</h3>
<div class="outline-text-3" id="text-11-3">
<p>
непредсказуемость появления какого-либо символа первичного алфавита.
</p>

<p>
Двоичная энтропия для независимых случайных событий x или состояний системы:
</p>
<ol class="org-ol">
<li>H(x) = - (от i = 1 до n)∑pi*log2(pi) , где pi - вероятность x (i=1&#x2026;n)</li>
<li>Частная энтропия Hi = -log2pi</li>
</ol>
</div>
</div>

<div id="outline-container-org5aa3a11" class="outline-3">
<h3 id="org5aa3a11"><span class="section-number-3">11.4.</span> Artificial general intelligence AGI or strong AI or full AI</h3>
<div class="outline-text-3" id="text-11-4">
<p>
Approaches:
</p>
</div>
<div id="outline-container-org28aff55" class="outline-4">
<h4 id="org28aff55"><span class="section-number-4">11.4.1.</span> Symbolic AI or Good Old Fashioned AI (GOFAI)</h4>
<div class="outline-text-4" id="text-11-4-1">
<p>
<a href="https://arxiv.org/pdf/1703.04368.pdf">https://arxiv.org/pdf/1703.04368.pdf</a>
</p>

<p>
based on high-level "symbolic" (human-readable) representations of problems, logic and search
</p>

<p>
"physical symbol systems hypothesis" - thinking is manipulation of symbols
</p>

<ul class="org-ul">
<li>symbols or strings are stored manually or incrementally in a <b>Knowledge Base</b>.</li>
<li>used to make intelligent conclusions and decisions based on the memorized facts and rules put together by
<b>propositional logic</b> (Логика высказываний) or first-order predicate calculus techniques (First-order
logic)</li>
</ul>
<p>
cons:
</p>
<ul class="org-ul">
<li>Patterns are not naturally inferred or picked up but have to be explicitly put together and spoon-fed to the
system</li>
<li>dynamically changing facts and rules are very hard to handle</li>
<li>learning procedures are monotonically incremental</li>
</ul>
</div>
</div>
<div id="outline-container-org84c6cd9" class="outline-4">
<h4 id="org84c6cd9"><span class="section-number-4">11.4.2.</span> Others</h4>
<div class="outline-text-4" id="text-11-4-2">
<ul class="org-ul">
<li>Deep learning</li>
<li>Bayesian networks</li>
<li>Evolutionary algorithms</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org8a8ab62" class="outline-3">
<h3 id="org8a8ab62"><span class="section-number-3">11.5.</span> Machine learning</h3>
<div class="outline-text-3" id="text-11-5">
<p>
Randomized algorithms fall into two rough categories:
</p>
<ul class="org-ul">
<li>Las Vegas algorithms always return precisely the correct answer. Consume a random amount</li>
</ul>
<p>
of resources, usually memory or time. Use <b>sampling</b>. Approximate the expectation by a corresponding average.
</p>
<ul class="org-ul">
<li>Monte Carlo algorithms return answers with a random amount of error. Error can typically be reduced by
expending more resources</li>
</ul>

<p>
MultiOutputClassifier(RandomForestClassifier(n<sub>estimators</sub> = 100, n<sub>jobs</sub> = 6))) - классификатор multi-target classification
</p>
</div>

<div id="outline-container-orgc919e2e" class="outline-4">
<h4 id="orgc919e2e"><span class="section-number-4">11.5.1.</span> ML techniques</h4>
<div class="outline-text-4" id="text-11-5-1">
</div>
<ol class="org-ol">
<li><a id="orgfb6e820"></a>linear<br />
<ol class="org-ol">
<li><a id="org840ebfb"></a>PCA<br />
<div class="outline-text-6" id="text-11-5-1-1-1">
<p>
уменьшает размерность и возвращает новые "components" на которые проецируются все фичи
</p>

<ul class="org-ul">
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a></li>
<li><a href="https://blog.bioturing.com/2018/06/14/principal-component-analysis-explained-simply/">https://blog.bioturing.com/2018/06/14/principal-component-analysis-explained-simply/</a></li>
</ul>

<p>
components_ - Principal Components - новые фичи на которые проецируются старые
</p>

<p>
How many principal components we can choose for our new feature subspace? A useful measure is the so-called
“explained variance ratio“. - насколько новая фича объясняет старые
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.decomposition <span style="color: #8ac6f2; font-weight: bold;">import</span> PCA
<span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.preprocessing <span style="color: #8ac6f2; font-weight: bold;">import</span> StandardScaler
<span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.pipeline <span style="color: #8ac6f2; font-weight: bold;">import</span> make_pipeline
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #cae682;">X</span> = np.array(df.drop([<span style="color: #95e454;">'result'</span>],1))
<span style="color: #cae682;">y</span> = np.array(df[<span style="color: #95e454;">'result'</span>])
<span style="color: #cae682;">scaler</span> = StandardScaler()
<span style="color: #cae682;">pca</span> = PCA()
<span style="color: #cae682;">pipeline</span> = make_pipeline(scaler, pca)
pipeline.fit(X, y)

<span style="color: #cae682;">features</span> = <span style="color: #e5786d;">range</span>(pca.n_components_)
<span style="color: #cae682;">feature_names</span> = <span style="color: #cae682;">features</span> = <span style="color: #e5786d;">range</span>(pca.n_components_)
plt.bar(features, pca.explained_variance_)

plt.xlabel(<span style="color: #95e454;">'PCA feature'</span>)
plt.ylabel(<span style="color: #95e454;">'variance'</span>)
plt.xticks(feature_names)
plt.show()

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Correlation between Features and Target Variable</span>
<span style="color: #cae682;">pca</span> = PCA(n_components=50)
<span style="color: #cae682;">X_new</span> = pca.fit_transform(X)
<span style="color: #cae682;">c</span> = DataFrame(X_new).corrwith(df[<span style="color: #95e454;">'result'</span>])
<span style="color: #e5786d;">print</span>(c.to_string())
</pre>
</div>
</div>
</li>
</ol>
</li>
<li><a id="org34d389f"></a>non-linear<br />
<div class="outline-text-5" id="text-11-5-1-2">
<ul class="org-ul">
<li>Regression Trees and Random Forest, which are tree-based non-linear algorithms</li>
<li>Gradient Boosting Machines (xgboost)</li>
<li>Support Vector Regression (SVR)</li>
<li>Neural Networks (NN) нейронные сети</li>
</ul>
</div>
</li>
<li><a id="orgeacc0f5"></a>common<br />
<div class="outline-text-5" id="text-11-5-1-3">
<ul class="org-ul">
<li>with images <a href="https://builtin.com/data-science/tour-top-10-algorithms-machine-learning-newbies">https://builtin.com/data-science/tour-top-10-algorithms-machine-learning-newbies</a></li>
<li>wide <a href="https://theappsolutions.com/blog/development/machine-learning-algorithm-types/">https://theappsolutions.com/blog/development/machine-learning-algorithm-types/</a></li>
</ul>
</div>
</li>
<li><a id="org3204339"></a>RandomForest<br />
<div class="outline-text-5" id="text-11-5-1-4">
<p>
from sklearn.ensemble import RandomForestClassifier
</p>
<ul class="org-ul">
<li>Ансамбль из sklearn.tree.DecisionTreeClassifier  on various sub-samples</li>
</ul>


<p>
<b>sklearn.tree.DecisionTreeClassifier</b>
</p>
<ul class="org-ul">
<li><a href="https://habr.com/en/company/ods/blog/322534/">https://habr.com/en/company/ods/blog/322534/</a></li>
</ul>


<p>
Плюсы:
</p>
<ul class="org-ul">
<li>Сильно несбалансированные классы</li>
<li>Порождение четких правил классификации, понятных человеку, например, "если возраст &lt; 25 и интерес к
мотоциклам, то отказать в кредите". Это свойство называют интерпретируемостью модели;</li>
<li>Деревья решений могут легко визуализироваться, то есть может "интерпретироваться" (строгого определения я не
видел) как сама модель (дерево), так и прогноз для отдельного взятого тестового объекта (путь в дереве);</li>
<li>Быстрые процессы обучения и прогнозирования;</li>
<li>Малое число параметров модели;</li>
<li>Поддержка и числовых, и категориальных признаков.</li>
</ul>

<p>
Минусы:
</p>
<ul class="org-ul">
<li>У порождения четких правил классификации есть и другая сторона: деревья очень чувствительны к шумам во
входных данных, вся модель может кардинально измениться, если немного изменится обучающая выборка (например,
если убрать один из признаков или добавить несколько объектов), поэтому и правила классификации могут сильно
изменяться, что ухудшает интерпретируемость модели;</li>
<li>Разделяющая граница, построенная деревом решений, имеет свои ограничения (состоит из гиперплоскостей,
перпендикулярных какой-то из координатной оси), и на практике дерево решений по качеству классификации
уступает некоторым другим методам;</li>
<li>Необходимость отсекать ветви дерева (pruning) или устанавливать минимальное число элементов в листьях дерева или максимальную глубину дерева для борьбы с переобучением. Впрочем, переобучение — проблема всех методов машинного обучения;</li>
<li>Нестабильность. Небольшие изменения в данных могут существенно изменять построенное дерево решений. С этой проблемой борются с помощью ансамблей деревьев решений (рассмотрим далее);</li>
<li>Проблема поиска оптимального дерева решений (минимального по размеру и способного без ошибок
классифицировать выборку) NP-полна, поэтому на практике используются эвристики типа жадного поиска признака
с максимальным приростом информации, которые не гарантируют нахождения глобально оптимального дерева;</li>
<li>Сложно поддерживаются пропуски в данных. Friedman оценил, что на поддержку пропусков в данных ушло около 50%
кода CART (классический алгоритм построения деревьев классификации и регрессии – Classification And
Regression Trees, в sklearn реализована улучшенная версия именно этого алгоритма);</li>
<li>Модель умеет только интерполировать, но не экстраполировать (это же верно и для леса и бустинга на
деревьях). То есть дерево решений делает константный прогноз для объектов, находящихся в признаковом
пространстве вне параллелепипеда, охватывающего все объекты обучающей выборки. В нашем примере с желтыми и
синими шариками это значит, что модель дает одинаковый прогноз для всех шариков с координатой &gt; 19 или &lt; 0.</li>
</ul>
</div>
</li>

<li><a id="orgcad0c5c"></a>XGBoost<br />
<div class="outline-text-5" id="text-11-5-1-5">
<ul class="org-ul">
<li>not require StandardScaler z=(x-mean)/std</li>
<li>XGBoost is not sensitive to monotonic transformations of its features for the same reason that decision
trees and random forests are not:  the model only needs to pick "cut points" on features to split a node</li>
<li>can enforce
<ul class="org-ul">
<li>Feature Interaction Constraints</li>
<li>Monotonic Constraints</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="org5152d75"></a><span class="todo TODO">TODO</span> Naive Bayes<br /></li>
<li><a id="org33922ac"></a>Метод ближайших соседей, KNeighbors, k-NN, knn<br />
<div class="outline-text-5" id="text-11-5-1-7">
<p>
<a href="https://github.com/spotify/annoy">https://github.com/spotify/annoy</a>
sklearn.neighbors.KNeighborsClassifier
</p>
</div>
<ol class="org-ol">
<li><a id="orge072fba"></a>how<br />
<div class="outline-text-6" id="text-11-5-1-7-1">
<p>
use metric, euclidian by default.
</p>

<p>
Find a predefined number of training samples closest in distance to the new point, and predict the label from
 these. By popularity or by distance.
</p>
<ul class="org-ul">
<li>k-nearest neighbor learning: user-defined constant.</li>
<li>radius-based neighbor learning: vary based on the local density of points.</li>
</ul>
</div>
</li>
<li><a id="org41e1535"></a>theory<br />
<div class="outline-text-6" id="text-11-5-1-7-2">
<p>
known as <b>lazy learner</b> or <b>non-generalizing</b> machine learning methods, since they simply “remember” all of
 its training data (possibly transformed into a fast indexing structure such as a Ball Tree or KD
 Tree). memories the entire training dataset and performs action on the dataset at the time of classification.
</p>

<p>
<b>nonparametric</b> - does not make assumptions about the data it is analyzing.
</p>

<p>
has implementations:
</p>
<ul class="org-ul">
<li>brute-force search - computation of distances between all pairs of points
<ul class="org-ul">
<li>based on routines in sklearn.metrics.pairwise.</li>
</ul></li>
<li>KDTree - use triangle inequality to reduce computations</li>
<li>BallTree - for very high dimensions</li>
</ul>

<p>
При взвешенном способе во внимание принимается не только количество попавших в область определённых классов, но и их удалённость от нового значения.
</p>
</div>
</li>
<li><a id="org517bf05"></a>Плюсы:<br />
<div class="outline-text-6" id="text-11-5-1-7-3">
<ul class="org-ul">
<li>robustness towards noisy data</li>
<li>Простая реализация;</li>
<li>Неплохо изучен теоретически;</li>
<li>Как правило, метод хорош для первого решения задачи, причем не только классификации или регрессии, но и,
например, рекомендации;</li>
<li>Можно адаптировать под нужную задачу выбором метрики или ядра (в двух словах: ядро может задавать операцию
сходства для сложных объектов типа графов, а сам подход kNN остается тем же). Кстати, профессор ВМК МГУ и
опытный участник соревнований по анализу данных Александр Дьяконов любит самый простой kNN, но с настроенной
метрикой сходства объектов.</li>
<li>Неплохая интерпретация, можно объяснить, почему тестовый пример был классифицирован именно так. Хотя этот
аргумент можно атаковать: если число соседей большое, то интерпретация ухудшается (условно: "мы не дали ему
кредит, потому что он похож на 350 клиентов, из которых 70 – плохие, что на 12% больше, чем в среднем по
выборке").</li>
<li>nonparametric</li>
</ul>
</div>
</li>
<li><a id="org1a52123"></a>Минусы:<br />
<div class="outline-text-6" id="text-11-5-1-7-4">
<ul class="org-ul">
<li>Метод считается быстрым в сравнении, например, с композициями алгоритмов, но в реальных задачах, как
правило, число соседей, используемых для классификации, будет большим (100-150), и в таком случае алгоритм
будет работать не так быстро, как дерево решений;</li>
<li>Если в наборе данных много признаков, то трудно подобрать подходящие веса и определить, какие признаки не важны для классификации/регрессии;</li>
<li>Зависимость от выбранной метрики расстояния между примерами. Выбор по умолчанию евклидового расстояния чаще
всего ничем не обоснован. Можно отыскать хорошее решение перебором параметров, но для большого набора данных
это отнимает много времени;</li>
<li>Нет теоретических оснований выбора определенного числа соседей — только перебор (впрочем, чаще всего это
верно для всех гиперпараметров всех моделей). В случае малого числа соседей метод чувствителен к выбросам, то
есть склонен переобучаться;</li>
<li>Как правило, плохо работает, когда признаков много, из-за "прояклятия размерности". Про это хорошо
рассказывает известный в ML-сообществе профессор Pedro Domingos – тут в популярной статье "A Few Useful
Things to Know about Machine Learning", также "the curse of dimensionality" описывается в книге Deep Learning
в главе "Machine Learning basics".</li>
</ul>

<p>
Metric Learning used to solve disadvantage of selecting classic distance metric.
</p>
</div>
</li>
<li><a id="orgb47ea43"></a>usage<br />
<div class="outline-text-6" id="text-11-5-1-7-5">
<ul class="org-ul">
<li><b>Classification</b> - majority voting</li>
<li><b>regression</b> - the average of the values is taken to be the final prediction</li>
</ul>


<ul class="org-ul">
<li>KNeighborsClassifier - classification based on K nearest neighbors of each query point.</li>
<li>RadiusNeighborsClassifier - fixed radious r.</li>
</ul>

<p>
select K:
</p>
<ul class="org-ul">
<li>Low values for K=(1,2) may be noisy and subject to the effects of outliers.</li>
<li>Large values smooth over things, category with only a few samples in it will always be out voted by other
categories.</li>
</ul>

<p>
metric, classifier: minkowski
</p>

<p>
Normalization is required.
</p>
</div>
<ol class="org-ol">
<li><a id="org7efcfe4"></a>outliers<br />
<div class="outline-text-7" id="text-11-5-1-7-5-1">
<p>
Outliers - a training examples surrounded by examples of other classes.
</p>

<ul class="org-ul">
<li><b>(k,r)NN class-outlier</b> - if its k nearest neighbors include more than r examples of other classes.
<ul class="org-ul">
<li>k&gt;r0&gt;</li>
</ul></li>
</ul>
</div>
</li>

<li><a id="org3d80691"></a>Elbow Curve Validation Technique in K-Nearest Neighbor Algorithm<br />
<div class="outline-text-7" id="text-11-5-1-7-5-2">
<p>
For a very low value of k (suppose k=1), the model overfits on the training data, which leads to a high error
 rate on the validation set. On the other hand, for a high value of k, the model performs poorly on both train
 and validation set.
</p>

<p>
<b>elbow curve of the validation error.</b>
</p>
</div>
</li>
</ol>
</li>

<li><a id="orgd31560d"></a>problem: not count density<br />
<div class="outline-text-6" id="text-11-5-1-7-6">
<p>
<a href="https://koaning.io/posts/high-on-probability-low-on-certainty/">https://koaning.io/posts/high-on-probability-low-on-certainty/</a>
</p>

<p>
They use GaussianMixture to score density
</p>
</div>
</li>
</ol>
</li>
<li><a id="org561b5f0"></a>Gradient boosting<br />
<div class="outline-text-5" id="text-11-5-1-8">
<ul class="org-ul">
<li>открытый курс <a href="https://habr.com/ru/company/ods/blog/327250/">https://habr.com/ru/company/ods/blog/327250/</a></li>
</ul>
<p>
technique for regression and classification problems - typically decision trees
</p>

<p>
Бустинг, использующий деревья решений в качестве базовых алгоритмов, называется градиентным бустингом над
 решающими деревьями, Gradient Boosting on Decision Trees, GBDT
</p>

<p>
steps:
</p>
<ul class="org-ul">
<li>Сначала мы моделируем с помощью простых методов и анализируем результат на предмет ошибок. Эти ошибки
означают точки данных, которые трудно вписать в существующую модель.</li>
<li>Затем, в более поздних моделях, мы особенно сосредотачиваемся на тех данных, которые трудно "уложить".</li>
<li>В конце мы группируем все методы, присваивая каждому из них вес.</li>
</ul>

<p>
objective is to minimize the loss of the model by adding weak learners using a gradient descent like procedure.
</p>
<ul class="org-ul">
<li>gradient descent procedure is used to minimize the loss when adding trees.</li>
<li>radient descent is used to minimize a set of parameters, such as the coefficients in a regression equation
or weights in a neural network</li>
</ul>

<p>
интрументы:
</p>
<ul class="org-ul">
<li>faster <a href="https://github.com/Microsoft/LightGBM">https://github.com/Microsoft/LightGBM</a></li>
<li>better <a href="https://github.com/dmlc/xgboost">https://github.com/dmlc/xgboost</a></li>
<li>CatBoost - Yandex</li>
<li>LightGBM - Microsoft</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org5d2e8bc"></a>вход<br />
<div class="outline-text-6" id="text-11-5-1-8-1">
<p>
На вход алгоритма нужно собрать несколько составляющих:
</p>
<ul class="org-ul">
<li>пары {xi,yi}</li>
<li>число итерация M</li>
<li>выбор функции потерь</li>
<li>выбор семейства функций базовых алгоритмов h(x,θ) c процедурой их обучения</li>
<li>дополнительные гиперпараметры h(x,θ), например глубина деревьев</li>
</ul>
</div>
</li>
<li><a id="org27b4180"></a>xgboost example<br />
<div class="outline-text-6" id="text-11-5-1-8-2">
<p>
<a href="https://www.kaggle.com/cbrogan/xgboost-example-python">https://www.kaggle.com/cbrogan/xgboost-example-python</a>
</p>
</div>
</li>

<li><a id="orgb0982fb"></a>как работает<br />
<div class="outline-text-6" id="text-11-5-1-8-3">
<p>
Функциональный градиентный спуск.
</p>

<p>
Придется ограничить свой поиск каким-то семейством функций
</p>
</div>
</li>

<li><a id="org418e767"></a>веса<br />
<div class="outline-text-6" id="text-11-5-1-8-4">
<p>
<a href="https://habr.com/en/company/ods/blog/327250/#2-gbm-algoritm">https://habr.com/en/company/ods/blog/327250/#2-gbm-algoritm</a>
задание весов для балансировки классов
</p>

<p>
общие требования разумности весов:
</p>
<ul class="org-ul">
<li>wi ∈R</li>
<li>wi &gt;=0</li>
<li>∑wi &gt;0</li>
</ul>

<p>
Веса позволяют существенно сократить время на подстройку самой функции потерь под решаемую задачу,
</p>

<p>
В общем случае, привязывая веса к значениям , мы можем прострелить себе колено.
</p>
</div>
</li>
<li><a id="org1f875a9"></a>History<br />
<div class="outline-text-6" id="text-11-5-1-8-5">
<ul class="org-ul">
<li>вопрос Можно ли из слабых моделей получить сильную</li>
<li>утвредительный ответ <a href="http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf">http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf</a></li>
<li>2003 Adaboost (with decision trees as the weak learners) Их общий подход заключался в жадном построении
линейной комбинации простых моделей (базовых алгоритмов) путем перевзвешивания входных данных. Каждая
последующая модель строилась таким образом, чтобы придавать больший вес и
предпочтение ранее некорректно предсказанным наблюдениям. см <a href="#org4eee0fb">8.19.5</a></li>
<li>1999 by Jerome Friedman. Gradient Boosting Machine (GBM) Но при построении следующей простой модели, она строится не просто на
перевзвешенных наблюдениях, а так, чтобы лучшим образом приближать общий градиент целевой функции.</li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="orgd5b8dbc"></a>k-fold cross-validation<br />
<div class="outline-text-5" id="text-11-5-1-9">
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">https://en.wikipedia.org/wiki/Cross-validation_(statistics)</a></li>
<li><a href="https://scikit-learn.org/stable/modules/cross_validation.html">https://scikit-learn.org/stable/modules/cross_validation.html</a></li>
</ul>

<p>
Does not waste too much data.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">round1</td>
<td class="org-left">round2</td>
</tr>

<tr>
<td class="org-left">fold1-test</td>
<td class="org-left">fold1</td>
</tr>

<tr>
<td class="org-left">fold2</td>
<td class="org-left">fold2-test</td>
</tr>

<tr>
<td class="org-left">fold3</td>
<td class="org-left">fold3</td>
</tr>
</tbody>
</table>

<p>
Types:
</p>
<ul class="org-ul">
<li>k-fold</li>
<li>stratified k-fold cross-validation - each partition contains roughly the same proportions of the two types
of class labels</li>
<li>repeated cross-validation the data is randomly split into k partitions several times</li>
</ul>


<p>
Кросс-валидация дает лучшую по сравнению с отложенной выборкой оценку качества модели на новых данных. Но
кросс-валидация вычислительно дорогостоящая, если данных много.
</p>

<p>
с ее помощью выбираются гиперпараметры моделей, сравниваются модели между собой, оценивается полезность новых
признаков в задаче и т.д
</p>

<pre class="example">
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5, scoring='gini')
</pre>


<pre class="example">
from sklearn.model_selection import KFold
kf = KFold(n_splits=2)
for train, test in kf.split(X): # train,test - indexes
</pre>
</div>
</li>

<li><a id="org21dcab9"></a>NOT Independent and Identically Distributed (i.i.d.)<br /></li>

<li><a id="org085e78c"></a><span class="todo TODO">TODO</span> Станислав семенов<br />
<div class="outline-text-5" id="text-11-5-1-11">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=NVKDSNM702k">https://www.youtube.com/watch?v=NVKDSNM702k</a></li>
<li><a href="https://www.youtube.com/watch?v=g335THJxkto">https://www.youtube.com/watch?v=g335THJxkto</a></li>
</ul>
</div>
</li>

<li><a id="org7cfb98b"></a>категориальные данные и smooth likelihood<br />
<div class="outline-text-5" id="text-11-5-1-12">
<p>
<a href="https://www.youtube.com/watch?v=NVKDSNM702k">https://www.youtube.com/watch?v=NVKDSNM702k</a>
</p>
</div>
</li>
<li><a id="org5bd4670"></a>Bayes Theorem (prior/likelihood/posterior/evidence)<br />
<div class="outline-text-5" id="text-11-5-1-13">
<p>
P(X|Y) = ( P(Y|X) * P(X) ) / P(Y)
Posterior = ( Likelihood * Prior ) /  Evidence
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgd90e5d4" class="outline-4">
<h4 id="orgd90e5d4"><span class="section-number-4">11.5.2.</span> terms</h4>
<div class="outline-text-4" id="text-11-5-2">
<p>
регрессия - набор методов использующих корреляцию между x и у - цель найти функцию - она же регрессия
</p>

<p>
линией регрессии - регрессия выражается линейной моделью первого порядка y=bx+a
</p>
</div>
</div>

<div id="outline-container-orgf29ce46" class="outline-4">
<h4 id="orgf29ce46"><span class="section-number-4">11.5.3.</span> Смещение и дисперсия для анализа переобучения</h4>
<div class="outline-text-4" id="text-11-5-3">
<ul class="org-ul">
<li><a href="https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D0%BB%D0%B5%D0%BC%D0%BC%D0%B0_%D1%81%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D1%8F%E2%80%93%D0%B4%D0%B8%D1%81%D0%BF%D0%B5%D1%80%D1%81%D0%B8%D0%B8">https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D0%BB%D0%B5%D0%BC%D0%BC%D0%B0_%D1%81%D0%BC%D0%B5%D1%89%D0%B5%D0%BD%D0%B8%D1%8F%E2%80%93%D0%B4%D0%B8%D1%81%D0%BF%D0%B5%D1%80%D1%81%D0%B8%D0%B8</a></li>
<li>Смещение - ошибку, вызванную упрощением предположений, принятых в методе
<ul class="org-ul">
<li>высокое - много ошибок на любых выбоках из той же совокупности</li>
<li>низкое - хорошо подогнана под обуч выборку</li>
</ul></li>
<li>Дисперсия -  как далеко метод обучения уведёт от среднего значения
<ul class="org-ul">
<li>высокая - любые две обуч выборки = разные модели</li>
<li>низкая - любые две обуч выборки = похожие модели</li>
</ul></li>
</ul>

<p>
выс С + низ Д = недообучение
</p>

<p>
низ С + выс Д = переобучение
</p>

<ul class="org-ul">
<li>Снижение размерности и отбор признаков могут уменьшить дисперсию путём упрощения моделей.</li>
<li>больше тренировочное множество приводит к уменьшению дисперсии</li>
<li>Добавление признаков (предсказателей) ведёт к уменьшению смещения за счёт увеличения дисперсии</li>
<li>В NN дисперсия увеличивается и смещение уменьшается с увеличением числа скрытых единиц</li>
</ul>
</div>
</div>
<div id="outline-container-org5c8671f" class="outline-4">
<h4 id="org5c8671f"><span class="section-number-4">11.5.4.</span> Regression vs. classification</h4>
<div class="outline-text-4" id="text-11-5-4">
<ul class="org-ul">
<li>A regression model predicts continuous values
<ul class="org-ul">
<li>What is the value of a house in California?</li>
</ul></li>
<li>classification model predicts discrete values
<ul class="org-ul">
<li>Is a given email message spam or not spam?</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org1d1660e" class="outline-4">
<h4 id="org1d1660e"><span class="section-number-4">11.5.5.</span> Reducing Loss (loss function) or cost function or residual <a id="orgf004120"></a></h4>
<div class="outline-text-4" id="text-11-5-5">
<ul class="org-ul">
<li>TODO <a href="https://aboveintelligent.com/deep-learning-basics-the-score-function-cross-entropy-d6cc20c9f972">https://aboveintelligent.com/deep-learning-basics-the-score-function-cross-entropy-d6cc20c9f972</a></li>
<li><a href="https://arxiv.org/pdf/1702.05659.pdf">https://arxiv.org/pdf/1702.05659.pdf</a></li>
<li><a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">https://en.wikipedia.org/wiki/Loss_functions_for_classification</a></li>
<li>Definition: Getting the examples right</li>
<li>optimization problem seeks to minimize a loss function</li>
</ul>
<p>
Metric articles:
</p>
<ul class="org-ul">
<li>P1 Regression <a href="https://www.kdnuggets.com/2018/04/right-metric-evaluating-machine-learning-models-1.html">https://www.kdnuggets.com/2018/04/right-metric-evaluating-machine-learning-models-1.html</a></li>
<li>P2 Classification <a href="https://www.kdnuggets.com/2018/06/right-metric-evaluating-machine-learning-models-2.html">https://www.kdnuggets.com/2018/06/right-metric-evaluating-machine-learning-models-2.html</a></li>
</ul>

<p>
loss - for single prediction, cost - for entire dataset (metric), norm - in math
</p>

<p>
Types:
</p>
<ul class="org-ul">
<li><b>MAE</b> Mean absolute error = (∑|yi-xi|)/n</li>
<li><b>MAPE</b> Mean absolute percentage error = 1/n * ∑ ((at-pt)/at) , a - actual, p - prediction ( best for
precition)</li>
<li><b>Mean square error (MSE)</b> average squared loss per example 1/n*∑(true<sub>label</sub> - prediction(x))2.
<ul class="org-ul">
<li>нельзя применять если есть выбросы</li>
<li>since n is constant f(x) and cf(x) have the same x minimum point, we can drop 1/n, L(y,o) = ∑(y-o)<sup>2</sup></li>
<li>partial derivative ∂/∂oL = ∂/∂oj(i)∑(y-oj)<sup>2</sup></li>
<li>we can remove sum becouse of the partial derivative for i ≠ j is 0.</li>
<li>∂/∂oL = -2(y-o) <a href="https://explained.ai/gradient-boosting/descent.html">https://explained.ai/gradient-boosting/descent.html</a></li>
<li>if using Sigmoid as the activation function, the quadratic loss function would suffer the problem of slow
convergence (learning speed)</li>
</ul></li>
<li><b>RMSE</b> - square root of MSE</li>
<li>RMSLE - (∑(log(|1-yi-xi|)-log(|1-xi|)))/n</li>
</ul>

<p>
If either predicted or the actual value is big : RMSE &gt; RMSLE
</p>


<p>
All loss functions o - output, y - true label, σ - probability estimate:
</p>
<ul class="org-ul">
<li>L1 loss = ∑|y-o| - Mean Absolute Error</li>
<li>L2 = ∑|y-o|<sup>2</sup> - Mean Squared Error</li>
<li>log (cross entropy) loss = -∑y*logσ(o)</li>
<li>log<sup>2</sup> squared log loss = -∑[y*logσ(o)]<sup>2</sup></li>
</ul>

<p>
Reducing error:
</p>
<ul class="org-ul">
<li>Stochastic Gradient Descent: one example at a time</li>
<li>Mini-Batch Gradient Descent: batches of 10-1000
<ul class="org-ul">
<li>Loss &amp; gradients are averaged over the batch</li>
</ul></li>
</ul>
</div>


<ol class="org-ol">
<li><a id="org5171e52"></a>comparision L1 and L2<br />
<div class="outline-text-5" id="text-11-5-5-1">
<ul class="org-ul">
<li>L1 - manhattan metric</li>
<li>L2 - euclidian metric</li>
</ul>

<p>
L2 is much more sensitive to outliers because the differences are squared, whilst L1 is the absolute
  difference and is therefore not as sensitive
</p>

<ul class="org-ul">
<li>L1 - yeild median</li>
<li>L2 - yeild mean</li>
</ul>

<p>
The median is the middle value in a set of data, which is calculated by finding the data point with the
 smallest sum of absolute differences from all other data points.
</p>

<p>
The mean is the average value of a set of data points, which is calculated by finding the coordinates of the
 point that minimizes the sum of the squared distances from all other points.
</p>


<p>
L1 regularization is the preferred choice when having a high number of features as it provides sparse
  solutions. Even, we obtain the computational advantage because features with zero coefficients can be
  avoided.
</p>

<p>
L1 regularization can be helpful in features selection by eradicating the unimportant features, whereas, L2
  regularization is not recommended for feature selection. (variance with L1 plays more)
</p>

<p>
L1 doesn’t have a closed form solution since it includes an absolute value and it is a non-differentiable
 function.  L1 regularization is relatively more expensive in computation, it can’t be solved in the context
 of matrix measurement and heavily relies on approximations.
</p>

<ul class="org-ul">
<li>mean median <a href="http://sepwww.stanford.edu/public/docs/gem/noiz/paper_html/node2.html">http://sepwww.stanford.edu/public/docs/gem/noiz/paper_html/node2.html</a></li>
<li>l1 vs l2 <a href="https://www.analyticssteps.com/blogs/l2-and-l1-regularization-machine-learning">https://www.analyticssteps.com/blogs/l2-and-l1-regularization-machine-learning</a></li>
</ul>
</div>
</li>
<li><a id="org6f8fd27"></a>cross-entropy cost function<br />
<div class="outline-text-5" id="text-11-5-5-2">
<ul class="org-ul">
<li>or Logistic Loss or Multinomial Logistic Loss</li>
<li><a href="https://gombru.github.io/2018/05/23/cross_entropy_loss/">https://gombru.github.io/2018/05/23/cross_entropy_loss/</a></li>
</ul>
<p>
<b>cross entropy</b> for classification with probability value between 0 and 1
</p>
<ul class="org-ul">
<li>CE  = - ∑y*log(x)</li>
<li>-y*log(p)+(1-y)log(1-p) - binary classification problem</li>
<li>x and y should be between [0,1] -&gt; softmax required</li>
</ul>

<p>
Categorical Cross-Entropy Loss CE = -∑ti*log(si) где si выход (0;1) ti - истынные si - полученные, i -
выходы - multi-class classification
</p>
<ul class="org-ul">
<li>если</li>
</ul>
</div>
</li>

<li><a id="org16f5f0c"></a>Hinge loss<br />
<div class="outline-text-5" id="text-11-5-5-3">
<ul class="org-ul">
<li>intended output t = ±1, prediction = y = (-2;1)</li>
<li>l(y) = max(0, 1-t*y)</li>
<li>for softsign</li>
</ul>

<p>
ex
</p>
<ul class="org-ul">
<li>t = 1
<ul class="org-ul">
<li>y = -1</li>
<li>l = 0,2 = 2</li>
</ul></li>
<li>t = -1
<ul class="org-ul">
<li>y = 1</li>
<li>l = 0,3 = 3</li>
</ul></li>
</ul>


<div class="org-src-container">
<pre class="src src-text">  l(y)
   ^
   |
  3+
   |\
   |  \
   |    \
   |      \
   |        \
   |          \
   |            \
  1+-------------+\
   |             |  \
   |             |    \
   +-------------+-----+---------&gt; y
  -2             0     1
</pre>
</div>
</div>
</li>
<li><a id="org6b9337b"></a>Note<br />
<div class="outline-text-5" id="text-11-5-5-4">
<ul class="org-ul">
<li>square loss function tends to penalize outliers excessively, leading to slower convergence rates (with
regards to sample complexity) than for the logistic loss or hinge loss functions.</li>
<li>logistic loss grows linearly for negative values which make it less sensitive to outliers.</li>
</ul>
</div>
</li>
<li><a id="org7b50ba7"></a>Additive Angular Mergin Loss for images<br />
<div class="outline-text-5" id="text-11-5-5-5">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1801.07698.pdf">https://arxiv.org/pdf/1801.07698.pdf</a></li>
<li><a href="https://www.kaggle.com/code/alifrahman/landmark-recognition2020-google">https://www.kaggle.com/code/alifrahman/landmark-recognition2020-google</a></li>
<li><a href="https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py">https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/blob/master/src/modeling/metric_learning.py</a></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org64c7dc6" class="outline-4">
<h4 id="org64c7dc6"><span class="section-number-4">11.5.6.</span> Regularization Overfeed problem</h4>
<div class="outline-text-4" id="text-11-5-6">
<ul class="org-ul">
<li>l1 l2 Not trust your examples too much <a href="http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/">http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/</a></li>
</ul>

<p>
<b>technique to prevent overfitting</b>
</p>
<ol class="org-ol">
<li>Explicit regularization - add term to loss function, term to penalize complexity of f(x)</li>
<li>all others</li>
</ol>

<p>
term example:
</p>
<ul class="org-ul">
<li>Loss = (y-y')<sup>2</sup> + b*b,  where y'= y(x<sub>i</sub>, b)</li>
</ul>

<p>
Strategies:
</p>
<ul class="org-ul">
<li>data augmentation</li>
<li>early stopping - get at the bottom of validation data lose curve.</li>
<li>Penalizing Model Complexity
<ul class="org-ul">
<li>lower training error</li>
<li>Prefer smaller weights</li>
<li>methods:
<ul class="org-ul">
<li>L1 (Lasso Regression) Least Absolute Shrinkage and Selection Operator
<ul class="org-ul">
<li>Cost function - ∑|(y-∑x*b)|+λ∑|b|</li>
</ul></li>
<li>L2 (Ridge Regression)
<ul class="org-ul">
<li>Cost function - ∑(y-∑x*b)<sup>2</sup>+λ∑b<sup>2</sup></li>
</ul></li>
<li>Dropout - randomly drop units from the neural network during training - prevents units from co-adapting
too much</li>
<li>artificial expansion of the training data</li>
</ul></li>
</ul></li>
</ul>


<p>
keras: Dense(32, activity<sub>regularizer</sub>=l1(0.001))
</p>
</div>
</div>
<div id="outline-container-orgf7d47c8" class="outline-4">
<h4 id="orgf7d47c8"><span class="section-number-4">11.5.7.</span> Sampling</h4>
<div class="outline-text-4" id="text-11-5-7">
<ul class="org-ul">
<li>magnitude more examples than trainable parameters</li>
<li>Simple models on large data sets generally beat fancy models on small data sets.</li>
<li>Серединные данные, не слишком частые и не слишком редкие</li>
<li>Reliability</li>
<li>Do unto training as you would do unto prediction. That is, the more closely your training task matches your
prediction task, the better your ML system will perform.</li>
<li>80% of the time on a machine learning project is spent constructing data sets and transforming data</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org0353a56"></a>Skew and Class Imbalance Problem<br />
<div class="outline-text-5" id="text-11-5-7-1">
<p>
A classification data set with skewed class proportions is called imbalanced.
</p>
<ul class="org-ul">
<li>majority classes and minority classes with smaller proportion</li>
</ul>

<p>
Degree of imbalance:
</p>
<ul class="org-ul">
<li>Mild 		20-40% of the data set</li>
<li>Moderate 	1-20% of the data set</li>
<li>Extreme 	&lt;1% of the data set</li>
</ul>

<p>
First try training on the true distribution. If the model works well and generalizes, you're done
</p>

<p>
approaches:
</p>
<ul class="org-ul">
<li>Cost function</li>
<li>Sampling
<ul class="org-ul">
<li>Oversampling - does not provide any additional information to the model.</li>
<li>SMOTE: Synthetic Minority Over-sampling Technique <a href="https://arxiv.org/abs/1106.1813">https://arxiv.org/abs/1106.1813</a>
<ul class="org-ul">
<li>more effective for binary</li>
</ul></li>
<li>ADASYN <a href="http://arxiv.org/abs/2105.04301v6">http://arxiv.org/abs/2105.04301v6</a></li>
<li>MUNGE</li>
</ul></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org17ef55a"></a>SMOTE<br />
<div class="outline-text-6" id="text-11-5-7-1-1">
<p>
Problem: kNN require that all features be scaled to be equal for kNN metric.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">SMOTE</span>(T, N:<span style="color: #e5786d;">int</span>, k:<span style="color: #e5786d;">int</span>):
    <span style="color: #f08080; font-style: italic;">"""</span>
<span style="color: #f08080; font-style: italic;">    Returns (N/100) * n_minority_samples synthetic minority samples.</span>

<span style="color: #f08080; font-style: italic;">    Parameters</span>
<span style="color: #f08080; font-style: italic;">    ----------</span>
<span style="color: #f08080; font-style: italic;">    T : array-like, shape = [n_minority_samples, n_features]</span>
<span style="color: #f08080; font-style: italic;">        Holds the minority samples</span>
<span style="color: #f08080; font-style: italic;">    N : percetange of new synthetic samples:</span>
<span style="color: #f08080; font-style: italic;">        n_synthetic_samples = N/100 * n_minority_samples. Can be &lt; 100.</span>
<span style="color: #f08080; font-style: italic;">    k : int. Number of nearest neighbours.</span>

<span style="color: #f08080; font-style: italic;">    Returns</span>
<span style="color: #f08080; font-style: italic;">    -------</span>
<span style="color: #f08080; font-style: italic;">    S : array, shape = [(N/100) * n_minority_samples, n_features]</span>
<span style="color: #f08080; font-style: italic;">    """</span>
    <span style="color: #cae682;">n_minority_samples</span>, <span style="color: #cae682;">n_features</span> = T.shape <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">rows, columns</span>

    <span style="color: #8ac6f2; font-weight: bold;">if</span> N &lt; 100:
        <span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">create synthetic samples only for a subset of T.</span>
        <span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">TODO: select random minortiy samples</span>
        <span style="color: #cae682;">N</span> = 100
        <span style="color: #8ac6f2; font-weight: bold;">pass</span>

    <span style="color: #8ac6f2; font-weight: bold;">if</span> (N % 100) != 0:
        <span style="color: #8ac6f2; font-weight: bold;">raise</span> <span style="color: #92a65e; font-weight: bold;">ValueError</span>(<span style="color: #95e454;">"N must be &lt; 100 or multiple of 100"</span>)

    <span style="color: #cae682;">NN</span> = N//100
    <span style="color: #e5786d;">print</span>(N/100, n_minority_samples)
    <span style="color: #cae682;">n_synthetic_samples</span> = <span style="color: #e5786d;">round</span>(NN * n_minority_samples) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">20%</span>
    <span style="color: #e5786d;">print</span>(n_synthetic_samples, n_features)
    <span style="color: #cae682;">S</span> = np.zeros(shape=(n_synthetic_samples, n_features))
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"S.shape"</span>, S.shape)

    <span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">Learn nearest neighbours</span>
    <span style="color: #cae682;">neigh</span> = NearestNeighbors(n_neighbors = k)
    neigh.fit(T)

    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"n_minority_samples"</span>, n_minority_samples) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">i - 0-&gt; rows</span>
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"N"</span>, N) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">n - 0 -&gt; N</span>
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">- for each source row</span>
    <span style="color: #8ac6f2; font-weight: bold;">for</span> i <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(n_minority_samples): <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">per row in source</span>
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">get most same rows</span>
        <span style="color: #cae682;">nn</span> = neigh.kneighbors([T[i]], return_distance=<span style="color: #e5786d; font-weight: bold;">False</span>)
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">- repeat for how many we need</span>
        <span style="color: #8ac6f2; font-weight: bold;">for</span> n <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(NN): <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">2</span>
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">- what row we will copy</span>
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">nn_index = nn[0][k-n-1]</span>
            <span style="color: #cae682;">nn_index</span> = nn[0][np.random.randint(1, k-1)]
            <span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">NOTE: nn includes T[i], we don't want to select it</span>
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">c = k-1</span>
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">while nn_index == i:</span>
            <span style="color: #fa8072;">#     </span><span style="color: #99968b; font-style: italic;"># nn_index = choice(nn[0])</span>
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">- new row will be between this and same one.</span>
            <span style="color: #cae682;">dif</span> = T[nn_index] - T[i] <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">row</span>
            <span style="color: #cae682;">gap</span> = np.random.random()
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">[i,:] - row</span>
            <span style="color: #cae682;">S</span>[i*NN + n, :] = T[i,:] + gap * dif[:]
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">S[n + i, :] = T.iloc[i].to_numpy() + gap * dif[:]</span>
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">-i -n1</span>
            <span style="color: #fa8072;">#    </span><span style="color: #99968b; font-style: italic;">-n2</span>
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">-i -n1 2+1</span>
            <span style="color: #fa8072;">#    </span><span style="color: #99968b; font-style: italic;">-n2</span>
    <span style="color: #8ac6f2; font-weight: bold;">return</span> S
</pre>
</div>
</div>
</li>
<li><a id="orga375f02"></a>links<br />
<div class="outline-text-6" id="text-11-5-7-1-2">
<ul class="org-ul">
<li><a href="http://www.chioka.in/class-imbalance-problem/">http://www.chioka.in/class-imbalance-problem/</a></li>
<li><a href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/">https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/</a></li>
<li><a href="https://www.activeloop.ai/resources/glossary/adaptive-synthetic-sampling-adasyn/">https://www.activeloop.ai/resources/glossary/adaptive-synthetic-sampling-adasyn/</a></li>
<li>Handling Imbalanced Data: A Case Study for Binary Class Problems <a href="https://arxiv.org/abs/2010.04326v1">https://arxiv.org/abs/2010.04326v1</a></li>
<li><a href="https://learn-scikit.oneoffcoder.com/imbalanced-learn.html">https://learn-scikit.oneoffcoder.com/imbalanced-learn.html</a></li>
</ul>
</div>
</li>
</ol>
</li>
</ol>
</div>

<div id="outline-container-org116d8ef" class="outline-4">
<h4 id="org116d8ef"><span class="section-number-4">11.5.8.</span> CRF Conditional random field</h4>
<div class="outline-text-4" id="text-11-5-8">
<p>
sequence modeling
</p>

<p>
Whereas a discrete classifier predicts a label for a single sample without considering "neighboring" samples,
 a CRF can take context into account; e.g., the linear chain CRF (which is popular in natural language
 processing) predicts sequences of labels for sequences of input samples.
</p>
</div>
</div>

<div id="outline-container-orge335edd" class="outline-4">
<h4 id="orge335edd"><span class="section-number-4">11.5.9.</span> типы обучения</h4>
<div class="outline-text-4" id="text-11-5-9">
</div>
<ol class="org-ol">
<li><a id="orgd1b4001"></a>supervised, unsupervised, reinforcement<br />
<div class="outline-text-5" id="text-11-5-9-1">
<p>
3 типа:
</p>
<ul class="org-ul">
<li>Обучение с учителем (supervised learning) - (x1,y1),(x2,y2),&#x2026;(xN,yN)
<ul class="org-ul">
<li>e.g. regression, classification.</li>
</ul></li>
<li>Обучение без учителя (unsupervised learning or deep learning) x1,x2,&#x2026;xN -&gt; ?
<ul class="org-ul">
<li>e.g. dimensionality reduction, clustering, outlier analysis, representation learning (feature extractors)</li>
</ul></li>
<li>Обучение с подкреплением (reinforcement learning) - an agent takes actions in an environment, which is
interpreted into a reward and a representation of the state. сеть постоянно улучшалась, играя с одной из
сетей, полученных ранее.  Instead of minimizing an error, reinforcement learning maximizes a reward.
<ul class="org-ul">
<li>по Розенблатт способов обучения:
<ul class="org-ul">
<li>Гамма-системой подкрепления - веса всех активных связей сначала изменяются на равную величину, а затем
из их всех весов связей вычитается другая величина, равная полному изменению весов всех активных связей,
делённому на число всех связей</li>
<li>Альфа-системой подкрепления - веса всех активных связей cij, которые ведут к элементу uj, изменяются на
одинаковую величину r, а веса неактивных связей за это время не изменяются.</li>
</ul></li>
</ul></li>
<li>Частичным подкреплением (Semi-supervised learning) - дополнительные неразмеченные данные
<ul class="org-ul">
<li>(x1,y1),(x2,y2),&#x2026;(xN,yN),xN+1,xN+2,&#x2026;xN+M</li>
<li>transductive inference - reasoning from observed, specific (training) cases to specific (test) cases</li>
<li>induction is reasoning from observed training cases to general rules</li>
</ul></li>
<li>Transfer learning - обучили модель на большом наборе данных, applying it to a different but related problem</li>
</ul>

<p>
Другая классификация
</p>
<ul class="org-ul">
<li>Контролируемое машинное обучение - логистическую регрессию, нейронные сети, дерево принятия решений,
градиентный бустинг, случайные леса, опорные векторы (SVM)</li>
<li>Неконтролируемое машинное обучение - заранее неизвестно, какие данные относятся к мошенническим операциям,
модель должна сама создать функцию, которая описывает структуру данных. - самоорганизующиеся карты, метод
k-средних, алгоритмы dbscan, ядерное сглаживание, одноклассовые SVM, метод главных компонент и т. д.</li>
</ul>

<p>
Zero-Shot, One-Shot, Few-Shot Learning
</p>
</div>
</li>

<li><a id="org0779fe2"></a>Continual Learning vs Retraining <a id="org9def081"></a><br />
<div class="outline-text-5" id="text-11-5-9-2">
<ul class="org-ul">
<li>2019 Continual Lifelong Learning with Neural Networks:A Review <a href="https://arxiv.org/pdf/1802.07569.pdf">https://arxiv.org/pdf/1802.07569.pdf</a></li>
<li>2020 Neural Network Retraining for Model Serving <a href="https://arxiv.org/pdf/2004.14203.pdf">https://arxiv.org/pdf/2004.14203.pdf</a></li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org788255b"></a>problems<br />
<div class="outline-text-6" id="text-11-5-9-2-1">
<p>
catastrophic forgetting - when re-trained, deep networks tend to forget how to perform previous tasks.
</p>
<ul class="org-ul">
<li>Progressive Networks - instantiate a new network "column" for each task.</li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org0e67a4e"></a>Online machine learning<br />
<div class="outline-text-5" id="text-11-5-9-3">
<ul class="org-ul">
<li>method of machine learning in which data becomes available in a sequential order and is used to update the
best predictor for future data at each step</li>
<li>uses out-of-core algorithms</li>
</ul>

<p>
used where
</p>
<ul class="org-ul">
<li>it is computationally infeasible to train over the entire dataset</li>
<li>it is necessary for the algorithm to dynamically adapt to new patterns in the data</li>
<li>data itself is generated as a function of time, e.g., stock price prediction.</li>
</ul>

<p>
libs:
</p>
<ul class="org-ul">
<li>river</li>
<li>float</li>
<li>creme</li>
<li>scikit-multiflow</li>
</ul>

<p>
Online training - Continue to feed in training data over time, regularly sync out updated version
</p>
<ul class="org-ul">
<li>use progressive validatin rather than batch training &amp; test</li>
<li>needs monitoring, model rollback &amp; data quarantine capabilities</li>
<li>well adapt to changes, staleness issues avoided</li>
</ul>
</div>
</li>
<li><a id="org4bff892"></a>Few-sample/shot learning (FSL): Zero-Shot, One-Shot, Few-Shot Learning<br />
<div class="outline-text-5" id="text-11-5-9-4">
<p>
data is the life-blood of training machine learning models that ensure their success
</p>

<dl class="org-dl">
<dt>One-shot learning</dt><dd>each new class has one labeled example. The goal is to make predictions for the new
classes based on this single example.</dd>
<dt>Few-shot learning</dt><dd>there is a limited number of labeled examples for each new class.</dd>
<dt>Zero-shot learning</dt><dd>there is absolutely no labeled data available for new classes. The goal is for the
algorithm to make predictions about new classes by using prior knowledge about the relationships that exist
between classes it already knows.</dd>
</dl>
</div>
<ol class="org-ol">
<li><a id="org1c88062"></a>approaches:<br />
<div class="outline-text-6" id="text-11-5-9-4-1">
<ul class="org-ul">
<li>Attribute-based approaches - the model uses relationships between attributes to generalize its knowledge and
apply the knowledge to new classes instead of relying on labeled examples.</li>
<li>Embedding-based approaches — the model infers information about new classes based on their proximity to
known classes in the embedding space.</li>
<li>Generative approaches — the model generates synthetic examples for unseen categories based on their semantic representation.</li>
<li>Metric-based models - the model learns a similarity metric between features of the input data and the
features of each class and then uses this metric to make predictions for new, unseen classes.</li>
<li>NN approach</li>
<li>Transfer learning-based models</li>
</ul>
</div>
</li>


<li><a id="org23e1946"></a>2018 Low-shot learning from imaginary data "Framework of Hallucinator" - Unsupervised Augmentation<br />
<div class="outline-text-6" id="text-11-5-9-4-2">
<p>
<a href="https://arxiv.org/pdf/1801.05401.pdf">https://arxiv.org/pdf/1801.05401.pdf</a>
</p>
</div>
</li>


<li><a id="orgc5f8283"></a>2023 A Survey on Machine Learning from Few Samples<br />
<div class="outline-text-6" id="text-11-5-9-4-3">
<p>
<a href="https://arxiv.org/pdf/2009.02653.pdf">https://arxiv.org/pdf/2009.02653.pdf</a>
</p>

<p>
terms:
</p>
<ul class="org-ul">
<li>task - is part of dataset with classes for specific knewledge domain</li>
<li>Dt - training dataset with few samples</li>
<li>Da - auxilliary dataset with many samples</li>
<li>Meta–Learning - part of the meta-training phase</li>
<li>Meta – Testing(Adaption) - models quickly adjust to novel tasks with the least amount of task-specific information.</li>
</ul>

<p>
The goal of the learning algorithm is to produce a mapping function f ∈ F : X → Y and minimize error, where x
 and y drawn from the joint distribution P(x,y) - which is not known for FSL
</p>

<p>
Constraint formed by each supervised sample can be regarded as a regularization performance == poor generalization.
</p>

<p>
FSL Orthogonal to zero-shot learning (ZSL). ZSL - entails concept-specific side information to support the
 cross-concept knowledge transfer.
</p>

<p>
current mainstream FSL approaches is the <b>meta learning</b> based FSL approaches, five major classe:
</p>
<ul class="org-ul">
<li>Learn-to-Measure</li>
<li>Learn-to-Finetune - finetune a base learner for task T using its few support samples and make the base
learner converge fast on these samples within several parameter update steps. base learner and a meta learner</li>
<li>Learn-to-Parameterize - param eterizing the base learner or some subparts of base learner for a novel task
so that it can address this task specifically. meta learner generate weights for base learner.</li>
<li>Learn-to-Adjust</li>
<li>Learn-to-Remember</li>
</ul>

<p>
types of FSL
</p>
<ul class="org-ul">
<li>Semi-supervised FSL - dataset also contains some unlabeled training samples</li>
<li>Unsupervised FSL - Da is fully unsupervised</li>
<li>Cross-domain FSL - sampled in different taks in datasets Dt != Da</li>
<li>Generalized FSL - model should inference on united label spaces yt U ya, rather than single yt.</li>
<li>Multimodal FSL - y and x in different modalities
<ol class="org-ol">
<li>multimodal matching -</li>
<li>multimodal fusion -</li>
</ol></li>
</ul>

<p>
The generative model based approaches and the discriminative model based approaches
</p>
<ul class="org-ul">
<li>discriminative models are better suited for classification tasks - estimates P(Y|X)
<ul class="org-ul">
<li>data augmentation - supervised or unsupervised</li>
<li>metric learning</li>
<li>meta learning</li>
</ul></li>
<li><p>
generative models are better suited for density estimation and unsupervised learning tasks - generate new
data samples based on a training set.  probabilistic in nature (estimates P(X)) rather than being
deterministic.  Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).
</p>
<ul class="org-ul">
<li>common to bridge the connection between x and y using some intermediate latent variables such that the</li>
</ul>
<p>
conditional distribution p(x|y) can be computed mathematically.
</p></li>
</ul>

<p>
History:
</p>
<ol class="org-ol">
<li>non-deep period (from 2000 to 2015) - more generative models - seek to estimate the joint distribution P(x,y) or
the conditional distribution P(X|Y) from the point of Bayesian decision.
<ol class="org-ol">
<li>Congealing algorithm</li>
<li>Variational Bayesian framework</li>
<li>Bayesian Program Learning (BPL)</li>
</ol></li>
<li>deep period (from 2015 to now) - more discriminative models - pursue a conditional distribution P (Y|X )
which can directly predict a probability given one observed sample.
<ol class="org-ol">
<li>Siamese CNN -</li>
</ol></li>
</ol>
</div>
</li>
</ol>
</li>
</ol>
</div>
<div id="outline-container-orgcc6b6d1" class="outline-4">
<h4 id="orgcc6b6d1"><span class="section-number-4">11.5.10.</span> Training, validation, and test sets</h4>
<div class="outline-text-4" id="text-11-5-10">
<p>
data used to build the final model commonly used in different stages of the creation of the model
</p>
<ol class="org-ol">
<li><b>training</b>  first - consist of pairs - 1)input vector or scalar 2) output vector or scalar - target (or
label)
<ul class="org-ul">
<li>result compared with the target - specific learning algorithm being used, the parameters of the model are
adjusted</li>
</ul></li>
<li><b>validation</b> - позволяет объективно оценить эффективность модели, после training dataset
<ul class="org-ul">
<li>для tuning the model's hyperparameters</li>
<li>used for regularization by early stopping</li>
</ul></li>
<li><b>test sets</b> - used to provide an unbiased evaluatioν ( also called a holdout dataset)
<ul class="org-ul">
<li>не может быть использован для выбора модели или тюнинговать</li>
</ul></li>
</ol>
</div>
</div>

<div id="outline-container-orgf23608b" class="outline-4">
<h4 id="orgf23608b"><span class="section-number-4">11.5.11.</span> с учителем</h4>
<div class="outline-text-4" id="text-11-5-11">
<ul class="org-ul">
<li>целевая переменной (или зависимая переменной) &lt;=  набора  предикторов (независимых  переменных)</li>
<li>Generalized  Linear Model(GLM) - specific types is Logistic regression and Linear models</li>
<li>Из набора  предикторов генерируем функцию.
<ul class="org-ul">
<li>линейная регрессия</li>
<li>логистическая регрессия</li>
<li>дерево  решений,</li>
<li>случайный лес</li>
</ul></li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org9537307"></a>линейная регрессия<br />
<div class="outline-text-5" id="text-11-5-11-1">
<ul class="org-ul">
<li>type of Linear model</li>
<li><a href="https://en.wikipedia.org/wiki/Simple_linear_regression">https://en.wikipedia.org/wiki/Simple_linear_regression</a></li>
<li>линия наилучшей подгонки - Y= a*X + b.</li>
<li>Line fitting - процесс оценки параметров</li>
</ul>

<p>
Виды:
</p>
<ul class="org-ul">
<li>простая линейная  регрессия - одной независимой переменной X</li>
<li>multiple linear regression - много независимых</li>
</ul>

<p>
Способы Line fitting:
</p>
<ul class="org-ul">
<li>метод наименьших квадратов ∑(y-f(x))<sup>2</sup> =0 -&gt; a,b - в ручную трудоемко</li>
<li>интерполяция и экстраполяция</li>
</ul>

<p>
Python: sklearn  linear<sub>model.LinearRegression</sub>()
</p>
</div>
</li>
<li><a id="orgfa94f26"></a>логистическая регрессия<br />
<div class="outline-text-5" id="text-11-5-11-2">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=yIYKR4sgzI8">https://www.youtube.com/watch?v=yIYKR4sgzI8</a></li>
</ul>
<p>
прогнозирует вероятность возникновения события путем подключения данных к функции логита
</p>
<ul class="org-ul">
<li>линия показывающая вероятность лежит между 0 и 1</li>
<li>тежяло сравнивать модель от многих переменных с простыми моделями</li>
<li>Y - Probability obese - 0 - 1 = функция распределения cumulative distribution function (CDF)</li>
<li>X - original data points. - на линии 1 - ДА и на линии 0 - НЕТ</li>
<li>may be transformed to log(y)=log(x/(1-x)) - log(odds of obesity)</li>
</ul>

<p>
метод maximum likelihood estimation:
</p>
<ul class="org-ul">
<li>для log(odds) находим линию кандидат</li>
<li>transform to y = e<sup>log</sup>(odds)/(1+e<sup>log</sup>(odds))   where log(odds) = log(x/(1-x))</li>
<li>перемножаем все y = верхние как = 0.91*0.9* нижние = (1-0.001)*(1-0.2) = log(0.91)+log(0.1) = log(ay)</li>
<li>получаем log(0.91*0.1) = -2.4</li>
</ul>

<p>
from sklearn.linear<sub>model</sub> import LogisticRegression
</p>
</div>
</li>

<li><a id="orge08a58a"></a>дерево решений<br />
<div class="outline-text-5" id="text-11-5-11-3">
<ul class="org-ul">
<li>используется в основном для задач классификации</li>
<li>Деревья принятия решений работают путем  деления  популяции  на  как  можно  более  разные  группы.</li>
<li>Gini, Хи-квадрат, энтропия. -???</li>
<li>from sklearn import tree</li>
<li>model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here</li>
</ul>
<p>
you can change the algorithm as gini or entropy (information gain) by default it
is gini
</p>
<ul class="org-ul">
<li># model = tree.DecisionTreeR</li>
</ul>
<p>
egressor() for regression
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orga23523c" class="outline-4">
<h4 id="orga23523c"><span class="section-number-4">11.5.12.</span> без учителя</h4>
<div class="outline-text-4" id="text-11-5-12">
<p>
Алгоритм Apriori
</p>
<ol class="org-ol">
<li>Кластеризация
<ul class="org-ul">
<li>алгоритм кластеризации K-means</li>
</ul></li>
<li>Сети Кохонена</li>
<li>Таксономия</li>
</ol>
</div>
</div>

<div id="outline-container-org8522d4d" class="outline-4">
<h4 id="org8522d4d"><span class="section-number-4">11.5.13.</span> Structured prediction <a id="orgac76ae2"></a></h4>
<div class="outline-text-4" id="text-11-5-13">
<p>
predicting structured objects in  supervised machine learning
</p>

<p>
Term:
</p>
<ul class="org-ul">
<li>structured output domain - область выходных значений</li>
</ul>

<p>
example:
</p>
<ul class="org-ul">
<li>Parsing or sequence-to-sequence</li>
<li>Sequence labeling</li>
</ul>

<p>
Techniques:
</p>
<ul class="org-ul">
<li>probabilistic graphical model (PGM)
<ul class="org-ul">
<li>Bayesian networks</li>
<li>random fields</li>
</ul></li>
<li>inductive logic programming</li>
<li>case-based reasoning</li>
<li>structured SVMs</li>
<li>Markov logic networks</li>
<li>constrained conditional models</li>
<li>Recurrent neural network - LSTMs and GRUs <a href="#org6c76edd">12.15.5</a></li>
</ul>
</div>
</div>
<div id="outline-container-orga906f80" class="outline-4">
<h4 id="orga906f80"><span class="section-number-4">11.5.14.</span> курс ML Воронцов ШАД <a href="http://www.machinelearning.ru">http://www.machinelearning.ru</a></h4>
<div class="outline-text-4" id="text-11-5-14">
<ul class="org-ul">
<li><a href="http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2%29">http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%28%D0%BA%D1%83%D1%80%D1%81_%D0%BB%D0%B5%D0%BA%D1%86%D0%B8%D0%B9%2C_%D0%9A.%D0%92.%D0%92%D0%BE%D1%80%D0%BE%D0%BD%D1%86%D0%BE%D0%B2%29</a></li>
<li><a href="https://yadi.sk/i/njk1o3VcmPbA4Q">https://yadi.sk/i/njk1o3VcmPbA4Q</a></li>
</ul>
</div>


<ol class="org-ol">
<li><a id="orgd8f7828"></a>Математические методы обучения по прецедентам<br />
<div class="outline-text-5" id="text-11-5-14-1">
<p>
<a href="http://www.machinelearning.ru/wiki/images/6/6d/Voron-ML-1.pdf">http://www.machinelearning.ru/wiki/images/6/6d/Voron-ML-1.pdf</a>
Ищется a:X-&gt;Y - приближение целевой функции
</p>

<p>
Feature f объекта х - результат измерения некоторой характеристики объекта. f:X-&gt;Df . Виды признаков:
</p>
<ul class="org-ul">
<li>Df={0,1} - бинарный</li>
<li>Df - конечное множество - f нормальный признак</li>
<li>Df - конечное упорядоченное множество - f порядковый признак</li>
<li>Df = R - f количественный признак</li>
</ul>
<p>
Пусть имеется набор признаков f1,&#x2026;,fn. Вектор (f1(x),&#x2026;,fn(x)) - признаковое описание объекта x∈X
</p>
<ul class="org-ul">
<li>Матрица объектов-признаков f1(x1)&#x2026;fn(x1)  f1(x2)..fn(x2)&#x2026;</li>
</ul>

<p>
Задачи обучения по прецедентам делятся:
</p>
<ul class="org-ul">
<li>Classification Y={1,&#x2026;,M}</li>
<li>Classification на M пересекающихся классов Y={0,1}<sup>M</sup></li>
<li>Regression estimation Восстановление регрессии Y=R</li>
<li>Forecasting - в будущем -  частный случай классификации и восстановления регрессии</li>
</ul>

<p>
Модель алгоритмов - семейство отображений A={g(x,θ), θ∈Q} где gXxQ-&gt;Y - фиксированная функция
</p>
<ul class="org-ul">
<li>Q - search space пространство поиска</li>
</ul>

<p>
Широко используются линейные модели g(x,θ)=∑θf(x)
</p>

<p>
Fitting or training or learning - Процесс подбора оптимального θ параметра модели а∈A
</p>

<p>
Learning algorithm - это отображение m:(XxY)-&gt;A
</p>

<p>
Loss function - Ф(a,x) - характеризует величину ошибки алгоритма a на объекте х.
</p>
<ul class="org-ul">
<li>Ф(a,x) = 0 то ответ корректный</li>
<li>Q(a,Xi)= (1/i)∑Ф(a,xi) - Функционал качества алгоритма a на выборке Xi. Или эмпирический риск или частота</li>
</ul>

<p>
При вероятностной постановке задачи вместо модели алгоритмов g(x,θ) аппроксимирующей неизвестную зависимость
у*(x) задаётся модель совместной плотности распределения объектов и ответов ф(x,y,θ) аппроксимирующая
неизвестную плотность p(x,y)
</p>
</div>
<ol class="org-ol">
<li><a id="org350bd41"></a>Принцип максимума правдоподобия<br />
<div class="outline-text-6" id="text-11-5-14-1-1">
<p>
Так как Xi независимы, то p(Xi) = p(x1,y1)*&#x2026;*p(xn,yn). Подставляя ф(x,y,θ) получаем <b>функцию правдоподобия</b>
</p>
<ul class="org-ul">
<li>L(θ, Xi)=Пф(xi,yi,θ)</li>
</ul>
</div>
</li>
<li><a id="org7c39901"></a>Likelihood function<br />
<div class="outline-text-6" id="text-11-5-14-1-2">
<p>
Функция правдоподобия - plausibility of a value for the parameter, given some data.
</p>

<p>
распределение вероятности зависит от параметра θ
</p>
<ol class="org-ol">
<li>Какова вероятность выпадения 12 очков в каждом из ста бросков двух костей?
<ul class="org-ul">
<li>условную вероятность событий x при заданном параметре θ</li>
<li>P(x)=P(x|θ)</li>
</ul></li>
<li>Насколько правдоподобно, что кости не шулерские, если из ста бросков в каждом выпало 12 очков
<ul class="org-ul">
<li>вероятность заданного события X при различных значениях параметра θ</li>
<li>L(θ)=L(x=X|θ) - насколько правдоподобно выбранное значение параметра θ при известном событии X</li>
</ul></li>
</ol>

<p>
Неформально: если вероятность позволяет нам предсказывать неизвестные результаты, основанные на известных
параметрах, то правдоподобие позволяет нам оценивать неизвестные параметры, основанные на известных
результатах.
</p>

<p>
Правдоподобие позволяет сравнить несколько вероятностных распределений с разными параметрами и оценить в
контексте какого из них наблюдаемые события наиболее вероятны.
</p>
</div>
</li>
</ol>
</li>
</ol>
</div>
<div id="outline-container-org7a6ab49" class="outline-4">
<h4 id="org7a6ab49"><span class="section-number-4">11.5.15.</span> метрики metrics</h4>
<div class="outline-text-4" id="text-11-5-15">
<ul class="org-ul">
<li><a href="https://stackabuse.com/understanding-roc-curves-with-python/">https://stackabuse.com/understanding-roc-curves-with-python/</a></li>
<li><a href="https://habr.com/ru/company/ods/blog/350440/">https://habr.com/ru/company/ods/blog/350440/</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgae5c43a" class="outline-4">
<h4 id="orgae5c43a"><span class="section-number-4">11.5.16.</span> <span class="todo TODO">TODO</span> problems</h4>
<div class="outline-text-4" id="text-11-5-16">
<p>
<b>saturated neuron</b> if activation functions have to compress an infinite range into a finite range. Веса
устанавливаются так, чтобы приблизиться к границам. Saturated neurons change their values slowly. It is
problem if neurons are wrong.  it erodes the plasticity of neural networks and usually results in worse test
performance
</p>

<p>
<b>data-sparsity</b>
<b>local optima</b>
</p>

<p>
<b>Схема винограда</b> Я выиграл приз и хотел положить его в чемодан, но не смог, потому что он слишком
 большой. Кто он? Тест на интеллект. Common sense.
</p>
</div>
</div>
<div id="outline-container-org1a2669e" class="outline-4">
<h4 id="org1a2669e"><span class="section-number-4">11.5.17.</span> эконом эффективность</h4>
<div class="outline-text-4" id="text-11-5-17">
<p>
специальные процедуры <b>оценки надежности</b>, после которых становится ясно, с какой вероятностью выходит из строя
 каждый элемент системы и как следствие, и вся система в целом. В сфере машинного обучения со временем
 появятся такие же стандарты.
</p>

<p>
<b>Релевантность</b> Все модели, которые работают в изменяющейся среде, требуют актуализации и диагностики.
</p>
</div>
<ol class="org-ol">
<li><a id="orgf17fc9d"></a>у нейросетей есть три больших минуса:<br />
<div class="outline-text-5" id="text-11-5-17-1">
<ul class="org-ul">
<li>Не ясно логика принятия решения, нельзя объяснить почему было принято решение.</li>
<li>Злоумышленник может «скормить» нейросети картинку с небольшим, еле видимым глазом, искажением. Программа не
сможет корректно распознать изображение и начнёт выдавать ошибки.
<ul class="org-ul">
<li>Чем сложнее модель и выше коэффициент Gini , тем больше вероятность получения некорректных
результатов. "Чем более сложную модель мы используем, тем тяжелее ее контролировать."</li>
</ul></li>
<li>Если нейросеть обучалась на неверных или неполных данных ,отклонения от заданной нормы будут казаться ей
неправильными. Дискриминация.</li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgf8c13f2" class="outline-4">
<h4 id="orgf8c13f2"><span class="section-number-4">11.5.18.</span> Spike-timing-dependent plasticity STDP</h4>
<div class="outline-text-4" id="text-11-5-18">
<ul class="org-ul">
<li><a href="https://www.groundai.com/project/stdp-based-spiking-deep-neural-networks-for-object-recognition/2">https://www.groundai.com/project/stdp-based-spiking-deep-neural-networks-for-object-recognition/2</a></li>
<li>альтернатива</li>
</ul>
</div>
</div>
<div id="outline-container-org36a50c2" class="outline-4">
<h4 id="org36a50c2"><span class="section-number-4">11.5.19.</span> non-linearity</h4>
<div class="outline-text-4" id="text-11-5-19">
<p>
Feedforward neural network with linear activation functions and n layers each having m hidden units (linear
neural network, for brevity) is equivalent to a linear neural network without hidden layers. Proof:
y=h(x)=bn+Wn(bn−1+Wn−1(…(b1+W1x)…))=bn+Wnbn−1+WnWn−1bn−2+⋯+WnWn−1…W1x=b'+W'x
</p>

<p>
adding layers ("going deep") doesn't increase the approximation power of a linear neural network at all,
unlike for nonlinear neural network.
</p>
</div>
</div>

<div id="outline-container-orga613b97" class="outline-4">
<h4 id="orga613b97"><span class="section-number-4">11.5.20.</span> math</h4>
<div class="outline-text-4" id="text-11-5-20">
<p>
y = f(w*x+b) - где f - бинарная функция активации = перцпетрон, или sigmod (0;1) - линейная Feedforward ANN
</p>

<p>
Δoutput is well approximated by Δo(Δwj,Δb) = ∑(∂o/∂w)Δw+(∂o/∂b)Δb
</p>

<p>
Parameters: 3 input, 4, 6, 1(sigmoid) = 3x4+4+4*6+6+6+1 = 53 parameters.
</p>
</div>
<ol class="org-ol">
<li><a id="org63c2e17"></a>units in layout<br />
<div class="outline-text-5" id="text-11-5-20-1">
<ul class="org-ul">
<li>Each of hidden units corresponds to a dimension (latent feature)</li>
<li>Edge weights between a movie and hidden layer are coordinate values (0.3, 0.9 0.2) = 3-dimension -&gt; 3 units</li>

<li>Higher-dimensional embeddings can more accurately represent the relationships between input values</li>
<li>But more dimensions increases the chance of overfitting and leads to slower training</li>
<li>Empirical rule-of-thumb dimensions=4_√(possible values)</li>
</ul>
<p>
Нейронная сетть 3-4-6-1   у=xA3x4+b4, у=xA4x6+b6, у=xA6x1+b1
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org68ef3d8" class="outline-4">
<h4 id="org68ef3d8"><span class="section-number-4">11.5.21.</span> optimal configuration</h4>
<div class="outline-text-4" id="text-11-5-21">
<ul class="org-ul">
<li><a href="https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw">https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw</a></li>
</ul>
<p>
what
</p>
<ol class="org-ol">
<li>number of layers and type</li>
<li>number of nodes in each</li>
</ol>

<p>
Layouts:
</p>
<ul class="org-ul">
<li>Input layout - equal to the number of features (columns) in your data</li>
<li>Output Layer - regression -&gt; 1 node, classifier -&gt;single node unless softmax is used in which case the
output layer has one node per class label</li>
<li>Hidden Layer -  the number of neurons in that layer is the mean of the neurons in the input and output layers</li>
</ul>
</div>
</div>

<div id="outline-container-org5f2bb33" class="outline-4">
<h4 id="org5f2bb33"><span class="section-number-4">11.5.22.</span> <span class="todo TODO">TODO</span> merging</h4>
<div class="outline-text-4" id="text-11-5-22">
<p>
<a href="https://ai.stackexchange.com/questions/8626/how-to-combine-input-from-different-types-of-data-sources">https://ai.stackexchange.com/questions/8626/how-to-combine-input-from-different-types-of-data-sources</a>
</p>
</div>
</div>
<div id="outline-container-org627b999" class="outline-4">
<h4 id="org627b999"><span class="section-number-4">11.5.23.</span> training, Inference mode, frozen state</h4>
<div class="outline-text-4" id="text-11-5-23">
<ul class="org-ul">
<li><a href="https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/">https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/</a></li>
<li>learning rate or step size, see <a href="#org9ef9da2">12.5.5</a></li>
<li>Momentum or Learning rate decay over each update. - linear combination of the gradient and the previous
update. especially used in the face of high curvature, small but consistent gradients, or noisy
gradients. Уменьшает learning rate со временем</li>
</ul>
</div>
</div>
<div id="outline-container-orgc7d4de4" class="outline-4">
<h4 id="orgc7d4de4"><span class="section-number-4">11.5.24.</span> MY NOTES</h4>
<div class="outline-text-4" id="text-11-5-24">
<ul class="org-ul">
<li>начинать выбор lr нужно с максимального значения, выбирая более стабильную кривую обучения + немного
меньше(по гуглу)</li>
<li>чем больше epoch тем больше модель требует именно такие входные данные</li>
<li>MaxPooling может не учитывать порядок слов в предложении и работает хуже Dense</li>
<li>чем проще модель тем она эффективнее</li>
<li>чтобы увеличить приоритет входа можно попробовать подвинуть его ближе к выходу и увеличить количество точек
в конкатенации</li>
<li>Правило левой руки - несколько тысяч примеров на один класс</li>
<li>Большое количество слоев уменьшает количество параметров, но усложняет обучение</li>
<li>мультислойная нейронная сеть с линейными функциями активации - по прежнему линейное преобразование</li>
<li>Different layers require different type of attention</li>
<li>Если от одной сети требуется несколько выводов-задач, лучше разделить их и натренировать отдельно.</li>
<li>чтобы увеличить число параметров у CNN нужно убрать один из последних слоев, а у соседнего увеличить количество фильтров</li>
<li>Reduce overtraining:
<ul class="org-ul">
<li>Dropout</li>
<li>reduce trainable parameters</li>
</ul></li>
<li>Хороший старт тоже важен.</li>
<li>Dropout:
<ul class="org-ul">
<li>большее значение на большем слое</li>
<li>основной инструмент регуляции</li>
</ul></li>
<li>Residual only MaxPool! and  concatenate
<ul class="org-ul">
<li>чем лучше residual, тем меньше loss и меньше accururacy</li>
<li>чтобы уменьшить Flatten - res2 = Conv2D, x = Add()([x, res2])  # residual</li>
</ul></li>
<li>CNN Flatten 23000 num<sub>classes</sub> =7 - тест запаздывает за train. 10111/7 - все нормально</li>
<li>Оптимизацию модели лучше проводить на испытаниях с низким lr, потому что обучение стабильнее и лучше отражает
качество модели</li>
</ul>
<p>
<b>CNN</b>
</p>
<ul class="org-ul">
<li>Сначала сделать наиболее быстро обучаемую СNN, потом добавить к ней Dense, ӕто замедлит оверфиттинг за счет
увеличения lr</li>
<li>Сначала подобрать идеальную кривую обучения для CNN, затем с Dense стараться пройти по ней.</li>
</ul>
<p>
-??????????????? никогда не используй Dropout перед сетью - используй его для увелечения независимости слоев
</p>
<ul class="org-ul">
<li>every FC layer can be replaced by a convolutional layer</li>
</ul>
</div>
</div>
<div id="outline-container-org191ac5c" class="outline-4">
<h4 id="org191ac5c"><span class="section-number-4">11.5.25.</span> Spatial Transformer Network (STN)</h4>
<div class="outline-text-4" id="text-11-5-25">
<ul class="org-ul">
<li>STN <a href="https://arxiv.org/pdf/1506.02025.pdf">https://arxiv.org/pdf/1506.02025.pdf</a>
<ul class="org-ul">
<li>spatial transformation capabilities</li>
<li>article <a href="https://habr.com/ru/company/newprolab/blog/339484/">https://habr.com/ru/company/newprolab/blog/339484/</a></li>
<li>1 <a href="https://kevinzakka.github.io/2017/01/10/stn-part1/">https://kevinzakka.github.io/2017/01/10/stn-part1/</a></li>
<li>2 <a href="https://kevinzakka.github.io/2017/01/18/stn-part2/">https://kevinzakka.github.io/2017/01/18/stn-part2/</a></li>
</ul></li>
</ul>

<p>
Spatial Transformer:
</p>
<ul class="org-ul">
<li>input image -&gt;</li>
<li>Localisation Network (any form, such as a fully-connected network or a convolutional network) -&gt;</li>
<li>θ transformation matrix
<ul class="org-ul">
<li>for affine 6-parameters</li>
<li>for attention:
<ul class="org-ul">
<li>[s 0 tx]</li>
<li>[0 s ty]</li>
</ul></li>
<li>plane projective transformation - 8 parameters</li>
<li>16-point thin plate spline transformation (TPS)</li>
</ul></li>
<li>SΤ warps an image: θ * input image = (x,y,1)</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org154ed42"></a>Inverse Compositional Spatial Transformer Networks<br />
<div class="outline-text-5" id="text-11-5-25-1">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=LV1slx9Ob7U">https://www.youtube.com/watch?v=LV1slx9Ob7U</a></li>
<li><a href="https://arxiv.org/pdf/1612.03897.pdf">https://arxiv.org/pdf/1612.03897.pdf</a></li>
<li><a href="https://github.com/chenhsuanlin/inverse-compositional-STN">https://github.com/chenhsuanlin/inverse-compositional-STN</a></li>
<li><a href="https://chenhsuanlin.bitbucket.io/inverse-compositional-STN/poster.pdf">https://chenhsuanlin.bitbucket.io/inverse-compositional-STN/poster.pdf</a></li>
</ul>

<p>
Проблемы оригинала:
</p>
<ul class="org-ul">
<li>Boundary effect - original information is not preserved</li>
<li>Single Transformation</li>
</ul>

<p>
Lucas-Kanade(LK) Algorithm
</p>

<p>
Image - I, p - transformation matrix, f - learnable geometric predictor (termed the localization network in
the original paper)
</p>
<ul class="org-ul">
<li>Iout(0) = Iin(p) , where p = f(Iin(0))</li>
</ul>

<p>
compositional STNs:
</p>

<p>
steps:
</p>
<ul class="org-ul">
<li>image = (100, 28, 28) - &gt; (100, 28, 28, 1)</li>
<li>pInit = data.genPerturbations(opt)</li>
<li>ICSTN(image, pInit)
<ul class="org-ul">
<li>for 4 times:
<ul class="org-ul">
<li>pInitMtrx = warp.vec2mtrx(pInit) (100, 3, 3) - initial random 100 transformations</li>
<li>imageWarp = transformImage(image, pInitMtrx) - with bilinear interpolation</li>
<li>dp = CNN(imageWarp) -&gt; opt.warpDim - size</li>
<li>warp.compose(pInit, dp)</li>
</ul></li>
<li>pMtrx = warp.vec2mtrx(opt,p)</li>
<li></li>
</ul></li>
<li>4 imageWarp to final CNN</li>
</ul>




<ul class="org-ul">
<li>data.genPerturbations - (100,8) #100-batch, 8 - opt.warpDim (homography matrix is a 3x3 matrix but with 8
DoF (degrees of freedom)) - random</li>
<li></li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-org1736492" class="outline-4">
<h4 id="org1736492"><span class="section-number-4">11.5.26.</span> Bayesian model averaging</h4>
<div class="outline-text-4" id="text-11-5-26">
<p>
instead of selecting single best model - Bayesian Model Averaging BMA uses a weighted average of each model's
individual prediction for the final predicted value
</p>
</div>
</div>

<div id="outline-container-org1752ef3" class="outline-4">
<h4 id="org1752ef3"><span class="section-number-4">11.5.27.</span> residual connection (or skip connection)</h4>
<div class="outline-text-4" id="text-11-5-27">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1605.06431.pdf">https://arxiv.org/pdf/1605.06431.pdf</a></li>
<li>Residual networks avoid the vanishing gradient problem by introducing short pathswhich can carry gradient</li>
</ul>
<p>
throughout the extent of very deep networks
</p>
</div>
</div>

<div id="outline-container-org8b9148c" class="outline-4">
<h4 id="org8b9148c"><span class="section-number-4">11.5.28.</span> vanishing gradient problem</h4>
<div class="outline-text-4" id="text-11-5-28">
<p>
the gradients get smaller and smaller until they’re almost negligible when they reach the first layers
</p>

<p>
why? Certain activation functions, like the sigmoid function, squishes a large input space into a small input
  space between 0 and 1. Therefore, a large change in the input of the sigmoid function will cause a small
  change in the output. Hence, the derivative becomes small.
</p>

<p>
<b>The problem arises when a large input space is mapped to a small one, causing the derivatives to disappear.</b>
</p>

<p>
solution:
</p>
<ul class="org-ul">
<li>relu</li>
<li>residuel networks</li>
<li>batch normalization layers</li>
</ul>

<p>
<a href="https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484">https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484</a>
</p>
</div>
</div>

<div id="outline-container-org9e436b5" class="outline-4">
<h4 id="org9e436b5"><span class="section-number-4">11.5.29.</span> Multi-task learning(MTL)</h4>
<div class="outline-text-4" id="text-11-5-29">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1702.04710.pdf">https://arxiv.org/pdf/1702.04710.pdf</a></li>
</ul>
<p>
learning tasks in parallel
</p>

<p>
Methods:
</p>
<dl class="org-dl">
<dt>Task grouping and overlap</dt><dd>просто выходные параметры общие</dd>
<dt>Exploiting unrelated tasks</dt><dd></dd>
</dl>

<p>
keras <a href="https://github.com/manashmandal/Multitask_Learning_Keras/blob/master/multilabel_with_missing_labels.py">https://github.com/manashmandal/Multitask_Learning_Keras/blob/master/multilabel_with_missing_labels.py</a>
</p>
</div>
</div>

<div id="outline-container-org246a4ba" class="outline-4">
<h4 id="org246a4ba"><span class="section-number-4">11.5.30.</span> many classes</h4>
<div class="outline-text-4" id="text-11-5-30">
<ul class="org-ul">
<li>NEURALNETWORK  FORMANY-CLASSFEW-SHOTLEARNING  WITHCLASSHIERARCHY <a href="https://openreview.net/pdf?id=rJlcV2Actm">https://openreview.net/pdf?id=rJlcV2Actm</a></li>
<li>Hierarchical softmax</li>
</ul>
</div>
</div>


<div id="outline-container-org52d937e" class="outline-4">
<h4 id="org52d937e"><span class="section-number-4">11.5.31.</span> super-convergence Fast Training with Large Learnign rate</h4>
<div class="outline-text-4" id="text-11-5-31">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1708.07120.pdf">https://arxiv.org/pdf/1708.07120.pdf</a></li>
</ul>

<p>
convergence [kənˈvɜːʤəns] - сходимость
</p>

<p>
typical, standard, or a piecewise-constant training regime:
</p>
<ol class="org-ol">
<li>using a global learning rate, (i.e.,≈0.1), for many epochs</li>
<li>until the test accuracyplateaus, and then continuing to train with a learning rate decreased by a factor of0.1</li>
</ol>

<p>
adaptive learning rate methods such as Nesterov momentum - do they lead to super-convergence
</p>

<p>
forms of regularization:
</p>
<ul class="org-ul">
<li>large learning rates</li>
<li>small batch sizes</li>
<li>weight decay</li>
<li>dropout</li>
</ul>

<p>
Reducing other forms of regularization and regularizing with very large learningrates makes training significantly more efficient.
</p>

<p>
large batch size is more effective than a small batch size for super-convergence training
</p>

<p>
gains from super-convergenceincrease as the available labeled training data becomes more limited
</p>
</div>
</div>

<div id="outline-container-orga74b7ba" class="outline-4">
<h4 id="orga74b7ba"><span class="section-number-4">11.5.32.</span> One Shot Learning &amp; Triple loss &amp; triple network</h4>
<div class="outline-text-4" id="text-11-5-32">
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Triplet_loss">https://en.wikipedia.org/wiki/Triplet_loss</a></li>
<li><a href="https://towardsdatascience.com/siamese-network-triplet-loss-b4ca82c1aec8">https://towardsdatascience.com/siamese-network-triplet-loss-b4ca82c1aec8</a></li>
<li>DEEP METRIC LEARNING USINGTRIPLET NETWORK <a href="https://arxiv.org/pdf/1412.6622.pdf">https://arxiv.org/pdf/1412.6622.pdf</a></li>
<li>example of triple network</li>
</ul>
<p>
Когда нужно рапознать лица к человеку и есть не больше 10 его фотографий.
</p>

<p>
Использую функцию сравнения изображений, выходы нейронной сети - encoding of image
</p>

<p>
Обучение:
</p>
<ul class="org-ul">
<li>берем Anchor фото</li>
<li>сравниваем его (encodings) сначала с positive (друго фото этого человека)</li>
<li>затем сравниваем с negative (другого человека)</li>
<li>считаем лосс и обновляем весы L = max(d(a,p)-d(a,n) + margin, 0)
<ul class="org-ul">
<li>d - dissimularity</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb56ef9d" class="outline-4">
<h4 id="orgb56ef9d"><span class="section-number-4">11.5.33.</span> Evaluation Metrices</h4>
<div class="outline-text-4" id="text-11-5-33">
<p>
<a href="https://scholar.google.com/scholar?cluster=11211211207326445005&amp;hl=en&amp;as_sdt=0,5">https://scholar.google.com/scholar?cluster=11211211207326445005&amp;hl=en&amp;as_sdt=0,5</a>
</p>

<ul class="org-ul">
<li>confidence - score for single input sample, how model confident for that class.(abstarct)</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org531addb"></a>types:<br />
<div class="outline-text-5" id="text-11-5-33-1">
<ul class="org-ul">
<li>binary
<ul class="org-ul">
<li>FDR=TPR=Recall=Sensitivity</li>
<li>MAR</li>
<li>Specificity</li>
<li>FAR=FPR</li>
<li>G-mean</li>
<li>Precision</li>
<li>F-measing</li>
<li>Accuracy</li>
<li>ROC-ACU,PRC-AUC</li>
<li>MCC</li>
</ul></li>

<li>window-based detection
<ul class="org-ul">
<li>NAB scoring algorithm - <a href="https://ieeexplore.ieee.org/abstract/document/7424283?casa_token=WpMp1lHmr5kAAAAA:wJdo4wdX2rnBozyT1qAzl4J4MCf0Q5Pf6XObQRXfC6OEDSEN8mO90iLnaCrtx3tV_EfBWqU8TbT5">https://ieeexplore.ieee.org/abstract/document/7424283?casa_token=WpMp1lHmr5kAAAAA:wJdo4wdX2rnBozyT1qAzl4J4MCf0Q5Pf6XObQRXfC6OEDSEN8mO90iLnaCrtx3tV_EfBWqU8TbT5</a></li>
<li>RandIndex - <a href="https://www.sciencedirect.com/science/article/pii/S0165168419303494?casa_token=vFPmPtIDVoIAAAAA:9p2F5e5vWqzbDhfXJtGkD7LwYjOcAVqT-IEZY24yYNAwhYEKF7FNIb4Y4hgV2v0Um3vvrPyeffE">https://www.sciencedirect.com/science/article/pii/S0165168419303494?casa_token=vFPmPtIDVoIAAAAA:9p2F5e5vWqzbDhfXJtGkD7LwYjOcAVqT-IEZY24yYNAwhYEKF7FNIb4Y4hgV2v0Um3vvrPyeffE</a></li>
</ul></li>

<li>detection time - evelauating diference in time (or point/index) between the predicted and actual change point
<ul class="org-ul">
<li>ADD=MAE=AnnotationError - absolute error</li>
<li>MSD(Mean signed difference) - considers the direction of the error (predicting before or after the actual change point time</li>
<li>MSE,RMSE,NRMSE - resulting measure will be very large if a few dramatic outliers exist in the classified data</li>
<li>ADD - <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9875/98751Z/Ensembles-of-detectors-for-online-detection-of-transient-changes/10.1117/12.2228369.short?SSO=1">https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9875/98751Z/Ensembles-of-detectors-for-online-detection-of-transient-changes/10.1117/12.2228369.short?SSO=1</a></li>
<li>Hausdorff - equal to the greatest temporal distance between a change point and its prediction</li>
</ul></li>
</ul>

<p>
other metrics:
</p>
<ul class="org-ul">
<li>worst-case mean detection delay, integral average detection delay, maximal conditional average delay to detection, mean time between false alarms,</li>
</ul>
<p>
<a href="https://medium.com/@katser/a-review-of-anomaly-detection-metrics-with-a-lot-of-related-information-736d88774712">https://medium.com/@katser/a-review-of-anomaly-detection-metrics-with-a-lot-of-related-information-736d88774712</a>
</p>

<p>
for tasks:
</p>
<ul class="org-ul">
<li>binary classification: precision, recall, specificity, F1, ROC, PR AUC</li>
<li>Multi-class: macro-averaging, weighted-averaging, macro-averaging</li>
<li>Multi-label: hamming loss, exact match ration, Jaccard index</li>
<li>statistical tests of significance: Paired Student's test, ANOVA, Kruskal-Wallis, Chi-squared test</li>
</ul>
</div>
</li>

<li><a id="org74d096c"></a>accuracy [ˈækjʊrəsɪ]<br />
<div class="outline-text-5" id="text-11-5-33-2">
<p>
accuracy = правильное решение/кол-во samples
</p>

<p>
типы:
</p>
<ol class="org-ol">
<li>label based  - accuracy: tf.reduce<sub>mean</sub>(tf.equal(tf.round(pred), y))</li>
<li>example based</li>
<li>Exact Match - 1/n∑I(Y=Z) where I - indicator function</li>
<li>accuracy - predicted correct labels to total labels. Overall [ˈəʊvərɔːl] - average</li>
<li>precision - predicted correct labels to predicted labels</li>
</ol>


<p>
Недостаток accuracy в чувствительности к downsampling
</p>
<ul class="org-ul">
<li>Мы имеем улучшение точности одобренных, а общая точность падает из-за увеличения количества одбренных в
проверочной выборке. Это увеличение было сделано, чтобы легче сравнивать метрики с метриками на обучающей
выборке. Что однако мешает сравнивать тестовые метрики между собой.</li>
<li>bad for imbalanced dataset</li>
</ul>

<p>
Точность 71% = (7880+722)/(3766 + 8339)  ,3766 - одобренных изначально, 7880 - отклонены
Точность одобренных 61% = 722/(722+459) ,722 - одобрено, 459 - ошиб. одоб.
Процент одобрения 10% = (722+459) / (3766 + 8339)
</p>

<p>
Точность 66% = (7880+988)/(5077 + 8339)  ,5077 - одобренных изначально
Точность одобренных 68% = 988/(988+459) ,988 - одобрено
Процент одобрения 11% = (988+459) / (5077 + 8339)
</p>

<p>
Во втором случае из-за увеличения числа одобренных, акцент в дроби смещается к отношению числа одобренных к одобренным изначально 988/5077, которое меньше отношения числа отклоненных  7880/8339.
Таким образом мы видим, что общая точность действительно снижается, однако для нас больше важно отношение одобренных, чем отклоненных, поэтому выбранный показатель точности Accuracy необходимо заменить например на F1, который показывет среднее между "Точность одобренных" и "Процент одобрения" или помнить, что наша Точность (Accuracy) имеет такой недостаток и не использовать downsapling.
</p>
</div>
</li>

<li><a id="org47baeb4"></a>precision* [prɪˈsɪʒən] and recall [rɪˈkɔːl]<br />
<div class="outline-text-5" id="text-11-5-33-3">
<ul class="org-ul">
<li><b>precision</b> "how useful the search results are" - how precise/accurate your model - Прецизионность
<ul class="org-ul">
<li>p is the number of correct positive results / number of all positive results returned ( false + true).</li>
<li>tp/(tp+fp)</li>
<li>high precision means - rare positive but all is good</li>
</ul></li>
<li><b>recall</b> or sensitivity "how complete the results are" - how many of the Actual Positives our model capture - Полнота
<ul class="org-ul">
<li>r is the number of correct positive results / number of all positives ( true positive + false negative)</li>
<li>tp/(tp+fn)</li>
</ul></li>
</ul>


<p>
Пример: радар определяет самолеты
</p>
<ol class="org-ol">
<li>с с с (с) (с) - perfect precision, bad recall</li>
<li>(c)()(c)()(c)()(c) - perfect recall, terrible precision</li>
<li>(c) (c) (c) (c) - Perfect precision and recall</li>
</ol>
</div>
</li>

<li><a id="org231c647"></a>F1 score [skɔː]<br />
<div class="outline-text-5" id="text-11-5-33-4">
<p>
 measure of a test's accuracy - balance between Precision and Recall - equally
f1 = ((r<sup>-1</sup> + p<sup>-1</sup>)/2)<sup>-1</sup> = 2*(p*r/p+r)
</p>

<ul class="org-ul">
<li>bad for imbalanced dataset</li>
</ul>


<div id="org2eb216c" class="figure">
<p><img src="imgs/precision-racall-f1.jpg" alt="precision-racall-f1.jpg" />
</p>
</div>
</div>
</li>
<li><a id="org891b211"></a>Fbeta and F2<br />
<div class="outline-text-5" id="text-11-5-33-5">
<p>
Fbeta=(1+B<sup>2</sup>)*(precision*recall)/(B<sup>2</sup>*precision+recall)
</p>

<p>
the more you care about recall over precision the higher beta you should choose
</p>

<p>
F2 score, recall is twice as important to us.
</p>
</div>
</li>

<li><a id="orgc68b4df"></a>confusion matrix<br />
<div class="outline-text-5" id="text-11-5-33-6">
<p>
Result of classification:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">TP</th>
<th scope="col" class="org-left">FP</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">FN</td>
<td class="org-left">TN</td>
</tr>
</tbody>
</table>

<ul class="org-ul">
<li>TP - ok</li>
<li>TN - ok</li>
<li>FP - must be negative</li>
<li>FN - must be positive</li>

<li>Type 1 Error - FP</li>
<li>Type 2 Error - FN</li>
</ul>

<p>
metrics:
</p>
<ul class="org-ul">
<li>Recall = TP / (TP + FN)</li>
<li>Precision = TP / (TP + FP)</li>
<li>F-Score = TP / (TP + FP)</li>
<li>F-мера (F-measure) =2*(Precision*Recall)/(precision+recall) =1/(a1/precision+(1-a)/recall), a∈[0,1] - задаёт соотношение весов точности и полноты</li>
</ul>

<p>
print("accuracy\t%f" % (np.round(ypred2) <code>= labels_test).mean())
print("loss\t\t%f" % (np.round(ypred2) !</code> labels<sub>test</sub>).mean())
</p>

<p>
sklearn.metrics.classification<sub>report</sub>(labels<sub>test</sub>, np.round(ypred2)) # all
</p>
</div>
</li>
<li><a id="orgdc3547c"></a>AUC ROC Curve<br />
<div class="outline-text-5" id="text-11-5-33-7">
<p>
AUC-ROC (Area Under Curve - Receiver Operating Characteristics) curve - is the model selection metric for
bi-milti class classification problem,
</p>

<p>
<b>ROC curve</b>
</p>
<ul class="org-ul">
<li>False Positive Rate (FPR) on the X-axis (</li>
<li>True Positive Rate (TPR) on the Y-axis</li>
<li>tells us how good the model is for distinguishing the given classes, in terms of the predicted probability.</li>
<li>насколько равномерно достигаются целевые классы + общее заполнение</li>
<li>FPR = FP / Neg(реальн) = FP / (FP + TN) - total number of negative</li>
<li>TPR = TP/ Pos(реальн) = TP / (TP + FN) - total number of positive</li>
</ul>

<p>
ideal value for <b>AUC</b> is 1 - use differentiation, hard to understand
</p>
<ul class="org-ul">
<li>AUC = ∫TPR d(FPR) - equal to the probability that a classifier will rank a randomly chosen positive instance
higher than a randomly chosen negative one</li>
<li>sklearn.metrics.roc<sub>auc</sub><sub>score</sub>(y<sub>true</sub>, y<sub>score</sub>)</li>
</ul>

<p>
pros:
</p>
<ul class="org-ul">
<li>good for imbalanced data</li>
</ul>

<p>
for multiclassification every class should have own curve
</p>

<p>
ROC AUC score is equivalent to calculating the rank correlation between predictions and targets. From an
 interpretation standpoint, it is more useful because it tells us that this metric shows how good at ranking
 predictions your model is. It tells you what is the probability that a randomly chosen positive instance is
 ranked higher than a randomly chosen negative instance.
</p>
<ul class="org-ul">
<li>What is</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org4af763a"></a>curve<br />
<div class="outline-text-6" id="text-11-5-33-7-1">
<div class="org-src-container">
<pre class="src src-text">
           0-class precision
     ^          ... ./
     |        ..   /
     |     ...   /
TPR(1)    .    /
     |    .  /
     |  .  /
     | . /
     |./
     |/-----------------&gt;
             FPR

</pre>
</div>
</div>
</li>
<li><a id="org1a6abcd"></a>illustration of ROC<br />
<div class="outline-text-6" id="text-11-5-33-7-2">
<ul class="org-ul">
<li><img src="https://miro.medium.com/max/720/1*SKn7aehckf2J8FVz9xnraQ.webp" alt="1*SKn7aehckf2J8FVz9xnraQ.webp" /></li>
<li><img src="https://miro.medium.com/max/720/1*SQe_g5Rs_VzaU5CUV_dzSA.webp" alt="1*SQe_g5Rs_VzaU5CUV_dzSA.webp" /></li>
</ul>
</div>
</li>
<li><a id="org0526147"></a>sklearn example<br />
<div class="outline-text-6" id="text-11-5-33-7-3">
<p>
roc<sub>auc</sub><sub>score</sub> == auc
</p>
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.datasets <span style="color: #8ac6f2; font-weight: bold;">import</span> make_classification
<span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.linear_model <span style="color: #8ac6f2; font-weight: bold;">import</span> LogisticRegression
<span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.model_selection <span style="color: #8ac6f2; font-weight: bold;">import</span> train_test_split
<span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.metrics <span style="color: #8ac6f2; font-weight: bold;">import</span> roc_curve, auc
<span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.metrics <span style="color: #8ac6f2; font-weight: bold;">import</span> roc_auc_score
<span style="color: #8ac6f2; font-weight: bold;">from</span> matplotlib <span style="color: #8ac6f2; font-weight: bold;">import</span> pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1075;&#1077;&#1085;&#1077;&#1088;&#1080;&#1088;&#1091;&#1077;&#1084; &#1076;&#1072;&#1090;&#1072;&#1089;&#1077;&#1090; &#1085;&#1072; 2 &#1082;&#1083;&#1072;&#1089;&#1089;&#1072;</span>
<span style="color: #cae682;">X</span>, <span style="color: #cae682;">y</span> = make_classification(n_samples=1000, n_classes=2, random_state=1)
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1088;&#1072;&#1079;&#1076;&#1077;&#1083;&#1103;&#1077;&#1084; &#1077;&#1075;&#1086; &#1085;&#1072; 2 &#1074;&#1099;&#1073;&#1086;&#1088;&#1082;&#1080;</span>
<span style="color: #cae682;">trainX</span>, <span style="color: #cae682;">testX</span>, <span style="color: #cae682;">trainy</span>, <span style="color: #cae682;">testy</span> = train_test_split(X, y, test_size=0.5, random_state=2)
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1086;&#1073;&#1091;&#1095;&#1072;&#1077;&#1084; &#1084;&#1086;&#1076;&#1077;&#1083;&#1100;</span>
<span style="color: #cae682;">model</span> = LogisticRegression(solver=<span style="color: #95e454;">'lbfgs'</span>)
model.fit(trainX, trainy)
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1087;&#1086;&#1083;&#1091;&#1095;&#1072;&#1077;&#1084; &#1087;&#1088;&#1077;&#1076;&#1082;&#1072;&#1079;&#1072;&#1085;&#1080;&#1103;</span>
<span style="color: #cae682;">lr_probs</span> = model.predict_proba(testX)
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1089;&#1086;&#1093;&#1088;&#1072;&#1085;&#1103;&#1077;&#1084; &#1074;&#1077;&#1088;&#1086;&#1103;&#1090;&#1085;&#1086;&#1089;&#1090;&#1080; &#1090;&#1086;&#1083;&#1100;&#1082;&#1086; &#1076;&#1083;&#1103; &#1087;&#1086;&#1083;&#1086;&#1078;&#1080;&#1090;&#1077;&#1083;&#1100;&#1085;&#1086;&#1075;&#1086; &#1080;&#1089;&#1093;&#1086;&#1076;&#1072;</span>
<span style="color: #cae682;">lr_probs</span> = lr_probs[:, 1]
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1088;&#1072;&#1089;&#1089;&#1095;&#1080;&#1090;&#1099;&#1074;&#1072;&#1077;&#1084; ROC AUC</span>
<span style="color: #cae682;">lr_auc</span> = roc_auc_score(testy, lr_probs)
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">'LogisticRegression: ROC AUC=%.3f'</span> % (lr_auc))
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1088;&#1072;&#1089;&#1089;&#1095;&#1080;&#1090;&#1099;&#1074;&#1072;&#1077;&#1084; roc-&#1082;&#1088;&#1080;&#1074;&#1091;&#1102;</span>
<span style="color: #cae682;">fpr</span>, <span style="color: #cae682;">tpr</span>, <span style="color: #cae682;">treshold</span> = roc_curve(testy, lr_probs)
<span style="color: #cae682;">roc_auc</span> = auc(fpr, tpr)
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1089;&#1090;&#1088;&#1086;&#1080;&#1084; &#1075;&#1088;&#1072;&#1092;&#1080;&#1082;</span>
plt.plot(fpr, tpr, color=<span style="color: #95e454;">'darkorange'</span>,
         label=<span style="color: #95e454;">'ROC &#1082;&#1088;&#1080;&#1074;&#1072;&#1103; (area = %0.2f)'</span> % roc_auc)
plt.plot([0, 1], [0, 1], color=<span style="color: #95e454;">'navy'</span>, linestyle=<span style="color: #95e454;">'--'</span>)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel(<span style="color: #95e454;">'False Positive Rate'</span>)
plt.ylabel(<span style="color: #95e454;">'True Positive Rate'</span>)
plt.title(<span style="color: #95e454;">'&#1055;&#1088;&#1080;&#1084;&#1077;&#1088; ROC-&#1082;&#1088;&#1080;&#1074;&#1086;&#1081;'</span>)
plt.legend(loc=<span style="color: #95e454;">"lower right"</span>)
plt.show()

</pre>
</div>
</div>
</li>
</ol>
</li>
<li><a id="org2825414"></a>Gini coefficient, Gini impurity index, G1<br />
<div class="outline-text-5" id="text-11-5-33-8">
<ul class="org-ul">
<li><a href="https://habr.com/ru/company/ods/blog/350440/">https://habr.com/ru/company/ods/blog/350440/</a></li>
<li><a href="https://dyakonov.org/2015/12/15/%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D0%BC%D1%8C%D1%82%D0%B5%D1%81%D1%8C-%D0%B4%D0%B6%D0%B8%D0%BD%D0%B8/">https://dyakonov.org/2015/12/15/%D0%B7%D0%BD%D0%B0%D0%BA%D0%BE%D0%BC%D1%8C%D1%82%D0%B5%D1%81%D1%8C-%D0%B4%D0%B6%D0%B8%D0%BD%D0%B8/</a></li>
<li><a href="https://github.com/oliviaguest/gini">https://github.com/oliviaguest/gini</a></li>
<li>В ML - метрика качества, которая часто используется при оценке предсказательных моделей в задачах бинарной</li>
</ul>
<p>
классификации в условиях сильной несбалансированности классов целевой переменной. how good the model is for
distinguishing the given classes
</p>



<ul class="org-ul">
<li>Обычный коэффициент Джини идеального алгоритма всегда будет равен 0.25</li>
<li>Gperfect = 0.25</li>
<li>Gnorm = Gmodel/Gperfect</li>
</ul>

<p>
gini<sub>normalized</sub> = 2 * roc<sub>auc</sub><sub>score</sub>(actual, predict) - 1
</p>
<ul class="org-ul">
<li>Предсказание идеального алгоритма является максимальным коэффициентом Джини для текущего набора данных и
зависит только от истинного распределения классов в задаче.</li>
<li>Коэффициент Джини случайного алгоритма равен 0</li>
<li>Значения нормализованного коэффициента Джини для обученного алгоритма находятся в интервале [0,1]</li>
</ul>

<p>
Gini = (AUC-0.5)/0.5 = 2*AUC - 1
</p>
<ul class="org-ul">
<li>(AUC - 0.5) площадь верхнего треугольника</li>
<li>/0.5 делить на площадь нижнего треугольника</li>
</ul>

<p>
G1 = 1 - ∑(Xk - X(k-1))*(Yk + Y(k-1))
</p>

<p>
Gini — то насколько «заполнена» верхняя половина квадрата, т.е. отношение площади над диагональю, к площади
треугольника под диагональю
</p>

<p>
Example:
</p>
<ul class="org-ul">
<li>accuracy	0.934783</li>
<li>auc 0.84375</li>
<li>gini 0.6875</li>
<li>0.0       0.98 precision</li>
<li>1.0       0.33 precision</li>
<li>(0.98 + 0.33) /2 = 0.655</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">without scikit-learn</span>
<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">gini</span>(actual, pred, cmpcol = 0, sortcol = 1):
    <span style="color: #8ac6f2; font-weight: bold;">assert</span>( <span style="color: #e5786d;">len</span>(actual) == <span style="color: #e5786d;">len</span>(pred) )
    <span style="color: #e5786d;">all</span> = np.asarray(np.c_[ actual, pred, np.arange(<span style="color: #e5786d;">len</span>(actual)) ], dtype=np.<span style="color: #e5786d;">float</span>)
    <span style="color: #e5786d;">all</span> = <span style="color: #e5786d;">all</span>[ np.lexsort((<span style="color: #e5786d;">all</span>[:,2], -1*<span style="color: #e5786d;">all</span>[:,1])) ]
    <span style="color: #cae682;">totalLosses</span> = <span style="color: #e5786d;">all</span>[:,0].<span style="color: #e5786d;">sum</span>()
    <span style="color: #cae682;">giniSum</span> = <span style="color: #e5786d;">all</span>[:,0].cumsum().<span style="color: #e5786d;">sum</span>() / totalLosses

    <span style="color: #cae682;">giniSum</span> -= (<span style="color: #e5786d;">len</span>(actual) + 1) / 2.
    <span style="color: #8ac6f2; font-weight: bold;">return</span> giniSum / <span style="color: #e5786d;">len</span>(actual)

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">gini_normalized</span>(a, p):
    <span style="color: #8ac6f2; font-weight: bold;">return</span> gini(a, p) / gini(a, a)
</pre>
</div>
</div>
<ol class="org-ol">
<li><a id="org0822680"></a>В экономике<br />
<div class="outline-text-6" id="text-11-5-33-8-1">
<p>
Показатель степени расслоения общества относительно какого-либо экономического признака - 0-1 или 0-100%
</p>

<p>
G = 1-[n]∑(Xk-X[k-1])*(Yk+Y[k-1])
</p>
<ul class="org-ul">
<li>n - число жителей</li>
<li>Xk - кумулятивная доля населения</li>
<li>Yk - кумулятивная доля дохода</li>
</ul>

<p>
7 человек получают 1 рубль в год, 1 человек — 10 рублей, 1 человек — 33 рубля и один человек — 50 рублей, суммарный доход = 100
</p>
<ul class="org-ul">
<li>n = 10</li>
<li>Xk = [1-n]∑k/n = np.cumsum(np.ones(10)/10) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0</li>
<li>Xk-1 = 0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9</li>
<li>Yk = ∑kД/сумма = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.17,0.50,1.00]</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #cae682;">x</span> = np.cumsum(np.ones(10)/10)
<span style="color: #cae682;">xk_1</span> = np.roll(x,1)
<span style="color: #cae682;">xk_1</span>[0] = 0
<span style="color: #cae682;">y</span> = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.17,0.50,1.00]
<span style="color: #cae682;">yk_1</span> = np.roll(y,1)
<span style="color: #cae682;">yk_1</span>[0] = 0

</pre>
</div>



<p>
np.sum((x - xk<sub>1</sub>) * (y + yk<sub>1</sub>))
</p>
</div>
</li>
<li><a id="org3de656b"></a>В ML<br />
<div class="outline-text-6" id="text-11-5-33-8-2">
<ul class="org-ul">
<li><a href="https://habr.com/en/company/ods/blog/350440/">https://habr.com/en/company/ods/blog/350440/</a></li>
</ul>

<p>
бинарная классификации для 15 объектов:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #cae682;">actual</span> = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
<span style="color: #cae682;">predict</span> = [0.9, 0.3, 0.8, 0.75, 0.65, 0.6, 0.78, 0.7, 0.05, 0.4, 0.4, 0.05, 0.5, 0.1, 0.1]

<span style="color: #cae682;">data</span> = <span style="color: #e5786d;">zip</span>(actual, predict)
<span style="color: #cae682;">sorted_data</span> = <span style="color: #e5786d;">sorted</span>(data, key=<span style="color: #8ac6f2; font-weight: bold;">lambda</span> d: d[1], reverse=<span style="color: #e5786d; font-weight: bold;">True</span>)
<span style="color: #cae682;">sorted_actual</span> = [d[0] <span style="color: #8ac6f2; font-weight: bold;">for</span> d <span style="color: #8ac6f2; font-weight: bold;">in</span> sorted_data] <span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">actual sorted by predict descending</span>

<span style="color: #cae682;">cumulative_actual</span> = np.cumsum(sorted_actual) / <span style="color: #e5786d;">sum</span>(actual)
<span style="color: #cae682;">cumulative_index</span> = np.arange(1, <span style="color: #e5786d;">len</span>(cumulative_actual)+1) / <span style="color: #e5786d;">len</span>(predict) <span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">or np.cumsum(np.ones(15)/15)</span>
<span style="color: #cae682;">cumulative_actual_perfect</span> = np.cumsum(<span style="color: #e5786d;">sorted</span>(actual, reverse=<span style="color: #e5786d; font-weight: bold;">True</span>)) / <span style="color: #e5786d;">sum</span>(actual) <span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">sort actual by descending</span>

<span style="color: #cae682;">x_values</span> = [0] + <span style="color: #e5786d;">list</span>(cumulative_index)
<span style="color: #cae682;">y_values</span> = [0] + <span style="color: #e5786d;">list</span>(cumulative_actual)
<span style="color: #cae682;">y_values_perfect</span> = [0] + <span style="color: #e5786d;">list</span>(cumulative_actual_perfect)

<span style="color: #cae682;">f1</span>, <span style="color: #cae682;">f2</span> = interp1d(x_values, y_values), interp1d(x_values, y_values_perfect) <span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">&#1092;&#1091;&#1085;&#1082;&#1094;&#1080;&#1080; &#1087;&#1086; &#1090;&#1086;&#1095;&#1082;&#1072;&#1084;</span>
<span style="color: #cae682;">S_pred</span> = quad(f1, 0, 1, points=x_values)[0] - 0.5 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1087;&#1083;&#1086;&#1097;&#1072;&#1076;&#1100; - &#1044;&#1078;&#1080;&#1085;&#1080; &#1076;&#1083;&#1103; &#1084;&#1086;&#1076;&#1077;&#1083;&#1080;</span>
<span style="color: #cae682;">S_actual</span> = quad(f2, 0, 1, points=x_values)[0] - 0.5 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1087;&#1083;&#1086;&#1097;&#1072;&#1076;&#1100; - &#1076;&#1078;&#1080;&#1085;&#1080; &#1076;&#1083;&#1103; &#1080;&#1076;&#1077;&#1072;&#1083;&#1072;</span>
<span style="color: #cae682;">G</span> = S_pred/ S_actual <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1082;&#1086;&#1101;&#1092;&#1092;&#1080;&#1094;&#1080;&#1077;&#1085;&#1090; &#1044;&#1078;&#1080;&#1085;&#1080;</span>


</pre>
</div>
</div>
</li>
</ol>
</li>

<li><a id="org80636ef"></a>K-S Kolomogorov Smirnov<br />
<div class="outline-text-5" id="text-11-5-33-9">
<p>
a measure of the degree of separation between the positive and negative distributions.
</p>

<p>
-&gt; Rank the N random numbers in ascending order.
-&gt; Calculate D+ as max(i/N-Ri) for all i in(1, N)
-&gt; Calculate D- as max(Ri-((i-1)/N)) for all i in(1, N)
-&gt; Calculate D as max(D+, D-)
-&gt; If D&gt;D(alpha)
    Rejects Uniformity
   else
    It fails to reject the Null Hypothesis.
</p>

<div class="org-src-container">
<pre class="src src-python">
<span style="color: #8ac6f2; font-weight: bold;">import</span> random

<span style="color: #cae682;">N</span> = <span style="color: #e5786d;">int</span>(<span style="color: #e5786d;">input</span>(<span style="color: #95e454;">"Enter the size of random numbers to be produced : "</span>))
<span style="color: #cae682;">D_plus</span> =[]
<span style="color: #cae682;">D_minus</span> =[]
<span style="color: #cae682;">_random</span> =[]

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Rank the N random numbers</span>
<span style="color: #8ac6f2; font-weight: bold;">for</span> i <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(0, N):
    _random.append(random.random())
    _random.sort()

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Calculate max(i/N-Ri)</span>
<span style="color: #8ac6f2; font-weight: bold;">for</span> i <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(1, N + 1):
    <span style="color: #cae682;">x</span> = i / N - _random[i-1]
    D_plus.append(x)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Calculate max(Ri-((i-1)/N))</span>
<span style="color: #8ac6f2; font-weight: bold;">for</span> i <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(1, N + 1):
    <span style="color: #cae682;">y</span> =(i-1)/N
    <span style="color: #cae682;">y</span> =_random[i-1]-y
    D_minus.append(y)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Calculate max(D+, D-)</span>
<span style="color: #cae682;">ans</span> = <span style="color: #e5786d;">max</span>(<span style="color: #e5786d;">max</span>(D_plus, D_minus))
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"Value of D is :"</span>)
<span style="color: #e5786d;">print</span>(ans)

</pre>
</div>
</div>
</li>
<li><a id="org6ff3a30"></a>k-fold cross validation<br />
<div class="outline-text-5" id="text-11-5-33-10">
<p>
is the gold-standard for evaluating the performance of a machine learning algorithm on unseen data with k set to 3, 5, or 10.
</p>
</div>
</li>
<li><a id="org9b9ea7a"></a>R<sup>2</sup> Pirson - r2<sub>score</sub> - Coefficient of determination<br />
<div class="outline-text-5" id="text-11-5-33-11">
<p>
for regression
</p>

<p>
Измеряет совместные колебания предсказаний и меток от их средних значений, нормализованных своими
 соответствующими диапазонами колебаний.
</p>
</div>
</li>

<li><a id="org95423ed"></a>Matthews Correlation Coefficient (MCC)<br />
<div class="outline-text-5" id="text-11-5-33-12">
<ul class="org-ul">
<li>for the classification problems</li>
<li>MCC is a metric that considers all possibilities of binary classification (TP, TN, FP, and FN)</li>
<li>robust to unbalanced datasets</li>
<li>between -1 and 1
<ul class="org-ul">
<li>-1 more mistakes</li>
<li>0 classifier is just predicting the most frequent class</li>
</ul></li>
</ul>
<p>
MCC = (TP*TN - FP*FN)/sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))
</p>
</div>
</li>
<li><a id="org74b4234"></a>TODO<br />
<div class="outline-text-5" id="text-11-5-33-13">
<ul class="org-ul">
<li>Статистика Колмогорова-Смирнова (вычисляется как максимальная разница между кумулятивными функциями
распределения «плохих» и «хороших» заемщиков. Выше в статье приводился рисунок с распределениями и этой
статистикой)</li>
<li>Коэффициент дивергенции (представляет собой оценку разницы математических ожиданий распределений скоринговых
баллов для «плохих» и «хороших» заемщиков, нормализованную дисперсиями этих распределений. Чем больше
значение коэффициента дивергенции, тем лучше качество модели.)</li>
</ul>

<p>
Не знаю как обстоят дела в России, хоть и живу здесь, но в Европе наиболее широко применяется коэффициент Джини, в Северной Америке — статистика Колмогорова-Смирнова.
</p>
</div>
</li>

<li><a id="org52734bc"></a>ranged based metrids<br />
<div class="outline-text-5" id="text-11-5-33-14">
<ul class="org-ul">
<li>Range-based Recall &amp; Precision (RR,PR)</li>
<li>Time-Series Aware Precision and Recall(TaP,TaR)</li>
</ul>

<p>
article "A Study on Performance Metrics for Anomaly Detection Based on Industrial Control System Operation
 Data"
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org4a3a948" class="outline-4">
<h4 id="org4a3a948"><span class="section-number-4">11.5.34.</span> forecast</h4>
<div class="outline-text-4" id="text-11-5-34">
<p>
y - actual, x - forecasted
</p>
<ul class="org-ul">
<li>Mean forecast error - mean(y-x) - one value - ~0 - good</li>
<li>Mean absolute error - mean(|mfe|/x) - one value</li>
</ul>

<p>
Рост или падение за период p:
(p.mean() - p[0])/p[0]   - [-1 &#x2026;. ∞]
</p>

<p>
<a href="https://facebook.github.io/prophet/">https://facebook.github.io/prophet/</a>
</p>
</div>
</div>
<div id="outline-container-org943d5d0" class="outline-4">
<h4 id="org943d5d0"><span class="section-number-4">11.5.35.</span> Machine Learning Crash Course Google <a href="https://developers.google.com/machine-learning/crash-course/ml-intro">https://developers.google.com/machine-learning/crash-course/ml-intro</a></h4>
<div class="outline-text-4" id="text-11-5-35">
<p>
Terms:
</p>
<ul class="org-ul">
<li>overfitting - хорошо на обучающей, плохо на новых</li>
<li>underfitting - возможно плохая модель</li>

<li>Kernel method or kernel trick - computing the inner products between the images of all pairs of data in
implicit, high-dimensional feature space without ever computing the coordinates of the data in that space</li>

<li>outliers - Values distant from most other values
<ul class="org-ul">
<li>Weights with high absolute values</li>
<li>Predicted values relatively far away from the actual values</li>
<li>Input data whose values are more than roughly 3 standard deviations from the mean.</li>
</ul></li>
<li>clipping - handling outliers - Clip all values over 60 to be exactly 60 - Clip all values under 40 to be
exactly 40</li>
</ul>

<p>
<b>Когда признаков слишком много, то легко переобучить</b>
</p>

<p>
<b>Machine Learning</b> is an algorithm that can learn from data without relying on rules-based programming.
</p>
<ul class="org-ul">
<li>describing your data with features a computer can understand</li>
<li>learning algorithm - Optimizing the weights on features</li>
</ul>

<p>
<b>Statistical Modelling</b> is formalization of relationships between variables in the form of mathematical equations.
</p>

<p>
<b>Deep Learning</b> - (dominant model - neural networks) - похожа на stacked logistic regression (Mathematical
 statistics) - uses multiple layers to progressively extract higher level features from the raw input
</p>
<ul class="org-ul">
<li>representation learning - automatically learn good features</li>
<li>Deep learning algorithms - to learn (multiple levels of) representation and an output</li>
<li>from raw input - sound, characters, words</li>
</ul>

<p>
<b>few-shot learning</b> algorithms - used when training data becomes costly
</p>
<ol class="org-ol">
<li>semi-supervised manner with unlabeled images - produce new data - add random noise</li>
<li>Parameter-level approach - parameter space can be limited - regularization techniques or loss functions are
often employed</li>
</ol>
</div>
</div>
<div id="outline-container-org108e2e1" class="outline-4">
<h4 id="org108e2e1"><span class="section-number-4">11.5.36.</span> Дилемма смещения–дисперсии Bias–variance tradeoff or Approximation-generalization tradeoff</h4>
<div class="outline-text-4" id="text-11-5-36">
<p>
The <b>bias error</b> <b>Смещение</b> is an error from erroneous assumptions in the learning algorithm. How well model
 fit to training data.
</p>
<ul class="org-ul">
<li>erroneous assumptions - ошибочные заключения</li>
<li>very small training error -&gt; very small bias</li>
<li>bias is a way of describing the difference between the actual, true relationship in our data</li>
</ul>

<p>
The <b>variance</b> <b>Дисперсия</b> is an error from sensitivity to small fluctuations in the training set.
</p>
<ul class="org-ul">
<li>how consistent a certain machine learning model is in its predictions when compared across similar datasets</li>
<li>small fluctuation of the error -&gt; small variance</li>
<li>model performs poorly, and does so consistently. - small variance</li>
</ul>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Training</th>
<th scope="col" class="org-left">Validation</th>
<th scope="col" class="org-left">&#xa0;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">high bias</td>
<td class="org-left">low variance</td>
<td class="org-left">underfitting</td>
</tr>

<tr>
<td class="org-left">low bias</td>
<td class="org-left">high variance</td>
<td class="org-left">overfitting</td>
</tr>
</tbody>
</table>

<p>
also
</p>
<ul class="org-ul">
<li>Models with high bias will have low variance.</li>
<li>Models with high variance will have a low bias.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgcf16e0b"></a>model complexity<br />
<div class="outline-text-5" id="text-11-5-36-1">
<div class="org-src-container">
<pre class="src src-text">variance
      |
      |                 |
       \               /-------bias
        \_           _/
          \__     __/
   ----------\---/-------------------&gt; Model complexity
</pre>
</div>
</div>
</li>

<li><a id="org083f310"></a>Algorithms<br />
<div class="outline-text-5" id="text-11-5-36-2">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Algorithm</th>
<th scope="col" class="org-left">Bias</th>
<th scope="col" class="org-left">Variance</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Linear Regression</td>
<td class="org-left">High Bias</td>
<td class="org-left">Less Variance</td>
</tr>

<tr>
<td class="org-left">Decision Tree</td>
<td class="org-left">Low Bias</td>
<td class="org-left">High Variance</td>
</tr>

<tr>
<td class="org-left">Bagging</td>
<td class="org-left">Low Bias</td>
<td class="org-left">High Variance (Less than Decision Tree)</td>
</tr>

<tr>
<td class="org-left">Random Forest</td>
<td class="org-left">Low Bias</td>
<td class="org-left">High Variance (Less than Decision Tree and Bagging)</td>
</tr>
</tbody>
</table>
</div>
</li>
</ol>
</div>

<div id="outline-container-orge55171e" class="outline-4">
<h4 id="orge55171e"><span class="section-number-4">11.5.37.</span> Explainable AI (XAI) and Interpretable Machine Learning (IML) models</h4>
<div class="outline-text-4" id="text-11-5-37">
<ul class="org-ul">
<li>2020 book <a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a></li>
<li><a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a></li>
<li>Interpretable Machine Learning with Python <a href="http://savvastjortjoglou.com/intrepretable-machine-learning-nfl-combine.html#Feature-Contributions">http://savvastjortjoglou.com/intrepretable-machine-learning-nfl-combine.html#Feature-Contributions</a></li>
<li>2021 <a href="https://www.ambiata.com/blog/2021-04-12-xai-part-1/">https://www.ambiata.com/blog/2021-04-12-xai-part-1/</a></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org76ad1c1"></a>terms<br />
<div class="outline-text-5" id="text-11-5-37-1">
<ul class="org-ul">
<li>narrative [ˈnærətɪv]</li>
<li>We torcher our data - обрабатываем наши данные</li>
</ul>
</div>
</li>
<li><a id="org1b899b4"></a>SHAP (SHapley Additive exPlanations)<br />
<div class="outline-text-5" id="text-11-5-37-2">
<ul class="org-ul">
<li>doc <a href="https://shap.readthedocs.io/en/latest/index.html">https://shap.readthedocs.io/en/latest/index.html</a></li>
<li>git <a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></li>
<li>wiki <a href="https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80_%D0%A8%D0%B5%D0%BF%D0%BB%D0%B8">https://ru.wikipedia.org/wiki/%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80_%D0%A8%D0%B5%D0%BF%D0%BB%D0%B8</a></li>
<li>An introduction to explainable AI with Shapley values <a href="https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html">https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html</a></li>
</ul>

<p>
Shapley value (Вектор Шепли)- how important is each player to the overall cooperation, and what payoff can he
or she reasonably expect? The Shapley value provides one possible answer to this question.
</p>

<p>
SHAP – значения интерпретируют влияние определенного значения признака в сопоставлении с прогнозом, которое мы
сделали бы, если бы этот признак принял бы некоторое базовое значение.
</p>

<ul class="org-ul">
<li>value function</li>
<li>Shapley value для каждого игрока - его вклад и мера выигрыша</li>
<li>the SHAP value for a specific feature is just the difference between the expected model output and the
partial dependence plot at the feature’s value</li>
<li>SHAP values of all the input features will always sum up to the difference between baseline (expected) model
output and the current model output for the prediction being explained.</li>
<li>SHAP values are sensitive to high correlations among different features.</li>
<li>SHAP values represent a descriptive approximation of the predictive model</li>
<li>each individual rows will have their own set of SHAP values ( for customer)</li>
<li>SHAP value of a feature represents the impact of the evidence provided by that feature on the model’s output</li>
</ul>

<p>
steps
</p>
<ol class="org-ol">
<li>create Explainer(model)</li>
<li>.shap<sub>values</sub>(X) - Estimate the SHAP values for a set of samples - matrix # samples # features</li>
</ol>
</div>
<ol class="org-ol">
<li><a id="org97fb966"></a>theory<br />
<div class="outline-text-6" id="text-11-5-37-2-1">
<p>
KernelSHAP. This method works by permuting feature values and making predictions on those permutations. Once
we have enough permutations, the Shapley values are estimated using linear regression
</p>
</div>
</li>
<li><a id="org0fc0e9b"></a>shap<sub>values</sub><br />
<div class="outline-text-6" id="text-11-5-37-2-2">
<p>
shape (rows,features)
</p>
</div>
</li>
<li><a id="orgcd56808"></a>supported algorithms:<br />
<div class="outline-text-6" id="text-11-5-37-2-3">
<ul class="org-ul">
<li>TreeExplainer: Support XGBoost, LightGBM, CatBoost and scikit-learn models by Tree SHAP.</li>
<li>DeepExplainer (DEEP SHAP): Support TensorFlow and Keras models by using DeepLIFT and Shapley values.</li>
<li>GradientExplainer: Support TensorFlow and Keras models.</li>
<li>KernelExplainer (Kernel SHAP): Applying to any models by using LIME and Shapley values.</li>

<li>“permutation”</li>
<li>“partition” - explain the output of any function.</li>
<li>“tree”</li>
<li>“kernel” -  special weighted linear regression to compute the importance of each feature</li>
<li>“sampling” - It is a good alternative to KernelExplainer when you want to use a large background set (as
opposed to a single reference value for example).</li>
<li>“linear”</li>
<li>“deep” - for deep learning models</li>
<li>“gradient”</li>
</ul>

<p>
Explainer - auto
LinerExplainer
TreeExplainer
DeepExplainer
KernelExplainer
PartitionExplainer
PermutationExplainer
SamplingExplainer
AdditiveEplainer
GPUTreeExplainer
GradientExplainer
</p>
</div>
</li>

<li><a id="orgffdb1a1"></a>expected<sub>value</sub><br />
<div class="outline-text-6" id="text-11-5-37-2-4">
<p>
property of Explainer - average model output over dataset
</p>
<ul class="org-ul">
<li>model.predict(data).maan(0) - средняя в столбце, если y - список - это число</li>
</ul>

<p>
feature pushed value higher - red, lower - blue
</p>
</div>
</li>
<li><a id="org49dd624"></a>interaction values<br />
<div class="outline-text-6" id="text-11-5-37-2-5">
<p>
<a href="https://h1ros.github.io/posts/explain-the-interaction-values-by-shap">https://h1ros.github.io/posts/explain-the-interaction-values-by-shap</a>
</p>

<p>
square for every record - numpy.ndarray
</p>

<p>
main effects are on the diagonal and the interaction effects are off-diagonal
</p>

<p>
SHAP interaction values are a generalization of SHAP values to higher order interactions.
</p>

<ol class="org-ol">
<li>summary plot</li>
<li>dependece plot for 2 features</li>
</ol>
</div>
</li>

<li><a id="orga1cb32b"></a>plot<br />
<div class="outline-text-6" id="text-11-5-37-2-6">
<ul class="org-ul">
<li>bar
<ul class="org-ul">
<li>single row of ShapV - shap value as a bar chart</li>
<li>multi-row of ShapV -  mean absolute value for each feature column as a bar chart</li>
</ul></li>
<li>waterfall - one-dimensional Explanation object -  explantion of a single prediction as a waterfall plot</li>
<li>scatter - column of SHAP - shap<sub>values</sub>[:,”Feature A”] -  value of the feature on the x-axis, SHAP value on y-axis
<ul class="org-ul">
<li>shap.plots.scatter(shap<sub>values</sub>[:,"RM"], color=shap<sub>values</sub>) - e SHAP value of that feature vs. the value of
the feature for all the examples in a dataset. If we pass the whole explanation tensor to the color
argument the scatter plot will pick the best feature to color by.</li>
</ul></li>
<li>heatmap - multi-row ShapV - ?</li>
<li>force -
<ul class="org-ul">
<li>single row of ShapV - waterfall in ine line</li>
<li>multi-row of ShapV - single rows rotated by 90 degree and stacked together</li>
</ul></li>
<li>text</li>
<li>image</li>
<li>partial<sub>dependence</sub></li>
<li>beeswarm - used as summary plot</li>
<li>decision</li>
</ul>

<p>
<b>SHAP Summary Plot</b>  <a href="https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html">https://shap-lrjball.readthedocs.io/en/latest/generated/shap.summary_plot.html</a>
</p>
<ul class="org-ul">
<li>feature importance with magnitude by classes
<ul class="org-ul">
<li>beeswarm - dots - instances and its densities.  Color is used to display the original value of a feature
<ul class="org-ul">
<li>default the features are ordered using shap<sub>values.abs.mean</sub>(0)</li>
</ul></li>
</ul></li>
</ul>

<p>
SHAP Dependence Plots -
</p>
</div>
</li>

<li><a id="org6f44494"></a>limitations<br />
<div class="outline-text-6" id="text-11-5-37-2-7">
<ul class="org-ul">
<li>we assume feature independence - not correlated</li>
<li>not for causal inference -
<ul class="org-ul">
<li>Shap is not a measure of “how important a given feature is in the real world”, it is simply “how important
a feature is to the model”. — Gianlucca Zuin</li>
</ul></li>
<li>human error - Confirmation bias —unconsciously favoring information that confirms your previously existing
beliefs</li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org403055d"></a>Model-Agnostic Interpretation Methods<br />
<div class="outline-text-5" id="text-11-5-37-3">
<ul class="org-ul">
<li>Partial Dependence Plot (PDP)</li>
</ul>
</div>
</li>
<li><a id="orga1fd50e"></a>Model-specific Interpretation Methods<br /></li>
<li><a id="org51e544a"></a>false positive<br />
<div class="outline-text-5" id="text-11-5-37-5">
<div class="org-src-container">
<pre class="src src-python">
<span style="color: #cae682;">gini1</span> = []

<span style="color: #cae682;">res21</span> = []
<span style="color: #cae682;">res22</span> = []
<span style="color: #cae682;">res23</span> = []
<span style="color: #cae682;">res24</span> = []

<span style="color: #cae682;">acc2</span> = []
<span style="color: #cae682;">gini2</span> = []
<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">run</span>():
    <span style="color: #8ac6f2; font-weight: bold;">for</span> train_index, test_index <span style="color: #8ac6f2; font-weight: bold;">in</span> skf.split(X, Y):
        <span style="color: #cae682;">X_train</span>, <span style="color: #cae682;">X_test</span> = X.iloc[train_index, :], X.iloc[test_index, :]
        <span style="color: #cae682;">Y_train</span>, <span style="color: #cae682;">Y_test</span> = Y.iloc[train_index, :], Y.iloc[test_index, :]

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1054;&#1073;&#1091;&#1095;&#1072;&#1077;&#1084; &#1085;&#1072; &#1092;&#1086;&#1083;&#1076;&#1077; &#1086;&#1090;&#1082;&#1083;&#1086;&#1085;&#1077;&#1085;&#1085;&#1099;&#1093; &#1072;&#1085;&#1076;&#1077;&#1088;&#1072;&#1081;&#1090;&#1077;&#1088;&#1086;&#1084;</span>
        <span style="color: #cae682;">dtrain</span> = xgb.DMatrix(X_train, Y_train[<span style="color: #95e454;">'under'</span>]) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">under</span>
        <span style="color: #cae682;">bst</span>: Booster = xgb.train(param, dtrain, num_round)

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1058;&#1077;&#1089;&#1090;&#1080;&#1088;&#1091;&#1077;&#1084; &#1085;&#1072; &#1086;&#1090;&#1082;&#1083;&#1086;&#1085;&#1077;&#1085;&#1085;&#1099;&#1093; &#1089;&#1080;&#1089;&#1090;&#1077;&#1084;&#1086;&#1081;</span>
        <span style="color: #cae682;">dtest</span> = xgb.DMatrix(X_test, Y_test[<span style="color: #95e454;">'system'</span>]) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">system</span>
        <span style="color: #cae682;">ypred2</span>: np.array = bst.predict(dtest)

        <span style="color: #cae682;">cn</span> = []
        <span style="color: #cae682;">cp</span> = []
        <span style="color: #8ac6f2; font-weight: bold;">for</span> i, x <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">enumerate</span>(Y_test[<span style="color: #95e454;">'system'</span>]):
            <span style="color: #8ac6f2; font-weight: bold;">if</span> x == 0:
                cn.append(ypred2[i])
            <span style="color: #8ac6f2; font-weight: bold;">if</span> x == 1:
                cp.append(ypred2[i])
        res21.append((np.<span style="color: #e5786d;">round</span>(cn) == 0).mean())
        res22.append((np.<span style="color: #e5786d;">round</span>(cn) == 1).mean())
        res23.append((np.<span style="color: #e5786d;">round</span>(cp) == 1).mean())
        res24.append((np.<span style="color: #e5786d;">round</span>(cp) == 0).mean())
        acc1.append((np.<span style="color: #e5786d;">round</span>(ypred2) == Y_test[<span style="color: #95e454;">'system'</span>]).mean())
        <span style="color: #cae682;">auc</span> = sklearn.metrics.roc_auc_score(Y_test[<span style="color: #95e454;">'system'</span>], ypred2)
        gini1.append(2 * auc - 1)


        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">&#1090;&#1077;&#1089;&#1090;&#1080;&#1088;&#1091;&#1077;&#1084; &#1085;&#1072; &#1086;&#1090;&#1082;&#1083;&#1086;&#1085;&#1077;&#1085;&#1085;&#1099;&#1093; &#1072;&#1085;&#1076;&#1077;&#1088;&#1072;&#1081;&#1090;&#1086;&#1088;&#1086;&#1084;</span>
        <span style="color: #cae682;">dtest</span> = xgb.DMatrix(X_test, Y_test[<span style="color: #95e454;">'under'</span>])
        <span style="color: #cae682;">ypred2</span>: np.array = bst.predict(dtest)

        <span style="color: #cae682;">cn</span> = []
        <span style="color: #cae682;">cp</span> = []
        <span style="color: #8ac6f2; font-weight: bold;">for</span> i, x <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">enumerate</span>(Y_test[<span style="color: #95e454;">'under'</span>]):
            <span style="color: #8ac6f2; font-weight: bold;">if</span> x == 0:
                cn.append(ypred2[i])
            <span style="color: #8ac6f2; font-weight: bold;">if</span> x == 1:
                cp.append(ypred2[i])
        res1.append((np.<span style="color: #e5786d;">round</span>(cn) == 0).mean())
        res2.append((np.<span style="color: #e5786d;">round</span>(cn) == 1).mean())
        res3.append((np.<span style="color: #e5786d;">round</span>(cp) == 1).mean())
        res4.append((np.<span style="color: #e5786d;">round</span>(cp) == 0).mean())
        acc2.append((np.<span style="color: #e5786d;">round</span>(ypred2) == Y_test[<span style="color: #95e454;">'under'</span>]).mean())
        <span style="color: #cae682;">auc</span> = sklearn.metrics.roc_auc_score(Y_test[<span style="color: #95e454;">'under'</span>], ypred2)
        gini2.append(2 * auc - 1)

    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"&#1056;&#1077;&#1079;&#1091;&#1083;&#1100;&#1090;&#1072;&#1090;&#1099; &#1082;&#1088;&#1086;&#1089;&#1089;-&#1074;&#1072;&#1083;&#1080;&#1076;&#1072;&#1094;&#1080;&#1080; &#1090;&#1077;&#1089;&#1090;&#1080;&#1088;&#1086;&#1074;&#1072;&#1085;&#1080;&#1103; &#1085;&#1072; &#1086;&#1090;&#1082;&#1083;&#1086;&#1085;&#1077;&#1085;&#1085;&#1099;&#1093; &#1089;&#1080;&#1089;&#1090;&#1077;&#1084;&#1086;&#1081;"</span>)
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"&#1058;&#1086;&#1095;&#1085;&#1086;&#1089;&#1090;&#1100;:"</span>, np.array(acc1).mean())
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"&#1050;&#1086;&#1101;&#1092;&#1092;&#1080;&#1094;&#1080;&#1077;&#1085;&#1090; gini:"</span>, np.array(gini1).mean())
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"TrueNegative/Negative &#1076;&#1083;&#1103; 0:</span><span style="color: #e5786d; font-weight: bold;">\t</span><span style="color: #95e454;">%f"</span> % np.array(res21).mean())
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"FalsePositive/Negative &#1076;&#1083;&#1103; 0:</span><span style="color: #e5786d; font-weight: bold;">\t</span><span style="color: #95e454;">%f"</span> % np.array(res22).mean())
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"TruePositive/Positive &#1076;&#1083;&#1103; 1:</span><span style="color: #e5786d; font-weight: bold;">\t</span><span style="color: #95e454;">%f"</span> % np.array(res23).mean())
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"FalseNegative/Positive &#1076;&#1083;&#1103; 1:</span><span style="color: #e5786d; font-weight: bold;">\t</span><span style="color: #95e454;">%f"</span> % np.array(res24).mean(), <span style="color: #95e454;">"</span><span style="color: #e5786d; font-weight: bold;">\n</span><span style="color: #95e454;">"</span>)

    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"&#1056;&#1077;&#1079;&#1091;&#1083;&#1100;&#1090;&#1072;&#1090;&#1099; &#1082;&#1088;&#1086;&#1089;&#1089;-&#1074;&#1072;&#1083;&#1080;&#1076;&#1072;&#1094;&#1080;&#1080; &#1090;&#1077;&#1089;&#1090;&#1080;&#1088;&#1086;&#1074;&#1072;&#1085;&#1080;&#1103; &#1085;&#1072; &#1086;&#1090;&#1082;&#1083;&#1086;&#1085;&#1077;&#1085;&#1085;&#1099;&#1093; &#1072;&#1085;&#1076;&#1077;&#1088;&#1072;&#1081;&#1090;&#1086;&#1088;&#1086;&#1084;"</span>)
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"&#1058;&#1086;&#1095;&#1085;&#1086;&#1089;&#1090;&#1100;:"</span>, np.array(acc2).mean())
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"&#1050;&#1086;&#1101;&#1092;&#1092;&#1080;&#1094;&#1080;&#1077;&#1085;&#1090; gini:"</span>, np.array(gini2).mean())
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"TrueNegative/Negative &#1076;&#1083;&#1103; 0:</span><span style="color: #e5786d; font-weight: bold;">\t</span><span style="color: #95e454;">%f"</span> % np.array(res1).mean())
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"FalsePositive/Negative &#1076;&#1083;&#1103; 0:</span><span style="color: #e5786d; font-weight: bold;">\t</span><span style="color: #95e454;">%f"</span> % np.array(res2).mean())
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"TruePositive/Positive &#1076;&#1083;&#1103; 1:</span><span style="color: #e5786d; font-weight: bold;">\t</span><span style="color: #95e454;">%f"</span> % np.array(res3).mean())
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"* FalseNegative/Positive &#1076;&#1083;&#1103; 1:</span><span style="color: #e5786d; font-weight: bold;">\t</span><span style="color: #95e454;">%f"</span> % np.array(res4).mean())
</pre>
</div>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgf315e2a" class="outline-3">
<h3 id="orgf315e2a"><span class="section-number-3">11.6.</span> Sampling</h3>
<div class="outline-text-3" id="text-11-6">
<p>
drawing random samples form statistical distibution to have constant distribuion.
</p>
<ul class="org-ul">
<li>Slice sampling - simplest techniques - require that distribution to be sampled be evaluable.</li>
<li>Markov chain Monte Carlo (MCMC)</li>
<li>rejection sampling</li>
</ul>
</div>
<div id="outline-container-org3527020" class="outline-4">
<h4 id="org3527020"><span class="section-number-4">11.6.1.</span> slice sampling</h4>
<div class="outline-text-4" id="text-11-6-1">
<ul class="org-ul">
<li>Choose a starting value x0 for which f(x0) &gt; 0.</li>
<li>Sample a y value uniformly between 0 and f(x0).</li>
<li>Draw a horizontal line across the curve at this y position.</li>
<li>Sample a point (x, y) from the line segments within the curve.</li>
<li>Repeat from step 2 using the new x value.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgba90cc2" class="outline-3">
<h3 id="orgba90cc2"><span class="section-number-3">11.7.</span> likelihood, the log-likelihood, and the maximum likelihood estimate</h3>
<div class="outline-text-3" id="text-11-7">
</div>
<div id="outline-container-orge53a65c" class="outline-4">
<h4 id="orge53a65c"><span class="section-number-4">11.7.1.</span> links</h4>
<div class="outline-text-4" id="text-11-7-1">
<p>
<a href="https://en.wikipedia.org/wiki/Likelihood_function">https://en.wikipedia.org/wiki/Likelihood_function</a>
</p>
</div>
</div>
</div>
<div id="outline-container-orgc08181f" class="outline-3">
<h3 id="orgc08181f"><span class="section-number-3">11.8.</span> Reinforcement learning (RL)</h3>
<div class="outline-text-3" id="text-11-8">
</div>
<div id="outline-container-orgfac2de1" class="outline-4">
<h4 id="orgfac2de1"><span class="section-number-4">11.8.1.</span> terms</h4>
<div class="outline-text-4" id="text-11-8-1">
<ul class="org-ul">
<li>Stochastic <i>stəˈkæstɪk</i> refers to the property of being well described by a random probability distribution.</li>
<li><b>autoregressive (AR) model</b> - model of random process where Xt = c + ∑Ai*Xt-i + et , where sum by i, et is
white noice</li>
<li><b>Optimal control</b> or just <a id="orgfe170be"></a> - is a branch of mathematical <b>optimization</b> that deals with finding a
<b>control</b> for a <b>dynamical system</b> over a period of time such that an <b>objective function</b> is optimized.</li>
<li>optimal control theory</li>
<li><b>control</b> is a variable chosen by the controller or agent to manipulate <b>state variables</b>, similar to an</li>
</ul>
<p>
actual control valve.
</p>
<ul class="org-ul">
<li><b>state variable</b> is one of the set of variables that are used to describe the mathematical "state" of a
<b>dynamical system</b>.</li>
<li><b>Phase space</b> <b>Фазовое пространство</b> or <b>state space</b> - space in which all possible "states" of a</li>
</ul>
<p>
<b>dynamical system</b> or a <b>control system</b> are represented.
</p>
<ul class="org-ul">
<li><b>Control system</b> - manages, commands, directs, or regulates the behavior of other devices or systems using
control loops.</li>
<li><b>Dynamical system</b> - is a system in which a function describes the time dependence of a point in an</li>
</ul>
<p>
ambient space, such as in a parametric curve.
</p>
<ul class="org-ul">
<li><b>agent</b> - Software programs that make intelligent decisions and they are the learners in RL. These agents
interact with the environment by actions and receive rewards based on there actions.</li>
<li><b>environment</b> -  is typically stated in the form of a Markov decision process (MDP)</li>
<li><b>transition</b> - Moving from one state to another</li>
<li><b>Conditional probability distribution</b> of Y given X, P(Y|X), is the probability distribution of Y when X is
known to be a particular value. may be expressed as functions containing the unspecified value x.</li>
<li><b>return</b> - total sum of reward the agent receives from the environment = r1+r2+r3, where 1,2,3 is states.</li>
<li><b>offline RL</b> the agent learns from a pre-recorded dataset of experiences, without interacting with the
environment.</li>
<li><b>action-value function</b> - expected reward for MAB</li>
<li>Q-values,  Action values -</li>
<li><b>sample-efficient</b> - the ability of an algorithm to learn an effective policy using a minimal number of
interactions or samples from the environment.</li>
</ul>
</div>
</div>
<div id="outline-container-org5072921" class="outline-4">
<h4 id="org5072921"><span class="section-number-4">11.8.2.</span> basic</h4>
<div class="outline-text-4" id="text-11-8-2">
<p>
area of machine learning concerned with how intelligent agents ought to take actions in an environment in
 order to maximize the notion of <b>cumulative reward</b>
</p>

<ul class="org-ul">
<li>RL is a basic machine learning paradigms, alongside supervised learning and unsupervised learning.</li>
<li>focused on finding a balance between exploration (of uncharted territory) and exploitation (of current
knowledge).</li>
</ul>

<p>
Many reinforcement learning algorithms are formulated within the framework of <b>Markov Decision Processes
 (MDPs)</b>, which describe the environment in terms of states, actions, transitions, and rewards. MDPs assume
 that the future state depends only on the current state and action, not on any previous states (Markovian
 property).
</p>
</div>
</div>
<div id="outline-container-orgb0488d5" class="outline-4">
<h4 id="orgb0488d5"><span class="section-number-4">11.8.3.</span> Exploration Strategy</h4>
<div class="outline-text-4" id="text-11-8-3">
</div>
<ol class="org-ol">
<li><a id="org4156dc0"></a>Boltzmann exploration<br />
<div class="outline-text-5" id="text-11-8-3-1">
<p>
Boltzmann distribution, also known as the softmax function.
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">boltzmann_exploration</span>(Q_values, temperature):
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Calculate the exponential values of the Q-values divided by the temperature</span>
    <span style="color: #cae682;">exp_values</span> = np.exp(Q_values / temperature)

    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Calculate the probabilities using the softmax function</span>
    <span style="color: #cae682;">action_probabilities</span> = exp_values / np.<span style="color: #e5786d;">sum</span>(exp_values)

    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Choose an action based on the calculated probabilities</span>
    <span style="color: #cae682;">chosen_action</span> = np.random.choice(<span style="color: #e5786d;">len</span>(Q_values), p=action_probabilities)

    <span style="color: #8ac6f2; font-weight: bold;">return</span> chosen_action

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Example usage</span>
<span style="color: #cae682;">Q_values</span> = np.array([1.0, 2.0, 0.5, 1.5])  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Action values (e.g., Q-values)</span>
<span style="color: #cae682;">temperature</span> = 0.1  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Temperature parameter</span>

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Select an action using Boltzmann exploration</span>
<span style="color: #cae682;">chosen_action</span> = boltzmann_exploration(Q_values, temperature)

<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"Action Values:"</span>, Q_values)
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"Selected Action:"</span>, chosen_action)
</pre>
</div>

<pre class="example">
Action Values: [1.  2.  0.5 1.5]
Selected Action: 1
</pre>
</div>
</li>
<li><a id="org994087a"></a>Boltzmann Exploration with Scheduled Temperature<br />
<div class="outline-text-5" id="text-11-8-3-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">boltzmann_exploration</span>(Q_values, temperature):

    <span style="color: #8ac6f2; font-weight: bold;">if</span> temperature &lt; 0.001:
        <span style="color: #cae682;">temperature</span> = 0.001
    <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"Q_values, temperature"</span>, Q_values, temperature)
    <span style="color: #cae682;">exp_values</span> = np.exp(np.divide(Q_values, temperature))
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print("exp_values, np.sum(exp_values)", exp_values, np.sum(exp_values))</span>
    <span style="color: #cae682;">action_probabilities</span> = exp_values / np.<span style="color: #e5786d;">sum</span>(exp_values)
    <span style="color: #cae682;">chosen_action</span> = np.random.choice(<span style="color: #e5786d;">len</span>(Q_values), p=action_probabilities)
    <span style="color: #8ac6f2; font-weight: bold;">return</span> chosen_action

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">linear_decay_temperature</span>(t, initial_temperature, total_time_steps):
    <span style="color: #cae682;">decay</span> = initial_temperature / total_time_steps
    <span style="color: #8ac6f2; font-weight: bold;">return</span> initial_temperature - decay * t

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">exponential_decay_temperature</span>(t, initial_temperature, decay_rate):
    <span style="color: #8ac6f2; font-weight: bold;">return</span> initial_temperature * np.exp(-decay_rate * t)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Example usage</span>
<span style="color: #cae682;">Q_values</span> = np.array([1.0, 2.0, 0.5, 1.5, 1.9, 1.8, 1.99])  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Action values (e.g., Q-values)</span>
<span style="color: #cae682;">initial_temperature</span> = 1.0
<span style="color: #cae682;">total_time_steps</span> = 100

<span style="color: #8ac6f2; font-weight: bold;">for</span> t <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(total_time_steps):
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">temperature = linear_decay_temperature(t, initial_temperature, total_time_steps)</span>
    <span style="color: #cae682;">temperature</span> = exponential_decay_temperature(t, initial_temperature, 0.1)
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print(temperature)</span>
    <span style="color: #cae682;">chosen_action</span> = boltzmann_exploration(Q_values, temperature)
    <span style="color: #e5786d;">print</span>(f<span style="color: #95e454;">"Time Step: </span>{t}<span style="color: #95e454;">, Temperature: </span>{temperature}<span style="color: #95e454;">, Chosen Action: </span>{chosen_action}<span style="color: #95e454;">"</span>)

</pre>
</div>

<pre class="example" id="orgfd2ff46">
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 1.0
Time Step: 0, Temperature: 1.0, Chosen Action: 0
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.9048374180359595
Time Step: 1, Temperature: 0.9048374180359595, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.8187307530779818
Time Step: 2, Temperature: 0.8187307530779818, Chosen Action: 5
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.7408182206817179
Time Step: 3, Temperature: 0.7408182206817179, Chosen Action: 3
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.6703200460356393
Time Step: 4, Temperature: 0.6703200460356393, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.6065306597126334
Time Step: 5, Temperature: 0.6065306597126334, Chosen Action: 4
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.5488116360940264
Time Step: 6, Temperature: 0.5488116360940264, Chosen Action: 4
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.49658530379140947
Time Step: 7, Temperature: 0.49658530379140947, Chosen Action: 4
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.44932896411722156
Time Step: 8, Temperature: 0.44932896411722156, Chosen Action: 5
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.4065696597405991
Time Step: 9, Temperature: 0.4065696597405991, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.36787944117144233
Time Step: 10, Temperature: 0.36787944117144233, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.33287108369807955
Time Step: 11, Temperature: 0.33287108369807955, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.301194211912202
Time Step: 12, Temperature: 0.301194211912202, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.2725317930340126
Time Step: 13, Temperature: 0.2725317930340126, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.24659696394160643
Time Step: 14, Temperature: 0.24659696394160643, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.22313016014842982
Time Step: 15, Temperature: 0.22313016014842982, Chosen Action: 3
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.20189651799465538
Time Step: 16, Temperature: 0.20189651799465538, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.1826835240527346
Time Step: 17, Temperature: 0.1826835240527346, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.16529888822158653
Time Step: 18, Temperature: 0.16529888822158653, Chosen Action: 4
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.14956861922263504
Time Step: 19, Temperature: 0.14956861922263504, Chosen Action: 4
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.1353352832366127
Time Step: 20, Temperature: 0.1353352832366127, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.1224564282529819
Time Step: 21, Temperature: 0.1224564282529819, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.11080315836233387
Time Step: 22, Temperature: 0.11080315836233387, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.1002588437228037
Time Step: 23, Temperature: 0.1002588437228037, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.09071795328941247
Time Step: 24, Temperature: 0.09071795328941247, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.0820849986238988
Time Step: 25, Temperature: 0.0820849986238988, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.07427357821433388
Time Step: 26, Temperature: 0.07427357821433388, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.06720551273974976
Time Step: 27, Temperature: 0.06720551273974976, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.06081006262521795
Time Step: 28, Temperature: 0.06081006262521795, Chosen Action: 4
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.05502322005640721
Time Step: 29, Temperature: 0.05502322005640721, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.049787068367863944
Time Step: 30, Temperature: 0.049787068367863944, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.0450492023935578
Time Step: 31, Temperature: 0.0450492023935578, Chosen Action: 4
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.04076220397836621
Time Step: 32, Temperature: 0.04076220397836621, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.036883167401239994
Time Step: 33, Temperature: 0.036883167401239994, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.033373269960326066
Time Step: 34, Temperature: 0.033373269960326066, Chosen Action: 4
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.0301973834223185
Time Step: 35, Temperature: 0.0301973834223185, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.02732372244729256
Time Step: 36, Temperature: 0.02732372244729256, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.024723526470339388
Time Step: 37, Temperature: 0.024723526470339388, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.02237077185616559
Time Step: 38, Temperature: 0.02237077185616559, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.02024191144580438
Time Step: 39, Temperature: 0.02024191144580438, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.01831563888873418
Time Step: 40, Temperature: 0.01831563888873418, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.016572675401761237
Time Step: 41, Temperature: 0.016572675401761237, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.014995576820477703
Time Step: 42, Temperature: 0.014995576820477703, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.013568559012200934
Time Step: 43, Temperature: 0.013568559012200934, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.012277339903068436
Time Step: 44, Temperature: 0.012277339903068436, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.011108996538242306
Time Step: 45, Temperature: 0.011108996538242306, Chosen Action: 6
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.010051835744633576
Time Step: 46, Temperature: 0.010051835744633576, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.009095277101695816
Time Step: 47, Temperature: 0.009095277101695816, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.008229747049020023
Time Step: 48, Temperature: 0.008229747049020023, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.007446583070924338
Time Step: 49, Temperature: 0.007446583070924338, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.006737946999085467
Time Step: 50, Temperature: 0.006737946999085467, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.006096746565515633
Time Step: 51, Temperature: 0.006096746565515633, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.0055165644207607716
Time Step: 52, Temperature: 0.0055165644207607716, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.004991593906910213
Time Step: 53, Temperature: 0.004991593906910213, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.004516580942612666
Time Step: 54, Temperature: 0.004516580942612666, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.004086771438464067
Time Step: 55, Temperature: 0.004086771438464067, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.003697863716482929
Time Step: 56, Temperature: 0.003697863716482929, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.003345965457471272
Time Step: 57, Temperature: 0.003345965457471272, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.0030275547453758127
Time Step: 58, Temperature: 0.0030275547453758127, Chosen Action: 1
Q_values, temperature [1.   2.   0.5  1.5  1.9  1.8  1.99] 0.0027394448187683684
</pre>
</div>
</li>

<li><a id="org0425e44"></a>Epsilon-Greedy Exploration Strategy<br />
<div class="outline-text-5" id="text-11-8-3-3">
<p>
shifts from exploration to exploitation: epsilon value can be decayed over time.
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">epsilon_greedy</span>(Q_values, epsilon):
    <span style="color: #8ac6f2; font-weight: bold;">if</span> np.random.rand() &lt; epsilon:
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Exploration: Choose a random action</span>
        <span style="color: #8ac6f2; font-weight: bold;">return</span> np.random.choice(<span style="color: #e5786d;">len</span>(Q_values))
    <span style="color: #8ac6f2; font-weight: bold;">else</span>:
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Exploitation: Choose the action with the highest Q-value</span>
        <span style="color: #8ac6f2; font-weight: bold;">return</span> np.argmax(Q_values)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Example usage</span>
<span style="color: #cae682;">Q_values</span> = np.array([1.0, 2.0, 0.5, 1.5])  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Action values (e.g., Q-values)</span>
<span style="color: #cae682;">epsilon</span> = 0.3  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Epsilon value</span>

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Select an action using epsilon-greedy</span>
<span style="color: #cae682;">chosen_action</span> = epsilon_greedy(Q_values, epsilon)

<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"Action Values:"</span>, Q_values)
<span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"Selected Action:"</span>, chosen_action)
</pre>
</div>

<pre class="example">
Action Values: [1.  2.  0.5 1.5]
Selected Action: 1
</pre>
</div>
</li>
</ol>
</div>

<div id="outline-container-org7e55299" class="outline-4">
<h4 id="org7e55299"><span class="section-number-4">11.8.4.</span> RL algorithms</h4>
<div class="outline-text-4" id="text-11-8-4">
</div>
<ol class="org-ol">
<li><a id="org25ff327"></a>model-based<br />
<div class="outline-text-5" id="text-11-8-4-1">
<p>
the agent (like Bob) builds an internal model of the environment:
</p>
<ul class="org-ul">
<li>state transitions</li>
<li>reward probabilities</li>
</ul>

<p>
<b>Dyna-Q</b> algorithm
</p>
</div>
</li>
<li><a id="orgb9f185b"></a>model-free<br />
<div class="outline-text-5" id="text-11-8-4-2">
<p>
agent has to rely on trial and error to discover the optimal policy.
</p>

<p>
explicitly building an internal model: (doesn't need to create a complex mental map of the room)
</p>
<ul class="org-ul">
<li>value of states and actions or the optimal strategy through trial and error.</li>
</ul>

<p>
algoritms: Q-Learning, SARSA (State-Action-Reward-State-Action), Policy gradient methods, Deep Q-Networks (DQN)
</p>
</div>
<ol class="org-ol">
<li><a id="org57e116e"></a>Q-learning<br />
<div class="outline-text-6" id="text-11-8-4-2-1">
<p>
Q-table:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-right">State</td>
<td class="org-left">Avaliable actions</td>
</tr>

<tr>
<td class="org-right">&#xa0;</td>
<td class="org-left">up down left righ</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-left">. . . . . . .</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-left">. . . . . . .</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-left">. . . . . . .</td>
</tr>
</tbody>
</table>

<p>
what
</p>
<ul class="org-ul">
<li>Initialization: Initializing the Q-table and defining the environment.</li>
<li>Epsilon-Greedy Policy: Choosing actions based on an epsilon-greedy strategy.</li>
<li>Q-Table Update: Updating the Q-values using the Q-learning update rule.</li>
<li>Training Loop: Iterating through episodes and steps to train the agent.</li>
<li>Testing: Evaluating the trained agent's performance.</li>
</ul>

<p>
Q-learning update formula:
</p>
<pre class="example">
Q_next = np.max(Q_table[next_state])
Q_table[state, action] = Q_table[state, action] + alpha * (reward + gamma * Q_next - Q_table[state, action])
</pre>


<p>
Which is equal to:
(1 - alpha) * Q<sub>table</sub>[state, action] + alpha * (reward + gamma * Q<sub>next</sub>)
</p>

<p>
temporal difference (TD) error.
</p>
<ul class="org-ul">
<li>difference between the current estimate of a value function and a better estimate of that value function
based on new information.</li>
<li>calculated as the <b>difference</b> between the predicted value at the <b>current time step</b> and the predicted value
at the <b>next time step</b>, adjusted by the reward received and the discount factor.
<ul class="org-ul">
<li>δt​=rt+1​+γ*V(st+1​)−V(st​) - the TD error at time step t.
<ul class="org-ul">
<li>rt+1​ is the reward received at time step t+1.</li>
<li>γ is the discount factor, which determines how much future rewards are valued.</li>
<li>V(st​)/V(st+1)​ -  is the current/next estimate of the value function at state st/st+1.</li>
</ul></li>
</ul></li>
<li>Temporal Difference learning - a class of model-free reinforcement learning methods.</li>
</ul>

<p>
Q-learnins is:
</p>
<ul class="org-ul">
<li>Temporal Difference learning approach - because it predicting a quantity that depends on future values of a
given signal, than waiting for the final outcome of an episode.</li>
<li>Off-Policy Learning - optimal value function (Q-function) independently of the policy being followed.</li>
<li>uses bootstrapping - the prediction at one time step is updated based on the prediction at the next time
step.</li>
</ul>

<p>
Here we use 30x30 grid. epsilon<sub>greedy</sub><sub>policy</sub> showed poor performance
 compared to boltzmann<sub>exploration</sub>.
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #8ac6f2; font-weight: bold;">import</span> random

<span style="color: #8ac6f2; font-weight: bold;">class</span> <span style="color: #92a65e; font-weight: bold;">GridWorldEnv</span>:
    <span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">__init__</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>, width=5, height=5):
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">width</span> = width
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">height</span> = height
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">state</span> = <span style="color: #e5786d; font-weight: bold;">None</span>
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">action_space</span> = np.array([0, 1, 2, 3])  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">0: up, 1: down, 2: left, 3: right</span>
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">n_actions</span> = <span style="color: #e5786d;">len</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>.action_space)
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">n_observations</span> = width * height

    <span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">reset</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>):
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">self.state = np.array([0, 0])  # Start at the top-left corner</span>
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">state</span> = 0  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Start at the top-left corner</span>
        <span style="color: #8ac6f2; font-weight: bold;">return</span> <span style="color: #8ac6f2; font-weight: bold;">self</span>.state

    <span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">step</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>, action):
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">x, y = self.state</span>
        <span style="color: #cae682;">x</span>, <span style="color: #cae682;">y</span> = <span style="color: #e5786d;">divmod</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>.state, <span style="color: #8ac6f2; font-weight: bold;">self</span>.width)

        <span style="color: #cae682;">reward</span> = -1  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Default reward for each step</span>
        <span style="color: #cae682;">done</span> = <span style="color: #e5786d; font-weight: bold;">False</span>

        <span style="color: #8ac6f2; font-weight: bold;">if</span> action == 0:  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Up</span>
            <span style="color: #cae682;">y</span> = <span style="color: #e5786d;">max</span>(0, y - 1)
        <span style="color: #8ac6f2; font-weight: bold;">elif</span> action == 1:  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Down</span>
            <span style="color: #cae682;">y</span> = <span style="color: #e5786d;">min</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>.height - 1, y + 1)
        <span style="color: #8ac6f2; font-weight: bold;">elif</span> action == 2:  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Left</span>
            <span style="color: #cae682;">x</span> = <span style="color: #e5786d;">max</span>(0, x - 1)
        <span style="color: #8ac6f2; font-weight: bold;">elif</span> action == 3:  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Right</span>
            <span style="color: #cae682;">x</span> = <span style="color: #e5786d;">min</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>.width - 1, x + 1)

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">- Calculate proximity reward around target corner</span>
        <span style="color: #cae682;">target_x</span>, <span style="color: #cae682;">target_y</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.width - 1, <span style="color: #8ac6f2; font-weight: bold;">self</span>.height - 1
        <span style="color: #cae682;">current_distance</span> = <span style="color: #e5786d;">abs</span>(x - target_x) + <span style="color: #e5786d;">abs</span>(y - target_y)
        <span style="color: #cae682;">next_x</span>, <span style="color: #cae682;">next_y</span> = <span style="color: #e5786d;">divmod</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>.state, <span style="color: #8ac6f2; font-weight: bold;">self</span>.width)
        <span style="color: #cae682;">next_distance</span> = <span style="color: #e5786d;">abs</span>(next_x - target_x) + <span style="color: #e5786d;">abs</span>(next_y - target_y)
        <span style="color: #8ac6f2; font-weight: bold;">if</span> next_distance &gt;= current_distance:
            <span style="color: #cae682;">reward</span> += 0.5  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Positive reward for moving closer to the target</span>

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Check if the agent reached the goal (bottom-right corner)</span>
        <span style="color: #8ac6f2; font-weight: bold;">if</span> x == <span style="color: #8ac6f2; font-weight: bold;">self</span>.width - 1 <span style="color: #8ac6f2; font-weight: bold;">and</span> y == <span style="color: #8ac6f2; font-weight: bold;">self</span>.height - 1:
            <span style="color: #cae682;">reward</span> = 10  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Reward for reaching the goal</span>
            <span style="color: #cae682;">done</span> = <span style="color: #e5786d; font-weight: bold;">True</span>

        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">state</span> = x * <span style="color: #8ac6f2; font-weight: bold;">self</span>.width + y

        <span style="color: #8ac6f2; font-weight: bold;">return</span> <span style="color: #8ac6f2; font-weight: bold;">self</span>.state, reward, done, {}

    <span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">action_space_sample</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>):
        <span style="color: #8ac6f2; font-weight: bold;">return</span> np.random.choice(<span style="color: #8ac6f2; font-weight: bold;">self</span>.action_space)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">- Envorinment</span>
<span style="color: #cae682;">env</span> = GridWorldEnv(30,30)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print("env.n_observations, env.n_actions", env.n_observations, env.n_actions)</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">100 , 4</span>
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">- initialization with zeroes.</span>
<span style="color: #cae682;">Q_table</span> = np.zeros((env.n_observations, env.n_actions))
<span style="color: #cae682;">alpha</span> = 0.03  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Learning rate</span>
<span style="color: #cae682;">gamma</span> = 0.95  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Discount factor</span>
<span style="color: #cae682;">epsilon</span> = 0.99  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Initial exploration rate</span>
<span style="color: #cae682;">epsilon_min</span> = 0.001  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Minimum exploration rate</span>
<span style="color: #cae682;">epsilon_decay</span> = 0.995  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Exploration rate decay</span>

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">epsilon_greedy_policy</span>(Q_table, state, epsilon):
    <span style="color: #f08080; font-style: italic;">"Return action."</span>
    <span style="color: #8ac6f2; font-weight: bold;">if</span> random.uniform(0, 1) &lt; epsilon:
        <span style="color: #8ac6f2; font-weight: bold;">return</span> env.action_space_sample()
    <span style="color: #8ac6f2; font-weight: bold;">else</span>:
        <span style="color: #8ac6f2; font-weight: bold;">return</span> np.argmax(Q_table[state])

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">boltzmann_exploration</span>(Q_table, state, temperature):
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Calculate the exponential values of the Q-values divided by the temperature</span>
    <span style="color: #cae682;">exp_values</span> = np.exp(Q_table[state] / temperature)
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Calculate the probabilities using the softmax function</span>
    <span style="color: #cae682;">action_probabilities</span> = exp_values / np.<span style="color: #e5786d;">sum</span>(exp_values)
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Choose an action based on the calculated probabilities</span>
    <span style="color: #8ac6f2; font-weight: bold;">return</span> np.random.choice(<span style="color: #e5786d;">len</span>(Q_table[state]), p=action_probabilities)


<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">update_Q_table</span>(Q_table, state, action, reward, next_state, done):
    <span style="color: #cae682;">Q_next</span> = np.<span style="color: #e5786d;">max</span>(Q_table[next_state]) <span style="color: #8ac6f2; font-weight: bold;">if</span> <span style="color: #8ac6f2; font-weight: bold;">not</span> done <span style="color: #8ac6f2; font-weight: bold;">else</span> 0
    <span style="color: #cae682;">Q_table</span>[<span style="color: #cae682;">state</span>, <span style="color: #cae682;">action</span>] = Q_table[state, action] + alpha * (reward + gamma * Q_next - Q_table[state, action])

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">- Main Training Loop</span>
<span style="color: #cae682;">episodes</span> = 1000
<span style="color: #cae682;">max_steps</span> = 100
<span style="color: #cae682;">done</span> = <span style="color: #e5786d; font-weight: bold;">False</span>
<span style="color: #8ac6f2; font-weight: bold;">for</span> episode <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(episodes):
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Q_table = np.zeros((env.n_observations, env.n_actions))</span>
    <span style="color: #cae682;">state</span> = env.reset()
    <span style="color: #cae682;">rewards</span> = 0.0

    <span style="color: #8ac6f2; font-weight: bold;">for</span> step <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(max_steps):
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">action = epsilon_greedy_policy(Q_table, state, epsilon)</span>
        <span style="color: #cae682;">action</span> = boltzmann_exploration(Q_table, state, epsilon)

        <span style="color: #cae682;">next_state</span>, <span style="color: #cae682;">reward</span>, <span style="color: #cae682;">done</span>, <span style="color: #cae682;">_</span> = env.step(action)
        <span style="color: #cae682;">rewards</span> += reward
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print(divmod(env.state, env.width), action, reward, epsilon)</span>
        update_Q_table(Q_table, state, action, reward, next_state, done)

        <span style="color: #cae682;">state</span> = next_state

        <span style="color: #8ac6f2; font-weight: bold;">if</span> done:
            <span style="color: #8ac6f2; font-weight: bold;">break</span>

    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Decay epsilon</span>
    <span style="color: #cae682;">epsilon</span> = <span style="color: #e5786d;">max</span>(epsilon_min, epsilon * epsilon_decay)

    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Print rewards every 100 episodes</span>
    <span style="color: #8ac6f2; font-weight: bold;">if</span> done <span style="color: #8ac6f2; font-weight: bold;">or</span> episode % max_steps == 0:
        <span style="color: #e5786d;">print</span>(f<span style="color: #95e454;">"</span>{done}<span style="color: #95e454;"> Episode </span>{episode+1}<span style="color: #95e454;">, Reward: </span>{rewards}<span style="color: #95e454;">, epsilon: </span>{epsilon}<span style="color: #95e454;">, step:</span>{step}<span style="color: #95e454;">"</span>)
    <span style="color: #8ac6f2; font-weight: bold;">if</span> done:
        <span style="color: #8ac6f2; font-weight: bold;">break</span>

<span style="color: #8ac6f2; font-weight: bold;">if</span> done:
    <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">- Testing</span>
    <span style="color: #cae682;">epsilon</span> = 0.01  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Set epsilon low for testing</span>
    <span style="color: #cae682;">test_episodes</span> = 100

    <span style="color: #8ac6f2; font-weight: bold;">for</span> episode <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(test_episodes):
        <span style="color: #cae682;">state</span> = env.reset()
        <span style="color: #cae682;">done</span> = <span style="color: #e5786d; font-weight: bold;">False</span>
        <span style="color: #cae682;">rewards</span> = 0.0

        <span style="color: #8ac6f2; font-weight: bold;">for</span> step <span style="color: #8ac6f2; font-weight: bold;">in</span> <span style="color: #e5786d;">range</span>(max_steps):
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">action = epsilon_greedy_policy(Q_table, state, epsilon)</span>
            <span style="color: #cae682;">action</span> = boltzmann_exploration(Q_table, state, epsilon)
            <span style="color: #cae682;">next_state</span>, <span style="color: #cae682;">reward</span>, <span style="color: #cae682;">done</span>, <span style="color: #cae682;">_</span> = env.step(action)
            <span style="color: #cae682;">rewards</span> += reward

            <span style="color: #cae682;">state</span> = next_state

            <span style="color: #8ac6f2; font-weight: bold;">if</span> done:
                <span style="color: #e5786d;">print</span>(<span style="color: #95e454;">"testing success"</span>, step)
                <span style="color: #8ac6f2; font-weight: bold;">break</span>

        <span style="color: #e5786d;">print</span>(f<span style="color: #95e454;">"Test Episode </span>{episode+1}<span style="color: #95e454;">, Reward: </span>{rewards}<span style="color: #95e454;">"</span>)
        <span style="color: #8ac6f2; font-weight: bold;">if</span> done:
            <span style="color: #8ac6f2; font-weight: bold;">break</span>

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">print(Q_table)</span>
</pre>
</div>

<pre class="example" id="org7f645d2">
False Episode 1, Reward: -71.5, epsilon: 0.98505, step:99
False Episode 101, Reward: -65.5, epsilon: 0.5967141684651915, step:99
False Episode 201, Reward: -68.5, epsilon: 0.36147180229136094, step:99
False Episode 301, Reward: -68.0, epsilon: 0.21896893145312793, step:99
False Episode 401, Reward: -65.0, epsilon: 0.13264490518426955, step:99
False Episode 501, Reward: -64.0, epsilon: 0.0803523621117462, step:99
False Episode 601, Reward: -62.0, epsilon: 0.0486750854694935, step:99
True Episode 644, Reward: -44.5, epsilon: 0.03923730721086434, step:93
testing success 97
Test Episode 1, Reward: -48.0
</pre>
</div>
</li>
</ol>
</li>
</ol>
</div>



<div id="outline-container-org5d5f6d5" class="outline-4">
<h4 id="org5d5f6d5"><span class="section-number-4">11.8.5.</span> environment is typically stated in the form of a Markov decision process (MDP)</h4>
<div class="outline-text-4" id="text-11-8-5">
<ul class="org-ul">
<li>S - environment and agent state space</li>
<li>A - set of actions</li>
<li>P(s,s') - probability of transtion from s to s' under action a.</li>
<li>R(s,s') - reward after transition</li>
</ul>

<p>
observability
</p>
<ul class="org-ul">
<li>full - agent observes the current environmental state</li>
<li>partial - with noise or not full</li>
</ul>

<p>
Problems:
</p>
<ul class="org-ul">
<li>model of the environment is known (planning problem)</li>
<li>simulation model of the environment (planning problem)</li>
<li>only way to collect information about the environment is to interact with it</li>
</ul>

<p>
trade-offs
</p>
<ul class="org-ul">
<li>long-term versus short-term reward trade-off</li>
<li>The exploration vs. exploitation trade-off</li>
</ul>
</div>
</div>
<div id="outline-container-org85c88f0" class="outline-4">
<h4 id="org85c88f0"><span class="section-number-4">11.8.6.</span> Dynamic programming</h4>
<div class="outline-text-4" id="text-11-8-6">
<p>
DP is both a <b>mathematical optimization</b> method and a computer programming method.
</p>

<p>
If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are
 applicable.
</p>

<p>
There is a relation between the value of the larger problem and the values of the sub-problems. In the
 optimization literature this relationship is called the <b>Bellman equation</b>.
</p>
</div>
</div>

<div id="outline-container-orgc43caad" class="outline-4">
<h4 id="orgc43caad"><span class="section-number-4">11.8.7.</span> Markov decision process (MDP)</h4>
<div class="outline-text-4" id="text-11-8-7">
<p>
Markov decision process (MDP) - is a discrete-time <b>stochastic</b> <a href="#orgfe170be">3</a> process.  It provides a mathematical
 framework for modeling decision making in situations where outcomes are partly random and partly under the
 control of a decision maker.
</p>

<p>
MDPs are useful for studying <b>optimization problems</b> solved via <b>dynamic programming</b>.
</p>

<p>
type (S, A, P, R, γ) - Markov decision process
</p>
<ul class="org-ul">
<li>S - state space, anything which can be useful in choosing actions. the statespace of the process is constant
through time.</li>
<li>A - action space (alternatively, A is set of actions available from state s)</li>
<li>P(s, s') - is a probability that action a in state s at time t will lead to state s' at time t+1.</li>
<li>R(s, s') - immediate reward (or expected immediate reward) received after transitioning from state s to
state s', due to action a.</li>
<li>γ - discount factor that is used to reduce the importance of the of future rewards. (optional)</li>
</ul>

<p>
reward calculation is considered to be the part of the environment
</p>


<p>
<b>policy function</b> π is is a (potentially probabilistic) mapping from state space S to
 action space A.
</p>

<p>
The goal in a Markov decision process is to find a good "policy" for the decision maker: a function π that
 specifies the action π(s) that the decision maker will choose when in state s.
</p>

<p>
<b>Markov property</b> refers to the memoryless property of a stochastic process. conditional probability
 distribution of future states of the process (conditional on both past and present values) depends only upon
 the present state.
</p>

<p>
classes of Markov process are the <b>Markov chain</b> and the <b>Brownian motion</b>.
</p>

<p>
<b>Discount Factor</b> (ɤ) - helps us to avoid infinity as a reward in continuous tasks.
</p>
<ul class="org-ul">
<li>0 - more importance is given to the immediate reward.</li>
<li>1 - more importance is given to future rewards</li>
<li>return G(t) = R(t+1) + ɤ*R(t+2) + ɤ<sup>2</sup>*R(t+3) + &#x2026;</li>
</ul>

<p>
<b>Value Function</b> determines how good it is for the agent to be in a particular state.
</p>
<ul class="org-ul">
<li>Bellman Equation for Value Function: v(s) = E[(R(t+1) + ɤ*v(S(t+1))) | St=s]
<ul class="org-ul">
<li>Immediate Reward of successor states + Discounted value of successor states.</li>
</ul></li>
</ul>

<p>
<b>policy</b> defines what actions to perform in a particular state s. defines a probability distribution over
 Actions (a∈ A) for each state (s ∈ S) at. π(a|s) is the probability that the agent with taking action (a ) at
 a particular time step (t). π(a|s) = P[At=a|St=s]
</p>

<p>
methods to solve:
</p>
<ul class="org-ul">
<li>Dynamic Programming (Value iteration and Policy iteration)</li>
<li>Monte-Claro methods</li>
<li>TD-Learning.</li>
</ul>

<p>
<b>State-action value function</b> or <b>Q-Function</b> - how good it (value) is for the agent to take action (a) in a state (s)
 with a policy π.
</p>
</div>
</div>

<div id="outline-container-org21ae7c1" class="outline-4">
<h4 id="org21ae7c1"><span class="section-number-4">11.8.8.</span> Markov chain</h4>
<div class="outline-text-4" id="text-11-8-8">
<p>
type of Markov process that has either a discrete state space or a discrete index set (often representing
 time), but the precise definition of a Markov chain varies.
</p>
</div>
</div>
<div id="outline-container-org45e5152" class="outline-4">
<h4 id="org45e5152"><span class="section-number-4">11.8.9.</span> Decision Transfromer</h4>
<div class="outline-text-4" id="text-11-8-9">
<p>
<a href="https://arxiv.org/pdf/2106.01345">https://arxiv.org/pdf/2106.01345</a>
</p>

<p>
Architecture that casts the problem of RL as conditional sequence modeling - simply outputs the optimal
 actions by leveraging a causally masked Transformer.
</p>
<ul class="org-ul">
<li>avoiding one of the “deadly triad”</li>
<li>avoids the need for discounting future rewards</li>
<li>can make use of existing transformer frameworks</li>
</ul>

<p>
Old conventional RL algorithms:
</p>
<ul class="org-ul">
<li>ﬁt value functions</li>
<li>compute policy gradients - training a policy through conventional RL algorithms like temporal difference
(TD) learning</li>
</ul>

<p>
<b>ofﬂine RL</b>, where we will task agents with learning policies
from suboptimal dat
</p>
</div>
</div>
<div id="outline-container-org1c611ca" class="outline-4">
<h4 id="org1c611ca"><span class="section-number-4">11.8.10.</span> Auto RL</h4>
<div class="outline-text-4" id="text-11-8-10">
<p>
<a href="https://arxiv.org/pdf/2405.15568">https://arxiv.org/pdf/2405.15568</a>
</p>

<ol class="org-ol">
<li>Task Generator(LLM) -&gt; task description</li>
<li>Environment Generator (LLM) -&gt; Env. code: simulated world + reward function.</li>
<li>Model of Interestingness (LLM) - ?</li>
<li>Train agent with RL</li>
<li>Success Detector (VLM/LLM)</li>
</ol>

<p>
Also “archive of tasks” used.
</p>
</div>
</div>
<div id="outline-container-org333e02a" class="outline-4">
<h4 id="org333e02a"><span class="section-number-4">11.8.11.</span> tools</h4>
<div class="outline-text-4" id="text-11-8-11">
<ul class="org-ul">
<li>algorithms: AlphaZero, AlphaGo</li>
<li>мультиагентное обучение</li>
<li>Tianshou - platform on PyTorch and Gymnasium. <a href="https://github.com/thu-ml/tianshou">https://github.com/thu-ml/tianshou</a></li>
</ul>


<p>
ROS2, wiki.ros.org - meta-operating system for your robot. runtime "graph" is a peer-to-peer network of processes
 (potentially distributed across machines)
</p>
</div>
</div>

<div id="outline-container-orgfe05efc" class="outline-4">
<h4 id="orgfe05efc"><span class="section-number-4">11.8.12.</span> links</h4>
<div class="outline-text-4" id="text-11-8-12">
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning">https://en.wikipedia.org/wiki/Reinforcement_learning</a></li>
<li><a href="https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da">https://towardsdatascience.com/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da</a></li>
<li>Recurrent Policy Gradients 2009 Юрген Шмидхубер <a href="https://mediatum.ub.tum.de/doc/1287506/file.pdf">https://mediatum.ub.tum.de/doc/1287506/file.pdf</a></li>
<li>course <a href="https://huggingface.co/learn/deep-rl-course/en/unit0/introduction">https://huggingface.co/learn/deep-rl-course/en/unit0/introduction</a></li>
<li>basic <a href="https://www.datacamp.com/tutorial/reinforcement-learning-python-introduction">https://www.datacamp.com/tutorial/reinforcement-learning-python-introduction</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf1bf663" class="outline-3">
<h3 id="orgf1bf663"><span class="section-number-3">11.9.</span> Distributed training</h3>
<div class="outline-text-3" id="text-11-9">
</div>
<div id="outline-container-orge93e4c2" class="outline-4">
<h4 id="orge93e4c2"><span class="section-number-4">11.9.1.</span> temrs</h4>
<div class="outline-text-4" id="text-11-9-1">
<ul class="org-ul">
<li>offloading - offload the sharded model parameters to CPUs.</li>
<li>Half-Precision - float16</li>
</ul>
</div>
</div>
<div id="outline-container-org6e6c7e8" class="outline-4">
<h4 id="org6e6c7e8"><span class="section-number-4">11.9.2.</span> all</h4>
<div class="outline-text-4" id="text-11-9-2">
<p>
<b>GPU cluster concept</b> - each node is equipped with a Graphics Processing Unit (<b>TPU clusters</b> that are more
 powerful than GPU clusters.)
</p>

<p>
types of distributed training:
</p>
<ul class="org-ul">
<li>Data parallelism (many copies of model) - not for large models</li>
<li><p>
Model parallelism (split model, all worker nodes use the same dataset)
</p>
<ul class="org-ul">
<li>neural network model should have a parallel architecture</li>
<li>hard to implement</li>
<li>параллелизм моделей чаще всего используется в сфере обработки естественных языков, в моделях, где</li>
</ul>
<p>
используются трансформеры, в таких проектах, как GPT-2, BERT, и в других подобных.
</p></li>
<li><b>GPU parallelism</b> - several GPUs in one computer</li>
</ul>

<p>
Synchronization methods:
</p>
<ul class="org-ul">
<li><p>
parameter server technique -  dividing all GPU nodes into two groups
</p>
<ul class="org-ul">
<li>if the global model parameters are synchronously shared across workers, you will wait until each worker</li>
</ul>
<p>
completes its iteration and returns the results which might be time-consuming
</p>
<ul class="org-ul">
<li>if you have only one parameter server, you will not benefit from adding more workers as your server will</li>
</ul>
<p>
have to work with more data from the workers which creates a bottleneck.
</p></li>
<li>an all-reduce technique - allows to add more workers without any limitations (used more often than a parameter server-based architecture)
<ul class="org-ul">
<li>tensorflow By default, uses the NVIDIA Collective Communication Library (NCCL) as the all-reduce implementation.</li>
</ul></li>
</ul>

<p>
tools:
</p>
<ul class="org-ul">
<li>NCCL and MPI (Message Passing Interface) - параллелизм модели - на каждом кусок сети.
<ul class="org-ul">
<li>Horovod - distributed training framework for TensorFlow, Keras, PyTorch, and MXNet</li>
<li>Gloo - Pytorch</li>
<li>NVCaffe - Caffe</li>
</ul></li>
<li>Parameter server (PS) - параллелизм данных - на каждом полная модель</li>
<li>Model Parallelism  for tensorflow <a href="https://github.com/tensorflow/mesh">https://github.com/tensorflow/mesh</a></li>
</ul>


<p>
Scalability:
</p>
<ul class="org-ul">
<li>t1 - time to complete</li>
<li>N processing elements</li>
<li>tN - amount of time to complete with N processors</li>
<li>Strong Scaling = t1 / ( N * tN ) * 100%</li>
<li>Weak scallig - constant and additional elements are used to solve a larger total problem (one that wouldn't
fit in RAM on a single node = ( t1 / tN ) * 100%</li>
</ul>

<p>
Automatic loss scaling - improve stability when training large models in mixed precision. Lower precision
 numerical formats introducing numerical instabilities during training, reducing the statistical performance
 of some models, potentially hampering statistical convergence. (<a href="https://arxiv.org/pdf/2112.11446.pdf">https://arxiv.org/pdf/2112.11446.pdf</a>). ALS
 aims to shift the gradient distribution across the dynamic range, so that underflow and overflow are
 prevented (as much as possible) in float-16.
</p>
<ul class="org-ul">
<li>loss scaling is not needed for some networks (e.g. image classification, Faster R-CNN), but necessary for
others (e.g. Multibox SSD, big LSTM language model).</li>
</ul>

<p>
Automatic Mixed Precision (AMP) - is the same as with fp16, except it'll use bf16. Nvidia.
</p>
<ul class="org-ul">
<li>Converting the model to use the float16 data type where possible.</li>
<li>Keeping float32 master weights to accumulate per-iteration weight updates.</li>
<li>Using loss scaling to preserve small gradient values.</li>
<li><a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html">https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html</a></li>
<li><a href="https://nvidia.github.io/apex/amp.html">https://nvidia.github.io/apex/amp.html</a></li>
</ul>

<p>
Distributed Data Parallel (DDP) - short: per-GPU copy of a model’s parameters, gradients and optimizer
 states. long: Applications using DDP should spawn multiple processes and create a single DDP instance per
 process. DDP registers an autograd hook for each parameter given by model.parameters() and the hook will fire
 when the corresponding gradient is computed in the backward pass. Then DDP uses that signal to trigger
 gradient synchronization across processes. (GPU devices cannot be shared across processes)
</p>

<p>
Fully Sharded Data Parallel (FSDP) - shards model’s parameters, gradients and optimizer states across
 data-parallel workers and can optionally offload the sharded model parameters to CPUs.
</p>
</div>
</div>
<div id="outline-container-org31881ce" class="outline-4">
<h4 id="org31881ce"><span class="section-number-4">11.9.3.</span> tips</h4>
<div class="outline-text-4" id="text-11-9-3">
<ul class="org-ul">
<li>When solving a deep learning problem GPU is more powerful than CPU</li>
<li>A CPU is good in the tasks where latency or per-core performance is important</li>
<li>CUDA is a tool that is used to communicate with a GPU</li>
<li>cuDNN is the library that is optimized for working on GPUs and has highly tuned implementations for standard
deep learning routines.  cuDNN provides highly tuned implementations for standard routines such as forward
and backward convolution, pooling, normalization, and activation layers.</li>
</ul>
</div>
</div>
<div id="outline-container-orgc248615" class="outline-4">
<h4 id="orgc248615"><span class="section-number-4">11.9.4.</span> paradigms</h4>
<div class="outline-text-4" id="text-11-9-4">
<ul class="org-ul">
<li><a href="https://colossalai.org/docs/concepts/paradigms_of_parallelism">https://colossalai.org/docs/concepts/paradigms_of_parallelism</a></li>
<li><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">https://lilianweng.github.io/posts/2021-09-25-train-large/</a></li>
</ul>
</div>
</div>

<div id="outline-container-org574300e" class="outline-4">
<h4 id="org574300e"><span class="section-number-4">11.9.5.</span> serverless computing and machine learning (SPIRT architecture)</h4>
<div class="outline-text-4" id="text-11-9-5">
<ul class="org-ul">
<li><a href="https://aihub.org/2024/03/26/interview-with-amine-barrak-serverless-computing-and-machine-learning/">https://aihub.org/2024/03/26/interview-with-amine-barrak-serverless-computing-and-machine-learning/</a></li>
<li><a href="https://ieeexplore.ieee.org/document/10366723/">https://ieeexplore.ieee.org/document/10366723/</a></li>
<li><img src="https://aihub.org/wp-content/uploads/2024/03/429753999_1605819696833977_5681116737691375829_n-1536x2048.jpg" alt="429753999_1605819696833977_5681116737691375829_n-1536x2048.jpg" /></li>
</ul>
<p>
inspired from the peer-to-peer paradigm. - fault tolerance
</p>

<p>
server to orchestrate the processing, and therefore they are exposed to a single point of failure.
</p>

<p>
model operations (gradient averaging , model updates) are significantly faster inside the database.
</p>

<p>
inherent statelessness necessitates strong communication methods, suggesting a pivotal role for database
</p>
<ul class="org-ul">
<li>Присущее statelessness требует надежных методов коммуникации, что предполагает ключевую роль баз данных.</li>
</ul>

<p>
We enhanced "in memory datatabase" to support ML training tasks.
</p>
</div>
</div>
<div id="outline-container-org4be89c8" class="outline-4">
<h4 id="org4be89c8"><span class="section-number-4">11.9.6.</span> links</h4>
<div class="outline-text-4" id="text-11-9-6">
<ul class="org-ul">
<li>comparision of distributed ml systems <a href="https://arxiv.org/pdf/1909.02061.pdf">https://arxiv.org/pdf/1909.02061.pdf</a></li>
<li>article GOOD <a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">https://lilianweng.github.io/posts/2021-09-25-train-large/</a></li>
<li><a href="https://huggingface.co/docs/transformers/v4.17.0/en/parallelism">https://huggingface.co/docs/transformers/v4.17.0/en/parallelism</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgdb2fdb3" class="outline-3">
<h3 id="orgdb2fdb3"><span class="section-number-3">11.10.</span> Federated learning (or collaborative learning)</h3>
<div class="outline-text-3" id="text-11-10">
<ul class="org-ul">
<li>distributed learning originally aims at parallelizing computing power,  training a single model on multiple servers</li>
<li>federated learning - aims at training on heterogeneous datasets</li>
</ul>
</div>
</div>
<div id="outline-container-org020165d" class="outline-3">
<h3 id="org020165d"><span class="section-number-3">11.11.</span> Statistical classification</h3>
<div class="outline-text-3" id="text-11-11">
<ul class="org-ul">
<li>categories - classes</li>
<li>observations - instances in machine learning</li>
<li>properties of observations - features  (grouped into a feature vector)</li>
<li>training set - observations (or instances) whose category membership is known</li>
<li><b>Classification</b> is an example of <b>pattern recognition</b>.</li>
<li>supervised learning</li>
<li>unsupervised procedure is known as clustering</li>
<li>Унитарный код (one-hot) - двоичный код фиксированной длины, 1 - прямой - 000010, Инверсный - 111101</li>
</ul>
</div>
<div id="outline-container-org880df58" class="outline-4">
<h4 id="org880df58"><span class="section-number-4">11.11.1.</span> in satistics</h4>
<div class="outline-text-4" id="text-11-11-1">
<ul class="org-ul">
<li>used logistic regression</li>
<li>properties of observations =  explanatory variables (or independent variables, regressors, etc.)</li>
<li>categories to be predicted are known as <b>outcomes</b> -  dependent variable</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgbc75a9c" class="outline-3">
<h3 id="orgbc75a9c"><span class="section-number-3">11.12.</span> Тематическое моделирование</h3>
<div class="outline-text-3" id="text-11-12">
<p>
к каким темам относится каждый документ коллекции
</p>
</div>
</div>

<div id="outline-container-org782d437" class="outline-3">
<h3 id="org782d437"><span class="section-number-3">11.13.</span> Популярные методы</h3>
<div class="outline-text-3" id="text-11-13">
<ul class="org-ul">
<li><a href="https://tproger.ru/translations/top-machine-learning-algorithms/">https://tproger.ru/translations/top-machine-learning-algorithms/</a></li>
<li>Cluster analysis - упорядочивающая объекты в сравнительно однородные группы</li>
<li>Collaborative filtering - это один из методов построения прогнозов (рекомендаций) в рекомендательных
системах[⇨], использующий известные предпочтения (оценки) группы это один из методов построения прогнозов
(рекомендаций) в рекомендательных системах[⇨], использующий известные предпочтения (оценки) группы</li>
</ul>
</div>
</div>

<div id="outline-container-org9851954" class="outline-3">
<h3 id="org9851954"><span class="section-number-3">11.14.</span> прогнозирование</h3>
<div class="outline-text-3" id="text-11-14">
<ol class="org-ol">
<li>временной ряд с аппроксимацией</li>
<li></li>
</ol>
</div>
</div>
<div id="outline-container-orga905719" class="outline-3">
<h3 id="orga905719"><span class="section-number-3">11.15.</span> Сейчас</h3>
<div class="outline-text-3" id="text-11-15">
<p>
крупные компании в какой-то мере монополизировали машинное обучение
</p>
<ul class="org-ul">
<li>вычислительные ресурсы и доступ к массивам данных</li>
</ul>
</div>
<div id="outline-container-org48496ee" class="outline-4">
<h4 id="org48496ee"><span class="section-number-4">11.15.1.</span> примеры</h4>
<div class="outline-text-4" id="text-11-15-1">
<p>
прогнозирования температуры поверхности дороги
</p>
<ul class="org-ul">
<li>погодные станции на автомагистралях</li>
<li>прогноз от Росгидромета</li>
</ul>

<p>
спрос на смартфоны
</p>
<ul class="org-ul">
<li>прогноз спрос производителий смартфонов на детали</li>
<li>прогноз спроса на детали всеми компаниями</li>
<li>зависимости между различными номенклатурами деталей</li>
</ul>

<p>
лидары (лазерные радары) -  в пространстве ориентируются самоуправляемые автомобили ???
</p>


<p>
Яндекс-такси <b>look-alike-модели</b> - предлагаем ее тем, кому это будет интересно
</p>
</div>
</div>
<div id="outline-container-orgba884dd" class="outline-4">
<h4 id="orgba884dd"><span class="section-number-4">11.15.2.</span> библиотеки</h4>
<div class="outline-text-4" id="text-11-15-2">
<p>
ML
</p>
<ul class="org-ul">
<li>Non distributed
<ul class="org-ul">
<li>Batch
<ul class="org-ul">
<li><b>R</b> language - visualisation features, which is essential to explore the data, package for machine
learning</li>
<li><b>Python</b> - scikit-learn</li>
<li><b>Weka</b> -  Java - GPL</li>
</ul></li>
<li>Stream
<ul class="org-ul">
<li><b>MOA</b> (Massive On-line Analysis) Java -GPL- is a framework for data stream mining</li>
</ul></li>
</ul></li>
<li>Distributed
<ul class="org-ul">
<li>Batch
<ul class="org-ul">
<li>Apache <b>Hadoop</b> (<i>həˈduːp</i>) -Java- GPL =&gt; <b>Mahout</b> -Java, Scala- -GPL- collaborative filtering,
classification, cluster analysis</li>
</ul></li>
<li>Stream
<ul class="org-ul">
<li>(Apache <b>S4</b>, <b>Storm</b>) =&gt; <b>SAMOA</b></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org7cfccd8" class="outline-3">
<h3 id="org7cfccd8"><span class="section-number-3">11.16.</span> kafka</h3>
<div class="outline-text-3" id="text-11-16">
<ul class="org-ul">
<li><a href="https://dzone.com/articles/5-machine-learning-trends-2018-combined-with-apach">https://dzone.com/articles/5-machine-learning-trends-2018-combined-with-apach</a></li>
</ul>

<p>
machine learning lifecycle:
</p>
<ol class="org-ol">
<li>Model training - analytic model - we feed historical data - continuous</li>
<li>Generating <b>predictions</b> - use an analytic model for making prediction - within an application or microservice</li>
</ol>

<p>
May be used with Kafka:
</p>
<ul class="org-ul">
<li><b>TensorFlow</b>  Java API, KSQL - streaming SQL</li>
</ul>
</div>
</div>

<div id="outline-container-orgab02882" class="outline-3">
<h3 id="orgab02882"><span class="section-number-3">11.17.</span> в кредитных орг-ях</h3>
<div class="outline-text-3" id="text-11-17">
<p>
Сбербанк, ВТБ - предсказательной силы показатель Джини (Gini)
</p>

<p>
Традиционные
</p>
<ul class="org-ul">
<li>оценка кредитных рисков</li>
<li>безопасность и противодействие мошенничеству</li>
<li>вторичные и кросс-продажи</li>
</ul>
<p>
Новые
</p>
<ul class="org-ul">
<li>бот-клиент</li>
<li>предиктивной  аналитики</li>
<li>оптимизации бизнес-процессов</li>
<li>сокращения издержек и повышения уровня STP</li>
<li>когнитивных вычислений, благодаря которому, в краткосрочной перспективе, банк сможет вывести на рынок
совершенно новые продукты и услуги, улучшить клиентский опыт и развить новые направления бизнеса</li>
</ul>
</div>
</div>

<div id="outline-container-org3907ccc" class="outline-3">
<h3 id="org3907ccc"><span class="section-number-3">11.18.</span> <span class="todo TODO">TODO</span> Сбербанк проекты</h3>
<div class="outline-text-3" id="text-11-18">
<ul class="org-ul">
<li><a href="http://futurebanking.ru/post/3224?fb_comment_id=1282932311748772_1870406629668001">http://futurebanking.ru/post/3224?fb_comment_id=1282932311748772_1870406629668001</a></li>
<li><a href="http://sberbank.ai/">http://sberbank.ai/</a></li>
</ul>

<p>
Сбербанк использует стандарт CRISP или Cross-Industry Standard Process for Data Mining/ Data Science,
определяющий, что каждая разработка roll out-модели должна идти согласно заданному жизненному циклу
</p>
</div>
</div>



<div id="outline-container-orgbfd6a2d" class="outline-3">
<h3 id="orgbfd6a2d"><span class="section-number-3">11.19.</span> KDTree simular</h3>
<div class="outline-text-3" id="text-11-19">
<ul class="org-ul">
<li><b>TODO:</b> <a href="https://habr.com/en/company/ods/blog/325422/">https://habr.com/en/company/ods/blog/325422/</a></li>
<li><b>TODO</b> <a href="https://scikit-learn.org/0.20/modules/generated/sklearn.preprocessing.StandardScaler.html">https://scikit-learn.org/0.20/modules/generated/sklearn.preprocessing.StandardScaler.html</a></li>
</ul>
</div>
</div>

<div id="outline-container-org9305ca3" class="outline-3">
<h3 id="org9305ca3"><span class="section-number-3">11.20.</span> Применение в банке</h3>
<div class="outline-text-3" id="text-11-20">
<ul class="org-ul">
<li><a href="https://habr.com/en/article/318152/">https://habr.com/en/article/318152/</a></li>
<li><a href="https://www.businessinsider.com/artificial-intelligence">https://www.businessinsider.com/artificial-intelligence</a></li>
</ul>


<p>
Payment:
</p>
<ul class="org-ul">
<li>Reducing payment fraud</li>
<li>Reducing false positives</li>
<li>Anti-money layndery</li>
<li>Conversational payments</li>
</ul>
<p>
Backend:
</p>
<ul class="org-ul">
<li>Automating existing processes</li>
<li>Aiding CSRs(back end) - Корпоративная социальная ответственность - добровольно принимают дополнительные меры
для повышения качества жизни работников и их семей, а также местного сообщества и общества в целом</li>
<li>Pre-empting problems</li>
</ul>
<p>
Front-end:
</p>
<ul class="org-ul">
<li>Securing dogital identity banking
<ul class="org-ul">
<li>Video, fingerprints, palm recognition, voice, радужной оболочке глаза, face</li>
</ul></li>
<li>Auto-saving and recommendations</li>
<li>Aiding CSRs</li>
<li>Improving iteractions across channels</li>
</ul>



<ul class="org-ul">
<li>Рекомендательная система - типа кал цента
<ul class="org-ul">
<li>интент - вопросы пользователей -
<ul class="org-ul">
<li>поиск</li>
<li>технический вопрос</li>
<li>отзыв</li>
<li>котика запостил</li>
</ul></li>
<li>Именованные сущности в банке - продукт, свойство, величина</li>
</ul></li>
<li>оптимизации обработки транзакций</li>
<li>кибербезопасности с выявлением мошенничества</li>
<li>персональных финансовых ассистентов и сверх-таргетированного маркетинга</li>
<li>детектируем паттерны поведения клиентов по транзакциям
<ul class="org-ul">
<li>1 предложить ему продукты или услуги, полезные для автовладельцев</li>
<li>2 предсказывать те или иные события, в том числе сам факт покупки</li>
</ul></li>
<li>мы видим, кто из клиентов банка копит деньги, помогаем формировать для них новые предложения</li>
<li>NLP - 1 создание библиотеки правил для извлечения сущностей
<ul class="org-ul">
<li>2 семантический анализатор текста</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org87dbfeb" class="outline-3">
<h3 id="org87dbfeb"><span class="section-number-3">11.21.</span> вспомогательные математические методы</h3>
<div class="outline-text-3" id="text-11-21">
<ul class="org-ul">
<li>Softmax - многомерный сигмойд, преобразовывает вектор в вектор q(z)i = e<sup>zi</sup>/∑e<sup>zk</sup>. Координата q трактуется
как вероятность того, что объект принадлежит к классу i. Область значений (0,1)
<ul class="org-ul">
<li>np.exp([1,2,3,4])/np.sum(np.exp([1,2,3,4])) -&gt; array([0.0320586 , 0.08714432, 0.23688282, 0.64391426])</li>
<li>np.sum(np.exp([1,2,3,4])/np.sum(np.exp([1,2,3,4]))) -&gt; 1.0</li>
</ul></li>
<li>Сигмоида - q(x)=1/(1+e<sup>-x</sup>)
<ul class="org-ul">
<li>1 / (1 + np.exp(-np.array([1,2,3,10]))) -&gt; array([0.73105858, 0.88079708, 0.95257413, 0.9999546 ])</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-org99a6ed0" class="outline-3">
<h3 id="org99a6ed0"><span class="section-number-3">11.22.</span> AutoML</h3>
<div class="outline-text-3" id="text-11-22">
<ul class="org-ul">
<li>Sberbank <a href="https://www.youtube.com/watch?v=jukZAtyJJ64">https://www.youtube.com/watch?v=jukZAtyJJ64</a></li>
</ul>
<p>
это процесс автоматизации сквозного процесса применения обучения машины к задачам реального мира
</p>

<p>
<b>by hands:</b>
</p>
<ul class="org-ul">
<li>сбор данных</li>
<li>пре-процессинг</li>
<li>конструктор факторов (features)</li>
<li>разработка ML алгоритма</li>
<li>выбор модели</li>
<li>валидация</li>
<li>продуктив</li>
</ul>

<p>
<b>AutoML</b> - генерация спецификаций моделей по выборке данных и выбор из них одной - главное автовалидация
 моделей - количественная оценить модельного риска (насколько выгодно инвестировать в её доработку)
</p>
<ul class="org-ul">
<li>Logistic Regression - дать или не дать - бинарная классификация</li>
<li>XGBoost - gradient boosting library - runs on major distributed environment (Hadoop, SGE, MPI)</li>
<li>SVM - Метод опорных векторов</li>
<li></li>
</ul>
</div>
<div id="outline-container-org455f76a" class="outline-4">
<h4 id="org455f76a"><span class="section-number-4">11.22.1.</span> <span class="todo TODO">TODO</span> <a href="https://github.com/sb-ai-lab/LightAutoML">https://github.com/sb-ai-lab/LightAutoML</a></h4>
</div>
<div id="outline-container-orgd2a78d3" class="outline-4">
<h4 id="orgd2a78d3"><span class="section-number-4">11.22.2.</span> Neuton AutoML <a href="https://neuton.ai/">https://neuton.ai/</a></h4>
<div class="outline-text-4" id="text-11-22-2">
<ul class="org-ul">
<li>Автоматический Feature engeering - различные сочетания столбцов</li>
<li>Feature importance for neural networks</li>
</ul>

<p>
классы задач
</p>
<ul class="org-ul">
<li>feature importance</li>
<li>ранжирование</li>
<li>стоп листы - выделение строк с низкой вероятностью</li>
<li>конверсия - выделение строк с высокой вероятностью</li>
<li>прогнозирование</li>
<li>сегментация</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgb5c04fb" class="outline-3">
<h3 id="orgb5c04fb"><span class="section-number-3">11.23.</span> Известные Датасеты</h3>
<div class="outline-text-3" id="text-11-23">
<p>
Binary classification
</p>
<ul class="org-ul">
<li><a href="https://www.kaggle.com/c/titanic">https://www.kaggle.com/c/titanic</a> - train.csv - Survived для каждого пассажира, обозначающий, выжил данный
пассажир или нет (0 для умерших, 1 для выживших).</li>
</ul>

<p>
MNIST - объёмная база данных образцов рукописного написания цифр
</p>

<p>
SVHN dataset - It can be seen as similar in flavor to MNIST -  images are of small cropped digist (over 600,000 digit images)
</p>

<p>
ImageNet - де факто стандарт сравнения CNN
</p>
<ul class="org-ul">
<li>rank 1 процент - accuracy - we compare if the class with the highest probability according to our network
matches the real tag</li>
<li>rank 2 процент - we compare if one of the 5 classes with higher probation according to our network matches
the real label</li>
</ul>
</div>
<div id="outline-container-org12c2e77" class="outline-4">
<h4 id="org12c2e77"><span class="section-number-4">11.23.1.</span> signatures</h4>
<div class="outline-text-4" id="text-11-23-1">
<p>
<b>On-line Handwritten Signature Database</b>
login and password required <a href="http://biometrics.sabanciuniv.edu/susig.html">http://biometrics.sabanciuniv.edu/susig.html</a>
</p>

<p>
<b>ICDAR</b> <a href="http://www.iapr-tc11.org/mediawiki/index.php?title=Datasets_List">http://www.iapr-tc11.org/mediawiki/index.php?title=Datasets_List</a>
</p>
<ul class="org-ul">
<li>2009 Signature Verification Competition (SigComp2009)*
<ul class="org-ul">
<li>usage <a href="https://github.com/gnbaron/signature-recognition">https://github.com/gnbaron/signature-recognition</a></li>
<li><a href="http://www.iapr-tc11.org/mediawiki/index.php?title=ICDAR_2009_Signature_Verification_Competition_(SigComp2009)">http://www.iapr-tc11.org/mediawiki/index.php?title=ICDAR_2009_Signature_Verification_Competition_(SigComp2009)</a></li>
</ul></li>
<li>ICFHR 2012 Signature Verification Competition (4NSigComp2012)
<ul class="org-ul">
<li><a href="http://www.iapr-tc11.org/mediawiki/index.php/ICFHR_2012_Signature_Verification_Competition_(4NSigComp2012)">http://www.iapr-tc11.org/mediawiki/index.php/ICFHR_2012_Signature_Verification_Competition_(4NSigComp2012)</a></li>
</ul></li>
</ul>


<p>
CEDAR handwriting <a href="https://cedar.buffalo.edu/Databases/index.html">https://cedar.buffalo.edu/Databases/index.html</a>
</p>

<p>
CEDAR signatures <a href="https://cedar.buffalo.edu/NIJ/data/signatures.rar">https://cedar.buffalo.edu/NIJ/data/signatures.rar</a>
</p>
<ul class="org-ul">
<li>It consists of 24 genuine and forged signatures each from 55 different signers.</li>
<li>usage <a href="https://github.com/rmalav15/signature-recognition">https://github.com/rmalav15/signature-recognition</a></li>
<li>usage <a href="https://github.com/uniyalvipin/signature-recognition">https://github.com/uniyalvipin/signature-recognition</a></li>
</ul>



<p>
handwritten signatures <a href="https://www.kaggle.com/divyanshrai/handwritten-signatures">https://www.kaggle.com/divyanshrai/handwritten-signatures</a>
</p>
<ul class="org-ul">
<li>30 people</li>
<li>NFI-00602023 is an image of signature of person number 023 done by person 006 - This is a forged signature</li>
<li>NFI-02103021 is an image of signature of person number 021 done by person 021 - genuine signature.</li>
</ul>


<p>
English Writer recognition dataset (not signatures)    IAM <a href="https://fki.tic.heia-fr.ch/databases/iam-handwriting-database">https://fki.tic.heia-fr.ch/databases/iam-handwriting-database</a>
</p>
</div>
</div>
</div>
<div id="outline-container-org8e7071e" class="outline-3">
<h3 id="org8e7071e"><span class="section-number-3">11.24.</span> игрушечные датасеты toy datasets</h3>
<div class="outline-text-3" id="text-11-24">
</div>
<div id="outline-container-org7b6c8a3" class="outline-4">
<h4 id="org7b6c8a3"><span class="section-number-4">11.24.1.</span> line with standard deviation</h4>
<div class="outline-text-4" id="text-11-24-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy
<span style="color: #8ac6f2; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">LINE ----------------------------------</span>
<span style="color: #cae682;">x</span> = np.random.rand(100)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Gaussian distribution N(mu,sigma ^2)</span>
<span style="color: #cae682;">sigma</span> = 0.1  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">mean</span>
<span style="color: #cae682;">mu</span> = 0  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">standard deviation</span>
<span style="color: #cae682;">N</span> = numpy.random.normal(mu, scale=sigma, size=x.shape[0])

<span style="color: #cae682;">y</span> = np.reshape(5 * x + 2 + N, -1)

plt.plot(x, y, <span style="color: #95e454;">'bo'</span>)
plt.show()
</pre>
</div>
</div>
</div>
<div id="outline-container-org9d36ed8" class="outline-4">
<h4 id="org9d36ed8"><span class="section-number-4">11.24.2.</span> two bloabs of Gaussian distributions N(mu,sigma ^2)</h4>
<div class="outline-text-4" id="text-11-24-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy
<span style="color: #8ac6f2; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np

k
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Toy Logistic Regression Data ---------------</span>
<span style="color: #cae682;">N</span> = 100
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Zeros form a Gaussian centered at (-1, -1)</span>
<span style="color: #cae682;">x_zeros</span> = np.random.multivariate_normal(
    mean=np.array((-1, -1)), cov=.1*np.eye(2), size=(N//2,))
<span style="color: #cae682;">y_zeros</span> = np.zeros((N//2,))
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Ones form a Gaussian centered at (1, 1)</span>
<span style="color: #cae682;">x_ones</span> = np.random.multivariate_normal(
    mean=np.array((1, 1)), cov=.1*np.eye(2), size=(N//2,))
<span style="color: #cae682;">y_ones</span> = np.ones((N//2,))

<span style="color: #cae682;">x_np</span> = np.vstack([x_zeros, x_ones])
<span style="color: #cae682;">y_np</span> = np.concatenate([y_zeros, y_ones])

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Save image of the data distribution</span>
plt.xlabel(r<span style="color: #95e454;">"$x_1$"</span>)
plt.ylabel(r<span style="color: #95e454;">"$x_2$"</span>)
plt.scatter(x_zeros[:, 0], x_zeros[:, 1], color=<span style="color: #95e454;">"blue"</span>)
plt.scatter(x_ones[:, 0], x_ones[:, 1], color=<span style="color: #95e454;">"red"</span>)
plt.title(<span style="color: #95e454;">"Toy Logistic Regression Data"</span>)
plt.show()

</pre>
</div>
</div>
</div>

<div id="outline-container-orgb2ad11a" class="outline-4">
<h4 id="orgb2ad11a"><span class="section-number-4">11.24.3.</span> cosine with standard deviation</h4>
<div class="outline-text-4" id="text-11-24-3">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy
<span style="color: #8ac6f2; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np


<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">COS(x) x in [-5,5] + N(0,1/5) ---------</span>
<span style="color: #cae682;">x</span> = np.array(np.arange(-5, 5, 0.1))

<span style="color: #cae682;">sigma</span> = 0.5  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">mean</span>
<span style="color: #cae682;">mu</span> = 0  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">standard deviation</span>
<span style="color: #cae682;">N</span> = numpy.random.normal(mu, scale=sigma, size=x.shape[0])

<span style="color: #cae682;">y</span> = np.reshape(np.cos(x) + N, -1)

plt.plot(x, y, <span style="color: #95e454;">'bo'</span>)
plt.show()
</pre>
</div>

<ul class="org-ul">
<li>После первого обучения мы всзвешиваем датасет на основе ошибок первого</li>
</ul>
</div>
</div>
<div id="outline-container-orgebe38e2" class="outline-4">
<h4 id="orgebe38e2"><span class="section-number-4">11.24.4.</span> normal distribution</h4>
<div class="outline-text-4" id="text-11-24-4">
</div>
<ol class="org-ol">
<li><a id="orge629a3e"></a>with scipy<br />
<div class="outline-text-5" id="text-11-24-4-1">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #8ac6f2; font-weight: bold;">from</span> scipy.stats <span style="color: #8ac6f2; font-weight: bold;">import</span> norm
<span style="color: #8ac6f2; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt

<span style="color: #cae682;">rv</span> = norm(loc=0, scale=1) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">loc (location) or mean  = 0 , scale (squared) or variance = 1</span>

<span style="color: #cae682;">x</span> = norm.rvs(size=1000) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">random variable</span>

<span style="color: #cae682;">pdf</span> = rv.pdf(x)
plt.scatter(x, pdf , color = <span style="color: #95e454;">'red'</span>)
plt.hist(x, 30, density=<span style="color: #e5786d; font-weight: bold;">True</span>)
plt.show()
</pre>
</div>

<pre class="example">
excess kurtosis of normal distribution (should be 0): -0.0024385251600711477
skewness of normal distribution (should be 0): 0.0013034391014922926
</pre>
</div>
</li>
<li><a id="org6329953"></a>with numpy<br />
<div class="outline-text-5" id="text-11-24-4-2">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #8ac6f2; font-weight: bold;">from</span> scipy.stats <span style="color: #8ac6f2; font-weight: bold;">import</span> norm
<span style="color: #8ac6f2; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt
<span style="color: #cae682;">mu</span>, <span style="color: #cae682;">sigma</span> = 0, 1 <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">mean and standard deviation</span>
<span style="color: #cae682;">x</span> = np.random.normal(mu, sigma, 1000)
<span style="color: #cae682;">pdf</span> = norm.pdf(x)
plt.scatter(x, pdf , color = <span style="color: #95e454;">'red'</span>)
plt.show()
</pre>
</div>
</div>
</li>

<li><a id="org64f5978"></a>pdf of line<br />
<div class="outline-text-5" id="text-11-24-4-3">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Importing required libraries</span>
<span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #8ac6f2; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #8ac6f2; font-weight: bold;">as</span> plt

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Creating a series of data of in range of 1-50.</span>
<span style="color: #cae682;">x</span> = np.linspace(1,50,200)

<span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">Creating a Function.</span>
<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">normal_dist</span>(x , mean , sd):
    <span style="color: #cae682;">prob_density</span> = (np.pi*sd) * np.exp(-0.5*((x-mean)/sd)**2)
    <span style="color: #8ac6f2; font-weight: bold;">return</span> prob_density

<span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">Calculate mean and Standard deviation.</span>
<span style="color: #cae682;">mean</span> = np.mean(x)
<span style="color: #cae682;">sd</span> = np.std(x)

<span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">Apply function to the data.</span>
<span style="color: #cae682;">pdf</span> = normal_dist(x,mean,sd)

<span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">Plotting the Results</span>
plt.plot(x, pdf , color = <span style="color: #95e454;">'red'</span>)

plt.xlabel(<span style="color: #95e454;">'Data points'</span>)
plt.ylabel(<span style="color: #95e454;">'Probability Density'</span>)
plt.show()
</pre>
</div>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org237fbd3" class="outline-3">
<h3 id="org237fbd3"><span class="section-number-3">11.25.</span> <span class="todo TODO">TODO</span> Genetic algorithms</h3>
<div class="outline-text-3" id="text-11-25">
<p>
by iterating, variation and combining target parameters. Neural network training can serve as an example of
 such a task.
</p>

<p>
<b>evolutionary computation</b> is a family of algorithms for global optimization.
</p>

<p>
<b>Soft computing</b> is a set of algorithms
</p>
<ul class="org-ul">
<li>Approximate reasoning - processing information (data) through fuzzy rules
<ul class="org-ul">
<li>Probablistic models</li>
<li>Multivalued &amp; Fuzzy Logics</li>
</ul></li>
<li>Functional approximation / Randomized Search
<ul class="org-ul">
<li>neural networks</li>
<li>evolutionary algorithms.</li>
</ul></li>
</ul>

<p>
<b>Classical logic</b> only permits conclusions that are either true or false. Fuzzy Logics - любые значения на
 отрезке [ 0 , 1 ].
</p>


<p>
links
</p>
<ul class="org-ul">
<li><a href="https://loginom.ru/blog/ga-math">https://loginom.ru/blog/ga-math</a></li>
<li><a href="https://en.wikipedia.org/wiki/Evolutionary_algorithm">https://en.wikipedia.org/wiki/Evolutionary_algorithm</a></li>
</ul>
</div>
</div>
<div id="outline-container-org548c011" class="outline-3">
<h3 id="org548c011"><span class="section-number-3">11.26.</span> Causal inference - причинно следственный вывод</h3>
<div class="outline-text-3" id="text-11-26">
<p>
Is determining effect (independent) of some event that is a component of some system. Provide the evidence of
 causality.
</p>
<ul class="org-ul">
<li>distinction between correlation and causation is important, <b>scientific method</b> used to solve this.</li>
</ul>

<p>
Inferring a cause - Identification of the cause or causes of a phenomenon, by establishing covariation of
 cause and effect, a time-order relationship with the cause preceding the effect, and the elimination of
 plausible alternative causes.
</p>
</div>
<div id="outline-container-org6a7afc9" class="outline-4">
<h4 id="org6a7afc9"><span class="section-number-4">11.26.1.</span> terms</h4>
<div class="outline-text-4" id="text-11-26-1">
<dl class="org-dl">
<dt>treatment</dt><dd>treatment group that receive intervention or treatment being studied.
<ul class="org-ul">
<li>In Factor Analysis: Any combination of factor levels is called a treatment.</li>
</ul></dd>
<dt>control group</dt><dd>do not receive the intervention or treatment. receive a placebo, the standard treatment, or
no treatment at all.</dd>
<dt>randomized controlled trials (RCTs)</dt><dd>units are randomly assigned to either the treatment or control group.</dd>
<dt>select bias</dt><dd>proper randomization is not achieved when selection of individuals, groups, or data for
analysis. <a href="https://en.wikipedia.org/wiki/Selection_bias">https://en.wikipedia.org/wiki/Selection_bias</a></dd>
<dt>feasible</dt><dd>достижимо</dd>
<dt>covariates</dt><dd>Characteristics of participants in an experiment. A variable that is possibly predictive of
the outcome under study.
<ul class="org-ul">
<li>can be an <b>independent variable</b> (i.e. of direct interest) or it can be an unwanted, <b>confounding
variable</b>.</li>
</ul></dd>
<dt>confounding</dt><dd>variable that influences both the dependent variable and independent variable, causing a
<b>spurious association</b>.  causal concept. <a href="https://en.wikipedia.org/wiki/Confounding_variables">https://en.wikipedia.org/wiki/Confounding_variables</a></dd>
<dt>Factor analysis</dt><dd>way to take a mass of data and shrinking it to a smaller data set that is more
manageable and more understandable with lower number of unobserved variables called <b>factors</b>, to describe
variability among observed, correlated variables.</dd>
<dt>Quasi-experiment</dt><dd>empirical interventional study used to estimate the causal impact of an intervention on
target population without random assignment. lacks of random assignment.</dd>
</dl>
</div>
</div>
<div id="outline-container-org4fa8d66" class="outline-4">
<h4 id="org4fa8d66"><span class="section-number-4">11.26.2.</span> related topics</h4>
<div class="outline-text-4" id="text-11-26-2">
<p>
Part of:
</p>
<dl class="org-dl">
<dt>inferential statistics</dt><dd>data analysis to infer properties of an underlying distribution of probability. by
testing hypotheses and deriving estimates.</dd>
<dt>Causal analysis</dt><dd>field of experimental design and statistics pertaining to establishing cause and effect.</dd>
</dl>

<p>
Other:
</p>
<dl class="org-dl">
<dt>etiology</dt><dd>answer why question.</dd>
<dt>Causal reasoning</dt><dd>process of identifying causality: the relationship between a cause and its
effect. <a href="math#MissingReference">math#MissingReference</a></dd>
<dt>inference of association</dt><dd>at change moment, while Causal inference for long after period.  correlation or
dependence is any statistical relationship.</dd>
<dt>descriptive statistics</dt><dd>solely concerned with properties of the observed data, and it does not rest on the
assumption that the data come from a larger population.</dd>
</dl>
</div>
</div>

<div id="outline-container-orga03fbc7" class="outline-4">
<h4 id="orga03fbc7"><span class="section-number-4">11.26.3.</span> problems:</h4>
<div class="outline-text-4" id="text-11-26-3">
<ul class="org-ul">
<li>determine causality</li>
<li>frequent errors: correlative results as causal, usage of incorrect methodologies, statistically significant
estimates as a result of data manipulations.</li>
<li></li>
</ul>
</div>
</div>
<div id="outline-container-orgf9b7c6b" class="outline-4">
<h4 id="orgf9b7c6b"><span class="section-number-4">11.26.4.</span> steps:</h4>
<div class="outline-text-4" id="text-11-26-4">
<ol class="org-ol">
<li>Identify the Treatment (the event or intervention you are testing) and Outcome Variables (the effects you are
measuring).</li>
<li>Quantitative or qualitative?
<ol class="org-ol">
<li>quaL - Explore, explain or understand a phenomenon. Cause and effect. Often starts with ‘why’. Often
focusing on meaning and experience. Tools: Methods include surveys, case studies, focus groups,
ethnography, and content analysis.
<ul class="org-ul">
<li>Exploratory: How do students at our school spend their weekends?</li>
<li>Predictive: Are people more likely to buy a product after a celebrity promotes it?</li>
<li>Interpretive: How do you feel about AI assisting in the publishing process in your research?</li>
</ul></li>
<li>quaN - To prove or disprove a hypothesis using empirical evidence and statistical analysis. Types:
Descriptive, Comparative, and Relationship-Based. Tools: Statistical software, experimental designs,
surveys, and other quantitative data collection and analysis methods.</li>
</ol></li>
<li>implies cause and effect) Causal relationships (quantitative study) or</li>
<li>What Research question:
<ul class="org-ul">
<li>Descriptive Question - To describe what is happening or what exists. To provide an accurate picture of a
situation.</li>
<li>Compare: “Why is it easier for men to lose weight than it is for women?” - Tools: Comparative studies,
ANOVA (Analysis of Variance), t-tests</li>
<li>Relationship-Based: To determine relationship between variables. Tools: Correlation analysis, regression
analysis.</li>
<li>Causal Questions: whether one variable causes or affects another. Check: Analyzes the response of the
effect variable after a cause of the effect variable is changed (causal inference). Or just find
association (inference of association) it is “Relationship-Based”. Tools: Experimental designs,
quasi-experimental designs, causal inference frameworks.</li>
</ul></li>
</ol>
<p>
to find cause or determining of effect or hypothesis testing.
</p>



<p>
causal relationships (quantitative study) or exploring a phenomenon (qualitative study)
</p>

<p>
Does research question to is to provide evidence of some known causality or to find cause?
</p>

<p>
Can you conduct a randomized experiment  or you are limited to observational data.
</p>

<p>
Complexity of question?
</p>

<p>
Availability of control in randomized experiment?
</p>


<ol class="org-ol">
<li>formulate a falsifiable <b>null hypothesis</b>, that will be testes. see <a href="math#MissingReference">math#MissingReference</a> and
<a href="#orgb310370">8.20</a></li>
<li>Statistical hypothesis testing - whether the data sufficiently supports a particular
hypothesis. <b>Statistical inference</b> is generally used to determine the difference between variations in the
original data that are random variation or the effect of a well-specified causal mechanism.
<ul class="org-ul">
<li><b>Frequentist statistical</b> inference is the use of statistical methods to determine the probability that the
data occur under the null hypothesis by chance;</li>
<li><b>Bayesian inference</b> is used to determine the effect of an independent variable.</li>
<li>epidemiological methods - collecting and measuring evidence of risk factors and effect and different ways
of measuring association between the two</li>
</ul></li>
</ol>
</div>
</div>

<div id="outline-container-org6d9e4b0" class="outline-4">
<h4 id="org6d9e4b0"><span class="section-number-4">11.26.5.</span> <span class="todo TODO">TODO</span> frameworks for causal inference</h4>
<div class="outline-text-4" id="text-11-26-5">
<ul class="org-ul">
<li>causal pie model (component-cause)</li>
<li>Pearl's structural causal model (causal diagram + do-calculus)</li>
<li>structural equation modeling</li>
<li>Rubin causal model (potential-outcome)</li>
</ul>
</div>
</div>

<div id="outline-container-orga93d3a5" class="outline-4">
<h4 id="orga93d3a5"><span class="section-number-4">11.26.6.</span> methods</h4>
<div class="outline-text-4" id="text-11-26-6">
</div>
<ol class="org-ol">
<li><a id="org0e91db4"></a>criteries to choose<br />
<div class="outline-text-5" id="text-11-26-6-1">
<ul class="org-ul">
<li>your research question</li>
<li>the available data</li>
<li>the underlying assumptions of each method</li>
</ul>

<p>
In most cases, <b>randomized controlled experiments</b> (when available) is enough, causal inference used to answer
 how and why causality had the effect it did.
</p>

<p>
experimental data or observational data?
</p>
<ul class="org-ul">
<li>experimental data - controlled experiments.</li>
<li>Observational Data - when experiments are not feasible due to ethical, practical, or cost reasons.
Collecting data from subjects without the researcher intervening or manipulating the variables of
interest. (frameworks like SCM and POF)</li>
</ul>
</div>
</li>

<li><a id="org5ab431d"></a>choosing<br />
<div class="outline-text-5" id="text-11-26-6-2">
<p>
Choosing steps for experimental data: (source <a href="https://www.uber.com/en-RS/blog/causal-inference-at-uber/">https://www.uber.com/en-RS/blog/causal-inference-at-uber/</a>)
</p>
<ol class="org-ol">
<li>Important pre-existing difference across groups? - CUPED / Diff-if-diff, Propensity Score Matching, IPTW.</li>
<li>Some people in treatment group didn't actually receive the treatment? - Complier Average Causal Effect (CACE).</li>
<li>Treatment effect varies significantly across subgroups? - Heterogeneous treatment effect, Uplift modeling,
Quantile regression.</li>
<li>Test why treatment did (or didn't) move outcome metric? - Mediation modeling.</li>
</ol>

<p>
Choosing steps for observational data:
</p>
<ol class="org-ol">
<li>Have repeated observations of the outcome over time?
<ul class="org-ul">
<li><i>YES</i> Have a control time series data from a non-treated unit?
<ul class="org-ul">
<li><i>NO</i> <b>Interrupted time series w/o control Single-group pre/post design.</b></li>
<li><i>YES</i> More than a handful of obs before and after intervention.
<ul class="org-ul">
<li><i>YES</i> <b>Interrupted time series synthetic control</b>.</li>
<li><i>NO</i> <b>difference-in-difference</b>.</li>
</ul></li>
</ul></li>
<li><i>NO</i> Treatment assignment depends on a sharpp cutoff?
<ul class="org-ul">
<li><i>YES</i> <b>Regression discontinuity.</b></li>
<li><i>NO</i> Have a third variable associated with the outcome only through the cause variable?
<ul class="org-ul">
<li><i>YES</i> <b>Intrumental Variable</b>.</li>
<li><i>NO</i> Have pre-intervention covariates? -&gt; <i>YES</i> -&gt; <b>Propensity score matching IPTW doubly robust
estimation.</b></li>
</ul></li>
</ul></li>
</ul></li>
</ol>
</div>
</li>
<li><a id="org0b7b51c"></a>all methods<br />
<div class="outline-text-5" id="text-11-26-6-3">
<ol class="org-ol">
<li>Experimental Approaches:
<ul class="org-ul">
<li>Randomized Controlled Trials (RCTs)</li>
<li>A/B Testing</li>
<li>Factorial Experiments</li>
</ul></li>
<li>Quasi-Experimental Approaches:
<ul class="org-ul">
<li>Regression Discontinuity Design (RDD)</li>
<li>Difference-in-Differences (DiD)</li>
<li>Propensity Score Matching (PSM)</li>
<li>Instrumental Variables (IV)</li>
</ul></li>
<li>Observational Approaches - Causal Modeling Approaches
<ul class="org-ul">
<li>Directed Acyclic Graphs (DAGs)</li>
<li>Structural Equation Modeling (SEM)</li>
<li>Causal Bayesian Networks</li>
</ul></li>
<li>Causal Frameworks:
<ul class="org-ul">
<li>Potential Outcomes Framework (Rubin Causal Model)</li>
<li>Pearl's Causal Inference Framework</li>
<li>Causal Diagrams and Graphical Models</li>
</ul></li>
</ol>

<p>
difference-in-differences approach (DID, DD) - mimic an experimental research design using observational study
 data, by studying the differential effect of a treatment on a 'treatment group' versus a 'control group' in a
 natural experiment.
</p>

<p>
DID is a special case of the CUPED method where the coefficient of pre-experiment data in the first model is
 equal to one.
</p>

<p>
CUPED - researchers first make predictions of the post-experiment data using the pre-experiment data to
 estimate what the baseline would be for each individual if there was no treatment. Researchers then adjust
 the observed post-experiment outcome using the predicted baseline to perform the experiment analysis.
</p>

<p>
treating the experiment as an observational study: using methods such as <b>propensity score matching</b> or
 <b>inverse probability of treatment weighting</b> (IPTW).
</p>

<p>
complier average causal effect (CACE) - when we sends out an email to customers, but not everyone in the
 treatment group who got the email actually opened it. <b>selection bias may arise</b>
</p>

<p>
Instrumental variables (IV) method - used to estimate causal relationships when controlled experiments are not
 feasible or when a treatment is not successfully delivered to every unit in a randomized experiment
</p>
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Instrumental_variables_estimation">https://en.wikipedia.org/wiki/Instrumental_variables_estimation</a></li>
</ul>
</div>
</li>

<li><a id="orgeaa3f64"></a>all methods in details<br />
<div class="outline-text-5" id="text-11-26-6-4">
<p>
Randomized Controlled Trials (RCTs)
    Definition: RCTs involve randomly assigning participants to either a treatment or a control group to evaluate the effect of the treatment.
    Strengths: RCTs are considered the gold standard for causal inference due to their ability to minimize confounding through randomization, ensuring that the treatment and control groups are similar in all respects except for the treatment.
    Limitations: RCTs can be expensive, time-consuming, and sometimes unethical or impractical to conduct.
    Proposed Changes:
        Improved Randomization Techniques: Enhance randomization methods to ensure better balance between groups, especially in smaller samples.
        Blinded Experiments: Use double-blinded or triple-blinded designs to reduce bias from both participants and researchers.
        Long-term Follow-Up: Incorporate longer follow-up periods to capture long-term effects of the treatment.
</p>

<p>
A/B Testing
</p>

<p>
Definition: A/B testing involves comparing two versions of a product, service, or intervention to determine which one performs better.
Strengths: A/B testing is widely used in industry settings due to its simplicity and the ability to quickly gather results.
Limitations: It may not be suitable for complex interventions or when the sample size is small.
Proposed Changes:
    Multi-Armed Bandit Designs: Use more sophisticated designs like multi-armed bandit algorithms to dynamically adjust the allocation of participants to different treatments based on interim results.
    Stratified Sampling: Ensure that the groups are stratified based on relevant covariates to improve balance.
</p>

<p>
Factorial Experiments
</p>

<p>
Definition: Factorial experiments involve testing multiple factors simultaneously to understand their individual and interactive effects.
Strengths: These experiments can efficiently estimate the effects of multiple factors and their interactions.
Limitations: They can become complex and resource-intensive as the number of factors increases.
Proposed Changes:
    Fractional Factorial Designs: Use fractional factorial designs to reduce the number of experiments needed while still capturing the main effects and some interactions.
    Sequential Experimentation: Conduct experiments in stages, analyzing results after each stage to inform the next set of experiments.
</p>

<p>
Quasi-Experimental Approaches
Regression Discontinuity Design (RDD)
</p>

<p>
Definition: RDD exploits a discontinuity in the treatment assignment based on a threshold of a continuous variable.
Strengths: RDD can provide causal estimates in situations where randomization is not feasible but there is a clear cutoff for treatment assignment.
Limitations: It relies on the assumption that the discontinuity is sharp and that there are no other factors affecting the outcome at the cutoff.
Proposed Changes:
    Fuzzy RDD: Use fuzzy RDD when the treatment is not sharply assigned at the cutoff, allowing for more flexibility in estimating the causal effect.
    Multiple Cutoffs: Analyze multiple cutoffs if available to increase the robustness of the estimates.
</p>

<p>
Difference-in-Differences (DiD)
</p>

<p>
Definition: DiD compares the change in outcomes over time between a treatment group and a control group.
Strengths: DiD can be used when there is a clear before-and-after period and a control group is available.
Limitations: It assumes that the trends in the treatment and control groups would have been the same in the absence of the treatment.
Proposed Changes:
    Synthetic Control Method: Use synthetic control methods to create a weighted average of control units that closely matches the pre-treatment trend of the treatment unit.
    Multiple Time Periods: Analyze multiple time periods to ensure the parallel trends assumption holds.
</p>

<p>
Propensity Score Matching (PSM)
</p>

<p>
Definition: PSM matches treated and control units based on their probability of receiving the treatment (propensity score).
Strengths: PSM can balance the covariates between treated and control groups in observational data.
Limitations: It relies on the unconfoundedness assumption and can be sensitive to the choice of matching method.
Proposed Changes:
    Coarsened Exact Matching (CEM): Use CEM, which coarsens the data into fewer strata to improve balance and reduce model dependence.
    Monotonic Imbalance Bounding (MIB): Implement MIB methods to generalize and extend existing matching methods, ensuring better statistical properties.
</p>

<p>
Instrumental Variables (IV)
</p>

<p>
Definition: IV uses an instrument (a variable that affects the treatment but not the outcome directly) to identify the causal effect.
Strengths: IV can address unmeasured confounding by leveraging an instrument that satisfies the necessary assumptions.
Limitations: Finding a valid instrument can be challenging, and the method assumes the instrument affects the outcome only through the treatment.
Proposed Changes:
    Multiple Instruments: Use multiple instruments if available to increase the robustness of the estimates.
    Sensitivity Analysis: Conduct sensitivity analyses to assess the robustness of the IV estimates to violations of the assumptions.
</p>

<p>
Observational Approaches
Directed Acyclic Graphs (DAGs)
</p>

<p>
Definition: DAGs are graphical representations of causal relationships between variables.
Strengths: DAGs help in identifying potential confounders and causal pathways, facilitating the design of causal inference studies.
Limitations: They require accurate knowledge of the causal structure, which can be challenging to determine.
Proposed Changes:
    Sensitivity Analysis: Perform sensitivity analyses to evaluate how robust the causal inferences are to different assumptions about the causal structure.
    Combining with Other Methods: Use DAGs in conjunction with other methods like matching or IV to ensure that the assumptions are met.
</p>

<p>
Structural Equation Modeling (SEM)
</p>

<p>
Definition: SEM involves specifying a set of equations that represent the causal relationships between variables.
Strengths: SEM can model complex relationships and account for latent variables.
Limitations: It requires strong assumptions about the model specification and can be sensitive to model misspecification.
Proposed Changes:
    Model Validation: Implement robust model validation techniques to ensure the model fits the data well.
    Bayesian SEM: Use Bayesian methods to incorporate prior knowledge and uncertainty into the model.
</p>

<p>
Causal Bayesian Networks
</p>

<p>
Definition: Causal Bayesian networks are probabilistic graphical models that represent causal relationships.
Strengths: They can handle complex causal structures and provide probabilistic estimates of causal effects.
Limitations: They require accurate specification of the causal structure and can be computationally intensive.
Proposed Changes:
    Learning Causal Structures: Use algorithms to learn the causal structure from data, rather than relying on prior knowledge.
    Incorporating Domain Knowledge: Integrate domain-specific knowledge to improve the accuracy of the causal structure.
</p>

<p>
Causal Frameworks
Potential Outcomes Framework (Rubin Causal Model)
</p>

<p>
Definition: This framework defines causal effects in terms of potential outcomes under different treatment assignments.
Strengths: It provides a clear and intuitive way to define causal effects and is widely used in causal inference.
Limitations: It relies on the assumption of no unmeasured confounding and requires identifying the potential outcomes.
Proposed Changes:
    Combining with Other Frameworks: Use this framework in conjunction with other frameworks like Pearl's causal inference framework to leverage their strengths.
    Sensitivity Analysis: Conduct sensitivity analyses to evaluate the robustness of the estimates to violations of the assumptions.
</p>

<p>
Pearl's Causal Inference Framework
</p>

<p>
Definition: This framework uses Structural Causal Models (SCMs) and do-calculus to represent and analyze causal relationships.
Strengths: It provides a rigorous mathematical foundation for causal inference and can handle complex causal structures.
Limitations: It requires a clear understanding of the causal structure and can be computationally intensive.
Proposed Changes:
    Automated Causal Discovery: Use algorithms to automatically discover the causal structure from data.
    Integration with Machine Learning: Combine SCMs with machine learning techniques to improve the estimation of causal effects.
</p>

<p>
Causal Diagrams and Graphical Models
</p>

<p>
Definition: These are visual representations of causal relationships that help in identifying confounders and causal pathways.
Strengths: They facilitate the identification of potential biases and the design of causal inference studies.
Limitations: They require accurate knowledge of the causal structure.
Proposed Changes:
    Dynamic Causal Diagrams: Use dynamic causal diagrams to represent time-varying causal relationships.
    Triangulation: Use triangulation by combining results from multiple approaches to strengthen causal inferences, as each approach may have different sources of bias.
</p>

<p>
General Proposals
Triangulation
</p>

<p>
Use multiple approaches (e.g., statistical and design-based methods) to triangulate evidence, as no single method can provide a definitive answer to a causal question. This helps in mitigating different sources of bias and strengthens the robustness of the findings.
</p>

<p>
Sensitivity Analysis
</p>

<p>
Conduct thorough sensitivity analyses to evaluate how robust the causal inferences are to different assumptions and model specifications. This is crucial for all methods to ensure that the results are not overly sensitive to specific assumptions.
</p>

<p>
Integration of Methods
</p>

<p>
Combine different methods (e.g., using DAGs with matching or IV) to leverage their strengths and mitigate their weaknesses. This integrated approach can provide more robust and reliable causal inferences.
</p>

<p>
Use of Advanced Statistical and Machine Learning Techniques
</p>

<p>
Incorporate advanced statistical and machine learning techniques to improve the estimation and validation of causal effects. This includes using Bayesian methods, machine learning algorithms for causal discovery, and robust model validation techniques.
</p>
</div>
</li>


<li><a id="org532911f"></a>Limits of methods<br />
<div class="outline-text-5" id="text-11-26-6-5">
<p>
RCTs: Randomization should ensure balance between groups.
PSM: Requires the unconfoundedness assumption.
IV: Requires the instrument to affect the outcome only through the treatment.
</p>
</div>
</li>


<li><a id="org887ec81"></a>Choosing the Appropriate Approach:<br />
<div class="outline-text-5" id="text-11-26-6-6">
<ol class="org-ol">
<li>Research Question and Study Design:
<ul class="org-ul">
<li>Start by clearly defining your research question and the causal effect you want to estimate.</li>
<li>Determine if you have the ability to conduct a randomized experiment or if you are limited to observational data.</li>
</ul></li>

<li>Assumptions and Limitations:
<ul class="org-ul">
<li>Understand the key assumptions and limitations of each causal inference method.</li>
<li>Assess whether your data and study design can meet the necessary assumptions.</li>
</ul></li>

<li>Data Availability and Quality:
<ul class="org-ul">
<li>Evaluate the availability and quality of your data, including the presence of potential confounding factors.</li>
<li>Determine if you have access to the necessary variables to apply a specific causal inference method.</li>
</ul></li>

<li>Complexity and Interpretability:
<ul class="org-ul">
<li>Consider the complexity of the causal inference method and its interpretability for your research context.</li>
<li>Simpler methods like RCTs, A/B testing, and DiD may be preferred if they can adequately address your research question.</li>
<li>More advanced methods like SEM and Causal Bayesian Networks may be necessary for complex causal relationships.</li>
</ul></li>

<li>Consultation and Guidance:
<ul class="org-ul">
<li>Consult with experts or literature in the field of causal inference to help you navigate the selection of the appropriate method.</li>
<li>Seek guidance from statisticians or methodologists who can provide insights on the suitability of different approaches for your specific research context.</li>
</ul></li>
</ol>
</div>
</li>
</ol>
</div>

<div id="outline-container-org4a86498" class="outline-4">
<h4 id="org4a86498"><span class="section-number-4">11.26.7.</span> links</h4>
<div class="outline-text-4" id="text-11-26-7">
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Causal_inference">https://en.wikipedia.org/wiki/Causal_inference</a></li>
<li><a href="https://www.uber.com/en-RS/blog/causal-inference-at-uber/">https://www.uber.com/en-RS/blog/causal-inference-at-uber/</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org161d157" class="outline-3">
<h3 id="org161d157"><span class="section-number-3">11.27.</span> <span class="todo TODO">TODO</span> Uplift modelling</h3>
<div class="outline-text-3" id="text-11-27">
<p>
Models the incremental impact of a treatment.
</p>

<p>
<b>uplift</b> - usually defined as the difference in response rate between a treated group and a randomized
control group. ( incremental effect )
</p>
<ul class="org-ul">
<li>many implement <b>lift</b> as the difference. (without predictive modeling)</li>
</ul>

<p>
ex.
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Group</th>
<th scope="col" class="org-left">Number of Customers</th>
<th scope="col" class="org-left">Responses</th>
<th scope="col" class="org-right">Response Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Treated</td>
<td class="org-left">1,000,000</td>
<td class="org-left">100,000</td>
<td class="org-right">10%</td>
</tr>

<tr>
<td class="org-left">Control</td>
<td class="org-left">1,000,000</td>
<td class="org-left">50,000</td>
<td class="org-right">5%</td>
</tr>
</tbody>
</table>

<p>
Here response rate uplift is 5%.
</p>

<p>
Перед созданием модели необходимо провести A/B-эксперимент , который заключается в следующем:
</p>
<ul class="org-ul">
<li>Часть активных пользователей продукта случайным образом делится на две группы: тестовую и контрольную.</li>
<li>К пользователям из тестовой группы применяются механизмы для удержания (бонусы, скидки, специальная коммуникация).</li>
<li><p>
Опыт пользователей из контрольной группы не меняется.
</p>

<p>
библиотека scikit-uplift
</p></li>
</ul>

<p>
Все базовые подходы можно разделить на два класса:
</p>
<ul class="org-ul">
<li>походы с применением одной модели</li>
<li><p>
подходы с применением двух моделей.
</p>

<p>
одна модель обучается одновременно на двух группах, при этом бинарный флаг коммуникации выступает в качестве
дополнительного признака. Каждый объект из тестовой выборки скорим дважды: с флагом коммуникации равным 1 и
равным 0. Вычитая вероятности по каждому наблюдению, получим искомый uplift.
</p></li>
</ul>

<p>
двух ML-моделей, которые будут предсказывать уход пользователей (как именно это делается, мы разбирали выше):
</p>
<ul class="org-ul">
<li>Модели, которая прогнозирует, что пользователь уйдет при отсутствии механизмов удержания. Для обучения этой
модели нужно использовать данные из контрольной группы эксперимента.</li>
<li>Модели, которая прогнозирует, что пользователь уйдет при наличии механизмов удержания. Для обучения этой
модели нужно использовать данные из тестовой группы эксперимента.</li>
</ul>
<p>
Две независимые модели:
</p>
<ul class="org-ul">
<li>Строится первая модель, оценивающая вероятность выполнения целевого действия среди клиентов, с которыми мы взаимодействовали.</li>
<li>Строится вторая модель, оценивающая ту же вероятность, но среди клиентов, с которыми мы не производили коммуникацию.</li>
<li>Затем для каждого клиента рассчитывается разность оценок вероятностей двух моделей.</li>
</ul>
<p>
Две зависимые модели (зависимое представление данных)
</p>
<ul class="org-ul">
<li>&#x2026;</li>
</ul>
<p>
Две зависимые модели (перекрестная зависимость)
</p>
<ul class="org-ul">
<li>..</li>
</ul>
</div>
<div id="outline-container-orgc747856" class="outline-4">
<h4 id="orgc747856"><span class="section-number-4">11.27.1.</span> <span class="todo TODO">TODO</span> Uplift modeling via Gradient Boosting paper</h4>
<div class="outline-text-4" id="text-11-27-1">
<p>
<a href="https://github.com/sb-ai-lab/uplift-via-gbdt">https://github.com/sb-ai-lab/uplift-via-gbdt</a>
</p>
</div>
</div>
<div id="outline-container-orgae5e8c3" class="outline-4">
<h4 id="orgae5e8c3"><span class="section-number-4">11.27.2.</span> dataset</h4>
<div class="outline-text-4" id="text-11-27-2">
<p>
Hillstrom Dataset. Этот набор данных содержит информацию о 64000 покупателей, совершивших покупку в течение 12 месяцев.
</p>
</div>
</div>
<div id="outline-container-org7b1b25a" class="outline-4">
<h4 id="org7b1b25a"><span class="section-number-4">11.27.3.</span> customers segmentation</h4>
<div class="outline-text-4" id="text-11-27-3">
<ul class="org-ul">
<li>The Persuadables : customers who only respond to the marketing action because they were targeted</li>
<li>The Sure Things  : customers who would have responded whether they were targeted or not</li>
<li>The Lost Causes  : customers who will not respond irrespective of whether or not they are targeted</li>
<li>The Do Not Disturbs or Sleeping Dogs : customers who are less likely to respond because they were targeted</li>
</ul>

<p>
Uplift modelling provides a scoring technique that can separate customers.
</p>
</div>
</div>
<div id="outline-container-org9387b6b" class="outline-4">
<h4 id="org9387b6b"><span class="section-number-4">11.27.4.</span> metrics</h4>
<div class="outline-text-4" id="text-11-27-4">
<ul class="org-ul">
<li>Uplift curve (или Uplift кривая).  - функция от количества объектов. В каждой точке кривой можно увидеть
накопленный к этому моменту uplift.</li>
<li>uplift@k – размер uplift на топ k процентах выборки</li>
</ul>
</div>
</div>
<div id="outline-container-orgd3014f5" class="outline-4">
<h4 id="orgd3014f5"><span class="section-number-4">11.27.5.</span> mts</h4>
<div class="outline-text-4" id="text-11-27-5">
<p>
Формирование сегментов для продвижения
</p>
<dl class="org-dl">
<dt>Look-alike</dt><dd>модель оценивает вероятность того, что клиент выполнит целевое действие.</dd>
<dt>Response модель</dt><dd>оценивает вероятность того, что клиент выполнит целевое действие при условии коммуникации.</dd>
<dt>Uplift модель</dt><dd>оценивает чистый эффект от коммуникации, пытаясь выбрать только тех клиентов, которые
совершат целевое действие только при нашем взаимодействии. Модель оценивает разницу в поведении клиента при
наличии воздействия и при его отсутствии.</dd>
</dl>

<p>
Retention. решается путем прогнозирование оттока пользователей (churn prediction)
</p>
<ul class="org-ul">
<li>альтернативный подход к улучшению Retention с помощью ML — uplift-моделирование</li>
</ul>
<p>
<a href="https://habr.com/ru/companies/ru_mts/articles/485980/">https://habr.com/ru/companies/ru_mts/articles/485980/</a>
</p>
</div>
</div>
</div>
<div id="outline-container-orgc4e71a6" class="outline-3">
<h3 id="orgc4e71a6"><span class="section-number-3">11.28.</span> A/B test</h3>
<div class="outline-text-3" id="text-11-28">
<p>
see <a href="math#MissingReference">math#MissingReference</a>
</p>
<ul class="org-ul">
<li>bootstrap - для решения специфичных задач или при вычислении в экспериментах статзначимости самых
неожиданных функций <a href="https://habr.com/ru/articles/762648/">https://habr.com/ru/articles/762648/</a></li>
</ul>

<p>
Многорукие бандиты - это один из методов тестирования гипотез, который можно применять в качестве альтернативы
 A/B-тестированию. Главное отличие в том, что бандиты позволяют оптимизировать выигрыш сразу после начала
 эксперимента и делают это автоматически. У нас возникла задача тестировать сразу много разных гипотез и
 автоматически отбрасывать совсем плохие, поэтому мы остановились именно на многоруких бандитах.
</p>
</div>


<div id="outline-container-org352a0ca" class="outline-4">
<h4 id="org352a0ca"><span class="section-number-4">11.28.1.</span> <span class="todo TODO">TODO</span> links</h4>
<div class="outline-text-4" id="text-11-28-1">
<ul class="org-ul">
<li><a href="https://bytepawn.com/five-ways-to-reduce-variance-in-ab-testing.html">https://bytepawn.com/five-ways-to-reduce-variance-in-ab-testing.html</a></li>
<li>optimizely.com/insights/blog</li>
<li>docs.geteppo.com/statistics</li>
<li>statsig.com/blog</li>
<li>online calculator  <a href="https://www.samplesizecalc.com/calculator">https://www.samplesizecalc.com/calculator</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgfed40cc" class="outline-3">
<h3 id="orgfed40cc"><span class="section-number-3">11.29.</span> A/B test - Multi-Armed Bandit (MAB) - reinforcement learning problem</h3>
<div class="outline-text-3" id="text-11-29">
<p>
see <a href="#org54a7d30">13.28.5</a>
</p>

<p>
Exploration-Exploitation Tradeoff - MAB optimize the overall reward or outcome over time
</p>

<p>
problem: 4 variants of change, which is better?
</p>

<p>
A/B testing approach:
</p>
<ul class="org-ul">
<li><b>Explore</b> the options on a small percentage of users</li>
<li><b>Exploit</b> - use winning veriant</li>
</ul>

<p>
A/B flaws:
</p>
<ul class="org-ul">
<li>Did you explore each variant enough?</li>
<li>Was the campaign sent to enough users before selecting a winning variant?</li>
<li>Are seasonal trends being accounted for?</li>
<li>Can you continuously Explore and Exploit?</li>
</ul>

<p>
MAB approach: optimises and drives more traffic to the best alternative <b>while</b> running the test, hence
 reducing missed opportunities.
</p>

<p>
<a href="https://medium.com/@sidjain1412/maximising-your-a-b-test-outcomes-with-multi-armed-bandits-f1bd39e236e3">https://medium.com/@sidjain1412/maximising-your-a-b-test-outcomes-with-multi-armed-bandits-f1bd39e236e3</a>
</p>
</div>
</div>
<div id="outline-container-org30ecfa0" class="outline-3">
<h3 id="org30ecfa0"><span class="section-number-3">11.30.</span> Regression</h3>
<div class="outline-text-3" id="text-11-30">
<p>
<b>Regression analysis</b> - statistical processes for estimating the relationships between a dependent variable
 and one or more independent variables.
</p>

<p>
A <b>regression model</b> predicts continuous values.
</p>

<p>
<b>Linear regression</b> - finding the straight line or hyperplane that best fits a set of points, y dependent is a
 liner combination of parameters
</p>
<ul class="org-ul">
<li>сумма Остатков (Residual) – разницей между фактическим и спрогнозированным значениями, равна 0, то есть они
распределены случайным образом вокруг нуля</li>
</ul>

<p>
Machine learning evaluation metrics. see <a id="orgbbcd2cd"></a>
</p>
<ul class="org-ul">
<li>MSE - mean squared error.			1/n * ∑((at-pt)<sup>2</sup>) where at is true y, pt - predicted y</li>
<li>MAE - mean absolute error			1/n * ∑|at-pt|</li>
<li>sMAPE</li>
<li>MAPE - mean absolute percentage error.	1/n * ∑ ((at-pt)/at) or 1/n * ∑ (1 - pt/at) - 0 no loss - inf big loss</li>
<li>MASE</li>
<li>MSPE</li>
<li>RMS</li>
<li>RMSE/RMSD</li>
<li>R2</li>
<li>MDA</li>
<li>MAD</li>
</ul>

<p>
MSE, RMSE is dependent on the scale of the data. It increases in magnitude if the scale of the error increases.
</p>
<ul class="org-ul">
<li>errors have physical dimensions and expressed in the units of the data under analysis (variable of interest</li>
</ul>

<p>
classifications:
</p>
<ul class="org-ul">
<li>scale-dependent measures (e.g. MSE, RMSE, MAE, MdAE);</li>
<li>measures based on percentage errors (e.g. MAPE, MdAPE, RMSPE, RMdSPE, sMAPE,</li>
</ul>
<p>
sMdAPE);
</p>
<ul class="org-ul">
<li>measures based on relative errors (e.g. MRAE, MdRAE, GMRAE);</li>
<li>relative measures (e.g. RelMAE, CumRAE);</li>
<li>scaled errors (e.g. MASE, RMSSE, MdASE)</li>
</ul>

<p>
2
</p>
<ul class="org-ul">
<li>Power distances which are based on mathematical expressions involving raising to power</li>
</ul>
<p>
(e.g. Euclidean, Manhattan, Mahalanobis, Heterogeneous distance);
</p>
<ul class="org-ul">
<li>Distances on distribution laws (probability-related) (e.g. Bhattacharya coefficient, Jensen,</li>
</ul>
<p>
Hellinger);
</p>
<ul class="org-ul">
<li>Correlation similarities and distances (e.g. Spearman, Kendall, Pearson);</li>
<li>Other similarities and distances which do not fit into the three main categories)</li>
</ul>
</div>
</div>
<div id="outline-container-org37c6e11" class="outline-3">
<h3 id="org37c6e11"><span class="section-number-3">11.31.</span> Similarity (ˌsiməˈlerədē/)</h3>
<div class="outline-text-3" id="text-11-31">
</div>
<div id="outline-container-org6799ebc" class="outline-4">
<h4 id="org6799ebc"><span class="section-number-4">11.31.1.</span> Cosine similarity, Orchini similarity, Otsuka–Ochiai similarity</h4>
<div class="outline-text-4" id="text-11-31-1">
<p>
the cosine of the angle between the vectors. applied to binary data.
</p>
<ul class="org-ul">
<li>cos θ = A*B / |A|*|B| - dot product / Euclidean magnitudes of A and B</li>
<li>∑(Ai*Bi)/sqrt(∑Ai<sup>2</sup>)*sqrt(∑Bi<sup>2</sup>)</li>
<li>|A| cos θ = scalar projection</li>
</ul>

<p>
always belongs to the interval [ − 1 , 1 ]
</p>
<ul class="org-ul">
<li>1 - proportional vectors</li>
<li>0 - orthogonal vectors</li>
<li>-1 opposite vectors</li>
</ul>

<p>
if required can be normolized to [ 0 , 1 ], cosine distance = [0, 2]
</p>

<p>
is not a true distance metric as it does not exhibit the <b>triangle inequality</b> property
</p>
<ul class="org-ul">
<li><p>
solution: convert to angular distance or Euclidean distance.
</p>
<ul class="org-ul">
<li>effective proxy for cosine distance can be obtained by L2 normalisation of the vectors (each term in each vector is first divided by the magnitude of the</li>
</ul>
<p>
vector, yielding a vector of unit length), followed by the application of normal Euclidean distance.
</p></li>
<li>or: the triangular inequality that does work for angular distances can be expressed directly in terms of the cosines;</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orge47be08" class="outline-3">
<h3 id="orge47be08"><span class="section-number-3">11.32.</span> <span class="todo TODO">TODO</span> Metric learning</h3>
<div class="outline-text-3" id="text-11-32">
<p>
<a href="https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity">https://developers.google.com/machine-learning/clustering/similarity/measuring-similarity</a>
</p>
</div>
</div>
<div id="outline-container-org5d373ed" class="outline-3">
<h3 id="org5d373ed"><span class="section-number-3">11.33.</span> Compressing Models</h3>
<div class="outline-text-3" id="text-11-33">
<p>
<b>Pruning</b> - methodology for inducing sparsity in weights and activations is called pruning. We can prune
 activations (usully), weights (less common) and biases. weights which match the pruning criteria are assigned
 a value of zero.
</p>

<p>
<b>Regularization</b> - any modification we make to a learning algorithm that is intended to reduce its
 generalization error, but not its training error.
</p>

<p>
<b>Quantization</b> - the process of reducing the number of bits that represent a number. FP32 or to 4/2/1-bits.
</p>

<p>
<b>Knowledge Distillation</b> or "teacher-student" - a small model is trained to mimic a pre-trained, larger model
 (or ensemble of models).
</p>

<p>
<b>Conditional Computation</b> - each input sample uses a different part
 of the model, latency is reduced.
</p>
<ul class="org-ul">
<li>ensembling with models trained at different tasks</li>
<li>cascading - boosting?</li>
<li>add small layers that learn which filters to use per input sample, and then enforce that during inference</li>
</ul>

<p>
<a href="https://intellabs.github.io/distiller">https://intellabs.github.io/distiller</a>
</p>
</div>
<div id="outline-container-orge935861" class="outline-4">
<h4 id="orge935861"><span class="section-number-4">11.33.1.</span> Knowledge Distillation</h4>
<div class="outline-text-4" id="text-11-33-1">
<p>
Student is a model with smaller number of parameters compared to the teacher model.
</p>
<ul class="org-ul">
<li>2006 <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf</a></li>
<li>2015 <a href="https://arxiv.org/abs/1503.02531">https://arxiv.org/abs/1503.02531</a></li>
<li>output logits can providing information as to which classes the teacher found more similar to the predicted
class.</li>
</ul>

<p>
problem: correct class 1, others close to 0
</p>
<ul class="org-ul">
<li>solution: temperature parameter - here is softmax function parameterized by the temperature:
<ul class="org-ul">
<li>pi = exp(zi/T)/ ∑j exp(zj/T), T=1&#x2026;∞, zi output of teacher.</li>
<li>probability pi of class i</li>
</ul></li>
</ul>

<p>
problem: teacher may be wrong
</p>
<ul class="org-ul">
<li>solution: calculate the "standard" loss between the student's predicted class probabilities and the
ground-truth labels - called <b>student loss</b> (with T=1)</li>
</ul>

<p>
overall loss function: L = α*H(y, σ(Zs)) + β*H(σ(Zt;T = τ), σ(Zs;T = τ))
</p>
<ul class="org-ul">
<li>τ, α and β are hyper parameters.</li>
<li>y is the ground truth label,</li>
<li>H is the cross-entropy loss function</li>
<li>σ is the softmax function, parametrized or not</li>
<li>Zs, Zt - are the logits of the student and teacher respectively.</li>
<li>β=1−α.</li>
<li>Some set α=1 while leaving β tunable. or α=β=0.5</li>
</ul>


<div id="orge18358c" class="figure">
<p><img src="./imgs/knowledge_distillation.png" alt="knowledge_distillation.png" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orga1f0a3f" class="outline-3">
<h3 id="orga1f0a3f"><span class="section-number-3">11.34.</span> Bayesian network</h3>
<div class="outline-text-3" id="text-11-34">
<p>
is a set of variables and their conditional dependencies. Directed acyclic graph (DAG). causal notation.
</p>

<p>
Ideal for taking an event that occurred and predicting the likelihood that any one of several possible known
 causes was the contributing factor.
</p>
</div>
</div>
</div>




<div id="outline-container-org6a8d0b4" class="outline-2">
<h2 id="org6a8d0b4"><span class="section-number-2">12.</span> Artificial Neural Network and deep learning <a id="org2601ee1"></a></h2>
<div class="outline-text-2" id="text-12">
<ul class="org-ul">
<li>международная конференция <a href="https://www.youtube.com/results?search_query=%23aijourney">https://www.youtube.com/results?search_query=%23aijourney</a></li>
<li><a href="https://developers.google.com/machine-learning/crash-course/ml-intro">https://developers.google.com/machine-learning/crash-course/ml-intro</a></li>
<li><a href="https://playground.tensorflow.org">https://playground.tensorflow.org</a></li>
<li>2019 course <a href="https://mlcourse.ai/">https://mlcourse.ai/</a></li>
<li>about playground <a href="https://cloud.google.com/blog/products/gcp/understanding-neural-networks-with-tensorflow-playground">https://cloud.google.com/blog/products/gcp/understanding-neural-networks-with-tensorflow-playground</a></li>
<li>сравнение <a href="https://en.wikipedia.org/wiki/Comparison_of_deep-learning_software">https://en.wikipedia.org/wiki/Comparison_of_deep-learning_software</a></li>
<li>article basic <a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/</a></li>
<li>math of ANN <a href="https://medium.com/deep-math-machine-learning-ai/chapter-7-artificial-neural-networks-with-math-bb711169481b">https://medium.com/deep-math-machine-learning-ai/chapter-7-artificial-neural-networks-with-math-bb711169481b</a></li>
<li>продвинутые лекции <a href="https://www.youtube.com/watch?v=92Ctk9OzlDg">https://www.youtube.com/watch?v=92Ctk9OzlDg</a></li>
<li>Geoffrey E. Hinton <a href="https://www.cs.toronto.edu/~hinton/">https://www.cs.toronto.edu/~hinton/</a></li>
</ul>
<p>
problems:
</p>
<ul class="org-ul">
<li>Catastrophic interference or catastrophic forgetting problem - forget previously learned information upon
learning new information <a href="https://en.wikipedia.org/wiki/Catastrophic_interference">https://en.wikipedia.org/wiki/Catastrophic_interference</a>
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">https://en.wikipedia.org/wiki/Feedforward_neural_network</a></li>
</ul></li>
<li>CNN and RNN tips <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks">https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks</a></li>
<li>book MIT Press <a href="http://www.deeplearningbook.org/">http://www.deeplearningbook.org/</a></li>
<li>механизм внимания ???</li>
<li>передаточная функция нейрона это сумматор и функция активации</li>
<li>скрытого уровня (hidden units) и нейронов-выходов (output units)</li>
<li>one <b>epoch</b> = one forward pass and one backward pass of all the training examples - чем больше epoch тем
больше модель требует именно такие входные данные</li>
<li><b>batch size</b> = the number of training examples in one forward/backward pass. The higher the batch size, the
more memory space you'll need. if you have 1000 training examples, and your batch size is 500, then it will
take 2 iterations to complete 1 epoch</li>
<li><b>Learning Rate</b> = см <a href="#org7ac0c5b">12.5.5</a></li>
<li>axon - отросток нейрона - выход</li>
<li>hyperparameter - дополнительная настройка для слоя</li>
<li>spatial (or temporal) dimension - пространственное или временное - 1D convolution layer (e.g. temporal convolution)</li>
<li>logits layer - last neuron layer - inverse of the sigmoid - from [0,1] to [-∞;+∞]</li>
<li>neural network - way of combining linear models. с нелинейными функциями</li>
<li>Линейная модель - y = xA + b    A ∈ Rnxm , b ∈ Rm , x - вектор - однослойная нейросеть  если все это от нелинейной функции</li>
<li>Функция активации - определяет выходной сигнал нейрона, который определяется входным сигналом или набором
входных сигналов</li>
<li>Функция потерь Loss function(or cost function) - является мерой расхождения между истинным значением оцениваемого параметра и
оценкой параметра. Используется как первый шаг в backward propogation</li>
<li>Negative Log-Likelihood(NLL)  L(y)=-log(y)</li>
<li>dense data (e.g. audio)</li>
<li>masking in RNN - allows us to handle variable length inputs in RNNs - going to be used to skip any input
with mask 0 by copying the previous hidden state of the cell;</li>
<li>waights initializations. Для разных моделей нужны разные инициализации. Нельзя нули - backward prop <a href="https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94">https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94</a></li>
</ul>

<p>
chars:
</p>
<ul class="org-ul">
<li>∘ Hadamard product (element-wise product) - a11*b11, a12*b12</li>
<li>⊕ element-wise plus</li>
<li>⊗ matrix multiplication</li>
</ul>
</div>
<div id="outline-container-orgafb7694" class="outline-3">
<h3 id="orgafb7694"><span class="section-number-3">12.1.</span> <span class="todo TODO">TODO</span> frameworks</h3>
<div class="outline-text-3" id="text-12-1">
</div>
<div id="outline-container-org220c4c5" class="outline-4">
<h4 id="org220c4c5"><span class="section-number-4">12.1.1.</span> history</h4>
<div class="outline-text-4" id="text-12-1-1">
<p>
Все поддерживают прикладной интерфейс OpenMP, языки Pyton, Java и C++ и платформу CUDA.
2022
</p>
<ul class="org-ul">
<li>TensorFlow.</li>
<li>Shogun.</li>
<li>Sci-Kit Learn.</li>
<li>PyTorch.</li>
<li>CNTK.</li>
<li>Apache MXNet.</li>
<li>H2O.</li>
<li>Apple's Core ML.</li>
</ul>

<p>
2017
</p>
<ul class="org-ul">
<li>TensorFlow</li>
<li>Theano</li>
<li>Keras</li>
<li>Lasagne</li>
<li>Caffe</li>
<li>DSSTNE</li>
<li>Wolfram Mathematica</li>
</ul>
</div>
</div>

<div id="outline-container-org4b890eb" class="outline-4">
<h4 id="org4b890eb"><span class="section-number-4">12.1.2.</span> list</h4>
<div class="outline-text-4" id="text-12-1-2">
<ul class="org-ul">
<li>PyTorch and Caffe2</li>
<li>Tensorflow</li>
<li>ONNX</li>
<li>Chainer, with extensions:
<ul class="org-ul">
<li>ChainerMN - usage on multiple GPUs with performance significantly faster than other deep learning frameworks</li>
<li>ChainerRL, ChainerCV and ChainerUI</li>
</ul></li>
<li>TVM <a href="https://tvm.apache.org/">https://tvm.apache.org/</a></li>
</ul>

<p>
<a href="https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software">https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software</a>
</p>
</div>
<ol class="org-ol">
<li><a id="orgd4dde5c"></a>retired<br />
<div class="outline-text-5" id="text-12-1-2-1">
<ul class="org-ul">
<li>MXNet - <a href="https://mxnet.apache.org">https://mxnet.apache.org</a></li>
<li>Darknet <a href="https://pjreddie.com/darknet/">https://pjreddie.com/darknet/</a></li>
<li>darknet-nnpack <a href="https://github.com/thomaspark-pkj/darknet-nnpack">https://github.com/thomaspark-pkj/darknet-nnpack</a></li>
<li>tiny-dnn <a href="https://github.com/tiny-dnn/tiny-dnn">https://github.com/tiny-dnn/tiny-dnn</a></li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-org8e2bb86" class="outline-3">
<h3 id="org8e2bb86"><span class="section-number-3">12.2.</span> History</h3>
<div class="outline-text-3" id="text-12-2">
<ul class="org-ul">
<li>1943 The perceptron was invented in 1943 by McCulloch and Pitts</li>
<li>1958 Frank Rosenblatt - perceptron implementation</li>
<li>1962 Widrow &amp; Hoff developed a learning procedure</li>
<li>1969 Perceptrons book shows limitation of Perceptrons by Marvin Minsky and Seymour Papert</li>
<li>1986 Backpropagation</li>
<li>1988 deep CNN - LeNet - for OCR <a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf</a></li>
<li>1997 Recurrent neural nerwork framework, LSTM by Schmidhuber &amp; Hochreiter</li>
<li>1998 Yann LeCun Deep Network - recognize handwritten ZIP codes on mailed envelopes</li>
<li>2010s, benefitting from cheap, powerful GPU-based computing systems</li>
<li>2010 CNN - AlexNet from Amazonwas the first winner of the ImageNet</li>
<li>2012 ResNet - Residual block</li>
<li>2014 - generative adversarial network (GAN)</li>
<li>2015 - Tensorflow</li>
<li>2016 - PyTorch</li>
<li>2016 DenseNet CNN architecture <a href="https://arxiv.org/abs/1608.06993">https://arxiv.org/abs/1608.06993</a></li>
<li>2016 - DyNet Dynamic Neural Networks</li>
<li>2017 Transformers - encoder–decoder architecture - Google - Attention is all you need paper</li>
<li>2018 BERT - Google  transformer-based -  language modeling, next sentence prediction</li>
<li>2018 AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks.</li>
<li>2018 GPT-1 OpenAI</li>
<li>2019 StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks.</li>
<li>2019 EfficientNet architecture -  применяется для обнаружения объектов -  <a href="https://arxiv.org/abs/1905.11946">https://arxiv.org/abs/1905.11946</a></li>
<li>2019 Tensorflow 2.0</li>
<li>2020 GPT-2/3 Micorosft - autoregressive language model that uses deep learning to produce human-like
text. GPT-3 – это ПО с закрытым программным кодом</li>
<li>2021 OpenAI company - DALL·E сеть, это версия GPT-3, генерировать изображения из текстовых описаний на
датасете из пар текст-изображение</li>
<li>2021 SberDevices ruGPT-3 (ruDALL-E Kandinsky) с действительно, открытым кодом.</li>
<li>2021 CLIP, CogView</li>
<li>2022 Stable Diffusion, Mindjourney, Chat GPT</li>
<li>2023 GPT-4</li>
</ul>

<p>
ResNet, ResNext, EfficientNet, EfficientDet, SSD, MaskRCNN, Unet, VNet, BERT, GPT-2, Tacotron2 and WaveGlow
</p>
</div>
<ol class="org-ol">
<li><a id="org15b167a"></a>links<br />
<div class="outline-text-5" id="text-12-2-0-1">
<ul class="org-ul">
<li><img src="https://miro.medium.com/max/720/1*vuW6DmvB7PeNM8vddesC3Q.webp" alt="1*vuW6DmvB7PeNM8vddesC3Q.webp" /></li>
<li><a href="https://www.tadviser.ru/index.php/%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F:%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_(Machine_Learning)">https://www.tadviser.ru/index.php/%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F:%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_(Machine_Learning)</a></li>
</ul>
</div>
</li>
</ol>
<div id="outline-container-orgea081cf" class="outline-4">
<h4 id="orgea081cf"><span class="section-number-4">12.2.1.</span> Перцептрон</h4>
<div class="outline-text-4" id="text-12-2-1">
<ul class="org-ul">
<li><a href="http://neerc.ifmo.ru/wiki/index.php?title=%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8,_%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD">http://neerc.ifmo.ru/wiki/index.php?title=%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8,_%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD</a></li>
<li>Возможности и ограничения перцептронов <a href="https://ru.wikipedia.org/wiki/%D0%92%D0%BE%D0%B7%D0%BC%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D0%B8_%D0%B8_%D0%BE%D0%B3%D1%80%D0%B0%D0%BD%D0%B8%D1%87%D0%B5%D0%BD%D0%B8%D1%8F_%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD%D0%BE%D0%B2">https://ru.wikipedia.org/wiki/%D0%92%D0%BE%D0%B7%D0%BC%D0%BE%D0%B6%D0%BD%D0%BE%D1%81%D1%82%D0%B8_%D0%B8_%D0%BE%D0%B3%D1%80%D0%B0%D0%BD%D0%B8%D1%87%D0%B5%D0%BD%D0%B8%D1%8F_%D0%BF%D0%B5%D1%80%D1%86%D0%B5%D0%BF%D1%82%D1%80%D0%BE%D0%BD%D0%BE%D0%B2</a></li>
<li>S, A, R слоя</li>
<li>Нейрон - покой или возбуждение - сумматор + функция активации</li>
<li>связь с весом.</li>
<li>каждый -
<ul class="org-ul">
<li>сумматор of boolean inputs ∑wixi-θ (dot product of two vectors w x) θ - bias constant or threshold</li>
<li>функция активации
<ul class="org-ul">
<li>threshold function 1 если ∑wixi-θ&gt;0(or ∑wixi&gt;θ) иначе 0. w и х - вектора</li>
</ul></li>
</ul></li>
</ul>
<p>
f(x) = sign(∑wixi-θ).
Обучение:
</p>
<ul class="org-ul">
<li>Метод коррекции ошибки - метод обучения перцептрона. Сначала весы случайные, не изменяется пока правильно,
если неправильно, то прибавляется или вычисляется что-то</li>
<li>Backpropagation метод обратного распространения ошибки "the backward propagation of errors," - метод
вычисления градиента, который используется при обновлении весов многослойного перцептрона
<ul class="org-ul">
<li>gradient of the loss function</li>
<li>передаточная функция нейрона должна быть дифференцируема</li>
</ul></li>
</ul>

<p>
Note:
</p>
<ul class="org-ul">
<li>Single layer perceptrons are only capable of learning <b>linearly separable</b> patterns. Два множества точек в
двумерном пространстве называются линейно сепарабельными (линейно разделимыми), если они могут быть
полностью отделены единственной прямой</li>
<li>dot product - quantifies how much one vector is going in the direction of the other</li>
</ul>

<p>
<b>теорема о сходимости перцептрона</b> FEC - независимо от начальных значений коэффициентов и порядка показа образцов
 при обучении, перцептрон за конечное число шагов научится различать два класса объектов, если только
 существует такая классификация
</p>
</div>
</div>
</div>

<div id="outline-container-org5ddd1e8" class="outline-3">
<h3 id="org5ddd1e8"><span class="section-number-3">12.3.</span> Evolution of Deep Learning</h3>
<div class="outline-text-3" id="text-12-3">
<ul class="org-ul">
<li>Statistical Modeling - math models and statistics based on insights and patterns observed in the data</li>
<li>Native Deep Learning - for every unique task, a new dataset was curated and a model was trained from scratch.</li>
<li>Transfer learning - even with smaller datasets, effective models could be developed by transferring knowledge.</li>
<li>Foundational Models - Transformers, possible to tran massive models and massive datasets, LLM.</li>
<li>AGI - every single task can be solved in zero-shot, without training</li>
</ul>
</div>
</div>
<div id="outline-container-org6fbf01d" class="outline-3">
<h3 id="org6fbf01d"><span class="section-number-3">12.4.</span> persons</h3>
<div class="outline-text-3" id="text-12-4">
<ul class="org-ul">
<li>Джефри Хинтон - Hinton - прижизненную славу классика, статьи в Nature</li>
<li>Ян Лекун - LeCun</li>
<li>Иешуа Бенджо - Bengio</li>
<li>Владимир Вапник</li>
<li>Эндрю Ын - Baidu - связал глубинное обучение с графическими процессорами</li>
<li>Christian S. Perone ML Research Engineer in London/UK <a href="https://blog.christianperone.com/">https://blog.christianperone.com/</a></li>
</ul>
<p>
google
</p>
<ul class="org-ul">
<li><a href="https://scholar.google.com/citations?view_op=search_authors&amp;hl=ru&amp;mauthors=label:machine_learning">https://scholar.google.com/citations?view_op=search_authors&amp;hl=ru&amp;mauthors=label:machine_learning</a></li>
<li><a href="https://scholar.google.com/citations?hl=ru&amp;view_op=search_authors&amp;mauthors=label:deep_learning">https://scholar.google.com/citations?hl=ru&amp;view_op=search_authors&amp;mauthors=label:deep_learning</a></li>
</ul>
</div>
</div>
<div id="outline-container-org2da93f9" class="outline-3">
<h3 id="org2da93f9"><span class="section-number-3">12.5.</span> Theory basis</h3>
<div class="outline-text-3" id="text-12-5">
</div>
<div id="outline-container-org41784ff" class="outline-4">
<h4 id="org41784ff"><span class="section-number-4">12.5.1.</span> NN definition (stanford)</h4>
<div class="outline-text-4" id="text-12-5-1">
<p>
NN consist of Trashold Login Unit (TLU):
</p>
<ul class="org-ul">
<li>inputs X</li>
<li>weights W</li>
<li>activation (treshold for perceptron) function
<ul class="org-ul">
<li>sum</li>
<li>treshold</li>
<li>bias (optional)</li>
</ul></li>
</ul>

<p>
TLU as dot product: X*W
</p>


<p>
f(x) = 1 if w*x+b&gt;0, 0 otherwhise
</p>
<ul class="org-ul">
<li>w and x is vectors</li>

<li><a href="https://ai.stanford.edu/~nilsson/MLBOOK.pdf">https://ai.stanford.edu/~nilsson/MLBOOK.pdf</a></li>
</ul>

<pre class="example">
   x1
   *\
     \
      \w1
       \         threshold
   x2 w2\       /
   *-----(∑)---[]-----&gt; f
        /
     w3/
      /
  x3 /
   */
</pre>
</div>
<ol class="org-ol">
<li><a id="org752b292"></a>weights, filters<br />
<div class="outline-text-5" id="text-12-5-1-1">
<p>
Each neuron in a neural network computes an output value by applying a specific function to the input values
  received from the receptive field in the previous layer. The function that is applied to the input values is
  determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively
  adjusting these biases and weights.
</p>

<p>
The vectors of weights and biases are called <b>filters</b> and represent particular <b>features</b> of the input (e.g., a
  particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This
  reduces the <b>memory footprint</b> because a single bias and a single vector of weights are used across all
  receptive fields that share that filter, as opposed to each receptive field having its own bias and vector
  weighting.
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org121b0a7" class="outline-4">
<h4 id="org121b0a7"><span class="section-number-4">12.5.2.</span> activation functions</h4>
<div class="outline-text-4" id="text-12-5-2">
<p>
∇ - nabla, gradient, pronounced "del", vector differential operator - result is a vector of partial derivatives
</p>

<p>
types:
</p>
<ul class="org-ul">
<li>saturating if lim-&gt;inf |∇f(u)| = 0</li>
<li>nonstaturating, such as ReLU (may be better, as they don't suffer from vanishing gradient)</li>
</ul>
<p>
types2:
</p>
<ul class="org-ul">
<li>Liner</li>
<li>ReLU max(0, a+v'b)</li>
<li>Heaviside</li>
<li>Logistic</li>
</ul>

<p>
function with vector result:
</p>
<ul class="org-ul">
<li><p>
softmax - range (0,1) - same count as inputs, np.exp(a)/np.sum(np.exp(a))
</p>
<ul class="org-ul">
<li>used as the last activation function, to normalize the output of a network to a probability distribution</li>
</ul>
<p>
over predicted output classes
</p>
<ul class="org-ul">
<li>the components will add up to 1</li>
</ul></li>
<li>maxout - range (-inf,inf) - max(z1,z2,z3)
<ul class="org-ul">
<li>can be interpreted as making a piecewise linear approximation to an arbitrary convex function</li>
</ul></li>
</ul>

<p>
from lowest→highest performing): logistic → tanh → ReLU → Leaky ReLU → ELU → SELU
</p>
<ul class="org-ul">
<li>to combat neural network overfitting: ReLU</li>
<li>reduce latency at runtime: leaky ReLU</li>
<li>for massive training sets: PReLU</li>
<li>for fast inference times: leaky ReLU</li>
<li>if your network doesn’t self-normalize: ELU</li>
<li>for an overall robust activation function: SELU</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org9653070"></a>most common<br />
<div class="outline-text-5" id="text-12-5-2-1">
<ul class="org-ul">
<li>Sigmoid - 1/(1+e<sup>-z</sup>) - range (0,1)</li>
<li>Liner - sum(w*x+b) - usually used for</li>
</ul>
<p>
regression problems
</p>
<ul class="org-ul">
<li>Tanh - Hyperbolic Tangent - range(-1,1) - faster, problem:vanishing gradient</li>
<li>ReLU - gradient vanishes when z&lt;0</li>
<li>softmax</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org86719f0" class="outline-4">
<h4 id="org86719f0"><span class="section-number-4">12.5.3.</span> Regularization</h4>
<div class="outline-text-4" id="text-12-5-3">
<p>
Tech for prevent overfitting (Early stopping, L1 and L2 Regularization, Dropout) - L1, L2 adds penalty to loss function
</p>

<p>
The Objective is maximizing the depth of the target convolutional neural network. Two constraints:
</p>
<ul class="org-ul">
<li>c-value of each layer should not be too small - measuring the capacity of a convolutional layer can learn new
and more complex patterns</li>
<li>the receptive field of the topmost convolutional layer in the feature-level should no larger than the size of input image</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgd9dc109"></a>Dilution or dropout<br />
<div class="outline-text-5" id="text-12-5-3-1">
<ul class="org-ul">
<li>Dilution refers to thinning weights</li>
</ul>

<p>
weak dilution and strong dilution
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org9e1cace" class="outline-4">
<h4 id="org9e1cace"><span class="section-number-4">12.5.4.</span> loss functions</h4>
<div class="outline-text-4" id="text-12-5-4">
<p>
for classification:
</p>
<ul class="org-ul">
<li>Quadratic</li>
<li>Cross-entropy</li>
<li>Likelihood - usually useed with softmax activation - equivalent to cross entropy, but for multiple</li>
</ul>
<p>
outcomes
</p>

<p>
for regression: - MSE
</p>

<p>
classification:
</p>
<ul class="org-ul">
<li>Binary Cross-Entropy Loss / Log Loss</li>
<li>Hinge Loss</li>
</ul>

<p>
Regression Losses:
</p>
<ul class="org-ul">
<li>Mean Square Error / Quadratic Loss / L2 Loss</li>
<li>Mean Absolute Error / L1 Loss</li>
<li>Huber Loss / Smooth Mean Absolute Error</li>
<li>Log-Cosh Loss</li>
<li>Quantile Loss</li>
</ul>
</div>
</div>

<div id="outline-container-org664f37f" class="outline-4">
<h4 id="org664f37f"><span class="section-number-4">12.5.5.</span> Backpropagation <a id="org9ef9da2"></a></h4>
<div class="outline-text-4" id="text-12-5-5">
<p>
As long as the activation function is differentiable, the whole neural network can be regarded as a
 differentiable function which can be opimized by gradient discent method.
</p>

<p>
ReLU - Non-differentiable at zero; however, it is differentiable anywhere else, and the value of the
 derivative at zero can be arbitrarily chosen to be 0 or 1.
</p>

<p>
way to optimize neural networks, stochastic gradient descent (SGD) is one of the most popular
</p>
<ul class="org-ul">
<li>article <a href="http://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BE%D1%88%D0%B8%D0%B1%D0%BA%D0%B8">http://neerc.ifmo.ru/wiki/index.php?title=%D0%9E%D0%B1%D1%80%D0%B0%D1%82%D0%BD%D0%BE%D0%B5_%D1%80%D0%B0%D1%81%D0%BF%D1%80%D0%BE%D1%81%D1%82%D1%80%D0%B0%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BE%D1%88%D0%B8%D0%B1%D0%BA%D0%B8</a></li>
<li>Stanford lect <a href="https://www.youtube.com/watch?v=isPiE-DBagM">https://www.youtube.com/watch?v=isPiE-DBagM</a></li>
<li>forward pass for the values</li>
<li>backward pass for the gradients</li>
</ul>
<p>
Перенумеруем все узлы (включая входы и выходы) числами от 1 до N.
</p>
<ul class="org-ul">
<li>wij - вес от i до j узла.</li>
<li>training examples - (x1,x2,t) where x1x2 - inputs, t correct output</li>
<li>common method for measuring the discrepancy between the expected output t and the actual output y
(discrepancy or error): E = 1/2*(t-y)<sup>2</sup> - по методу наименьших квадратов. 1/2 не имеет роли, так как
изчезает при дифференцировании.</li>
</ul>

<p>
Алгоритм: BackPropagation (η,α,{xid,td}, steps) - i step, d - количество samles, η - скорость <b>learning rate</b>
<a id="org7ac0c5b"></a>, α — коэффициент инерциальности для сглаживания резких скачков при перемещении по
поверхности целевой функции
</p>

<ol class="org-ol">
<li>wij - маленькими случайными значениями</li>
<li>steps раз i = 1&#x2026;n:
<ol class="org-ol">
<li>подаем {xid}=(1,1,0) {td} - вектор выходов без ошибки.</li>
<li>Для всех k∈Outpits δk=ok(1-ok)(tk-ok)</li>
<li>для уровней j начиная с последнего δj=oj*(1-oj)*[k∈Children(j)]∑δk*wjk</li>
<li>для всех ребра в итерации n
<ul class="org-ul">
<li>Δwij(n)= α*Δwij(n-1)+(1-α)*η*δj*oi</li>
<li>wij(n)=wij(n-1) + Δwij(n)</li>
</ul></li>
</ol></li>
<li>добавлять к каждому весу Δwij = -n*∂E/∂wij где 0&lt;n&lt;1 - задает скорость движения это и есть</li>
<li>выражать поправку для узла более низкого уровня (входа) через поправки более высокого (выход)</li>
</ol>


<p>
Недостатки алгоритма:
</p>
<ul class="org-ul">
<li>Паралич сети - значения весов могут в результате коррекции стать очень большими величинамий - Обычно этого
избегают уменьшением размера шага η, но это увеличивает время обучения</li>
<li>Локальные минимумы - осуществляет спуск вниз по поверхности ошибки, Поверхность ошибки сложной сети сильно
изрезана и состоит из холмов, долин, Сеть может попасть в локальный минимум (неглубокую долину), когда рядом
имеется гораздо более глубокий минимум.</li>
<li>Размер шага - Размер шага должен браться конечным. Если размер шага фиксирован и очень мал, то сходимость
слишком медленная, если же он фиксирован и слишком велик, то может возникнуть паралич или постоянная
неустойчивость. Эффективно увеличивать шаг до тех пор, пока не прекратится улучшение оценки в данном
направлении антиградиента и уменьшать, если такого улучшения не происходит</li>
</ul>

<p>
Gradient (nabla) ∇f(x,y,z) = (∂f/∂x, ∂f/∂y, ∂f/∂z)
</p>
</div>

<ol class="org-ol">
<li><a id="org91b49b1"></a>Gradient discent и его виды (finding the minimum of a function)<br />
<div class="outline-text-5" id="text-12-5-5-1">
<p>
Gradient descent is based on the observation: F(x)  decreases fastest if x goes in direction of the negative gradient of F
</p>

<p>
метод нахождения локального экстремума (минимума или максимума) функции с помощью движения вдоль
градиента. Основная идея метода заключается в том, чтобы идти в направлении наискорейшего спуска, а это
направление задаётся антиградиентом -∇F(xj) or -∇θJ(θ).
</p>
<ul class="org-ul">
<li>F(v):X-&gt;R</li>
<li>v{j+1} = xj-λ*∇F(xj) где λ - задает скорость</li>
<li>or θ = θ+Δθ = θ - η*</li>
</ul>

<p>
Если нужно минимизировать функцию ошибок E(wij)
</p>
<ul class="org-ul">
<li>добавлять к весу будем дельту  Δwij = -n*∂E/∂wij где n = λ</li>
</ul>

<p>
3 Types of Gradient Descent:
</p>
<ol class="org-ol">
<li><b>Stochastic gradient descent</b> - method uses randomly selected (or shuffled) samples to evaluate the
gradients - calculates the error and updates the model for each example - функция ошибок имеет свойство
аддитивности - на всем наборе = сумма для каждой точки.
<ul class="org-ul">
<li>pro
<ul class="org-ul">
<li>вычилсяем градиент на одной точке.</li>
<li>simplest to understand and implement, especially for beginners</li>
<li>increased model update frequency can result in faster learning on some problems.</li>
<li>he noisy update process can allow the model to avoid local minima (e.g. premature convergence).</li>
</ul></li>
<li>con
<ul class="org-ul">
<li>complicates convergence to the exact minimum</li>
<li>The noisy learning process down the error gradient can also make it hard for the algorithm to settle on
an error minimum for the model.</li>
</ul></li>
</ul></li>
<li>Batch gradient descent - calculates the error for each example in the training dataset, but only updates
the model after all training examples have been evaluated - epoch
<ul class="org-ul">
<li>more stable error gradient and may result in a more stable convergence on some problems.</li>
<li>more computationally efficient and parallel processing based implementations</li>
</ul></li>
<li>Mini-batch gradient descent - takes the best of both worlds - используется в нейронных сетях</li>
</ol>

<p>
Chalanges of Mini-batch g d:
</p>
<ul class="org-ul">
<li>Choosing a proper learning rate can be difficult</li>
<li>schedules and thresholds - have to be defined in advance and are thus unable to adapt to a dataset's
characteristics</li>
<li>If our data is sparse - we might not want a larger update for rarely occurring features</li>
<li>avoiding getting trapped in their numerous suboptimal local minima - saddle point</li>
</ul>

<p>
<b>gradient clipping</b> used for SGD commonly occur in recurrent networks in the area where the recurrent network
 behaves approximately linearly.
</p>
</div>
</li>
<li><a id="org8a23010"></a>gradient<br />
<div class="outline-text-5" id="text-12-5-5-2">
<ul class="org-ul">
<li>∇F оператора набла</li>
<li>grad F</li>
</ul>
<p>
градиент функции φ в точке x перпендикулярен её линии уровня
</p>

<p>
F = x*2, gradF = 2*x
</p>

<p>
x - 0.01*(2*x)
</p>
<ul class="org-ul">
<li>0.01 - ηx</li>
</ul>
</div>
</li>
<li><a id="orgd47791a"></a>optimization algorithms - виды оптимизационных алгоритмов<br />
<div class="outline-text-5" id="text-12-5-5-3">
<ul class="org-ul">
<li><a href="http://ruder.io/optimizing-gradient-descent/">http://ruder.io/optimizing-gradient-descent/</a></li>
</ul>


<p>
Optimization Problem Types - Convex Optimization
</p>
<ul class="org-ul">
<li>convex - выпуклый - one optimal solution, which is globally optimal or you might prove
that there is no feasible solution to the problem. is at least NP-hard
<ul class="org-ul">
<li>potentially many local minima</li>
<li>Saddle points</li>
<li>Very flat regions</li>
<li>Widely varying curvature</li>
</ul></li>
<li>non-stationary and non-convex problems - optimization may have multiple locally optimal points and it can
take a lot of time to identify whether the problem has no solution or if the solution is global. Hence, the
efficiency in time of the convex optimization problem is much better.</li>
</ul>


<p>
terms:
</p>
<ul class="org-ul">
<li>Momentum - retained gradient is multiplied by a value called "Coefficient of Momentum" which is the
percentage of the gradient retained every iteration. preventing oscillations [ɒsɪˈleɪʃnz]</li>
<li>Averaging -  records an average of its parameter vector over time w=1/t*[t]∑wi</li>
<li>Adagrad - adaptive gradient algorithm. still has a base learning rate - сохраняет все градиенты</li>
<li>RMSProp - Root Mean Square Propogation</li>
<li>Nesterov (NAG) - more accurate step in the gradient direction by updating the parameters with the momentum
step before computing the gradient</li>
<li>Adadelta</li>
<li>Adam - RMSprop and momentum</li>
<li>AdaMax</li>
<li>Nadam - Adam and NAG</li>
<li>AMSGrad</li>
</ul>

<p>
<b>Which to use?</b>
</p>
<ul class="org-ul">
<li>data is sparse - adaptive learning-rate methods</li>
<li>Adam - might be the best overall choice</li>
<li>SGD without momentum and a simple learning rate annealing schedule - slow but efficient</li>
</ul>

<p>
see <a href="#orgd376911">40.1.5</a>
</p>

<p>
From simple to complex:
</p>
<ul class="org-ul">
<li>GD</li>
<li>SGD - lr should be set,  solution may be trapped at the saddle point</li>
<li>NAG - accumulating the previous gradient as <b>momentum</b> to accelerate the current gradient - difficult to choose a suitable</li>
</ul>
<p>
learning rate.
</p>
<ul class="org-ul">
<li>AdaGrad - learning rate is adaptively adjusted according to the sum of the squares of all historical
gradients. - training time increases, the accumulated gradient will become larger and larger, making the
learning rate tend to zero.</li>
<li>Adam - Combine the adaptive methods and the momentum method.</li>
</ul>
</div>
</li>

<li><a id="org640dfa5"></a>Gradient averaging<br />
<div class="outline-text-5" id="text-12-5-5-4">
<ul class="org-ul">
<li><a href="https://gchlebus.github.io/2018/06/05/gradient-averaging.html">https://gchlebus.github.io/2018/06/05/gradient-averaging.html</a></li>
</ul>
<p>
technique -  compute gradients in each iteration and apply an average of them less frequently
</p>
</div>
</li>
<li><a id="org860cd0c"></a>SGD with momentum, Nesterov<br />
<div class="outline-text-5" id="text-12-5-5-5">
<p>
Momentum is a method that helps accelerate SGD
</p>
<ul class="org-ul">
<li>usually 0.9</li>
<li>v = self.momentum * m - lr * g   # velocity, m-moment(previous Vt-1), g-gradient</li>
<li>Nesterov: new<sub>p</sub> = p + self.momentum * v - lr * g</li>
<li>NoNesterov: new<sub>p</sub> = p + v 	# p-parameter</li>
</ul>

<p>
Generally momentum is set to 0.5 until the initial learning stabilizes and then is increased to 0.9 or higher
</p>

<p>
Decay:
</p>
<ul class="org-ul">
<li>lr * 1/ (1+ decay * iterations)</li>
<li>1e-6 * 1 / (1 + 0.8 *20) = 5.88235294117647e-08</li>
<li>1e-6 * 1 / (1 + 0.999 *20) = 4.766444232602478e-08</li>
</ul>


<p>
simple:
</p>
<ul class="org-ul">
<li>params = params - learning<sub>rate</sub> * params<sub>grad</sub></li>
</ul>
<p>
moment:
</p>
<ul class="org-ul">
<li>params = params - (momentum* Ut-1 + learning<sub>rate</sub> * params<sub>grad</sub>)</li>
</ul>
<p>
Nesterov:
</p>
<ul class="org-ul">
<li>?</li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-org2f886bb" class="outline-4">
<h4 id="org2f886bb"><span class="section-number-4">12.5.6.</span> limits of NN</h4>
<div class="outline-text-4" id="text-12-5-6">
<ul class="org-ul">
<li>overfit</li>
<li>Data is biased</li>
<li>easy to fool</li>
<li>prone to catastrophic forgetting</li>
<li>multitask? general intelligenece?</li>
<li>Explainable / interpretable AI</li>
<li>do not generalize to different viewpoints - can be forced to interpolate with enough data (generalization), but cannot extrapolate.</li>
<li>AIs do not form their own goals</li>
</ul>
</div>
</div>
<div id="outline-container-orga121ed5" class="outline-4">
<h4 id="orga121ed5"><span class="section-number-4">12.5.7.</span> Self-organization</h4>
<div class="outline-text-4" id="text-12-5-7">
<ul class="org-ul">
<li>statistical approach - tries to extract the most relevant information from the distribution of unlabeled data (autoencoders, etc).</li>
<li>self-organization - tries to understand the principles of organization of natural systems and use them to create efficient algorithms.</li>
</ul>
</div>
</div>

<div id="outline-container-orgb5aa143" class="outline-4">
<h4 id="orgb5aa143"><span class="section-number-4">12.5.8.</span> <span class="todo TODO">TODO</span> Universal approximation theorem</h4>
<div class="outline-text-4" id="text-12-5-8">
<p>
put limits on what neural networks can theoretically learn.
</p>

<p>
Neural networks with an unbounded (non-polynomial) activation function have the <b>universal approximation
 property</b>. (non linear activation function also)
</p>
</div>
</div>
</div>
<div id="outline-container-org2c6349a" class="outline-3">
<h3 id="org2c6349a"><span class="section-number-3">12.6.</span> STEPS</h3>
<div class="outline-text-3" id="text-12-6">
<ul class="org-ul">
<li>task type - classification, regression, etc..</li>
<li>final layout for model - multiclassification</li>
<li>select loss function</li>
<li>data augmentation
<ul class="org-ul">
<li>preprocess and save on hard disk most of the work</li>
<li>create dataset with links to files</li>
<li>map function "encode<sub>single</sub><sub>sample</sub>" to dataset - read links and simple encoding only</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgd868be3" class="outline-3">
<h3 id="orgd868be3"><span class="section-number-3">12.7.</span> Конспект универ</h3>
<div class="outline-text-3" id="text-12-7">
</div>
<div id="outline-container-orgcb90a09" class="outline-4">
<h4 id="orgcb90a09"><span class="section-number-4">12.7.1.</span> введение</h4>
<div class="outline-text-4" id="text-12-7-1">
<p>
иск. инт - научная дисциплина на стыке кибернетики, лингвистики, психологии, программирования
</p>

<p>
ССИ = знания + стратегия обработки знаний.
</p>

<p>
Функции СИИ:
</p>
<ol class="org-ol">
<li>представление - 1,3 связаны</li>
<li>обучение - на стыке обоих</li>
<li>рассуждение - способность решать задачи</li>
</ol>

<p>
Условия разумности системы:
</p>
<ul class="org-ul">
<li>описывать и решать широкий круг задач</li>
<li>понимать явную и неявную инфу</li>
<li>иметь механизм управления определяющий операции, выполняемые для решения отдельных задач</li>
</ul>
<p>
Типа поиск
</p>
<ol class="org-ol">
<li>правила -&gt;2</li>
<li>данные(области)</li>
<li>управ воздействие -&gt;1</li>
</ol>
</div>
</div>

<div id="outline-container-org43b16b5" class="outline-4">
<h4 id="org43b16b5"><span class="section-number-4">12.7.2.</span> Обучение</h4>
<div class="outline-text-4" id="text-12-7-2">
<p>
Простейшая модель с обратной связью:
</p>
<ol class="org-ol">
<li>Среда - воздействие</li>
<li>Элемент обучения - знания</li>
<li>База знаний - решение</li>
<li>Исполнительный элемент -&gt;1 обратная связь.</li>
</ol>

<p>
Способы
</p>
<ul class="org-ul">
<li>индивидуальный - общие шаблоны и правила создаются на основе практического опыта ( на основе подобия потоков данных)</li>
<li>дедуктивный - для определения конкретных фактов используются общие факты ( док-во теорем)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orga13decc" class="outline-3">
<h3 id="orga13decc"><span class="section-number-3">12.8.</span> Data Augmentation</h3>
<div class="outline-text-3" id="text-12-8">
</div>
<div id="outline-container-orge689c6c" class="outline-4">
<h4 id="orge689c6c"><span class="section-number-4">12.8.1.</span> image libraries</h4>
<div class="outline-text-4" id="text-12-8-1">
<p>
<a href="https://albumentations.ai/">https://albumentations.ai/</a> <a href="https://github.com/albumentations-team/albumentations">https://github.com/albumentations-team/albumentations</a>
</p>
<ul class="org-ul">
<li>part of the PyTorch ecosystem.</li>
<li>classification, semantic segmentation, instance segmentation, object detection, and pose estimation.</li>
<li>photos, medical images, satellite imagery, manufacturing and industrial applications, Generative Adversarial Networks.</li>
</ul>
</div>
</div>
<div id="outline-container-org0ecef6e" class="outline-4">
<h4 id="org0ecef6e"><span class="section-number-4">12.8.2.</span> CA conventional augmentation</h4>
<div class="outline-text-4" id="text-12-8-2">
<p>
affine transformation
</p>
</div>

<ol class="org-ol">
<li><a id="org9b55145"></a><span class="todo TODO">TODO</span> mixup<br /></li>
<li><a id="orgc6b8338"></a><span class="todo TODO">TODO</span> cutout<br /></li>
<li><a id="orgad2a2f3"></a><span class="todo TODO">TODO</span> random erasing<br /></li>
<li><a id="orgc4719f5"></a><span class="todo TODO">TODO</span> random image cropping and patching (RICAP)<br /></li>
<li><a id="orgf5352d5"></a><span class="todo TODO">TODO</span> cutout<br /></li>
<li><a id="orgabcd619"></a>example<br />
<div class="outline-text-5" id="text-12-8-2-6">
<p>
I used affine transformation for both training augmentation and testing augmentation. The training
augmentation is more aggressive comparing to the testing augmentation. For training, the scale range is
0.2~2.0, the shear range is -0.7~0.7, the ratio range is 0.6~1.4, the rotation range is –pi~pi; for testing
the scale range 0.6~1.4, the shear range is -0.5~0.5, the ratio range is 0.8~1.2, the rotation range is
–pi~pi.
</p>

<p>
All parameters are randomly sampled from uniform distribution
</p>

<p>
The stronger the fitting power a CNN has, the more aggressive augmentation should be applied.
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org4956c65" class="outline-4">
<h4 id="org4956c65"><span class="section-number-4">12.8.3.</span> <span class="todo TODO">TODO</span> AutoAugment method and Fast AutoAugment method</h4>
<div class="outline-text-4" id="text-12-8-3">
<ul class="org-ul">
<li>reducing the heuristics of data augmentation has attracted increasing attention</li>
<li>searches appropriate data augmentation policies using reinforcement learning</li>
</ul>
</div>
</div>
<div id="outline-container-org3cc404f" class="outline-4">
<h4 id="org3cc404f"><span class="section-number-4">12.8.4.</span> <span class="todo TODO">TODO</span> RandAugment</h4>
</div>
<div id="outline-container-org82e2b26" class="outline-4">
<h4 id="org82e2b26"><span class="section-number-4">12.8.5.</span> <span class="todo TODO">TODO</span> Self-paced Augmentation</h4>
<div class="outline-text-4" id="text-12-8-5">
<p>
<a href="https://arxiv.org/pdf/2010.15434.pdf">https://arxiv.org/pdf/2010.15434.pdf</a>
</p>
<ul class="org-ul">
<li>curriculum learning -  strategy that transitions training from easy to difficult samples in a gradual manner
<ul class="org-ul">
<li><a href="http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf">http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf</a></li>
</ul></li>
<li>change training loss</li>
</ul>

<p>
steps:
</p>
<ol class="org-ol">
<li>feed samples batch to NN</li>
<li>calc training loss but do not change weights</li>
<li>augmentation several samples in bath by using calced training loss ( if loss &gt; threshold)</li>
<li>feed this new batch</li>
</ol>
</div>
</div>
<div id="outline-container-orge5e0144" class="outline-4">
<h4 id="orge5e0144"><span class="section-number-4">12.8.6.</span> Data normalization and  Feature scaling <a id="org9fb3736"></a></h4>
<div class="outline-text-4" id="text-12-8-6">
<ul class="org-ul">
<li><a href="https://scikit-learn.org/0.22/modules/preprocessing.html">https://scikit-learn.org/0.22/modules/preprocessing.html</a></li>
<li><a href="https://scikit-learn.org/0.22/auto_examples/preprocessing/plot_all_scaling.html">https://scikit-learn.org/0.22/auto_examples/preprocessing/plot_all_scaling.html</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_scaling">https://en.wikipedia.org/wiki/Feature_scaling</a></li>
</ul>

<p>
<b>Standardization (Z-score Normalization)</b> mean removal and variance scaling
transform the data to center and scale it by dividing non-constant features - получить нулевое
матожидание(mean) и единичную дисперсию(np.std)
</p>
<ul class="org-ul">
<li>mean = 0 print(np.nanmean(data, axis=0))</li>
<li>std = 1 print(np.nanstd(data, axis=0))</li>
</ul>
<pre class="example">
scale = np.nanstd(data, axis=0)
data /= scale
mean = np.nanmean(data, axis=0)
data -= mean
</pre>


<p>
<b>Mean normalization</b>
</p>
<ul class="org-ul">
<li>data = (np.array(data) - np.mean(data)) / (max(data) - min(data))</li>
</ul>

<p>
<b>Scaling features to a range</b> or <b>min-max scaling</b> or min-max normalization
</p>
<ul class="org-ul">
<li>x<sub>norm</sub> = (x - x<sub>min</sub>)/(x<sub>max</sub> - x<sub>min</sub>) - [0,1]</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #fa8072;">#</span><span style="color: #99968b; font-style: italic;">min-max of [0, 1]</span>
<span style="color: #cae682;">data</span> = (np.array(data) - <span style="color: #e5786d;">min</span>(data))/ (<span style="color: #e5786d;">max</span>(data) - <span style="color: #e5786d;">min</span>(data))
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">or</span>
<span style="color: #cae682;">data_min</span> = np.nanmin(data, axis=0)
<span style="color: #cae682;">data_max</span> = np.nanmax(data, axis=0)
<span style="color: #cae682;">data</span> = (np.array(data) - data_min) / (data_max - data_min)
<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">or</span>
<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">scale10</span>(data: <span style="color: #e5786d;">list</span>) -&gt; <span style="color: #e5786d;">list</span>:
    <span style="color: #cae682;">data_min</span> = np.nanmin(data, axis=0)
    <span style="color: #cae682;">data_max</span> = np.nanmax(data, axis=0)

    <span style="color: #cae682;">scale</span> = (1 - 0) / (data_max - data_min)
    <span style="color: #cae682;">min_</span> = 0 - data_min * scale

    <span style="color: #cae682;">data</span> = np.array(data, dtype=np.<span style="color: #e5786d;">float</span>)
    <span style="color: #cae682;">data</span> = scale * data
    <span style="color: #cae682;">data</span> += min_
    <span style="color: #8ac6f2; font-weight: bold;">return</span> data
</pre>
</div>
</div>
</div>
<div id="outline-container-orgbb3d834" class="outline-4">
<h4 id="orgbb3d834"><span class="section-number-4">12.8.7.</span> Boosting</h4>
<div class="outline-text-4" id="text-12-8-7">
<ul class="org-ul">
<li>После первого обучения мы подготавливаем датасет выбирая чаще те значения которые показывали большую ошибку.</li>
</ul>
</div>
</div>
<div id="outline-container-orgdbc1e10" class="outline-4">
<h4 id="orgdbc1e10"><span class="section-number-4">12.8.8.</span> Input One-Hot Encode Контрастное кодирование</h4>
<div class="outline-text-4" id="text-12-8-8">
<ul class="org-ul">
<li><a href="https://www.researchgate.net/profile/Kedar_Potdar/publication/320465713_A_Comparative_Study_of_Categorical_Variable_Encoding_Techniques_for_Neural_Network_Classifiers/links/59e6f9554585151e5465859c/A-Comparative-Study-of-Categorical-Variable-Encoding-Techniques-for-Neural-Network-Classifiers.pdf">https://www.researchgate.net/profile/Kedar_Potdar/publication/320465713_A_Comparative_Study_of_Categorical_Variable_Encoding_Techniques_for_Neural_Network_Classifiers/links/59e6f9554585151e5465859c/A-Comparative-Study-of-Categorical-Variable-Encoding-Techniques-for-Neural-Network-Classifiers.pdf</a></li>
<li>One Hot Coding 1 - 001 - 2 - 010 3 - 100 Avoid OneHot for high cardinality columns and decision tree-based algorithms.</li>
<li>One-cold - 1 - 000 2 - 001 3 - 010 4 - 100</li>
<li>Ordinal coding один вход в виде числа 1 - 1  2 - 2</li>
<li>Binary Coding - 1 - 01 2 - 10 3 - 11</li>
<li>Sum coding - ?</li>
<li>Dummy coding
<ul class="org-ul">
<li>Nationality C1 	C2 	C3</li>
<li>French 	0 	0 	0 - control group</li>
<li>Italian 	1 	0 	0</li>
<li>German 	0 	1 	0</li>
<li>Other 	0 	0 	1</li>
</ul></li>
<li>Контрастное кодирование C1 - Французы и итальянцы имеют больший оптимизм по сравнению с немцами С2 -
французы и итальянцы имеют отличие в их оптимизме
<ul class="org-ul">
<li>Правила:
<ol class="org-ol">
<li>Сумма контрастных коэффициентов по каждой кодовой переменной (по всем группам) должна равняться нулю. В
нашем случае, 1/3 + 1/3 – 2/3 = 0, 1/2 – 1/2 + 0 = 0.</li>
<li>Разность между суммой положительных (различных) коэффициентов и суммой отрицательных (различных)
коэффициентов должна равняться 1. В нашем случае, 1/3 – (–2/3) = 1, 1/2 – (–1/2) = 1.</li>
<li>Кодовые переменные должны быть ортогональны
<ul class="org-ul">
<li>НациональностьC1 	C2</li>
<li>французы 	+0,33 	+0,50</li>
<li>итальянцы 	+0,33 	−0,50</li>
<li>немцы 	−0,66 	0</li>
</ul></li>
</ol></li>
</ul></li>
</ul>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Encoding Technique</td>
<td class="org-right">Accuracy (Percentage)</td>
</tr>

<tr>
<td class="org-left">One Hot Coding</td>
<td class="org-right">90</td>
</tr>

<tr>
<td class="org-left">Ordinal Coding</td>
<td class="org-right">81</td>
</tr>

<tr>
<td class="org-left">Sum Coding</td>
<td class="org-right">95</td>
</tr>

<tr>
<td class="org-left">Helmert Coding</td>
<td class="org-right">89</td>
</tr>

<tr>
<td class="org-left">Polynomial Coding</td>
<td class="org-right">91</td>
</tr>

<tr>
<td class="org-left">Backward Difference Coding</td>
<td class="org-right">95</td>
</tr>

<tr>
<td class="org-left">Binary Coding</td>
<td class="org-right">90</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="outline-container-org1dba684" class="outline-3">
<h3 id="org1dba684"><span class="section-number-3">12.9.</span> Major network Architectures</h3>
<div class="outline-text-3" id="text-12-9">
<ul class="org-ul">
<li><a href="https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32">https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32</a></li>
<li>ResNet - Residual network - y = f(x) +x</li>
<li>Highway Network - residual with sigmoid activation  y = f(x)*T(x) + x*(1-T(x) )<a href="https://github.com/trangptm/HighwayNetwork/blob/master/ConvHighway.py">https://github.com/trangptm/HighwayNetwork/blob/master/ConvHighway.py</a>
<ul class="org-ul">
<li>skip connection concept</li>
<li>like LSTM with forget gates g(x)*x + t(x)*h(x), ResNet - where g(x)=t(x)=const=1</li>
</ul></li>
<li>Dense Network TODO
<ul class="org-ul">
<li>skip connection concept at extreme</li>
<li>AveragePooling2D</li>
</ul></li>
</ul>

<p>
cuDNN orient: ResNet, ResNext, EfficientNet, EfficientDet, SSD, MaskRCNN, Unet, VNet, BERT, GPT-2, Tacotron2 and WaveGlow
</p>
</div>

<div id="outline-container-orge62418b" class="outline-4">
<h4 id="orge62418b"><span class="section-number-4">12.9.1.</span> Unet</h4>
<div class="outline-text-4" id="text-12-9-1">
<p>
<a href="https://arxiv.org/pdf/1505.04597">https://arxiv.org/pdf/1505.04597</a> 2015 U-Net: Convolutional Networks for Biomedical Image Segmentation
</p>

<p>
for
</p>
<ul class="org-ul">
<li>quantification tasks such as cell detection and shape measurements in biomedical image data</li>
<li>employed in diffusion models for iterative image denoising.</li>
</ul>

<p>
features:
</p>
<ul class="org-ul">
<li>have two parts: encoder and decoder - conventional autoencoder architecture</li>
<li>Skip Connections for each compression step to decoder corresponding upsampling step</li>
</ul>
<p>
-U-Net uses <b>padding and mirroring</b> to ensure that images can be segmented continuously,
 even at the borders. This is particularly important when dealing with large images where the resolution might
 be limited by GPU memory capacity.
</p>

<p>
compresses the input in <b>non linear way</b>
</p>

<p>
<b>upsampling blocks</b> - <a href="https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html">https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html</a>
</p>
<ul class="org-ul">
<li>nn.Upsample(scale<sub>factor</sub>=2, mode='bilinear'</li>
<li>(convolution =&gt; [BatchNorm] =&gt; ReLU) * 2</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #cae682;">x2</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.down1(x1)
<span style="color: #cae682;">x3</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.down2(x2)
<span style="color: #cae682;">x4</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.down3(x3)
<span style="color: #cae682;">x5</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.down4(x4)
<span style="color: #cae682;">x</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.up1(x5, x4)
<span style="color: #cae682;">x</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.up2(x, x3)
<span style="color: #cae682;">x</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.up3(x, x2)
<span style="color: #cae682;">x</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.up4(x, x1)
<span style="color: #cae682;">logits</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.outc(x)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orga23d0e5" class="outline-3">
<h3 id="orga23d0e5"><span class="section-number-3">12.10.</span> Activation Functions φ(net)</h3>
<div class="outline-text-3" id="text-12-10">
<p>
net = ∑wixi = x
</p>
<ul class="org-ul">
<li>threshold function Перцептрон</li>
<li><b>Sigmoid</b> - σ = L/(1+e<sup>(-k(x-x0))</sup> - R -&gt;(0,1) range - Used for the binary classification task.
<ul class="org-ul">
<li>L - curve's maximum value (1)</li>
<li>k - steepness ( крутизна) (1)</li>
<li>x0 - Sigmoid’s midpoint (0)</li>
</ul></li>
<li>Hyperbolic tangent <b>tanh</b> (x) = (1 - e<sup>-2x</sup>)/(1 + e<sup>-2x</sup>) - R -&gt;(-1;1) range</li>
<li>Rectified Linear Units (<b>ReLU</b>) or rectifier [ˈrektɪfaɪə] - f(net) = max(0,x) - neuron can die - never
activated
<ul class="org-ul">
<li>smooth approximation f(x) = ln(1+e<sup>x</sup>). Its derivative f'(x) = e<sup>x</sup>/(1+e<sup>x</sup>) = 1/(1+e<sup>-x</sup>)</li>
<li>Leaky and Parametric ReLU - attempt to fix the “dying ReLU” problem f(x)=0.01x (x&lt;0) and f(x)=x (x&gt;=0)</li>
<li>Gaussian Error Linear Unit (GELU) cdf = 0.5 * (1.0 + tf.tanh( (np.sqrt(2 / np.pi) * (x + 0.044715 *
tf.pow(x, 3))))).  f(x) = x*cdf</li>
</ul></li>
<li>Softmax - σ = e<sup>xi</sup>/∑e<sup>x</sup> - convert all nodes to [0,1] range
<ul class="org-ul">
<li>Used for multi-classification neural network output</li>
</ul></li>
<li>Maxout - f(x) = max(xi) - просто больший</li>
</ul>
</div>
<div id="outline-container-orgfd64a0b" class="outline-4">
<h4 id="orgfd64a0b"><span class="section-number-4">12.10.1.</span> links</h4>
<div class="outline-text-4" id="text-12-10-1">
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Activation_function">https://en.wikipedia.org/wiki/Activation_function</a></li>
<li><a href="https://medium.com/@cmukesh8688/activation-functions-sigmoid-tanh-relu-leaky-relu-softmax-50d3778dcea5">https://medium.com/@cmukesh8688/activation-functions-sigmoid-tanh-relu-leaky-relu-softmax-50d3778dcea5</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgfe5b879" class="outline-3">
<h3 id="orgfe5b879"><span class="section-number-3">12.11.</span> виды сетей и слоев</h3>
<div class="outline-text-3" id="text-12-11">
<p>
Spiking neural networks (SNNs) are artificial neural network models that more closely mimic natural neural
networks. Spike-timing-dependent plasticity (STDP) - learning-rule unsupervised
</p>

<p>
Fundamental:
</p>
<ul class="org-ul">
<li>Rate-based</li>
<li>Spike-based</li>
</ul>

<p>
old:
</p>
<ul class="org-ul">
<li>Multilayer perceptron - fully connected - each node in one layer connects connects to every node
in the following layer</li>
</ul>

<p>
New:
</p>
<ol class="org-ol">
<li>однослойная нейронная сеть может успешно решить лишь задачу линейной сепарации ∑ax+b</li>
<li>Dense(Fully-connected FC layer) (FNN) or Multilayer perceptron
<ul class="org-ul">
<li>pro: допускает отсутствие структуры</li>
<li>con: много заучиваемых параметров</li>
</ul></li>
<li>Locally Connected Networks LCN - filters not</li>
<li>convolutional neural networks (CNNs)
<ul class="org-ul">
<li>normal
<ul class="org-ul">
<li>pros:
<ul class="org-ul">
<li>go-to model on every image related problem</li>
<li>computationally efficient</li>
</ul></li>
<li>cons:
<ol class="org-ol">
<li>Backpropagation - Метод обратного распространения ошибки - неопределённо долгий процесс обучения</li>
<li>Translation invariance - плохая трансляционная инвариантность - отсутствие инфы об ориентации</li>
<li>Pooling layers - суммируют значения на величину kernel, чаще всего max</li>
</ol></li>
</ul></li>
<li>Fully CNN - has BilinearUpSampling2D as last layer -  used for semantic segmentation</li>
</ul></li>
<li>Recurrent neural network(RNN) Рекуррентная сеть (deep in time) - directed graph along a temporal sequnece (по
временной последовательности) - can use their internal state (memory) to process sequences of inputs
<ul class="org-ul">
<li>perceptron network</li>
<li>Long short Term Memory (LSTM) - has feedback connections that make it a "general purpose computer" - can
process single data points or sequences of data
<ul class="org-ul">
<li>Budurectional RNN (BRNN/BLSTM)</li>
<li>non-peephole (default)</li>
<li>Peephole LSTM</li>
</ul></li>
</ul></li>
<li>Recursive neural network (RNTNs) рекурсивная (deep in structure) - useful for natural-language processing -
в виде дерева, где листья - слова</li>
<li>Feedforward neural network - wherein connections between the nodes do not form a cycle</li>
<li>Random Forest (RF) не сеть - классификации, регрессии и кластеризации - можно использовать для оценки
качества статей
<ul class="org-ul">
<li>pros:
<ul class="org-ul">
<li>Способность эффективно обрабатывать данные с большим числом признаков и классов.</li>
<li>Нечувствительность к масштабированию (и вообще к любым монотонным преобразованиям) значений признаков.</li>
<li>Одинаково хорошо обрабатываются как непрерывные, так и дискретные признаки. Существуют методы
построения деревьев по данным с пропущенными значениями признаков.</li>
<li>Существуют методы оценивания значимости отдельных признаков в модели.</li>
<li>Внутренняя оценка способности модели к обобщению (тест по неотобранным образцам out-of-bag).</li>
<li>Высокая параллелизуемость и масштабируемость.</li>
</ul></li>
<li>con: много заучиваемых параметров</li>
</ul></li>
<li>Generative adversarial networks (GANs) <a href="https://arxiv.org/pdf/1406.2661.pdf">https://arxiv.org/pdf/1406.2661.pdf</a> - две конкурирующие нейронные сети</li>
<li>Variational Autoencoders (VAE) <a href="http://kvfrans.com/variational-autoencoders-explained/">http://kvfrans.com/variational-autoencoders-explained/</a> <a href="https://arxiv.org/pdf/1312.6114.pdf">https://arxiv.org/pdf/1312.6114.pdf</a></li>
<li>Transformer: “Attention is All you Need”</li>
</ol>

<p>
CRF Conditional Random Fields - NN dense as a final clasifier layout
</p>
</div>

<div id="outline-container-org4a88c0f" class="outline-4">
<h4 id="org4a88c0f"><span class="section-number-4">12.11.1.</span> Основные типы:</h4>
<div class="outline-text-4" id="text-12-11-1">
<ul class="org-ul">
<li>FeedForward NN</li>
<li>Recurrent NN
<ul class="org-ul">
<li>pro: processing input of any length</li>
<li>con: hard to parallelize</li>
</ul></li>
<li>Recursive neural network</li>
<li>Spatial Transformer Network used before CNN</li>
<li>Graph Neural Networks (GNNs)</li>
</ul>


<p>
All Generative NN archintectures are generalized into 2
 types:
</p>
<ul class="org-ul">
<li>Transformer architecture - sequence generator</li>
<li>Diffusion architecture - iterative refinement</li>
</ul>


<p>
<b><b>Convolutional Neural Networks (CNNs):</b></b> CNNs can be viewed as a special case of Transformers where the
 attention mechanism is restricted to local neighborhoods.
</p>

<p>
<b><b>Recurrent Neural Networks (RNNs):</b></b> RNNs can be viewed as a special case of Transformers where the attention
 mechanism is restricted to sequential dependencies.
</p>

<p>
<b><b>Graph Neural Networks (GNNs):</b></b> GNNs can be viewed as a special case of Transformers where the attention
 mechanism is applied to graph data.
</p>
</div>
</div>

<div id="outline-container-org5028a3f" class="outline-4">
<h4 id="org5028a3f"><span class="section-number-4">12.11.2.</span> Dense layer or fully-connected layer</h4>
<div class="outline-text-4" id="text-12-11-2">
<p>
whose inside neurons connect to every neuron in the preceding layer, same as a traditional multilayer
  perceptron neural network (MLP)
</p>
</div>
</div>
</div>
<div id="outline-container-orga298211" class="outline-3">
<h3 id="orga298211"><span class="section-number-3">12.12.</span> Layer Normalization and Batch Normalization</h3>
<div class="outline-text-3" id="text-12-12">
<ul class="org-ul">
<li>Batch Normalization <a href="https://arxiv.org/pdf/1502.03167.pdf">https://arxiv.org/pdf/1502.03167.pdf</a></li>
<li>Layer Normalization <a href="https://arxiv.org/pdf/1607.06450.pdf">https://arxiv.org/pdf/1607.06450.pdf</a></li>
<li>Batch Normalized Recurrent Neural Networks <a href="https://arxiv.org/pdf/1510.01378.pdf">https://arxiv.org/pdf/1510.01378.pdf</a></li>
</ul>

<p>
problem:  distribution of each layer’s inputs changes during training (internal covariate shift)
</p>

<p>
solution: normalize tensor by mean and variance
</p>

<p>
(gamma*(x-mu))/sigma + beta , where gamme - scale, beta - offset
</p>
<ul class="org-ul">
<li>mean		mu</li>
<li>variance	sigma</li>
<li>offset	beta</li>
<li>scale	gamm</li>
</ul>

<p>
saturating
</p>
</div>
</div>

<div id="outline-container-orgd185141" class="outline-3">
<h3 id="orgd185141"><span class="section-number-3">12.13.</span> hybrid networks</h3>
<div class="outline-text-3" id="text-12-13">
<ol class="org-ol">
<li>CNN + RNN - by Andrej Karpathy and Li Fei-Fei - natural-language descriptions of images and their regions</li>
<li>seq2seq or encoder-decoder or Neural machine translation (NMT)
<ul class="org-ul">
<li>pros: вся последовательность читается и только потом выдается решение</li>
<li>cons:
<ul class="org-ul">
<li>выходная последовательность может иметь другую длину чем входная</li>
<li>вектор передатчик - bottleneck</li>
</ul></li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-org8204404" class="outline-3">
<h3 id="org8204404"><span class="section-number-3">12.14.</span> Dynamic Neural Networks</h3>
<div class="outline-text-3" id="text-12-14">
<p>
Tensorflow uses static dataflow graphs
</p>

<p>
Dynamic computation graph like Pytorch and DyNet
</p>

<p>
cons:
</p>
<ul class="org-ul">
<li>Difficulty in debugging:</li>
<li>Handling more complex data types increases the complexity of computation graph formalism and implementation, and reduces opportunities for optimization.</li>
</ul>

<p>
in Tensorflow creating a dataflow graph per sample takes 70% of the overall running time.
</p>

<p>
DyNet is the first framework to perform dynamic batching in dynamic declaration.
</p>

<p>
TensorFlow Fold - state-of-the-art framework for dynamic NNs (is not an official Google product.)
</p>

<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1701.03980.pdf">https://arxiv.org/pdf/1701.03980.pdf</a></li>
</ul>
</div>
</div>
<div id="outline-container-org0683413" class="outline-3">
<h3 id="org0683413"><span class="section-number-3">12.15.</span> MLP, CNN, RNN, etc.</h3>
<div class="outline-text-3" id="text-12-15">
</div>
<div id="outline-container-org14c242d" class="outline-4">
<h4 id="org14c242d"><span class="section-number-4">12.15.1.</span> LCN</h4>
<div class="outline-text-4" id="text-12-15-1">
<ul class="org-ul">
<li><a href="https://medium.com/machine-learning-for-li/different-convolutional-layers-43dc146f4d0e">https://medium.com/machine-learning-for-li/different-convolutional-layers-43dc146f4d0e</a></li>
</ul>
<p>
In Locally-Connected Layer each neuron (pixel) has its own filter.
cons:
</p>
<ul class="org-ul">
<li>could increase the number of parameters and if you do not have enough data, you might end up with an over-fitting issue</li>
</ul>
<p>
pros:
</p>
<ul class="org-ul">
<li>let your network to learn different types of feature for different regions of the input</li>
</ul>
</div>
</div>
<div id="outline-container-org363632e" class="outline-4">
<h4 id="org363632e"><span class="section-number-4">12.15.2.</span> CNN <a id="org6f1939c"></a></h4>
<div class="outline-text-4" id="text-12-15-2">
<ul class="org-ul">
<li>samples in depth <a href="https://www.youtube.com/watch?v=JB8T_zN7ZC0">https://www.youtube.com/watch?v=JB8T_zN7ZC0</a></li>
<li>article <a href="https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2">https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2</a></li>
<li>article Stanford <a href="https://cs231n.github.io/convolutional-networks/">https://cs231n.github.io/convolutional-networks/</a></li>
<li>(shallow-and-wide vs deep) for Text Classification <a href="https://arxiv.org/pdf/1707.04108.pdf">https://arxiv.org/pdf/1707.04108.pdf</a></li>
<li>2014 Kim <a href="https://www.aclweb.org/anthology/D14-1181.pdf">https://www.aclweb.org/anthology/D14-1181.pdf</a></li>
<li>Transposed Convolution <a href="https://arxiv.org/pdf/1603.07285.pdf">https://arxiv.org/pdf/1603.07285.pdf</a></li>
<li>article <a href="https://pyimagesearch.com/2021/05/14/convolutional-neural-networks-cnns-and-layer-types/">https://pyimagesearch.com/2021/05/14/convolutional-neural-networks-cnns-and-layer-types/</a></li>
</ul>

<p>
For tesks
</p>
<ul class="org-ul">
<li>classification</li>
<li>localisation</li>
<li>semantic segmentation</li>
<li>action recognition</li>
</ul>

<p>
Properties:
</p>
<ul class="org-ul">
<li>soft translation-invariance - same object with slightly change of <b>orientation</b> or <b>position</b> might not fire up
the neuron that is supposed to recognize that object</li>
<li>Pooling losing valuable information - CNN does not take into account important spatial hierarchies between
simple and complex objects (Local information processing)</li>
</ul>

<p>
Types of convolution:<a href="https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d">https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d</a>
</p>
<ul class="org-ul">
<li>Dilated Convolutions - spacing in kernel</li>
<li>Transposed Convolutions - spacing in input and create convolution</li>
</ul>

<p>
x - source, w - filter = w[0]*x[0] + w[1]*x[1] +
</p>
</div>
<ol class="org-ol">
<li><a id="orgb576ff8"></a>fundamental<br />
<div class="outline-text-5" id="text-12-15-2-1">
<ul class="org-ul">
<li><b>Convolution</b> in CNN - operation to merge two sets - input and convolution kernel - to produce feature map.
<ul class="org-ul">
<li>convolution kernel/filter (receptive field) - прогон фильтра это нахождение фичи на изображении</li>
<li>pooling - (downsampling) позволяет быть более устойчивым к сдвигам изображения - common: 2x2 applied with
a stride of 2</li>
<li>filter values - initialized randomly - [-1,0,1] -  normal distribution or other distributions</li>
<li><b>Stride</b> specifies how much we move the convolution filter at each step. By default the value is 1. Для уменьшения выхода.</li>
<li><b>dilation</b> - когда применяется фильтр, между его ячеек устанавливается зазор. 0 - нет 1 - есть - Для
уменьшения выхода. Позволяет заострить внимание на более отдаленных учатках</li>
<li>1x1 convolutions - used when input is 3 channel - doing 3-dimensional dot products</li>
</ul></li>
</ul>
</div>
</li>

<li><a id="orgbe3699a"></a>history<br />
<div class="outline-text-5" id="text-12-15-2-2">
<ul class="org-ul">
<li>1989 ConvNet  - CONV - RELU - POOL - FC</li>
<li>1998 LeNet</li>
<li>2012 AlexNet</li>
<li>2014 Inception you only see once</li>
<li>VGG</li>
<li>2015 ResNet
<ul class="org-ul">
<li>YOLO Algorithm and YOLO Object Detection</li>
</ul></li>
<li>2016 DenceNet</li>
<li>2017 ResNeXt</li>
<li>2018 Channel Boosted CNN</li>
<li>2019 EfficientNet</li>
</ul>
</div>
</li>
<li><a id="org338e956"></a>Models AlexNet, MobileNet, Inception-v3, EfficientNet<br />
<div class="outline-text-5" id="text-12-15-2-3">
<ul class="org-ul">
<li><a href="https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">https://towardsdatascience.com/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc</a></li>
</ul>

<p>
<b>EfficientNet</b>
</p>
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1905.11946.pdf">https://arxiv.org/pdf/1905.11946.pdf</a></li>
<li><a href="https://github.com/qubvel/efficientnet/blob/aa1edcaa2bbbf878f78164c4d45f46acabe59fab/efficientnet/model.py">https://github.com/qubvel/efficientnet/blob/aa1edcaa2bbbf878f78164c4d45f46acabe59fab/efficientnet/model.py</a></li>
<li><a href="https://www.kaggle.com/rsmits/keras-efficientnet-b3-training-inference">https://www.kaggle.com/rsmits/keras-efficientnet-b3-training-inference</a></li>
</ul>

<p>
Inception v1
</p>
<ul class="org-ul">
<li>target object may have different size in image</li>
<li>hard to select right kernel size</li>
<li>Solution:   3 different sizes of filters (1x1, 3x3, 5x5) at the same level -&gt; concatination</li>
<li>Maxpool -&gt; 1x1 with redused size -&gt; 3x3 -&gt;</li>
<li>instead of residual connection - two middle FC outs (auxiliary loss) - total<sub>loss</sub> = real<sub>loss</sub> + 0.3 *
aux<sub>loss</sub><sub>1</sub> + 0.3 * aux<sub>loss</sub><sub>2</sub></li>
<li>auxiliary loss is purely used for training purposes, and is ignored during inference</li>
</ul>


<p>
Inception v2
</p>
<ul class="org-ul">
<li><b>representational bottleneck</b> - Reducing the dimensions too much may cause loss of information</li>
<li>Factorize 5x5 convolution to two 3x3. 5x5  2.78 times more expensive</li>
<li>3x3 equivalent 1x3 convolution -&gt; 3x1 convolution - 33% more cheaper</li>
<li>share same 1x1 before 1x3 and 3x1</li>
</ul>

<p>
Inception v4
</p>
<ul class="org-ul">
<li>Reduction Blocks was introduced:
<ul class="org-ul">
<li>3x3 maxpool stride 2</li>
<li>3x3 conv strive 2</li>
<li>1x1 conv k -&gt; 3x3 conv 1 -&gt; 3x3 stride 2</li>
</ul></li>
</ul>
</div>
</li>

<li><a id="org20f1251"></a>PROBLEMS<br />
<ol class="org-ol">
<li><a id="orgb0b4ccc"></a>Rotation problem<br />
<div class="outline-text-6" id="text-12-15-2-4-1">
<ul class="org-ul">
<li>Group Equivariant Convolutional Networks <a href="http://proceedings.mlr.press/v48/cohenc16.pdf">http://proceedings.mlr.press/v48/cohenc16.pdf</a>
<ul class="org-ul">
<li><a href="https://github.com/tscohen/gconv_experiments">https://github.com/tscohen/gconv_experiments</a></li>
</ul></li>
<li>Rotation Equivariant <a href="https://arxiv.org/pdf/1705.08623.pdf">https://arxiv.org/pdf/1705.08623.pdf</a></li>
<li>Rotation Equivariant CNNs for Digital Pathology <a href="https://arxiv.org/pdf/1806.03962.pdf">https://arxiv.org/pdf/1806.03962.pdf</a>
<ul class="org-ul">
<li><a href="https://github.com/basveeling/keras-gcnn">https://github.com/basveeling/keras-gcnn</a></li>
</ul></li>
<li>H-Net Harmonic Networks: Deep Translation and Rotation Equivariance <a href="http://visual.cs.ucl.ac.uk/pubs/harmonicNets/index.html">http://visual.cs.ucl.ac.uk/pubs/harmonicNets/index.html</a>
<ul class="org-ul">
<li><a href="https://github.com/deworrall92/harmonicConvolutions">https://github.com/deworrall92/harmonicConvolutions</a></li>
<li><a href="https://github.com/codeaudit/harmonicConvolutions">https://github.com/codeaudit/harmonicConvolutions</a></li>
</ul></li>
<li>Approach CFNet <a href="https://academic.oup.com/bioinformatics/article/35/14/i530/5529148">https://academic.oup.com/bioinformatics/article/35/14/i530/5529148</a>
<ul class="org-ul">
<li><a href="https://github.com/bchidest/CFNet">https://github.com/bchidest/CFNet</a></li>
<li><a href="https://openreview.net/forum?id=BJepX2A9tX">https://openreview.net/forum?id=BJepX2A9tX</a></li>
</ul></li>
<li>G-CNN+DFT
<ul class="org-ul">
<li>DFT 2018 <a href="https://faculty.ucmerced.edu/mhyang/papers/eccv2018_dft.pdf">https://faculty.ucmerced.edu/mhyang/papers/eccv2018_dft.pdf</a></li>
</ul></li>
</ul>

<p>
Terms: Daniel Worrall <a href="https://www.youtube.com/watch?v=TlzRyHbWeP0&amp;feature=youtu.be">https://www.youtube.com/watch?v=TlzRyHbWeP0&amp;feature=youtu.be</a>
</p>
<ul class="org-ul">
<li>Equivariance - Something Not affected by a specified group action. f:S-&gt;T is equivariant with respect to g:
g(f(s)) = f(g(s)) . Mapping preserve algebraic structure of transformation.</li>
<li>Invariance or symmetrie - "no variance" at all. Maximum value m' = m is invariant to translation. While its location will
be (x',y') = (x-u,y-v) is equivariant, meaning that is varies "equally" with the distortion. f(I)=f(F(I)) -
ignore entirely.
<ul class="org-ul">
<li>geometric translation, rotation, pixel normalization - bunch of <b>symmetries</b> of function f(I)</li>
</ul></li>
<li>distortion - искажение</li>
</ul>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Max/average pooling</th>
<th scope="col" class="org-left">translation invariance</th>
<th scope="col" class="org-left">shape preserving</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">without - FC layers</td>
<td class="org-left">no</td>
<td class="org-left">yes</td>
</tr>

<tr>
<td class="org-left">with m/a pooling</td>
<td class="org-left">yes</td>
<td class="org-left">no</td>
</tr>

<tr>
<td class="org-left">DFT magnitude pooling</td>
<td class="org-left">yes</td>
<td class="org-left">yes</td>
</tr>
</tbody>
</table>

<p>
Comparision:
</p>
<ul class="org-ul">
<li>G-convs - good discriminativity, okay equivariance</li>
<li>H-convs - good equivariance, okay discriminativity</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org99282da"></a>CapsNet<br />
<div class="outline-text-7" id="text-12-15-2-4-1-1">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1710.09829.pdf">https://arxiv.org/pdf/1710.09829.pdf</a></li>
<li><a href="https://openreview.net/pdf?id=HJWLfGWRb">https://openreview.net/pdf?id=HJWLfGWRb</a></li>
<li><a href="https://becominghuman.ai/understanding-capsnet-part-1-e274943a018d">https://becominghuman.ai/understanding-capsnet-part-1-e274943a018d</a></li>
<li><a href="https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc">https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc</a></li>
<li>original tensor <a href="https://github.com/debarko/CapsNet-Tensorflow">https://github.com/debarko/CapsNet-Tensorflow</a></li>
<li>newer tensor <a href="https://github.com/capsnet/CapsNet-Gravitational-Lensing">https://github.com/capsnet/CapsNet-Gravitational-Lensing</a></li>
<li>keras <a href="https://github.com/XifengGuo/CapsNet-Keras">https://github.com/XifengGuo/CapsNet-Keras</a></li>
</ul>

<p>
Solve 3D object rotation invariant problem
</p>
</div>
</li>
</ol>
</li>
<li><a id="org496c179"></a>Shift invariant problem<br />
<div class="outline-text-6" id="text-12-15-2-4-2">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1904.11486.pdf">https://arxiv.org/pdf/1904.11486.pdf</a></li>
</ul>
</div>
</li>

<li><a id="orgbeaf375"></a>Scale invariant<br />
<div class="outline-text-6" id="text-12-15-2-4-3">
<p>
Equivariance Over Scale <a href="https://arxiv.org/pdf/1905.11697.pdf">https://arxiv.org/pdf/1905.11697.pdf</a>
</p>
</div>
</li>

<li><a id="org402f1f6"></a>Нейронные сети предпочитают текстуры<br />
<div class="outline-text-6" id="text-12-15-2-4-4">
<p>
<a href="https://habr.com/ru/company/ods/blog/453788/">https://habr.com/ru/company/ods/blog/453788/</a>
</p>
</div>
</li>
</ol>
</li>
<li><a id="org6e6cff7"></a>shallow-and-wide CNN<br />
<div class="outline-text-5" id="text-12-15-2-5">
<ul class="org-ul">
<li>A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence
Classification” <a href="https://arxiv.org/pdf/1510.03820.pdf">https://arxiv.org/pdf/1510.03820.pdf</a></li>
<li>неглубокая, но широкая - 6 filters, maxpooling, -&gt; concatenated -&gt; batch normalization</li>
<li>применение для анализа логов <a href="http://www.nada.kth.se/~ann/exjobb/bjorn_annergren.pdf">http://www.nada.kth.se/~ann/exjobb/bjorn_annergren.pdf</a></li>
</ul>
</div>
</li>
<li><a id="orgf80cfdc"></a>CNN-based attention maps<br />
<div class="outline-text-5" id="text-12-15-2-6">
<p>
terms:
</p>
<ul class="org-ul">
<li>salient regions - выступающие регионы</li>
</ul>
<p>
articles:
</p>
<ul class="org-ul">
<li><a href="https://stackoverflow.com/questions/44731990/cnn-attention-activation-maps">https://stackoverflow.com/questions/44731990/cnn-attention-activation-maps</a></li>
<li><a href="https://www.groundai.com/project/query-based-attention-cnn-for-text-similarity-map/2">https://www.groundai.com/project/query-based-attention-cnn-for-text-similarity-map/2</a></li>
</ul>

<p>
Types:
</p>
<ul class="org-ul">
<li>Functions (gradients, saliency map): These methods visualize how a change in input space affects the
prediction</li>
<li>Signal (deconvolution, Guided BackProp, PatternNet): the signal (reason for a neuron's activation) is
visualized. So this visualizes what pattern caused the activation of a particular neuron.</li>
<li>Attribution (LRP, Deep Taylor Decomposition, PatternAttribution): these methods visualize how much a single
pixel contributed to the prediction. As a result you get a heatmap highlighting which pixels of the input
image most strongly contributed to the classification.</li>
</ul>

<p>
Models:
</p>
<ul class="org-ul">
<li>Hourglass (Bottom-up top-down feedforward) - human pose and image segmentation
<a href="https://arxiv.org/pdf/1603.06937.pdf">https://arxiv.org/pdf/1603.06937.pdf</a> <a href="https://arxiv.org/pdf/1704.06904.pdf">https://arxiv.org/pdf/1704.06904.pdf</a>
<ul class="org-ul">
<li>reaches its lowest resolution at 4x4 pixels</li>
<li><a href="https://github.com/wbenbihi/hourglasstensorlfow/blob/master/yolo_net.py">https://github.com/wbenbihi/hourglasstensorlfow/blob/master/yolo_net.py</a></li>
</ul></li>
<li>attention gate model</li>
<li>Max Average Hourglass without residual (CBAM) <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf">http://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf</a></li>
</ul>
</div>

<ol class="org-ol">
<li><a id="orgd828bd5"></a>Attention in CNN<br />
<div class="outline-text-6" id="text-12-15-2-6-1">
<ul class="org-ul">
<li><a href="https://openreview.net/pdf?id=HyzbhfWRW">https://openreview.net/pdf?id=HyzbhfWRW</a></li>
</ul>
</div>
</li>
<li><a id="orgce4775a"></a><span class="todo TODO">TODO</span> Attention Gated Networks<br />
<div class="outline-text-6" id="text-12-15-2-6-2">
<ul class="org-ul">
<li><a href="https://openreview.net/pdf?id=BJtn7-3sM">https://openreview.net/pdf?id=BJtn7-3sM</a></li>
<li><a href="https://github.com/ozan-oktay/Attention-Gated-Networks">https://github.com/ozan-oktay/Attention-Gated-Networks</a></li>
<li><a href="https://arxiv.org/pdf/1803.07294.pfd">https://arxiv.org/pdf/1803.07294.pfd</a></li>
</ul>
<p>
encoder -&gt; decoder
</p>
</div>
</li>
<li><a id="org6f27f9a"></a>Residual Attention Network for Image Classification<br />
<div class="outline-text-6" id="text-12-15-2-6-3">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1704.06904.pdf">https://arxiv.org/pdf/1704.06904.pdf</a></li>
<li><a href="https://www.youtube.com/watch?v=Deq1BGTHIPA">https://www.youtube.com/watch?v=Deq1BGTHIPA</a></li>
<li>keras <a href="https://github.com/qubvel/residual_attention_network/blob/master/models/models.py">https://github.com/qubvel/residual_attention_network/blob/master/models/models.py</a></li>
</ul>



<p>
<b>residual block</b> - остаточный - размер сохраняет
</p>
<ul class="org-ul">
<li>3:
<ul class="org-ul">
<li>x = BatchNormalization()(input)</li>
<li>x = Activation('relu')(x)</li>
<li>x = Conv2D(filters=(output<sub>channels</sub> // 4), (1, 1))(x)</li>
</ul></li>
<li>x = Add()[x, input] - residual connection</li>
</ul>

<p>
<b>attention<sub>block</sub></b>
</p>
<ul class="org-ul">
<li>MaxPool2D   # 1</li>
<li>skip<sub>connections</sub> = []</li>
<li>for encoder<sub>depth</sub>-1
<ul class="org-ul">
<li>residual block</li>
<li>skip<sub>connections.append</sub>(output<sub>skip</sub><sub>connection</sub>) # сохраняем слой 2-n</li>
<li>MaxPool2D # 2 - n</li>
<li>residual<sub>block</sub></li>
</ul></li>

<li>skip<sub>connections</sub> = list(reversed(skip<sub>connections</sub>))</li>
<li>for encoder<sub>depth</sub>-1
<ul class="org-ul">
<li>residual<sub>block</sub></li>
<li>UpSampling2D # 2 - n</li>
<li>Add()([output<sub>soft</sub><sub>mask</sub>, skip<sub>connections</sub>[i]])</li>
</ul></li>

<li>residual<sub>block</sub></li>
<li>UpSampling2D # 1</li>
<li>Activation('sigmoid')</li>
<li>Attention: (1 + output<sub>soft</sub><sub>mask</sub>) * input</li>
</ul>
</div>
</li>

<li><a id="orge2bd8e5"></a>Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer<br />
<div class="outline-text-6" id="text-12-15-2-6-4">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1612.03928.pdf">https://arxiv.org/pdf/1612.03928.pdf</a></li>
<li>pythorch <a href="https://github.com/szagoruyko/attention-transfer">https://github.com/szagoruyko/attention-transfer</a></li>
</ul>
</div>
</li>
<li><a id="org5c3ccb4"></a>Attention gate<br />
<div class="outline-text-6" id="text-12-15-2-6-5">
<ul class="org-ul">
<li><a href="https://www.sciencedirect.com/science/article/pii/S1361841518306133#fig0002">https://www.sciencedirect.com/science/article/pii/S1361841518306133#fig0002</a></li>
</ul>

<p>
additive attention gate:
</p>
<ul class="org-ul">
<li>g + x(down) -&gt; relu -&gt; softsign -&gt;up -&gt; полученный фильти умножить на x</li>
</ul>

<p>
attention-gated classification model:
</p>
<ul class="org-ul">
<li>CNN -&gt; выход из последнего используем как g для сложения с выходами более высокими. Всех их подаем на FC</li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org10af6c9"></a>Temporal Convolutional Networks<br />
<div class="outline-text-5" id="text-12-15-2-7">
<ul class="org-ul">
<li>article <a href="https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-3-7f6633fcc7c7">https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-3-7f6633fcc7c7</a></li>
<li>науч раб <a href="https://arxiv.org/pdf/1811.10166.pdf">https://arxiv.org/pdf/1811.10166.pdf</a></li>
</ul>
</div>
</li>
<li><a id="org41ed416"></a>Atrous convolution (a.k.a. convolution with holes or dilated convolution).<br />
<div class="outline-text-5" id="text-12-15-2-8">
<ul class="org-ul">
<li>article <a href="https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5</a></li>
<li>tensorflow <a href="https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d">https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d</a></li>
</ul>
</div>
</li>
<li><a id="org5a18d2c"></a>calc output size<br />
<div class="outline-text-5" id="text-12-15-2-9">
<p>
<b>Conv Layer</b>
</p>
<ul class="org-ul">
<li>input volume size (W)</li>
<li>filter or  receptive field (F)</li>
<li>stride (S) - смещаем фильтр на 1 или больше?</li>
<li>the amount of padding used (P) on the border</li>
</ul>

<p>
(W−F+2P)/S+1
</p>

<p>
For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2
we would get a 3x3 output:
</p>
<ul class="org-ul">
<li>(7-3+0)/1 +1 = 5</li>
<li>(7-3+0)/2 +1 = 3</li>
</ul>


<p>
Pooling: W=6 F=2 O=3 (6-2)/2 + 1 = 3
</p>

<p>
<b>Padding</b> recommended: P = (F-1)/2 when S=1
</p>
</div>
</li>
<li><a id="orgc631e94"></a>Pooling layer<br />
<div class="outline-text-5" id="text-12-15-2-10">
<p>
To reduce the dimensionality It is common to periodically insert a Pooling layer in-between successive Conv
layers. =&gt; reduce the number of parameters, which both shortens the training time and combats
overfitting. Downsampling the feature map while keeping the important information.
</p>

<p>
То же что и Convolution о Фильтр накладывается смещаясь на всю свою длинну F=2 S=2. Обычно функция Max.
</p>
</div>
</li>

<li><a id="org95633c3"></a>Fully-convolutional networks(FCN)<br />
<div class="outline-text-5" id="text-12-15-2-11">
<ul class="org-ul">
<li>“Fully convolutional networks for semantic segmentation”, Proc. IEEE Int. Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 3431-3440, IEEE, 2015</li>
<li><a href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1">https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1</a></li>
<li><a href="https://github.com/aurora95/Keras-FCN">https://github.com/aurora95/Keras-FCN</a></li>
</ul>

<p>
An FC layer has nodes connected to all activations in the previous layer, hence, requires a fixed size of
input data. The only difference between an FC layer and a convolutional layer is that the neurons in the
convolutional layer are connected only to a local region in the input. However, the neurons in both layers
still compute dot products. Since their functional form is identical every FC layer can be replaced by a
convolutional layer
</p>

<p>
Обычная сверточная сеть обученная на 100x100 пробегает входом по более большому изображению и определяет
тепловую карту где находится конкретный класс. Для нахождения.
</p>
</div>
</li>

<li><a id="orga3cb15a"></a>1x1 convolution<br />
<div class="outline-text-5" id="text-12-15-2-12">
<p>
used to decrease the number of feature maps.
1×1 filter
</p>
</div>
</li>
<li><a id="org29f5624"></a>calc output shape after Conv2D<br />
<div class="outline-text-5" id="text-12-15-2-13">
<ul class="org-ul">
<li>Output Height = (Input Height - Filter Height + 2 * Padding) / Stride + 1</li>
<li>Output Width = (Input Width - Filter Width + 2 * Padding) / Stride + 1</li>
</ul>
</div>
</li>
<li><a id="org4616e05"></a>keras<br />
<div class="outline-text-5" id="text-12-15-2-14">
<ul class="org-ul">
<li><b>Conv2D</b> ( -
<ul class="org-ul">
<li>64, - number of output filters (depth)</li>
<li>(2, 2), - kernel<sub>size</sub> of filter</li>
<li>padding='same', - case-insensitive  - ("same" add with -inf ) ("valid" - no padding (default))</li>
<li>input<sub>shape</sub>=(400, 400, 1),</li>
<li>dtype=tf.float32))</li>
<li><b>default</b>:
<ul class="org-ul">
<li>strides=(1, 1)</li>
</ul></li>
</ul></li>
</ul>


<p>
<b>LocallyConnected2D</b> - weights are unshared, that is, a different set of filters is applied at each different patch of the input.
</p>
</div>
</li>

<li><a id="org2418c74"></a>fine-tuning<br />
<div class="outline-text-5" id="text-12-15-2-15">
<ul class="org-ul">
<li><b>fine-tuning</b> - retraining the head of a network to recognize classes it was not originally intended
for.</li>
</ul>
<p>
for layer in baseModel.layers: layer.trainable = False
</p>
</div>
</li>
<li><a id="org38e5f9c"></a>Instance Segmentation<br />
<div class="outline-text-5" id="text-12-15-2-16">
<p>
Mask Region based Convolution Neural Networks
</p>
<ol class="org-ol">
<li>Object detection</li>
<li>Semantic Segmentation</li>
</ol>
</div>
</li>
<li><a id="orgcb9fc18"></a>Object Detection<br />
<div class="outline-text-5" id="text-12-15-2-17">
<ul class="org-ul">
<li>history <a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4">https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4</a></li>
</ul>

<p>
R-CNN - proposed regions to CNN classifier + CNN tighten the bounding boxes
</p>

<p>
Fast R-CNN - source image to CNN -&gt; proposed regions compared with CNN exit grid -&gt; (softmax + bbox
 regressor). <img src="https://blog.paperspace.com/content/images/2020/09/Fig03-1.jpg" alt="Fig03-1.jpg" />
</p>
<ul class="org-ul">
<li>объединённая loss-функция for (softmax + bbox regressor)</li>
<li>steps:
<ol class="org-ol">
<li>The first module generates 2,000 region proposals using the Selective Search algorithm</li>
<li>After being resized to a fixed pre-defined size, the second module extracts a feature vector of length 4,096 from each region proposal.</li>
<li>The third module uses a pre-trained SVM algorithm to classify the region proposal to either the background
or one of the object classes.</li>
</ol></li>
</ul>

<p>
Faster R-CNN - новый модуль Region Proposal Network (RPN)
</p>
<ul class="org-ul">
<li>one networks CNN -&gt; sliding window 3x3 -&gt;1) 2k score 2) 4k coordinates where k - anchor boxes (shapes)</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org2d22b11"></a>One and two stage detectors:<br />
<div class="outline-text-6" id="text-12-15-2-17-1">
<ul class="org-ul">
<li>Two-stage/proposal (Region based detectors) - first pass is used to generate a set of proposals or potential
object locations, second to refine these proposals and make final predictions
<ul class="org-ul">
<li>RCNN</li>
<li>Fast RCNN</li>
<li>Faster RCNN</li>
<li>RFCN</li>
<li>Mask RCNN</li>
</ul></li>
<li>One-stage/proposal-free - Single-shot object detection (less effective in detecting small objects)
<ul class="org-ul">
<li>YOLO - CNN based, fast inference speed, simple architecture and requires minimal training data</li>
<li>SSD</li>
</ul></li>
</ul>
</div>
</li>

<li><a id="orgf64188f"></a>metrics<br />
<div class="outline-text-6" id="text-12-15-2-17-2">
<p>
between the predicted and the ground truth <b>bounding boxes</b>,
</p>
<ul class="org-ul">
<li>Intersection over Union (IoU) = Area of Overlap / Area of Union
<ul class="org-ul">
<li>Union - overlap</li>
</ul></li>
<li>Average Precision (AP) -  calculated as the area under a precision vs. recall curve for a set of predictions.
<ul class="org-ul">
<li>mean Average Precision (mAP)</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="org6faea39"></a>region proposal<br />
<div class="outline-text-6" id="text-12-15-2-17-3">
<ul class="org-ul">
<li>introduction <a href="https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491">https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491</a>
<ul class="org-ul">
<li><a href="https://github.com/jrieke/shape-detection">https://github.com/jrieke/shape-detection</a></li>
</ul></li>
</ul>
<p>
region proposal algorithms to hypothesize object locations
</p>
<ul class="org-ul">
<li>SPPnet 2014</li>
<li>Fast R-CNN 2015</li>
</ul>

<p>
Fast R-CNN <a href="https://arxiv.org/pdf/1504.08083.pdf">https://arxiv.org/pdf/1504.08083.pdf</a>
</p>

<p>
Tensorflow API <a href="https://www.youtube.com/watch?v=rWFg6R5ccOc">https://www.youtube.com/watch?v=rWFg6R5ccOc</a>
</p>


<p>
<b>Faster-RCNN</b> two modules:
</p>
<ul class="org-ul">
<li>Region Proposal Network (RPN) <a href="https://arxiv.org/pdf/1506.01497v3.pdf">https://arxiv.org/pdf/1506.01497v3.pdf</a></li>
<li>Faster-RCNN <a href="https://github.com/smallcorgi/Faster-RCNN_TF">https://github.com/smallcorgi/Faster-RCNN_TF</a></li>
<li>Faster R-CNN implementation for rotated boxes <a href="https://github.com/runa91/FRCNN_git">https://github.com/runa91/FRCNN_git</a></li>
</ul>

<p>
RPN - output set of rectangular object proposals
</p>
</div>
</li>

<li><a id="org3162600"></a>YOLO<br />
<div class="outline-text-6" id="text-12-15-2-17-4">
<p>
YOLO stands for “you only look once”
</p>

<p>
Intersection over union (IOU) is a phenomenon in object detection that describes how boxes overlap.
</p>

<p>
IOU is equal to 1 if the predicted bounding box is the same as the real box.
</p>

<p>
last layer YOLOv1 predicts a cuboidal output - (1, 1470) from final fully connected layer and reshaping it to size (7, 7, 30)
</p>

<p>
S × S grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.
</p>

<p>
techniques
</p>
<ul class="org-ul">
<li><b>non-maximum suppression (NMS)</b> - post-processing step to filter out redundant bounding boxes. This technique ensures that only the most
confident and accurate bounding boxes are retained while eliminating duplicates or overlapping
boxes.</li>
<li>1x1 Convolution Layers: after backbone, 1x1 convolutions are applied to reduce dimensionality,
perform channel-wise transformations, and introduce non-linearity into the feature maps.</li>
</ul>

<p>
bounding box:
</p>
<ul class="org-ul">
<li>Width (bw)</li>
<li>Height (bh)</li>
<li>Class (for example, person, car, traffic light, etc.)- This is represented by the letter c.</li>
<li>Bounding box center (bx,by)</li>
</ul>

<p>
history:
</p>
<ul class="org-ul">
<li>2015 YOLO -  20 convolution layers, capable of processing at a maximum rate of 45 frames per second</li>
<li>2016 YOLO v2 - CNN backbone called Darknet-19 (a variant of the VGGNet architecture - progressive
convolution and pooling layers), anchor boxes - set of predefined bounding boxes of different aspect ratios
and scales, new loss function</li>
<li>2018 YOLO v3 - Darknet-53 (variant of the ResNet), anchor boxes with different scales and aspect ratios,
feature pyramid networks" (FPN)</li>
<li>2019 YOLO v4 - CSPNet Cross Stage Partial Network (variant of the ResNet architecture for OD task, 54
convolutional layers), new method for generating the anchor boxes, called "k-means clustering.", GHM loss -
variant of the focal loss function</li>
<li>2020 YOLO v5 - EfficientNet network architecture, "spatial pyramid pooling" (SPP), CIoU loss - variant of
the IoU loss function</li>
<li>2020 YOLO v6 - "dense anchor boxes" - new method for generating the anchor boxes</li>
<li>2021 YOLO v7 - uses nine anchor boxes, new loss function called “focal loss.”, can process images at a rate
of 155 frames per second</li>
</ul>

<p>
FPN - pyramid of feature maps, with each level of the pyramid being used to detect objects at a different scale. This helps to improve the detection performance on small objects, as the model is able to see the objects at multiple scales.
</p>

<p>
links
</p>
<ul class="org-ul">
<li>YOLO <a href="https://arxiv.org/abs/1506.02640">https://arxiv.org/abs/1506.02640</a></li>
<li>YOLOv2 <a href="https://arxiv.org/pdf/1612.08242">https://arxiv.org/pdf/1612.08242</a>
<ul class="org-ul">
<li>YOLOv2 loss <a href="https://www.v7labs.com/blog/pytorch-loss-functions">https://www.v7labs.com/blog/pytorch-loss-functions</a></li>
</ul></li>
<li>YOLOv3 <a href="https://arxiv.org/pdf/1804.02767.pdf">https://arxiv.org/pdf/1804.02767.pdf</a></li>
<li>YOLOv4 <a href="https://arxiv.org/pdf/2004.10934.pdf">https://arxiv.org/pdf/2004.10934.pdf</a></li>
<li>YOLOv6 <a href="https://arxiv.org/pdf/2209.02976.pdf">https://arxiv.org/pdf/2209.02976.pdf</a>
<ul class="org-ul">
<li>EfficientNet-L2 <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_Self-Training_With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper.pdf">https://openaccess.thecvf.com/content_CVPR_2020/papers/Xie_Self-Training_With_Noisy_Student_Improves_ImageNet_Classification_CVPR_2020_paper.pdf</a></li>
</ul></li>
<li>YOLOv7 <a href="https://arxiv.org/pdf/2207.02696.pdf">https://arxiv.org/pdf/2207.02696.pdf</a></li>
<li>article <a href="https://www.section.io/engineering-education/introduction-to-yolo-algorithm-for-object-detection/">https://www.section.io/engineering-education/introduction-to-yolo-algorithm-for-object-detection/</a></li>
<li>article <a href="https://www.geeksforgeeks.org/yolo-you-only-look-once-real-time-object-detection/">https://www.geeksforgeeks.org/yolo-you-only-look-once-real-time-object-detection/</a></li>
<li>article <a href="https://www.v7labs.com/blog/yolo-object-detection">https://www.v7labs.com/blog/yolo-object-detection</a></li>
</ul>
</div>
</li>
<li><a id="org27ff568"></a><span class="todo TODO">TODO</span> Faster R-CNN 2015<br />
<div class="outline-text-6" id="text-12-15-2-17-5">
<p>
<a href="https://arxiv.org/abs/1506.01497">https://arxiv.org/abs/1506.01497</a>
</p>

<p>
object detection
</p>

<p>
Region Proposal Network (RPN) - shares full-image convolutional features with the detection network
</p>
<ul class="org-ul">
<li>takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an
objectness score.</li>
<li>slide a small network over the convolutional feature map output by the last shared convolutional layer.</li>
<li>a box-regression layer (reg) and a box-classification layer -  fullyconnected layers</li>
</ul>

<p>
deep CNN -&gt; Fast R-CNN detector - both nets share a common set of convolutional layers (shareable convolutional layers)
</p>
<ol class="org-ol">
<li>feature map -&gt; 2,3</li>
<li>proposals -&gt; 3</li>
<li>RoI pooling</li>
</ol>
</div>
</li>
<li><a id="orgf7b2a52"></a>2018 Mask R-CNN - object detection or<br />
<div class="outline-text-6" id="text-12-15-2-17-6">
<p>
instance segmentation - combines elements from the classical computer vision tasks of object detection
</p>
<ul class="org-ul">
<li>object detection - the goal is to classify individual objects and localize each using a bounding box</li>
<li>semantic segmentation - the goal is to classify each pixel into a fixed set of categories without
differentiating object instances</li>
</ul>

<p>
instance segmentation, bounding-box object detection, and person keypoint detection
</p>

<p>
<a href="https://github.com/facebookresearch/Detectron">https://github.com/facebookresearch/Detectron</a>
</p>

<p>
Faster R-CNN by adding a branch for predicting segmentation masks on each Region of Interest (RoI), in
 parallel with the existing branch for classification and bounding box regression.
</p>
<ul class="org-ul">
<li>The mask branch is a small FCN applied to each RoI</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org5232baf"></a>backbone network architectures:<br />
<div class="outline-text-7" id="text-12-15-2-17-6-1">
<ul class="org-ul">
<li>ResNeXt{50,101,152}</li>
<li>ResNet{50,101,152}</li>
<li>Feature Pyramid Networks (with ResNet/ResNeXt)</li>
<li>VGG16</li>
</ul>
</div>
</li>
</ol>
</li>

<li><a id="orged2d988"></a>Notes<br />
<div class="outline-text-6" id="text-12-15-2-17-7">
<ul class="org-ul">
<li>лучше x и у заменить на центр [x+w/2, y+h/2, w, h]  # save coordinats</li>
</ul>
</div>
</li>
</ol>
</li>

<li><a id="orgfe53cd3"></a>image segmentation<br />
<div class="outline-text-5" id="text-12-15-2-18">
<p>
U-Net - convolutional neural network with residual connections - downsampling and upsampling
</p>
<ul class="org-ul">
<li>output is same size image with binary mask</li>
<li><a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/</a></li>
<li><a href="https://arxiv.org/pdf/1505.04597.pdf">https://arxiv.org/pdf/1505.04597.pdf</a></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org03ea4e4" class="outline-4">
<h4 id="org03ea4e4"><span class="section-number-4">12.15.3.</span> RNN recurrent [rɪˈkʌrənt] повторяющийся</h4>
<div class="outline-text-4" id="text-12-15-3">
<ul class="org-ul">
<li><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks">https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks</a></li>
<li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/</a></li>
<li>smallest rnn <a href="https://gist.github.com/karpathy/d4dee566867f8291f086#file-min-char-rnn-py">https://gist.github.com/karpathy/d4dee566867f8291f086#file-min-char-rnn-py</a></li>
</ul>
<p>
Class of neural networks
</p>
<ul class="org-ul">
<li>x -U-&gt; s -V-&gt;o     s(t-1)-W-&gt;s(t)-W-&gt;s(t+1)</li>
<li>current hidden state st=f(U(xt)+W(s(t-1))) - current input + previous hiden state</li>
<li>f is ReLU or tanh</li>
<li>ot = softmax(V(st))</li>
<li>s(t-1) - typically initialized to all zeroes</li>
</ul>
<p>
<b>Advantages</b>
</p>
<ul class="org-ul">
<li>RNN has no layouts it reduces the total number of parameters we need to learn</li>
<li>Possibility of processing input of any length (one to many, many t one, many to many(during), many to
many(after))</li>
<li>Model size not increasing with size of input</li>
<li>Computation takes into account historical information</li>
<li>Weights are shared across time</li>
</ul>
<p>
<b>Drawbacks</b>
</p>
<ul class="org-ul">
<li>hard to parallelize</li>
<li>Computation being slow</li>
<li>Difficulty of accessing information from a long time ago</li>
<li>Cannot consider any future input for the current state</li>
</ul>
<p>
<b>Usage</b>
</p>
<ul class="org-ul">
<li>Generating Text</li>
<li>Machine Translation - key difference is that our output only starts after we have seen the complete input</li>
</ul>

<p>
<b>Structure</b>
</p>
<ul class="org-ul">
<li>^   ^   ^</li>
<li>O &gt; O &gt; O</li>
<li>^   ^   ^</li>
</ul>

<p>
<b>Deep (Bidirectional) RNNs</b>  multiple layers
</p>
<ul class="org-ul">
<li>higher learning capacity (but we also need a lot of training data)</li>
</ul>
<p>
<b>CNN to RNN connection для описания</b>
</p>
<ul class="org-ul">
<li>st=f(U(xt)+W(s(t-1)) + CNNoutput)</li>
<li>слово - один шаг RNN сети с одним и тем же СNN входом</li>
<li>&lt;start&gt; - начальное слово</li>
<li>&lt;end&gt; - обучающее слово конца для RNN</li>
<li>RNN will work better with attention over the different parts of image ( Image Captioning with Attention)
<ul class="org-ul">
<li>CNN -&gt; LxD - grid of vectors, one for special location in image</li>
<li>at each step we put LxD and add weight to vector of step</li>
<li>RNN output = 1 dictribution of vacabulary 2 dictribution over image locations
<ul class="org-ul">
<li>Soft attention - features from all image</li>
<li>hard attention - select exactly one location</li>
</ul></li>
</ul></li>
</ul>
<p>
<b>RNNs visual question answering</b>
</p>
<ol class="org-ol">
<li>CNN with attemtion -&gt; RNN question words - one word per step</li>
<li>out of end step of RNN +(concatenate) another CNN</li>
<li>softmax</li>
</ol>
</div>
<ol class="org-ol">
<li><a id="org09c85ff"></a>Backpropagation Through Time (BPTT)<br />
<div class="outline-text-5" id="text-12-15-3-1">
<ul class="org-ul">
<li>Stanford University <a href="https://www.youtube.com/watch?v=6niqTuYFZLQ">https://www.youtube.com/watch?v=6niqTuYFZLQ</a></li>
</ul>
<p>
In order to calculate the gradient at t=4 we would need to backpropagate 3 steps and sum up the gradients.
</p>
<ul class="org-ul">
<li>have difficulties learning long-term dependencies = vanishing/exploding gradient problem</li>
</ul>

<p>
Problems:
</p>
<ul class="org-ul">
<li>Exploding gradients
<ul class="org-ul">
<li>Gradient clipping: scale gradient if its norm is too big</li>
</ul></li>
<li>Vanishing gradients
<ul class="org-ul">
<li>change RNN architecture</li>
</ul></li>
</ul>

<p>
<b>Truncated</b> Backpropagation Through Time - Carry hidden states forward in time forever, but only backpropogate
for some smaller number of steps
</p>
</div>
</li>
<li><a id="org768ba58"></a>Bidirectional RNNs<br />
<div class="outline-text-5" id="text-12-15-3-2">
<p>
want to look at both the left and the right context
</p>
<ul class="org-ul">
<li>two RNNs</li>
<li>both get input x</li>
<li>one get input from t+1, one get input from t-1</li>
<li>o = computed based on the hidden state of both RNNs</li>
</ul>

<p>
<b>Structure</b>
</p>
<ul class="org-ul">
<li>^   ^   ^ - concat of two</li>
<li>O &lt; O &lt; O</li>
<li>O &gt; O &gt; O</li>
<li>^   ^   ^ - input to two</li>
</ul>

<pre class="example">
model.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10), merge_mode='concat'))
model.add(Bidirectional(LSTM(10)))
</pre>


<p>
Обычно вход - это слова, и выход выдается сразу
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgaf4820e" class="outline-4">
<h4 id="orgaf4820e"><span class="section-number-4">12.15.4.</span> RNTNs recursive [riːˈkɜːsɪv]</h4>
<div class="outline-text-4" id="text-12-15-4">
<ul class="org-ul">
<li><a href="https://papers.nips.cc/paper/5551-deep-recursive-neural-networks-for-compositionality-in-language.pdf">https://papers.nips.cc/paper/5551-deep-recursive-neural-networks-for-compositionality-in-language.pdf</a></li>
<li>Stanford <a href="https://www.youtube.com/watch?v=RfwgqPkWZ1w">https://www.youtube.com/watch?v=RfwgqPkWZ1w</a></li>
<li>Sentiment <a href="https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf">https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf</a></li>
</ul>
<p>
Recurrent vs Recursive:
</p>
<ul class="org-ul">
<li>Recurrent это тоже дерево, только сдвинуто вершиной к концу предложения</li>
</ul>

<p>
two leave (two inputs) -&gt; neural network -&gt;
</p>
<ol class="org-ol">
<li>result when two vectors are merged</li>
<li>Score of how plausable [ˈplɔːzəbl] правдопадобны</li>
</ol>

<p>
Виды
</p>
<ol class="org-ol">
<li>Standard RNNs - Paraphrase detection</li>
<li>Matrix-Vector RNNs - Relation classification</li>
<li>Recursive Neural Tensor Networs - Sentiment Analysis</li>
<li>Tree LSTMs - Phrase simularity - hardest</li>
</ol>
</div>
</div>
<div id="outline-container-org6d4a97f" class="outline-4">
<h4 id="org6d4a97f"><span class="section-number-4">12.15.5.</span> LSTM <a id="org6c76edd"></a></h4>
<div class="outline-text-4" id="text-12-15-5">
<ul class="org-ul">
<li>article <a href="https://ahmedhanibrahim.wordpress.com/2016/10/09/another-lstm-tutorial/">https://ahmedhanibrahim.wordpress.com/2016/10/09/another-lstm-tutorial/</a></li>
<li>Deep Learning for Time Series Forecasting <a href="https://machinelearningmastery.com/start-here/#deep_learning_time_series">https://machinelearningmastery.com/start-here/#deep_learning_time_series</a></li>
</ul>
<p>
see <a href="#orgac76ae2">11.5.13</a>
</p>
<ul class="org-ul">
<li>Learning to Forget: Continual Prediction with LSTM <a href="https://pdfs.semanticscholar.org/e10f/98b86797ebf6c8caea6f54cacbc5a50e8b34.pdf">https://pdfs.semanticscholar.org/e10f/98b86797ebf6c8caea6f54cacbc5a50e8b34.pdf</a></li>
</ul>
<p>
type of RNN
</p>
<ul class="org-ul">
<li>W, U - weights</li>
<li>i - input gate - controls the extent to which a new value flows into the cell</li>
<li>o - output gate - value in the cell is used to compute the output activation</li>
<li>f - forget gate -  controls the extent to which a value remains in the cell</li>
<li>c - memory cell or just cell</li>
</ul>

<p>
Pros:
</p>
<ul class="org-ul">
<li>only elementwise operations</li>
<li>easier to avoid gradient problems of RNN</li>
<li>we maintain gradient on cell state</li>
</ul>

<p>
Cons:
</p>
<ul class="org-ul">
<li>training только от начала до конца так как hidden state должен инициализироваться в начале</li>
<li>predict only at one step - because state pass from before to next step</li>
<li>batch может состоять только повторяющихся данных - дней, месяцев</li>
<li>неравномерно понимает последовательность - гибче в начале - грубее к концу</li>
</ul>

<p>
well-suited to
</p>
<ul class="org-ul">
<li>classifying</li>
<li>processing</li>
<li>making predictions based on time series data</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org2390d04"></a>Architecture<br />
<div class="outline-text-5" id="text-12-15-5-1">
<p>
<a href="https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/">https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/</a>
</p>

<p>
Vanilla LSTM:
</p>
<ul class="org-ul">
<li>model.add(LSTM(50, activation='relu', input<sub>shape</sub>=(n<sub>steps</sub>, n<sub>features</sub>)))</li>
<li>model.add(Dense(1))</li>
</ul>
<p>
Stacked LSTM:
</p>
<ul class="org-ul">
<li>model.add(LSTM(50, activation='relu', return<sub>sequences</sub>=True, input<sub>shape</sub>=(n<sub>steps</sub>, n<sub>features</sub>)))</li>
<li>model.add(LSTM(50, activation='relu'))</li>
<li>model.add(Dense(1))</li>
</ul>
<p>
Bidirectional LSTM
</p>

<p>
CNN LSTM - CNN can interpret each subsequence of two time steps and provide a time series of interpretations
of the subsequences to the LSTM model to process as input.
</p>

<p>
ConvLSTM
</p>
</div>
</li>
<li><a id="org53672d6"></a>limitation Autoregression<br />
<div class="outline-text-5" id="text-12-15-5-2">
<p>
An autoregression (AR) approach was used to model these problems.
This means that the next time step was taken as a function of some number of past (or lag) observations.
</p>

<p>
examples:
</p>
<ul class="org-ul">
<li>Mackey-Glass Series</li>
<li>Chaotic Laser Data (Set A)</li>
</ul>

<p>
LSTM learned to tune into the fundamental oscillation of each series but was unable to accurately follow the signal.
</p>
</div>
</li>
<li><a id="orgc6ea696"></a>LSTM with a forget gate<br />
<div class="outline-text-5" id="text-12-15-5-3">
<p>
[Hochreiter et al.,1997]
<b>Inputs:</b>
</p>
<ul class="org-ul">
<li>cell state = ct-1</li>
<li>hidden state vector = ht-1</li>
<li>input vector = xt</li>
</ul>
<p>
<b>Outputs:</b>
</p>
<ul class="org-ul">
<li>cell state = ct</li>
<li>hidden state vector = ht</li>
</ul>

<p>
forward pass:
</p>
<ul class="org-ul">
<li>• - Hadamard product -тупое поэлементное умножение two matrices of the same dimensions</li>
<li>ft=σg(Wf*xt+Uf*ht-1 + bf) - σg - sigmoid - основной фильтр забывания</li>
<li>it=σg(Wi*xt+Ui*ht-1 + bi) - какие значения следует обновить</li>
<li>ot=σg(Wo*xt+Uo*ht-1 + bo)</li>
<li>ct=ft•ct-1 + it•σc(Wc*xt+Uc*ht-1+bc) - σc - tanh (вектор новых значений-кандидатов которые можно добавить в
состояние ячейки)</li>
<li>ht=ot•σh(ct) - σh - tanh or σh(x)=x - фильтруем старый скрытый вход по новому состоянию</li>
<li>initial c0=0, h0=0</li>
</ul>

<p>
Compact:
</p>
<ul class="org-ul">
<li>(i f o g) = (σ σ σ tanh)W(ht-1 xt)</li>
<li>ct = f • ct-1 + i•g</li>
<li>ht = o • tanh(ct)</li>
</ul>
</div>
</li>
<li><a id="orgadd637e"></a>Peephole LSTM<br />
<div class="outline-text-5" id="text-12-15-5-4">
<ul class="org-ul">
<li>One output</li>
<li>Peephole connections allow the gates to access the constant error carousel (CEC), whose activation is the
cell state.</li>
</ul>
</div>
</li>
<li><a id="orgd787391"></a>Simple Recurrent Units (SRU)<br />
<div class="outline-text-5" id="text-12-15-5-5">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/1709.02755.pdf">https://arxiv.org/pdf/1709.02755.pdf</a></li>
</ul>
</div>
</li>
<li><a id="org6bef7e0"></a>Gated recurrent units (GRUs) 2014<br />
<div class="outline-text-5" id="text-12-15-5-6">
<ul class="org-ul">
<li>fewer parameters than LSTM</li>
<li>better performance on certain smaller datasets</li>
</ul>

<p>
performance on certain tasks was found to be similar to that of LSTM:
</p>
<ul class="org-ul">
<li>polyphonic music modeling</li>
<li>speech signal modeling</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgb173af5" class="outline-4">
<h4 id="orgb173af5"><span class="section-number-4">12.15.6.</span> Attention, SAN self-attention, Transformer</h4>
<div class="outline-text-4" id="text-12-15-6">
<ul class="org-ul">
<li>2017 Attention Is All You Need <a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></li>
<li>2019 UNIVERSAL TRANSFORMERS <a href="https://arxiv.org/pdf/1807.03819.pdf">https://arxiv.org/pdf/1807.03819.pdf</a></li>
<li>article Self Attention <a href="https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d">https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d</a></li>
<li>article <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html</a></li>
<li>статья Transformer <a href="https://habr.com/ru/post/341240/">https://habr.com/ru/post/341240/</a></li>
<li>Pytorch seq2seq <a href="https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb">https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb</a></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org310cc8f"></a>seq2seq<br />
<div class="outline-text-5" id="text-12-15-6-1">
<p>
LSTM
</p>

<div class="org-src-container">
<pre class="src src-artist">                                   decoder
                         /---------------------------\
                 hidden
                 state   Wo     ai      ni      &lt;EOS&gt;
                    |
                    |
+-+     +-+     +-+ |   +-+     +-+     +-+     +-+
| |     | |     | | |   | |     | |     | |     | |
| |     | |     | | |   | |     | |     | |     | |
| |     | |     | | |   | |     | |     | |     | |
| |     | |     | | |   | |     | |     | |     | |
| +----&gt;| +----&gt;| +----&gt;| +----&gt;| +----&gt;| +----&gt;| +
| |     | |     | |     | |     | |     | |     | |
| |     | |     | |     | |     | |     | |     | |
| |     | |     | |     | |     | |     | |     | |
| |     | |     | |     | |     | |     | |     | |
| |     | |     | |     | |     | |     | |     | |
+-+     +-+     +-+     +-+     +-+     +-+     +-+

 I     want     to     &lt;EOS&gt;
\--------------------------/
      encoder

</pre>
</div>

<p>
Enhances:
</p>
<ol class="org-ol">
<li>problem: hidden state mutate and first state fade out. solution: add first state to all mutated hidden states</li>
<li>pr: one lavel of LSTM is simple. solution: make LSTM deem and separate encoder input from decoder output</li>
<li>pass decode sub-layer to encoder sub-layer at every step</li>
<li>pr: next decoder step don't know about preview decoder output softmax. solution: add decoder output to next
encoder sub-layer.</li>
<li>pr: "I" is very importent to "Wo". solution: make <b>reverse</b> of ecoder sequence to "to want I"</li>
<li>pr: All information compressed in last hidden state, we need return to encoder state. solution: ATTENTION!</li>
</ol>
<p>
<a href="https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3">https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3</a>
</p>
</div>
</li>
<li><a id="org668698d"></a>RNN with attention<br />
<div class="outline-text-5" id="text-12-15-6-2">
<div class="org-src-container">
<pre class="src src-artist">                decode
          /---------------\
           yt      y(t+1)
           +-+    +-+    +-+
           | |    | |    | |                         LSTM
       ---&gt;|S|---&gt;|S|---&gt;|S|
           | |   ^| |    | |  S - get all h + a
 |-------- +-+   |+-+    +-+
 |               |
 --&gt;+-+          |softmax = of all a - show which h is more important for y(t+1)
 --&gt;| +--&gt;       \
 |  +-+ at,1    --+--
 |             (--|--)
 |-+            ----- _
   |         _/  /  \  \__ at,4          a - attention - one digit.
   |   at,1_/   |    \_   \__
   |    __/     /at,2  \     \___
       /       |        \at,3    \
   +---+     +-+-+     +-\-+     +---+
   |   |     |   |     |   |     |   |
   |hb +----&gt;|h2 +----&gt;|h3 +----&gt;|   |
   |   |     |   |     |   |     |   |
   +---+     +---+     +---+     +---+    Bi-directional LSTM
   +---+     +---+     +---+     +---+
   |   |     |   |     |   |     |   |
   | hf|&lt;----|h2 |&lt;----|h3 |&lt;----|   |
   |   |     |   |     |   |     |   |
   +---+     +---+     +---+     +---+
 h=[hf,hb]

   \---------------------------------/
               encoder
</pre>
</div>

<p>
Allow to visualize attantion as correlation matrix between encoder and decoder.
</p>
</div>
</li>

<li><a id="orgfe6117e"></a>attention<br />
<div class="outline-text-5" id="text-12-15-6-3">
<p>
NEURAL MACHINE TRANSLATION <a href="https://arxiv.org/pdf/1409.0473.pdf">https://arxiv.org/pdf/1409.0473.pdf</a>
</p>

<p>
based on (RNN) Encoder–Decoder
</p>
<ul class="org-ul">
<li>X - encoder input</li>
<li>Y - decoder output - использует attention на hidden state si - f(s(i-1),y(i-1),ci) - concatenation,
fully-connected layer with a nonlinear activation. У декодера hadden state становится чуть больше.</li>
</ul>

<p>
terms:
</p>
<ul class="org-ul">
<li><b>score</b> or <b>content-based function</b> -</li>
<li><b>context vector</b> - output of attention layer (and encoder), depends on a sequence of annotations - позволяет
понять какая из hiddent state of encoder важнее
<ul class="org-ul">
<li>ci = (j)∑aij*hj</li>
</ul></li>
<li><b>attention or align</b> - насколько релеванты друг другу yi, hi или s и h.
<ul class="org-ul">
<li>aij =softmax(eij) - цифры от 0 до 1</li>
<li>eij = a(s(i-1),hj) , s - предыдущий hidden state декодера</li>
</ul></li>
<li>function f is a  g = g(ui-</li>
</ul>

<p>
Luong et al. describe a few more attention models that offer improvements and simplifications <a href="https://arxiv.org/abs/1508.04025">https://arxiv.org/abs/1508.04025</a>
</p>
<ul class="org-ul">
<li>score - основа для aign.
<ul class="org-ul">
<li>dot ht*st</li>
<li>general</li>
<li>concat</li>
</ul></li>
<li>align = softmax(score)</li>
</ul>

<p>
models (whether the “attention”is placed on all source positions or on only a fewsource positions.):
</p>
<ul class="org-ul">
<li>global - con-sider all the hidden states of the encoder</li>
<li>local</li>
</ul>
</div>
</li>

<li><a id="org939681d"></a>Self-attention<br />
<div class="outline-text-5" id="text-12-15-6-4">
<ul class="org-ul">
<li>CNN-LSTM <a href="http://proceedings.mlr.press/v37/xuc15.pdf">http://proceedings.mlr.press/v37/xuc15.pdf</a></li>
</ul>

<p>
<b>Self-attention</b>, also known as <b>intra-attention</b>
</p>

<p>
SAN:
</p>
<ul class="org-ul">
<li>large memory requirement to store the alignment scores</li>
</ul>

<p>
soft - essentially the same type of attention as in Bahdanau et al., 2015.
</p>
<ul class="org-ul">
<li>Pro: the model is smooth and differentiable.</li>
<li>Con: expensive when the source input is large.</li>
</ul>
<p>
hard - selects one patch of the image to attend to at a time
</p>
<ul class="org-ul">
<li>Pro: less calculation at the inference time.</li>
<li>Con: the model is non-differentiable and requires more complicated techniques such as variance reduction or
reinforcement learning to train. (Luong, et al., 2015)</li>
</ul>
</div>
</li>
<li><a id="org3dba103"></a>Transformer <a id="org10d2176"></a><br />
<div class="outline-text-5" id="text-12-15-6-5">
<p>
Seq2seq or Neural machine translation (NMT) without RNN
</p>
<ul class="org-ul">
<li>Encoder + Decoder</li>
<li>Main part: <b>multi-head self-attention mechanism</b></li>
<li>At each step the model is auto-regressive, consuming the previously generated symbols as additional input
when generating the next.</li>
<li>Encoder - is designed to attend to all words in the input sequence regardless of their position in the
sequence. generates an attention-based representation with capability to locate a specific piece of
information from a large context.</li>
<li>Decoder - modified to attend only to the preceding words. Function to retrieve information from the encoded
representation.  The first multi-head attention submodule is masked to prevent positions from attending to
the future.</li>
</ul>

<p>
Позиционное кодирование критически необходимо только для энкодерам, а вот декодеры (GPT, LLaMA и тд) могут
 прекрасно работать и без него! Похоже, что каузальные маски внимания (которые не позволяют заглядывать в
 правый контекст) сами по себе являются отличным источником информации о позиции токенов. И более того,
 трансформер БЕЗ позиционного кодирования лучше обобщается на размер контекста, выходящий за длину примеров из
 обучения, даже по сравнению с такими мудрёными методами, как Rotary или ALiBi.
</p>

<p>
Key, value, query come from retrieval systems. Query to search, map query against a set of keys(video title,
 description, etc.) associated with candidate in the database, then present you the best matched videos
 (values).
</p>
</div>

<ol class="org-ol">
<li><a id="org2aa5759"></a>Encoder:<br />
<div class="outline-text-6" id="text-12-15-6-5-1">
<p>
Input:
</p>
<ol class="org-ol">
<li>padding [“&lt;pad&gt;”, “&lt;pad&gt;”, “&lt;pad&gt;”, “Hello”, “, “, “how”, “are”, “you”, “?”] -&gt; [5, 5, 5, 34, 90, 15, 684,
55, 193]</li>
<li><p>
words to vacabID and to vects (emb<sub>dim</sub>)
</p>
<ul class="org-ul">
<li>Token Embeddings - модель ищет эмбеддинг слова в своей матрице эмбеддингов. Embedding size:
768(small), 1600(extra large) - count of tokens is является гиперпараметром, который мы можем</li>
</ul>
<p>
устанавливать, и, по сути, равен длине самого длинного предложения в обучающем корпусе.
</p></li>
<li>Positional Encoding - add numbers between [-1,1] using predetermined (non-learned) sinusoidal functions to
the token embeddings - relative positions, not absolute. Из за отказа от реккурентности все входные нейроны
не имеют позиции (self-attention operation is permutation invariant).
<ul class="org-ul">
<li>pij =
<ul class="org-ul">
<li>if j is even(четное) = sin(i/ (j/ 10000<sup>d</sup>*emb<sub>dim</sub>))</li>
<li>if j is odd = cos(i/ ((j-1)/ 10000<sup>d</sup>*emb<sub>dim</sub>))</li>
</ul></li>
</ul></li>
<li>Multi-Head Self-Attention.(with Scaled Dot-Product Attention). headi = Q,K and V</li>
<li>Positional-wise fully connected feed-forward network.</li>
<li>Residual connection around each of the two sub-layers folloed by layer normalization.</li>
</ol>

<p>
Since the encoder attends to all words in the input sequence, irrespective if they precede or succeed the
 word under consideration, then the Transformer encoder is bidirectional.
</p>

<p>
Layer Normalization
</p>
</div>
</li>

<li><a id="orgb2d16b3"></a><span class="todo TODO">TODO</span> Decoder<br /></li>
<li><a id="orgc1d2664"></a>applications:<br />
<div class="outline-text-6" id="text-12-15-6-5-3">
<ul class="org-ul">
<li>BERT is an example of encoder-only model;</li>
<li>GPT are decoder-only models.</li>
<li>T5 (Encoder-Decoder)</li>
</ul>
</div>
</li>
<li><a id="orgab4749b"></a>links<br />
<div class="outline-text-6" id="text-12-15-6-5-4">
<ul class="org-ul">
<li>Transformer explained <a href="https://www.youtube.com/watch?v=4Bdc55j80l8">https://www.youtube.com/watch?v=4Bdc55j80l8</a></li>
<li>BERT 2018 <a href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></li>
<li>Tensorflow tutorial <a href="https://www.tensorflow.org/beta/tutorials/text/transformer">https://www.tensorflow.org/beta/tutorials/text/transformer</a></li>
<li>Sber <a href="https://github.com/sberbank-ai/ner-bert">https://github.com/sberbank-ai/ner-bert</a></li>
<li>Google <a href="https://github.com/google-research/bert">https://github.com/google-research/bert</a></li>
<li>Attention <a href="https://habr.com/ru/post/341240/">https://habr.com/ru/post/341240/</a></li>
<li><a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</a></li>
<li><a href="https://machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning">https://machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning</a></li>
</ul>
</div>
</li>
<li><a id="org261fd45"></a>BERT<br />
<div class="outline-text-6" id="text-12-15-6-5-5">
<p>
Context-free models such as word2vec or GloVe generate a single word embedding representation for each word
in the vocabulary, whereas BERT takes into account the context for each occurrence of a given word.
</p>

<p>
BERT tokenizer was trained separately to learn the vocabulary and subword tokenization rules
</p>
</div>
</li>
<li><a id="org67797d4"></a>decoders/autoregressive (AR) vs encoders/autoencoding (AE) vs Encoder-Decoder/seq2seq models<br />
<div class="outline-text-6" id="text-12-15-6-5-6">
<p>
decoders/autoregressive (AR)
</p>
<ul class="org-ul">
<li>AR language model is only trained to encode a uni-directional context (either forward or backward)</li>
<li>each token is predicted and conditioned on the previous token. every token can only at tend to previous
tokens in the self-attention layers</li>
<li></li>

<li>Pros: AR language models are good at generative NLP tasks. Since AR models utilize causal attention to
predict the next token, they are naturally applicable for generating content. The other advantage of AR
models is that generating data for them is relatively easy, since you can simply have the training objective
be to predict the next token in a given corpus. generating long sequences of text with high accuracy</li>
<li>Cons: AR language models have some disadvantages, it only can use forward context or backward context, which
means it can’t use bidirectional context at the same time.</li>
</ul>

<p>
encoders/autoencoding (AE) - BERT
</p>
<ul class="org-ul">
<li>generate all its outputs at once. inputs and output positions of each token are the same</li>
<li>pros: understanding context within given texts in order to perform more sophisticated tasks as sentiment
analysis or NLU.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org1d8d823"></a>SOTAs<br />
<div class="outline-text-7" id="text-12-15-6-5-6-1">
<p>
decoders/autoregressive (AR)
</p>
<ul class="org-ul">
<li>GPT &#x2026;</li>
</ul>

<p>
encoder/autoregressive (AR)
</p>
<ul class="org-ul">
<li>BERT</li>
<li>ELECTRA</li>
</ul>

<p>
Encoder-Decoder/seq2seq models
</p>
<ul class="org-ul">
<li>T5</li>
<li>BART</li>
<li>BigBird</li>
</ul>
</div>
</li>
<li><a id="org5c122cf"></a>links<br />
<div class="outline-text-7" id="text-12-15-6-5-6-2">
<p>
<a href="https://aman.ai/primers/ai/autoregressive-vs-autoencoder-models/">https://aman.ai/primers/ai/autoregressive-vs-autoencoder-models/</a>
</p>
</div>
</li>
</ol>
</li>

<li><a id="org2b750bd"></a>Multi-Head Attention<br />
<div class="outline-text-6" id="text-12-15-6-5-7">
<ul class="org-ul">
<li>permutation-equivariant, and thus, outputs the same values no matter in what order we enter the inputs
(inputs and outputs are permuted equally). That is why it have residual connection around to keep
information about positions.</li>
</ul>
</div>
</li>
<li><a id="org37b3f64"></a>multi-head self-attention mechanism<br />
<div class="outline-text-6" id="text-12-15-6-5-8">
<p>
<b>self-attention mechanism</b>
</p>

<p>
attention score - softmax(Q*K<sub>T</sub>/sqrt(dk)) ( not exist in original article)
</p>
<ol class="org-ol">
<li>dot product of Query with all keys</li>
<li>divide each Dot by sqrt of K size - to prevent small gradients</li>
<li>apply a softmax to get weights on the values</li>
<li>score * V, then sum up</li>
</ol>

<p>
Attention(Q,K,V) = softmax(Q*K<sub>T</sub>/sqrt(dk))*V
</p>
<ul class="org-ul">
<li>Have something from other words, but can not dominate.</li>
</ul>

<p>
Q, K, V - is result of multiplication of Input vector to W<sub>Q</sub>,  W<sub>K</sub> and W<sub>V</sub> matrices
</p>

<ol class="org-ol">
<li>Compute Dot Products: (query . key) to obtain a matrix of similarity scores.</li>
<li>Normalize Scores: Divide the similarity scores by the square root of the key dimension to scale and
stabilize the attention weights.</li>
<li>Apply Softmax: Apply a softmax function to the normalized scores to obtain a probability distribution over
the keys, assigning higher weights to more relevant key-value pairs.</li>
<li>Weight Values: Multiply the softmax weights by the value matrix to obtain the weighted values.</li>
<li>Sum Weighted Values: Sum the weighted values along the key dimension to obtain the attended representation.</li>
</ol>

<p>
<b>multi-head attantion</b> - is extension of self-attention. reducing the computational cost of attending to all positions.
</p>
<ul class="org-ul">
<li>head<sub>i</sub> = Attention(Q*WiQ,K*WiK,V*WiV), where i is 8 for ex. - heach head have reduced dimension.</li>
<li>MultiHead(Q,V,K) = Concat(Head1,Head2 .. Headi)*Wo</li>
<li>it allow to look at different positions</li>
</ul>
</div>
</li>
<li><a id="org36dc374"></a>Keras implementation of multi-head self-attention mechanism<br />
<div class="outline-text-6" id="text-12-15-6-5-9">
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> tensorflow <span style="color: #8ac6f2; font-weight: bold;">import</span> math, matmul, reshape, shape, transpose, cast, float32
<span style="color: #8ac6f2; font-weight: bold;">from</span> tensorflow.keras.layers <span style="color: #8ac6f2; font-weight: bold;">import</span> Dense, Layer
<span style="color: #8ac6f2; font-weight: bold;">from</span> keras.backend <span style="color: #8ac6f2; font-weight: bold;">import</span> softmax

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Implementing the Scaled-Dot Product Attention</span>
<span style="color: #8ac6f2; font-weight: bold;">class</span> <span style="color: #92a65e; font-weight: bold;">DotProductAttention</span>(Layer):
    <span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">__init__</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>, **kwargs):
        <span style="color: #e5786d;">super</span>(DotProductAttention, <span style="color: #8ac6f2; font-weight: bold;">self</span>).__init__(**kwargs)

    <span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">call</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>, queries, keys, values, d_k, mask=<span style="color: #e5786d; font-weight: bold;">None</span>):
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Scoring the queries against the keys after transposing the latter, and scaling</span>
        <span style="color: #cae682;">scores</span> = matmul(queries, keys, transpose_b=<span style="color: #e5786d; font-weight: bold;">True</span>) / math.sqrt(cast(d_k, float32))

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Apply mask to the attention scores</span>
        <span style="color: #8ac6f2; font-weight: bold;">if</span> mask <span style="color: #8ac6f2; font-weight: bold;">is</span> <span style="color: #8ac6f2; font-weight: bold;">not</span> <span style="color: #e5786d; font-weight: bold;">None</span>:
            <span style="color: #cae682;">scores</span> += -1e9 * mask

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Computing the weights by a softmax operation</span>
        <span style="color: #cae682;">weights</span> = softmax(scores)

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Computing the attention by a weighted sum of the value vectors</span>
        <span style="color: #8ac6f2; font-weight: bold;">return</span> matmul(weights, values)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Implementing the Multi-Head Attention</span>
<span style="color: #8ac6f2; font-weight: bold;">class</span> <span style="color: #92a65e; font-weight: bold;">MultiHeadAttention</span>(Layer):
    <span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">__init__</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>, h, d_k, d_v, d_model, **kwargs):
        <span style="color: #e5786d;">super</span>(MultiHeadAttention, <span style="color: #8ac6f2; font-weight: bold;">self</span>).__init__(**kwargs)
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">attention</span> = DotProductAttention()  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Scaled dot product attention</span>
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">heads</span> = h  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Number of attention heads to use</span>
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">d_k</span> = d_k  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Dimensionality of the linearly projected queries and keys</span>
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">d_v</span> = d_v  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Dimensionality of the linearly projected values</span>
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">d_model</span> = d_model  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Dimensionality of the model</span>
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">W_q</span> = Dense(d_k)  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Learned projection matrix for the queries</span>
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">W_k</span> = Dense(d_k)  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Learned projection matrix for the keys</span>
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">W_v</span> = Dense(d_v)  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Learned projection matrix for the values</span>
        <span style="color: #8ac6f2; font-weight: bold;">self</span>.<span style="color: #cae682;">W_o</span> = Dense(d_model)  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Learned projection matrix for the multi-head output</span>

    <span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">reshape_tensor</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>, x, heads, flag):
        <span style="color: #8ac6f2; font-weight: bold;">if</span> flag:
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)</span>
            <span style="color: #cae682;">x</span> = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))
            <span style="color: #cae682;">x</span> = transpose(x, perm=(0, 2, 1, 3))
        <span style="color: #8ac6f2; font-weight: bold;">else</span>:
            <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)</span>
            <span style="color: #cae682;">x</span> = transpose(x, perm=(0, 2, 1, 3))
            <span style="color: #cae682;">x</span> = reshape(x, shape=(shape(x)[0], shape(x)[1], <span style="color: #8ac6f2; font-weight: bold;">self</span>.d_k))
        <span style="color: #8ac6f2; font-weight: bold;">return</span> x

    <span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">call</span>(<span style="color: #8ac6f2; font-weight: bold;">self</span>, queries, keys, values, mask=<span style="color: #e5786d; font-weight: bold;">None</span>):
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Rearrange the queries to be able to compute all heads in parallel</span>
        <span style="color: #cae682;">q_reshaped</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.reshape_tensor(<span style="color: #8ac6f2; font-weight: bold;">self</span>.W_q(queries), <span style="color: #8ac6f2; font-weight: bold;">self</span>.heads, <span style="color: #e5786d; font-weight: bold;">True</span>)
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Resulting tensor shape: (batch_size, heads, input_seq_length, -1)</span>

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Rearrange the keys to be able to compute all heads in parallel</span>
        <span style="color: #cae682;">k_reshaped</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.reshape_tensor(<span style="color: #8ac6f2; font-weight: bold;">self</span>.W_k(keys), <span style="color: #8ac6f2; font-weight: bold;">self</span>.heads, <span style="color: #e5786d; font-weight: bold;">True</span>)
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Resulting tensor shape: (batch_size, heads, input_seq_length, -1)</span>

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Rearrange the values to be able to compute all heads in parallel</span>
        <span style="color: #cae682;">v_reshaped</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.reshape_tensor(<span style="color: #8ac6f2; font-weight: bold;">self</span>.W_v(values), <span style="color: #8ac6f2; font-weight: bold;">self</span>.heads, <span style="color: #e5786d; font-weight: bold;">True</span>)
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Resulting tensor shape: (batch_size, heads, input_seq_length, -1)</span>

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Compute the multi-head attention output using the reshaped queries, keys and values</span>
        <span style="color: #cae682;">o_reshaped</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.attention(q_reshaped, k_reshaped, v_reshaped, <span style="color: #8ac6f2; font-weight: bold;">self</span>.d_k, mask)
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Resulting tensor shape: (batch_size, heads, input_seq_length, -1)</span>

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Rearrange back the output into concatenated form</span>
        <span style="color: #cae682;">output</span> = <span style="color: #8ac6f2; font-weight: bold;">self</span>.reshape_tensor(o_reshaped, <span style="color: #8ac6f2; font-weight: bold;">self</span>.heads, <span style="color: #e5786d; font-weight: bold;">False</span>)
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Resulting tensor shape: (batch_size, input_seq_length, d_v)</span>

        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Apply one final linear projection to the output to generate the multi-head attention</span>
        <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Resulting tensor shape: (batch_size, input_seq_length, d_model)</span>
        <span style="color: #8ac6f2; font-weight: bold;">return</span> <span style="color: #8ac6f2; font-weight: bold;">self</span>.W_o(output)



<span style="color: #8ac6f2; font-weight: bold;">from</span> numpy <span style="color: #8ac6f2; font-weight: bold;">import</span> random

<span style="color: #cae682;">input_seq_length</span> = 5  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Maximum length of the input sequence</span>
<span style="color: #cae682;">h</span> = 8  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Number of self-attention heads</span>
<span style="color: #cae682;">d_k</span> = 64  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Dimensionality of the linearly projected queries and keys</span>
<span style="color: #cae682;">d_v</span> = 64  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Dimensionality of the linearly projected values</span>
<span style="color: #cae682;">d_model</span> = 512  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Dimensionality of the model sub-layers' outputs</span>
<span style="color: #cae682;">batch_size</span> = 64  <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Batch size from the training process</span>

<span style="color: #cae682;">queries</span> = random.random((batch_size, input_seq_length, d_k))
<span style="color: #cae682;">keys</span> = random.random((batch_size, input_seq_length, d_k))
<span style="color: #cae682;">values</span> = random.random((batch_size, input_seq_length, d_v))

<span style="color: #cae682;">multihead_attention</span> = MultiHeadAttention(h, d_k, d_v, d_model)
<span style="color: #e5786d;">print</span>(multihead_attention(queries, keys, values))
</pre>
</div>
</div>
</li>
<li><a id="org187645a"></a>Encoder vs Decoder vs Encode-Decoder<br />
<div class="outline-text-6" id="text-12-15-6-5-10">
<p>
The encoder processes the input sequence and transforms it into a fixed-sized context vector, while the
decoder takes this context vector and generates the output sequence.
</p>

<ul class="org-ul">
<li>Encoder-decoder models are suitable for sequence-to-sequence tasks</li>
<li>decoder-only models are effective in generative tasks.</li>
<li>Encoder-only models are useful for classification tasks and providing contextual representations.</li>
</ul>
</div>
</li>
<li><a id="orgbb95dc3"></a>links<br />
<div class="outline-text-6" id="text-12-15-6-5-11">
<p>
Based on self attention or  Attention Is All You Need 2017 <a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a>
</p>
<ul class="org-ul">
<li>all(bad) <a href="https://habr.com/ru/articles/490842/">https://habr.com/ru/articles/490842/</a></li>
<li>Architecture <a href="https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3">https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3</a></li>
<li>multi-head attention in Keras explained <a href="https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/">https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/</a>
<ul class="org-ul">
<li>attention <a href="https://machinelearningmastery.com/the-transformer-attention-mechanism">https://machinelearningmastery.com/the-transformer-attention-mechanism</a></li>
<li>Transformer explained <a href="https://machinelearningmastery.com/the-transformer-model/">https://machinelearningmastery.com/the-transformer-model/</a></li>
</ul></li>
<li>the best book about transformer <a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html">https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html</a></li>
<li>multi-head <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html">https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html</a></li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org75b226b"></a>auto-regressive property<br />
<div class="outline-text-5" id="text-12-15-6-6">
<p>
Transformer decoder is autoregressive at inference time and non-autoregressive at training time.
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orge02ff0e" class="outline-4">
<h4 id="orge02ff0e"><span class="section-number-4">12.15.7.</span> NeRF</h4>
<div class="outline-text-4" id="text-12-15-7">
<p>
3D computer vision problem - reconstructing the 3D shape from images
</p>
<ol class="org-ol">
<li>NeRF <a href="https://arxiv.org/abs/2003.08934">https://arxiv.org/abs/2003.08934</a></li>
<li>RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs — <a href="https://arxiv.org/abs/2112.00724">https://arxiv.org/abs/2112.00724</a></li>
<li>pixelNeRF: Neural Radiance Fields from One or Few Images — <a href="https://arxiv.org/abs/2012.02190">https://arxiv.org/abs/2012.02190</a></li>
</ol>

<p>
The training time is very long.
</p>

<p>
Instant Neural Graphics Primitives with a Multiresolution Hash Encoding — <a href="https://nvlabs.github.io/instant-ngp/">https://nvlabs.github.io/instant-ngp/</a>
</p>

<p>
Camera pose of each image is required.
</p>

<p>
GNeRF: GAN-based Neural Radiance Field without Posed Camera — <a href="https://arxiv.org/abs/2103.15606">https://arxiv.org/abs/2103.15606</a>
NeRF- -: Neural Radiance Fields Without Known Camera Parameters — <a href="https://arxiv.org/abs/2102.07064">https://arxiv.org/abs/2102.07064</a>
</p>

<p>
Other Interesting NeRF-related paper
</p>

<p>
Zero-Shot Text-Guided Object Generation with Dream Fields — <a href="https://ajayj.com/dreamfields">https://ajayj.com/dreamfields</a>
Block-NeRF: Scalable Large Scene Neural View Synthesis — <a href="https://arxiv.org/abs/2202.05263">https://arxiv.org/abs/2202.05263</a>
</p>
</div>
</div>
<div id="outline-container-orgf4b659c" class="outline-4">
<h4 id="orgf4b659c"><span class="section-number-4">12.15.8.</span> Autoencoders</h4>
<div class="outline-text-4" id="text-12-15-8">
<p>
Denoising Autoencoders/Stacked Denoising Autoencoders
 <a href="https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf">https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf</a>
</p>

<p>
Autoencoder - encoder-decoder very simply architecture - train reconstruction of the original input.
</p>
<ul class="org-ul">
<li>minimal hidden layout for sufficient resolution.</li>
<li>used for : reduce noise, demensionality reduction (sometimes better than PCA), data compression, anomaly
detection.</li>
</ul>

<p>
Denoising Autoencoder - input is partially corrupted by adding noises to or masking some values of the input
 vector in a stochastic manner.
</p>
<ul class="org-ul">
<li>was created to avoid overfitting and improve the robustness.</li>
</ul>
<p>
<a href="https://lilianweng.github.io/posts/2018-08-12-vae/#reparameterization-trick">https://lilianweng.github.io/posts/2018-08-12-vae/#reparameterization-trick</a>
</p>
</div>
</div>
<div id="outline-container-org0a72468" class="outline-4">
<h4 id="org0a72468"><span class="section-number-4">12.15.9.</span> Variational Autoencoders (VAE)</h4>
<div class="outline-text-4" id="text-12-15-9">
<p>
Variational Autoencoders
</p>
<ul class="org-ul">
<li>4 key components: an encoder, the latent space, a decoder and a loss function</li>
<li>used for: generate scenery in video games - we train the neural network to understand what characteristics trees have,
VAE to generate new images of trees that still look like trees.</li>
<li>Points in the latent space that are closer together are understood to be more similar to each other</li>
<li>X -&gt; F (latent space)</li>
<li>loss: typical expression for the mean squared error (MSE) between the input data, X, and the output data, X’</li>
<li>Z = g(θX+b) - output of each layout, θ - weights, g - activation</li>
<li>L(X,X') = ||X = X'||<sup>2</sup> - MSE</li>
</ul>

<p>
problem: trouble separating points that have features which are too similar.
</p>
<ul class="org-ul">
<li>solution: change from representing the latent space as a discrete set of points to instead represent it as a
probability distribution. encoder is going to learn to represent the latent space as a Gaussian probability
density. q, is the Gaussian probability density, and it represents the probability that we get a certain
value z<sub>i</sub> given a certain input, x<sub>i</sub>. For encoder q(z given x), for decoder  p(x given z)</li>
</ul>

<p>
reparameterization trick -
</p>
</div>
<ol class="org-ol">
<li><a id="orgcc24d1e"></a>links<br />
<div class="outline-text-5" id="text-12-15-9-1">
<p>
<a href="https://sassafras13.github.io/VAE/">https://sassafras13.github.io/VAE/</a>
</p>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-orga8c1c96" class="outline-3">
<h3 id="orga8c1c96"><span class="section-number-3">12.16.</span> batch and batch normalization</h3>
<div class="outline-text-3" id="text-12-16">
<p>
batch normalization - normalize the activations of a given input volume before passing it into the next layer in the network.
</p>

<p>
Reduces the amount by what the hidden unit values shift around (covariance shift) ковариационного сдвига
</p>

<p>
Самый простой способ - получить нулевое матожидание(mean) и единичную дисперсию(np.std)
</p>

<p>
batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers.
</p>

<p>
BatchNormalization - дифференцируемое преобразование, ставится перед активацией
</p>

<p>
adds two trainable parameters to each layer
</p>

<p>
batch normalization lets SGD do the denormalization by changing only these two weights for each activation,
instead of losing the stability of the network by changing all the weights.
</p>

<p>
The biggest drawback of batch normalization is that it can actually slow down the wall time
</p>

<p>
<b>with Dropout</b> <a href="https://arxiv.org/pdf/1801.05134.pdf">https://arxiv.org/pdf/1801.05134.pdf</a>
</p>
<ul class="org-ul">
<li>network even performs worse and unsatisfactorilywhen it is equipped with BN and Dropout simultaneously</li>
<li>BN eliminates the need for Dropout in some cases</li>
</ul>
</div>
</div>

<div id="outline-container-org45549ac" class="outline-3">
<h3 id="org45549ac"><span class="section-number-3">12.17.</span> patterns of design</h3>
<div class="outline-text-3" id="text-12-17">
<ul class="org-ul">
<li>count of parameters decrease close to final layer.</li>
</ul>

<p>
Andrej Karpathy recommends the overfit then regularize approach — “first get a model large enough that it can
  overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to
  improve the validation loss).”
</p>

<p>
<b>Probabilistic layer</b> - outputs are usually interpreted in terms of class membership probabilities
</p>
<ul class="org-ul">
<li>Logistic probabilistic activation.</li>
<li>SoftMax probabilistic activation.</li>
</ul>

<p>
Configurations:
</p>
<ul class="org-ul">
<li>Aproximation model - usually contains a scaling layer, several perceptron layers, an unscaling layer, and a
bounding layer.</li>
<li>Classification - requires a scaling layer, one or several perceptron layers, and a probabilistic layer. It might also contain
a principal component layer.</li>
<li>Forecasting - scaling layer, a long-short term memory layer, a perceptron layer, an unscaling layer and a
bounding layer.</li>
<li>Auto association (learn a compressed or reduced representation of the input data)</li>
<li>Text classification</li>
</ul>

<p>
<b>Weight initialization method</b>
</p>
<ul class="org-ul">
<li>When using ReLU or leaky RELU, use He initialization</li>
<li>When using SELU or ELU, use LeCun initialization</li>
<li>When using softmax, logistic, or tanh, use Glorot initialization</li>
<li>Most initialization methods come in uniform and normal distribution flavors.</li>
</ul>

<p>
<a href="https://wandb.ai/site/articles/fundamentals-of-neural-networks">https://wandb.ai/site/articles/fundamentals-of-neural-networks</a>
</p>
</div>
</div>
<div id="outline-container-orgb7ddf90" class="outline-3">
<h3 id="orgb7ddf90"><span class="section-number-3">12.18.</span> <span class="todo TODO">TODO</span> MultiModal Machine Learning (MMML)</h3>
<div class="outline-text-3" id="text-12-18">
<p>
Modality - the way in which something happens or is experienced (ex. sensory modalities)
</p>

<p>
diffusion models for personalized image generation <a href="https://arxiv.org/pdf/2409.13346">https://arxiv.org/pdf/2409.13346</a>
</p>
</div>

<div id="outline-container-orgac2b040" class="outline-4">
<h4 id="orgac2b040"><span class="section-number-4">12.18.1.</span> theory</h4>
<div class="outline-text-4" id="text-12-18-1">
</div>
<ol class="org-ol">
<li><a id="org18e2056"></a>history of deep MMML<br />
<div class="outline-text-5" id="text-12-18-1-1">
<ul class="org-ul">
<li>Multimodal deep learning [ICML 2011]</li>
<li>Multimodal learning with Deep Boltzmann Machines [NIPS 2012] (joint multimodal)</li>
<li>Visual attention: Show, Attend and Tell: Neural Image Caption Generation with Visual Attnetion [ICML 2015]</li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org3df113c" class="outline-4">
<h4 id="org3df113c"><span class="section-number-4">12.18.2.</span> real world task for MMML</h4>
<div class="outline-text-4" id="text-12-18-2">
<ul class="org-ul">
<li>Affect recognition
<ul class="org-ul">
<li>emotion</li>
<li>persuasion</li>
<li>personality traits</li>
</ul></li>
<li>Media description
<ul class="org-ul">
<li>image captioning</li>
<li>video captioning</li>
<li>visual question answering</li>
</ul></li>
<li>Event recognition
<ul class="org-ul">
<li>action recognition</li>
<li>segmentation</li>
</ul></li>
<li>Multimedia information retrieval
<ul class="org-ul">
<li>content based/cross-media</li>
</ul></li>
</ul>
<p>
new
</p>
<ul class="org-ul">
<li>Генератор описания к изображениям</li>
<li>Генератор изображения из текста</li>
<li>Визуальный ответ на вопрос (VQA)</li>
<li>Визуально-языковое представление</li>
<li>речь-текст</li>
</ul>
</div>
</div>
<div id="outline-container-org2e08cbc" class="outline-4">
<h4 id="org2e08cbc"><span class="section-number-4">12.18.3.</span> <span class="todo TODO">TODO</span> core challenges in deep MMML</h4>
<div class="outline-text-4" id="text-12-18-3">
<dl class="org-dl">
<dt>Representation</dt><dd>Learn how to represent and summarize multimodal data in a way that exploits the
complementarity and redundancy.
<ul class="org-ul">
<li>join representations (to one thing) or coordinated representations (vectors in vector spaces)</li>
</ul></dd>
<dt>Alignment</dt><dd></dd>

<dt>(no term)</dt><dd>Fusion</dd>
<dt>(no term)</dt><dd>Translation</dd>
<dt>(no term)</dt><dd><p>
Co-Learning
</p>

<p>
link  arxiv.org/abs/1705.09406
</p></dd>
</dl>


<p>
на практике сложно комбинировать различный уровень шума и конфликты между модальностями.  модальности имеют
 различное количественное влияние на результаты прогнозирования.
</p>
</div>
</div>
<div id="outline-container-org7424cc6" class="outline-4">
<h4 id="org7424cc6"><span class="section-number-4">12.18.4.</span> current major systems</h4>
<div class="outline-text-4" id="text-12-18-4">
</div>
<ol class="org-ol">
<li><a id="org0dc9083"></a>LayoutLMv3<br />
<div class="outline-text-5" id="text-12-18-4-1">
<p>
<a href="https://arxiv.org/abs/2204.08387">https://arxiv.org/abs/2204.08387</a>
</p>
</div>
</li>
<li><a id="org6f151fd"></a>DALL.E (oponai)<br />
<div class="outline-text-5" id="text-12-18-4-2">
<p>
— искусственный интеллект, разработанный OpenAI для эффективного преобразования текста в изображение. Система
 распознает широкий спектр понятий, произносимых на естественном языке. ИИ по сути представляет собой
 нейронную сеть, состоящую из 12 миллиардов параметров.
<a href="https://openai.com/blog/dall-e/">https://openai.com/blog/dall-e/</a>
</p>
</div>
</li>
<li><a id="orgcb3e6f6"></a>CLIP (openai)<br />
<div class="outline-text-5" id="text-12-18-4-3">
<p>
— еще одна мультимодальная система искусственного интеллекта, разработанная OpenAI для успешного выполнения
 широкого набора задач визуального распознавания. Имея набор категорий, описанных на естественном языке, CLIP
 может быстро классифицировать изображение по одной из этих категорий.
</p>

<p>
on a large body of work on zero-shot transfer,  the text paired with images
</p>

<p>
<a href="https://openai.com/index/clip/">https://openai.com/index/clip/</a>
<a href="https://openai.com/blog/clip/">https://openai.com/blog/clip/</a>
</p>
</div>
</li>
<li><a id="orge76555b"></a>ALIGN (google)<br />
<div class="outline-text-5" id="text-12-18-4-4">
<p>
— это модель искусственного интеллекта, обученная Google на зашумленном наборе данных с большим количеством
 пар изображение-текст. Модель достигла наилучшей точности в нескольких тестах поиска изображений и текста.
</p>

<p>
<a href="https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html">https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html</a>
</p>
</div>
</li>
<li><a id="orgaf30e7a"></a>MURAL (google)<br />
<div class="outline-text-5" id="text-12-18-4-5">
<p>
— это модель искусственного интеллекта, разработанная Google AI для сопоставления изображения, текста и перевода одного языка на другой. В модели используется многозадачное обучение, применяемое к парам изображение-текст в сочетании с парами перевода на более чем 100 языках.
</p>
</div>
</li>
<li><a id="orgc937583"></a>VATT (google)<br />
<div class="outline-text-5" id="text-12-18-4-6">
<p>
недавний проект Google AI по созданию мультимодальной модели на основе видео-аудио-текста. VATT может делать
 прогнозы мультимодальностей на основе необработанных данных. Он не только генерирует описания событий в
 видео, но также может подтягивать видео по запросу, классифицировать аудиоклипы и идентифицировать объекты на
 изображениях.
<a href="https://arxiv.org/abs/2104.11178">https://arxiv.org/abs/2104.11178</a>
</p>
</div>
</li>
<li><a id="org21b6b06"></a>FLAVA (META)<br />
<div class="outline-text-5" id="text-12-18-4-7">
<p>
модель, обученная Meta на изображениях и 35 языках. Хорошо зарекомендовала себя во множестве мультимодальных
 задачах.
<a href="https://medium.com/syncedreview/facebook-ais-flava-foundational-model-tackles-vision-language-and-vision-language-tasks-all-at-56b662185207">https://medium.com/syncedreview/facebook-ais-flava-foundational-model-tackles-vision-language-and-vision-language-tasks-all-at-56b662185207</a>
</p>
</div>
</li>

<li><a id="orgfa2af8b"></a>NUWA (Microsoft)<br />
<div class="outline-text-5" id="text-12-18-4-8">
<p>
это совместное предприятие Microsoft Research и Пекинского университета, которое занимается генерацией
изображений и видео для задач по созданию мультимедиа. По текстовой подсказке или эскизу модель может
предсказать следующий видеокадр и заполнить неполные изображения.
<a href="https://github.com/microsoft/NUWA">https://github.com/microsoft/NUWA</a>
</p>
</div>
</li>
<li><a id="org962fd3a"></a>Florence (Microsoft)<br />
<div class="outline-text-5" id="text-12-18-4-9">
<p>
, способной моделировать пространство, время и модальность. Модель может решать многие популярные задачи
 видеоязыка.
 <a href="https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/">https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/</a>
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgb0b78c9" class="outline-4">
<h4 id="orgb0b78c9"><span class="section-number-4">12.18.5.</span> datasets</h4>
<div class="outline-text-4" id="text-12-18-5">
<p>
Набор данных о мультимодальном корпусе чувствительности (MOSI) - Аннотированный набор данных 417 видео в
 миллисекунду с аннотированными аудиофункциями. Всего имеется 2199 аннотированных точек данных, в которых
 интенсивность настроений определяется от сильно отрицательной до сильно положительной с линейной шкалой от −3
 до +3.
</p>
</div>
</div>
<div id="outline-container-orgeddca32" class="outline-4">
<h4 id="orgeddca32"><span class="section-number-4">12.18.6.</span> multimodal RAG for documents</h4>
<div class="outline-text-4" id="text-12-18-6">
<p>
<a href="https://huggingface.co/learn/cookbook/multimodal_rag_using_document_retrieval_and_vlms">https://huggingface.co/learn/cookbook/multimodal_rag_using_document_retrieval_and_vlms</a>
<a href="https://huggingface.co/blog/manu/colpali">https://huggingface.co/blog/manu/colpali</a>
<a href="https://arxiv.org/pdf/2407.01449">https://arxiv.org/pdf/2407.01449</a>
</p>
</div>
</div>
<div id="outline-container-org7e94ed4" class="outline-4">
<h4 id="org7e94ed4"><span class="section-number-4">12.18.7.</span> links</h4>
<div class="outline-text-4" id="text-12-18-7">
<ul class="org-ul">
<li><a href="https://www.cs.cmu.edu/~morency/MMML-Tutorial-ACL2017.pdf">https://www.cs.cmu.edu/~morency/MMML-Tutorial-ACL2017.pdf</a></li>
<li><a href="https://russianblogs.com/article/2633412875/">https://russianblogs.com/article/2633412875/</a></li>
<li><a href="https://habr.com/ru/companies/wunderfund/articles/724608/">https://habr.com/ru/companies/wunderfund/articles/724608/</a></li>
<li><a href="https://huggingface.co/docs/transformers/model_doc/layoutlmv3">https://huggingface.co/docs/transformers/model_doc/layoutlmv3</a></li>
<li><a href="https://dzen.ru/a/Y55OEHzC6kDXlUMP">https://dzen.ru/a/Y55OEHzC6kDXlUMP</a></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgecfcfa4" class="outline-3">
<h3 id="orgecfcfa4"><span class="section-number-3">12.19.</span> challanges</h3>
<div class="outline-text-3" id="text-12-19">
<p>
Data Overload -  (I/O) operations - shared parallel file system
</p>
<ul class="org-ul">
<li>intercepts I/O traffic and processes it on the compute node to reduce the data workload on the shared file system</li>
<li>Few shot learning</li>
</ul>

<p>
Scaling Code
</p>

<p>
Human Interpretability
</p>

<p>
Data-Poor Problems
</p>
<ul class="org-ul">
<li>Employ refinement approaches like interpolation and cost function mitigation to overcome this data deficiency.</li>
</ul>

<p>
Implausible Results:
</p>
<ul class="org-ul">
<li>Develop methods that blend deep learning with physics-based constraints to advance domain science.</li>
</ul>
</div>
</div>

<div id="outline-container-orgba382f9" class="outline-3">
<h3 id="orgba382f9"><span class="section-number-3">12.20.</span> GAN Generative adversarial network <a id="org3ee3e47"></a></h3>
<div class="outline-text-3" id="text-12-20">
<ul class="org-ul">
<li>2014 Generative adversarial networks (GANs) <a href="https://arxiv.org/pdf/1406.2661.pdf">https://arxiv.org/pdf/1406.2661.pdf</a></li>
<li>2016 UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS <a href="https://arxiv.org/pdf/1511.06434.pdf">https://arxiv.org/pdf/1511.06434.pdf</a></li>
</ul>

<p>
GANs provide an attractive alternative to maximum likelihood techniques.
</p>
</div>
</div>
<div id="outline-container-orgc39c8fb" class="outline-3">
<h3 id="orgc39c8fb"><span class="section-number-3">12.21.</span> inerpretation</h3>
<div class="outline-text-3" id="text-12-21">
<p>
IR forms (or graphs )
</p>

<p>
ML frameworks have either graph abstractions built into the programming model (e.g., TF) or the evaluation
 model (e.g., TVM), or a language frontend (e.g., Relay) that can be deterministically converted into IRs.
</p>

<p>
Graph capture for an <b>eager-first ML framework</b> like PyTorch is non-trivial and design space in itself.
</p>
</div>
</div>
<div id="outline-container-orge8767df" class="outline-3">
<h3 id="orge8767df"><span class="section-number-3">12.22.</span> multiclass(multi-class) classification problem or large number of classes problem</h3>
<div class="outline-text-3" id="text-12-22">
<p>
When number of classes is small softmax activation fonction for last layer is used.
</p>

<p>
Losses: logistic (for binary) and softmax (multiclass).
</p>

<p>
full softmax training - for every training example we compute all classes logits. [ x, x, x, x, x]
</p>

<p>
Solutions:
</p>
<dl class="org-dl">
<dt>Candidate sampling</dt><dd>set of methods. Selecting a random subset of classes (candidates) for each training example, rathen
than computing the loss over all classes. Candidates consist of the target classes associated with the
training example and a randomly chosen set of sampled classes.</dd>
<dt>Embeddings and Distance-based classification</dt><dd>Each class as embedding vector and use distance-based
measures to classify new instances.</dd>
<dt>One vs Rest</dt><dd>.</dd>
<dt>Naive Bayes Classifier</dt><dd>.</dd>
<dt>Multistage Classification</dt><dd>Divide each class into hierarchical structure</dd>
<dt>Boosting algorithms</dt><dd>.</dd>
</dl>
<p>
<a href="https://www.tensorflow.org/extras/candidate_sampling.pdf">https://www.tensorflow.org/extras/candidate_sampling.pdf</a>
</p>
</div>
</div>
<div id="outline-container-orgfb01a0f" class="outline-3">
<h3 id="orgfb01a0f"><span class="section-number-3">12.23.</span> Design Patterns for neural networks</h3>
<div class="outline-text-3" id="text-12-23">
<p>
<a href="https://arxiv.org/pdf/1611.00847v3.pdf">https://arxiv.org/pdf/1611.00847v3.pdf</a>
</p>
<ol class="org-ol">
<li>Architectural Structure follows the Application</li>
<li>Proliferate Paths - based on the idea that ResNets can be an exponentialensemble of networks with different lengths</li>
<li>Strive for Simplicity - fewer types of units and  keeping  the  network  as  simple  as  possible</li>
<li>Increase Symmetry - sign of beauty and quality
<ul class="org-ul">
<li>for CNN - activations are downsampled and the number of channels increased fromthe input to the final
layer</li>
</ul></li>
<li>Design Pattern 5: Pyramid Shape - smooth downsamplingcombined with an increase in the number of channels
throughout the architecture</li>
<li>Design Pattern 6: Over-train - trained on a harder problem than necessary to improve generalization performance</li>
<li>Design Pattern 7: Cover the Problem Space -  training data is another way toimprove generalization
<ul class="org-ul">
<li>augmentation</li>
<li>sorting! - from samplest to hardest</li>
</ul></li>
<li>Design Pattern 8: Incremental Feature Construction - common thread throughout many of the more successful
architectures is to make each layer’s“job” easier.
<ul class="org-ul">
<li>shorter skip connections in ResNet - better</li>
</ul></li>
<li>Design Pattern 9: Normalize Layer Inputs - We feel that normalization puts all the layer’s input samples on
more equal footing (analogous to a unitsconversion scaling), which allows back-propagation to train more
effectively</li>
<li>Input Transition - based on the common occurrence that the output from the first layer of aCNN
significantly increases the number of channels from 3. - Here, the trade-off is that of cost versus
accuracy</li>
<li>Available Resources Guide Layer Widths - Choose the number of outputs of the first layer based on memory
andcomputational resources and desired accuracy</li>
<li>Design Pattern 12: Summation Joining -
<ul class="org-ul">
<li>summation causes the layers to learn theresidual (the difference from the input)</li>
<li>mean keeps the output smooth if branches are randomly dropped.</li>
</ul></li>
<li>Down-sampling Transition - when down-sampling by pooling or using a stride greater than 1, agood way to
combine branches is to concatenate the output channels, hence smoothly accomplishingboth joining and an
increase in the number of channels that typically accompanies down-sampling.</li>
<li>Maxout for Com-petition - when each branch is composed of different sized kernels, Maxout is useful
forincorporating scale invariance in an analogous way to how max pooling enables translation invari-ance</li>
</ol>
</div>
</div>

<div id="outline-container-org1b70b4b" class="outline-3">
<h3 id="org1b70b4b"><span class="section-number-3">12.24.</span> Ways to optimize training of neural network</h3>
<div class="outline-text-3" id="text-12-24">
<ul class="org-ul">
<li>efficient <b>optimizers</b> - AdamW, Adam</li>
<li>Utilize <b>hardware accelerators</b> (GPUs/TPUs)</li>
<li>Max out the <b>batch size</b>.</li>
<li>Use <b>Bayesian Optimization</b> if hyperparameter search space is big.</li>
<li>Set max<sub>workers</sub> in *DataLoader*(pyTorch)/tf.data.Dataset</li>
<li>set pin<sub>memory</sub> in <b>DataLoader</b></li>
<li>Use <b>mixed precision training</b></li>
<li>Try <b>initializers</b> He or Xavier for fast convergence</li>
<li>Use <b>activation checkpointing</b> to optimize memory (run-time will go up)</li>
<li>Utilize <b>multi-GPU training</b> through Model/Data/Pipeline/Tensor parallelism.</li>
<li><b>distributed training</b> like FSDP(PyTorch)</li>
<li>Normalize data after transferring to GPU (for integer data, like pixels) not on CPU or before training.</li>
<li>Use <b>gradient accumulation</b> (may have marginal improvement at times)</li>
<li>Always use DistributedDataParallel, not DataParallel.</li>
<li>torch.rand(2,2,device=&#x2026;) creates tensor directly on GPU.
<ul class="org-ul">
<li>torch.rand(2,2).cude() - first create at CPU then transfer to GPU.</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2586203" class="outline-2">
<h2 id="org2586203"><span class="section-number-2">13.</span> Natural Language Processing (NLP)</h2>
<div class="outline-text-2" id="text-13">
<ul class="org-ul">
<li>2017 использованией нейронных сетей <a href="https://habr.com/ru/company/ods/blog/347524/">https://habr.com/ru/company/ods/blog/347524/</a>
<a href="https://www.youtube.com/watch?v=1Chk1Mi-yZ0">https://www.youtube.com/watch?v=1Chk1Mi-yZ0</a></li>
<li>Сбербанк 2018 <a href="http://www.nanonewsnet.ru/news/2018/izuchaem-sintaksicheskie-parsery-dlya-russkogo-yazyka">http://www.nanonewsnet.ru/news/2018/izuchaem-sintaksicheskie-parsery-dlya-russkogo-yazyka</a>
<a href="https://habr.com/en/company/sberbank/blog/418701/">https://habr.com/en/company/sberbank/blog/418701/</a></li>
<li>comp science, ai, linguistics</li>
<li>Goal: accept orders, question answering, Understanding the meaning</li>
<li><a href="https://en.wikipedia.org/wiki/Phrase_structure_grammar">https://en.wikipedia.org/wiki/Phrase_structure_grammar</a></li>
<li><a href="https://events.yandex.ru/lib/talks/3516/">https://events.yandex.ru/lib/talks/3516/</a></li>
<li>- "SpaCy и DeepPavlov для решения NLU задач" <a href="https://www.youtube.com/watch?v=WVhA3YpIek4">https://www.youtube.com/watch?v=WVhA3YpIek4</a></li>
<li>AllenNLP - <a href="https://allennlp.org">https://allennlp.org</a> - on PyTorch</li>
<li>2017 best practices <a href="http://ruder.io/deep-learning-nlp-best-practices/">http://ruder.io/deep-learning-nlp-best-practices/</a></li>
<li>The Role of Complex NLP in Transformers for Text Ranking <a href="https://arxiv.org/pdf/2207.02522.pdf">https://arxiv.org/pdf/2207.02522.pdf</a></li>
</ul>
<p>
Language - discrete, symbolic, categorical signaling syste.
</p>

<p>
Meaning of word - high dimension vector.
</p>

<p>
<b>word level CNN vs character level CNN</b> = word level CNN = f-мера лучше, но у character level меньше модель
размером
</p>

<p>
Algorithms ??
</p>
<ul class="org-ul">
<li>CRF</li>
<li>MEMM</li>
<li>HMM</li>
</ul>

<p>
Three Dimensions of NLP: language, content(empathy), emotion
</p>
</div>
<div id="outline-container-orgdd01816" class="outline-3">
<h3 id="orgdd01816"><span class="section-number-3">13.1.</span> history</h3>
<div class="outline-text-3" id="text-13-1">
<p>
Traditional LM was based on n-gram count statistics (Bahlet al., 1983) and various smoothing techniques where
 proposed to imporve the estimation of rare events (Katz, 1987; Kneser and Ney 1995).
</p>

<p>
In the past two decades, NN have been sucessfuly applied to the LM task: feed forward, RNN, LSTM.
</p>

<p>
More recently transformer networks, based on self-attention, have led to improvements, especially for
 capturing long range dependencies (Vaswani et al., 2017 ; Radford et al., 2018 ; Dai et al. 2019)
</p>
<ul class="org-ul">
<li>Attention Is All You Need <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
<li>Improving Language Understanding by Generative Pre-Training <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a>
<ul class="org-ul">
<li>Train: with Generative Pre-Training and discriminative fine-tuning.</li>
<li>Transformer Decoder model</li>
<li>masked self-attention heads, Adam</li>
</ul></li>
<li>Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</li>
</ul>

<p>
history:
</p>
<ul class="org-ul">
<li>2016 - HAN (Hierarchical Attention Network) by Yang et al - two bidirectional LSTM for two levels of
attention mechanisms: word-level and sentence-level. - sentiment analysis, topic classification, and question
answering</li>
</ul>
</div>
</div>
<div id="outline-container-org65458e6" class="outline-3">
<h3 id="org65458e6"><span class="section-number-3">13.2.</span> NLP pyramid</h3>
<div class="outline-text-3" id="text-13-2">
<ul class="org-ul">
<li>Pragmatics</li>
<li>Semantics</li>
<li>syntax</li>
<li>Morhology</li>
</ul>

<p>
process:
</p>
<ul class="org-ul">
<li>Tokenization</li>
<li>stemming (optional)</li>
<li>removing the punctuation (optional)</li>
<li>Embedding - word to vector</li>
<li>Model architectures</li>
</ul>
</div>
</div>

<div id="outline-container-org68303af" class="outline-3">
<h3 id="org68303af"><span class="section-number-3">13.3.</span> Tokenization</h3>
<div class="outline-text-3" id="text-13-3">
<ol class="org-ol">
<li>converting a sequence of characters into a sequence of tokens (words to numbers)</li>
<li>converted into a sequence of numerical vectors that can be processed by a neural network. (words to vectors)</li>
</ol>
</div>
</div>

<div id="outline-container-orgbb87f50" class="outline-3">
<h3 id="orgbb87f50"><span class="section-number-3">13.4.</span> Sentiment analysis definition (Liu 2010)</h3>
<div class="outline-text-3" id="text-13-4">
<p>
Sentiment analysis is defined by the 5-tuple
</p>
<ul class="org-ul">
<li>E is the targe entity</li>
<li></li>
</ul>
</div>
</div>
<div id="outline-container-orgd8f6893" class="outline-3">
<h3 id="orgd8f6893"><span class="section-number-3">13.5.</span> Approaches:</h3>
<div class="outline-text-3" id="text-13-5">
<ol class="org-ol">
<li>Rule-based methods - NLTK
<ul class="org-ul">
<li>Types
<ul class="org-ul">
<li>Regex</li>
<li>Context-free grammars - yargy
<ul class="org-ul">
<li>не умеет в условия if and or</li>
</ul></li>
</ul></li>
<li>Cons you cannot know all words in list = low Recall</li>
<li>Pros = high precision</li>
</ul></li>
<li>Brobabilistic modeling and machine learning - faster than Deep learning,
<ul class="org-ul">
<li>Likelihood maximization</li>
<li>Linear classifiers</li>
<li>Conditional Random Fields(CRF)</li>
<li>Pros:
<ul class="org-ul">
<li>good for  <b>sequence labeling</b> - set of independent classification tasks</li>
<li>allow us not to be blinded with the hype - word2vec, distributional semantics</li>
</ul></li>
</ul></li>
<li>Deep learning
<ul class="org-ul">
<li>Recurrent Neural Networks (RNN)</li>
<li>Convolutional Neural Networks (CNN)</li>
</ul></li>
</ol>
</div>
</div>

<div id="outline-container-org70fcc59" class="outline-3">
<h3 id="org70fcc59"><span class="section-number-3">13.6.</span> Machine learning steps:</h3>
<div class="outline-text-3" id="text-13-6">
<ol class="org-ol">
<li>Training data with markup</li>
<li>Feature engineering - Capitalized, occur on some list,</li>
<li>Model - depends of some parameters(will be trained) and require some features</li>
</ol>
<p>
Deep learning difference:
</p>
<ul class="org-ul">
<li>features not required</li>
<li>many parameters</li>
</ul>
</div>
</div>

<div id="outline-container-orgb15792d" class="outline-3">
<h3 id="orgb15792d"><span class="section-number-3">13.7.</span> Математические методы анализа текстов</h3>
<div class="outline-text-3" id="text-13-7">
<ul class="org-ul">
<li>Мурат Апишев. Математические методы анализа текстов 2018
<a href="http://www.machinelearning.ru/wiki/images/5/53/Mel_lain_msu_nlp_sem_1.pd">http://www.machinelearning.ru/wiki/images/5/53/Mel_lain_msu_nlp_sem_1.pd</a></li>
<li>какае-то книга на Новосибирском универе <a href="https://nsu.ru/xmlui/bitstream/handle/nsu/1446/Text_AlperinBL.pdf">https://nsu.ru/xmlui/bitstream/handle/nsu/1446/Text_AlperinBL.pdf</a></li>
</ul>
</div>

<div id="outline-container-orgcccdb14" class="outline-4">
<h4 id="orgcccdb14"><span class="section-number-4">13.7.1.</span> Определения:</h4>
<div class="outline-text-4" id="text-13-7-1">
<ul class="org-ul">
<li>веб-пауки - парсят страницы - результат plain text</li>
<li>Corpus linguistics -  раздел языкознания, занимающийся разработкой, созданием и использованием текстовых корпусов</li>
<li>corpus [ˈkɔːpəs] (plural corpora or corpuses) - large and structured set of texts (nowadays usually
electronically stored and processed).</li>
<li>Seme Се́ма - smallest unit of meaning, which enables one to describe words multilingually</li>
<li>фонема φώνημα «звук»</li>
<li>Морфе́ма - smallest grammatical unit in a language</li>
<li>sememe - σημαίνω — «обозначаю» , language unit of meaning, analogous to a morpheme. smallest unit of meaning
recognized in semantics</li>
<li>Collocation - словосочетание -</li>
<li>L-грамма - последовательность и L&gt;=1 последовательно идущих слов (токенов) текста. Внутри предложения,
скользящим окном.</li>
</ul>
</div>
</div>

<div id="outline-container-org12e9f78" class="outline-4">
<h4 id="org12e9f78"><span class="section-number-4">13.7.2.</span> схема извлечения ключевых фраз</h4>
<div class="outline-text-4" id="text-13-7-2">
<ul class="org-ul">
<li>предварительная обработка текста;</li>
<li>отбор кандидатов в ключевые фразы
<ul class="org-ul">
<li>L-граммным методом - скользящее окно, каждая фраза, попавшая в скользящее окно, обрабатывается независимо</li>
<li>стоп-словари и фильтрация по морфологическим признакам - удаление предлогов, междометий и т.д</li>
</ul></li>
<li>вычисление признаков для каждого кандидата - позволяющих принять решение, является ли данный кандидат
ключевой фразой, или нет</li>
<li>отбор ключевых фраз из числа кандидатов</li>
</ul>
</div>
</div>

<div id="outline-container-org0fcffbe" class="outline-4">
<h4 id="org0fcffbe"><span class="section-number-4">13.7.3.</span> Оценка эффективности извлечения ключевых фраз:</h4>
<div class="outline-text-4" id="text-13-7-3">
<p>
точность и полнота = F-мера. сравнивают  ключевые  слова,  найденные  автоматически,  с  ключевыми  словами,
выделенными  читателями-экспертами.
</p>
<ul class="org-ul">
<li>Precision = |Texp ∩ Ta| div |Ta|</li>
<li>Recall = |Texp ∩ Ta| div |Texp| - количества экспертных ключевых фраз, найденных автоматически, к общему количеству экспертных
ключевых фраз</li>
</ul>
</div>
</div>
<div id="outline-container-org673acdd" class="outline-4">
<h4 id="org673acdd"><span class="section-number-4">13.7.4.</span> предобработка plain text</h4>
<div class="outline-text-4" id="text-13-7-4">
<ul class="org-ul">
<li>токенизация</li>
<li>приведение к нижнему регистру</li>
<li>удаление стоп-слов - and or not but,&#x2026;.</li>
<li>удаление пунктуации</li>
<li>фильтрация по частоте/длине/соответствию регулярному выражению</li>
<li>лемматизация или стемминг Lemmatization and Stemming (отрезание окончания и формообразующего суффикса)
<ul class="org-ul">
<li>replace wordform with lemma Lemma [ˈlemə] (вспомогательное утверждение)</li>
<li>using dictionary</li>
</ul></li>
<li>Морфологический анализ (применяется библиотека Stanford CoreNLP) сопоставляет каждому слову набор тегов
частеречной разметки (Penn Treebank Tag Set).</li>
</ul>
</div>
</div>

<div id="outline-container-org64a32c3" class="outline-4">
<h4 id="org64a32c3"><span class="section-number-4">13.7.5.</span> Коллокаци Collocations</h4>
<div class="outline-text-4" id="text-13-7-5">
<ul class="org-ul">
<li><a href="http://www.nltk.org/howto/collocations.html">http://www.nltk.org/howto/collocations.html</a></li>
<li>N-граммы - усточивые последовательности из N слов, идущих подряд («машина опорных векторов»)
<ul class="org-ul">
<li>биграммы - два слова</li>
<li>униграмма - одно слово</li>
</ul></li>
<li>Коллокация - устойчивое сочетание слов, не обязательно идущих подряд («Он <b>сломал</b> своему противнику
*руку*»)
<ul class="org-ul">
<li>Соединённые Штаты Америки, Европейский Союз</li>
<li>Машина опорных векторов, испытание Бернулли</li>
<li>Крепкий чай, крутой кипяток, свободная пресса</li>
</ul></li>
<li>collocational window - (usually a window of 3 to 4 words on each side of a word</li>
<li>mean oﬀset - среднее расстояние между словами фразы. 1/2(2+3) Если второе слово перед первым 1/2(-1+3)</li>
<li>variance measures -</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="orge97a3ea"></a>Способы:<br />
<div class="outline-text-5" id="text-13-7-5-1">
<ul class="org-ul">
<li>Извлечение биграмм на основе частот и морфологических шаблонов.</li>
<li>Поиск разрывных коллокаций.</li>
<li>Извлечение биграмм на основе мер ассоциации и статистических критериев.</li>
<li>Алгоритм TextRank для извлечения словосочетаний.</li>
<li>Rapid Automatic Keyword Extraction.</li>
<li>Выделение ключевых слов по tf-idf.</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org80089ea"></a>прямой подсчет количества пар (freq);<br />
<div class="outline-text-6" id="text-13-7-5-1-1">
<p>
двусловия упорядочиваются по убыванию их встречаемости в тексте (т.е. частоты встречаемости отдельных слов не учитываются)
</p>
</div>
</li>

<li><a id="orge25ecba"></a>t‑статистика Стьюдента, x<sup>2</sup>, отношение функций правдоподобия (LR)<br />
<div class="outline-text-6" id="text-13-7-5-1-2">
<p>
три метода заключаются в проверке статистических гипотез, соответствующих случайной или неслучайной «встрече»
слов в паре
</p>
</div>
</li>
<li><a id="org88f3370"></a>KEA keyword extraction algorithm наивный Байесовский классификатор Naive Bayes<br />
<div class="outline-text-6" id="text-13-7-5-1-3">
<ul class="org-ul">
<li><a href="https://habr.com/en/post/120194/">https://habr.com/en/post/120194/</a></li>
<li><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">https://en.wikipedia.org/wiki/Naive_Bayes_classifier</a></li>
</ul>
<p>
Два признака для классификации TF-IDF и признак первого вхождения(first occurrence) - называются «стандартными
признаками» - используются везде.
</p>
</div>
</li>

<li><a id="org0936429"></a>TF-IDF <a id="org4b8425b"></a><br />
<div class="outline-text-6" id="text-13-7-5-1-4">
<p>
the importance or relevance of string representations in a document amongst a collection of documents
</p>

<ul class="org-ul">
<li>TF-IDF показывает специфичность данной <b>фразы t</b> по отношению к остальным фразам <b>документа D</b> и вычисляется как
произведение TF (Term Frequency) на IDF (Inversed Document Frequency)
<ul class="org-ul">
<li>TFIDF(t,D) = (freq(t,D)/size(d)) * |log2(df(t)/N)|</li>
</ul></li>
</ul>

<p>
(freq(t,D)/size(d)) - TF (term frequency) - Number of times the word appears in a document (raw count).
</p>
<ul class="org-ul">
<li>где freq(t,D) - число вхождений фразы t в документ D</li>
<li>size(d) - числов слов в D</li>
</ul>

<pre class="example">
|log2(df(t)/N)| - IDF (inverse document frequency) - how common (or uncommon) a word is amongst the corpus
</pre>

<ul class="org-ul">
<li>df(t) - число документов рассматриваемого текстового корпуса, содержащих t</li>
<li>N - количество документов в корпусе</li>
<li>first occurrence - вычисляется как позиция первого вхождения первого слова фразы, деленная на количество
слов в документе - [0..1]</li>
</ul>
</div>
</li>


<li><a id="org7cb4158"></a>Association measures Меры ассоциации биграмм<br />
<div class="outline-text-6" id="text-13-7-5-1-5">
<p>
<b>Contingency table (Таблица сопряжённости)</b> - a type of table in a matrix format that displays the
(multivariate) frequency distribution of the variables
</p>
<ul class="org-ul">
<li>Строки - значениям одной переменной x, столбцы — значениям другой переменной y</li>
<li>На пересечении - частота совместного появления f(x,y)</li>
<li>Сумма частот по строке - маргинальной частотой строки, маргинальной частотой столбца - marginal totals</li>

<li>x1 - f(x1y1) - f(x1y2)</li>
<li>x2 - f(x2y1) - f(x2y2)</li>
</ul>


<p>
significance of the difference between f(x1y1) and f(x1y2):
</p>
<ul class="org-ul">
<li>Pearson's chi-squared test (χ2)</li>
<li>G-tests are likelihood-ratio</li>
<li>etc.</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org4444856"></a>PMI — pointwise mutual information<br />
<div class="outline-text-7" id="text-13-7-5-1-5-1">
<ul class="org-ul">
<li><a href="https://habr.com/en/post/140739/">https://habr.com/en/post/140739/</a></li>
<li>is a measure of association used in information theory and statistics</li>
</ul>
</div>
</li>
</ol>
</li>
</ol>
</li>

<li><a id="orgf1ab6c8"></a>морфологические шаблоны-фильтры<br />
<div class="outline-text-5" id="text-13-7-5-2">
<ul class="org-ul">
<li>Шаблон - Пример</li>
<li>[Прил. + Сущ.]		файловая система</li>
<li>[Прич. + Сущ.]		вытесняющая многозадачность</li>
<li>[Сущ. + Сущ., Род.п.]	менеджер памяти</li>
<li>[Сущ. + Сущ., Твор.п.]	управление ресурсами</li>
<li>[Сущ. + ‘-’ + Сущ.]		файл-сервер</li>
</ul>

<p>
Nominative case — именительный падеж
Genitive — родительный
Accusative — винительный
Dative — дательный
Instrumental — творительный
Prepositional — предложный
ending — окончание
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org5bb41df" class="outline-4">
<h4 id="org5bb41df"><span class="section-number-4">13.7.6.</span> Полезные модули</h4>
<div class="outline-text-4" id="text-13-7-6">
<ul class="org-ul">
<li>nltk — один из основных модулей Python для анализа текстов, содержит множество инструментов.</li>
<li>re/regex — модули для работы с регулярными выражениями</li>
<li>pymorphy2/pymystem3 — лемматизаторы 4. Специализированные модули для обучения моделей (например, CRF)</li>
<li>numpy/pandas/scipy/sklearn — модули общего назначения</li>
<li>codecs — полезный модуль для работы с кодировками при использовании Python 2.*</li>
</ul>

<p>
HTML/XML parser Python -  дерево синтаксического разбора
</p>
<ul class="org-ul">
<li>Beautiful Soup</li>
<li>lxml</li>
</ul>

<p>
import matplotlib.pyplot as plt - построение граффиков
</p>
</div>
</div>
</div>

<div id="outline-container-org30fd8a1" class="outline-3">
<h3 id="org30fd8a1"><span class="section-number-3">13.8.</span> Извлечение именованных сущностей NER (Named-Entity Recognizing)</h3>
<div class="outline-text-3" id="text-13-8">
<ul class="org-ul">
<li>keras and tensorflow <a href="https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede">https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede</a></li>
<li><b>Semantic role labeling</b> - близкое понятие</li>
<li>Conditional random fields (CRFs) - class of statistical modeling method often applied in pattern recognition
and machine learning and used for structured prediction</li>
<li>что-то близкое <a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis">https://en.wikipedia.org/wiki/Latent_semantic_analysis</a></li>
<li>хороший сайт <a href="https://nlpub.ru">https://nlpub.ru</a></li>
<li></li>
</ul>




<p>
Tools <a href="https://en.wikipedia.org/wiki/Outline_of_natural_language_processing#Natural_language_processing_toolkits">https://en.wikipedia.org/wiki/Outline_of_natural_language_processing#Natural_language_processing_toolkits</a> :
</p>
<ul class="org-ul">
<li>Stanford NLP    3.9.2 	2018-10-16 	 <a href="https://habr.com/en/post/414175/">https://habr.com/en/post/414175/</a></li>
<li>OpenNLP - java - perceptron based machine learning -  <a href="https://ru.bmstu.wiki/OpenNLP">https://ru.bmstu.wiki/OpenNLP</a></li>
<li>Python github.com/nltk/nltk</li>
<li>spaCy - python - 	spacy.io <a href="https://github.com/explosion/spaCy">https://github.com/explosion/spaCy</a>
<ul class="org-ul">
<li>почти нет поддержки русского</li>
</ul></li>
<li>Apache UIMA - infrastructure, components, frameworks</li>
<li><a href="https://github.com/natasha/natasha">https://github.com/natasha/natasha</a></li>
<li>NLTK - можно только на части речи. Что-то сложнее через костыли
<ul class="org-ul">
<li><a href="https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html">https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html</a></li>
<li><a href="https://developer.ibm.com/articles/os-pythonnltk/">https://developer.ibm.com/articles/os-pythonnltk/</a></li>
</ul></li>
</ul>

<p>
аннотирования слов IOB:
</p>
<ul class="org-ul">
<li>POS (Part of Speech — часть речи)</li>
<li>Chunk - Noun chunks - phrase that have a noun as their head "the lavish green grass" or "the world’s largest
tech fund"</li>
<li>EntityType - PERSON, ORG, MONEY</li>
</ul>

<p>
<a href="https://habr.com/ru/articles/763542/">https://habr.com/ru/articles/763542/</a>
</p>
</div>

<div id="outline-container-orgfc2c3e0" class="outline-4">
<h4 id="orgfc2c3e0"><span class="section-number-4">13.8.1.</span> Approaches to NER</h4>
<div class="outline-text-4" id="text-13-8-1">
<ul class="org-ul">
<li>CNN <a href="https://towardsdatascience.com/what-is-wrong-with-convolutional-neural-networks-75c2ba8fbd6f">https://towardsdatascience.com/what-is-wrong-with-convolutional-neural-networks-75c2ba8fbd6f</a></li>
<li>CNN <a href="https://skymind.ai/wiki/convolutional-network">https://skymind.ai/wiki/convolutional-network</a></li>

<li>rule based - NLTK, yargy</li>
<li>Machine Learning Approaches
<ul class="org-ul">
<li>multi-class classification - problem: ignore context</li>
<li>Conditional Random Field (CRF) - problem: able to capture the features of the current and previous labels
in a sequence but it cannot understand the context of the forward labels</li>
</ul></li>
<li>Deep Learning Approaches
<ul class="org-ul">
<li>convolutional neural networks (CNNs) Problems:
<ol class="org-ol">
<li>Backpropagation - Метод обратного распространения ошибки - неопределённо долгий процесс обучения</li>
<li>Translation invariance - плохая трансляционная инвариантность - отсутствие инфы об ориентации</li>
<li>Pooling layers</li>
</ol></li>
<li>bidirectional Long short Term Memory (LSTM)  is an artificial recurrent neural network (RNN)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgb86a546" class="outline-4">
<h4 id="orgb86a546"><span class="section-number-4">13.8.2.</span> Deep learning</h4>
<div class="outline-text-4" id="text-13-8-2">
<p>
sentence representation:
</p>
<ol class="org-ol">
<li>Recurrent Neural Networks - sequence modeling</li>
<li>Convolutional Neural Networks - much faster</li>
<li>Recursive Neural Networks (Tree-LSTMs, DAG-LSTMs) - use hierarchical structure with help of syntax of
language</li>
</ol>

<p>
Morphology can help to build word embeddings
</p>
</div>
</div>



<div id="outline-container-org3f67980" class="outline-4">
<h4 id="org3f67980"><span class="section-number-4">13.8.3.</span> characteristics of the token &amp; text in a surrounding window</h4>
<div class="outline-text-4" id="text-13-8-3">
<p>
<a href="https://slideplayer.com/slide/4965710/">https://slideplayer.com/slide/4965710/</a>
</p>
<ul class="org-ul">
<li>lexical items -</li>
<li>stemmed lexical items - stemmed version of the target token</li>
<li>shape - orphographic pattern of the target word</li>
<li>character affix - character-level affixes of the target and surrounding words</li>
<li>pos</li>
<li>syntactic chunk labels - base-phrase chunk label</li>
<li>gazetter or name list - presence of the word in one or more named entity lists</li>
<li>Predictive token(s) - presence of predictive words in surrounding text</li>
<li>Bag of words/Bag of N-gramds - Words and/or N-grams <b>occurring</b> in surrounding context</li>
<li>TF-IDF - статистическая мера, используемая для оценки важности слова в контексте документа</li>
</ul>
</div>
</div>

<div id="outline-container-org5c6376e" class="outline-4">
<h4 id="org5c6376e"><span class="section-number-4">13.8.4.</span> Shape/orthographic features</h4>
<div class="outline-text-4" id="text-13-8-4">
<ul class="org-ul">
<li>lower</li>
<li>Capitalized</li>
<li>All caps</li>
<li>mixed case - eBay</li>
<li>Capitalized character with period - H.</li>
<li>Ends in digit - A9</li>
<li>Contains hyphen - H-P</li>
</ul>
</div>
</div>
<div id="outline-container-org1d9475c" class="outline-4">
<h4 id="org1d9475c"><span class="section-number-4">13.8.5.</span> Metrics</h4>
<div class="outline-text-4" id="text-13-8-5">
<p>
<b>false positives and false negatives have a business cost in a NER task</b>
</p>
<ul class="org-ul">
<li>F1 score because we need a balance between precision and recall - точностью и полнота</li>
</ul>
</div>
</div>
<div id="outline-container-orgf24cc63" class="outline-4">
<h4 id="orgf24cc63"><span class="section-number-4">13.8.6.</span> С использованием нейронных сетей (CNN):</h4>
<div class="outline-text-4" id="text-13-8-6">
<ul class="org-ul">
<li>spacy vs Stanford NER <a href="https://towardsdatascience.com/a-review-of-named-entity-recognition-ner-using-automatic-summarization-of-resumes-5248a75de175">https://towardsdatascience.com/a-review-of-named-entity-recognition-ner-using-automatic-summarization-of-resumes-5248a75de175</a></li>
<li>spaCy - convolutional neural network  <a href="https://en.wikipedia.org/wiki/SpaCy">https://en.wikipedia.org/wiki/SpaCy</a></li>
<li>OpenNER - Named Entity Resolution - заточен на обычные тексты со словарями</li>
<li>NLTK + sckit-learn - TF-IDF vector</li>
<li>Stanford CoreNLP or Stanford Named Entity Recognizer (NER) - Conditional random field - statistical modeling
method - Doesn’t assume that features are independent - Java implementation  <a href="https://nlp.stanford.edu/software/CRF-NER.shtml">https://nlp.stanford.edu/software/CRF-NER.shtml</a></li>
<li>DeepPavlov - all the components required for building chatbots - TensorFlow and Keras - <a href="https://deeppavlov.ai/">https://deeppavlov.ai/</a> <a href="https://github.com/deepmipt/DeepPavlov">https://github.com/deepmipt/DeepPavlov</a></li>
</ul>

<p>
сверточных нейронных сетей  <a href="https://habr.com/en/company/ods/blog/353060/">https://habr.com/en/company/ods/blog/353060/</a>
Лучше Рекуррентные нейронные сети
</p>
<ul class="org-ul">
<li><img src="https://hsto.org/getpro/habr/post_images/642/8cf/505/6428cf505ac1e9e1cf462e1ec8fe9a68.gif" alt="6428cf505ac1e9e1cf462e1ec8fe9a68.gif" /></li>
</ul>
</div>
</div>

<div id="outline-container-org7a33ff5" class="outline-4">
<h4 id="org7a33ff5"><span class="section-number-4">13.8.7.</span> Apache OpenNLP</h4>
<div class="outline-text-4" id="text-13-8-7">
<ul class="org-ul">
<li>sentence segmentation</li>
<li>part-of-speech tagging</li>
<li>named entity extraction</li>
<li>chunking</li>
<li>parsing</li>
<li>language detection</li>
<li>coreference resolution - отношение между именами - ссылаются на один и тот же объект (ситуацию) внеязыковой
действительности - референт</li>
</ul>
</div>
</div>

<div id="outline-container-org2a4b06f" class="outline-4">
<h4 id="org2a4b06f"><span class="section-number-4">13.8.8.</span> Natasha</h4>
<div class="outline-text-4" id="text-13-8-8">
<p>
Natasha - это собрание правил для ярги-парсера
</p>
<ul class="org-ul">
<li><a href="https://github.com/natasha/natasha">https://github.com/natasha/natasha</a></li>
<li><a href="https://github.com/natasha/yargy">https://github.com/natasha/yargy</a></li>
<li><a href="https://habr.com/en/post/349864/">https://habr.com/en/post/349864/</a></li>
<li><a href="https://www.youtube.com/watch?time_continue=1027&amp;v=NQxzx0qYgK8">https://www.youtube.com/watch?time_continue=1027&amp;v=NQxzx0qYgK8</a></li>
<li>yargy ярги-парсер -</li>
</ul>

<p>
Недостатки:
</p>
<ul class="org-ul">
<li>правила для извлечения имён не до конца документированы.</li>
<li>Вручную составленные правила.</li>
<li>Медленная скорость работы.</li>
<li>Ошибки в стандартных правилах.</li>
</ul>
<p>
Достоинства
</p>
<ul class="org-ul">
<li>заявляет, что  Яндекс не раскрывает свои правила для Томита-парсера.</li>
</ul>


<p>
Extractors:
</p>
<ul class="org-ul">
<li>NamesExtractor - NAME,tagger=tagger</li>
<li>SimpleNamesExtractor - SIMPLE<sub>NAME</sub></li>
<li>PersonExtractor - PERSON, tagger=tagger</li>
<li>DatesExtractor - DATE</li>
<li>MoneyExtractor - MONEY</li>
<li>MoneyRateExtractor - MONEY<sub>RATE</sub></li>
<li>MoneyRangeExtractor - MONEY<sub>RANGE</sub></li>
<li>AddressExtractor - ADDRESS, tagger=tagger</li>
<li>LocationExtractor - LOCATION</li>
<li>OrganisationExtractor - ORGANISATION</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org0c190f6"></a>yargy<br />
<div class="outline-text-5" id="text-13-8-8-1">
<p>
Извлечение структурированной информации из текстов на русском языке
</p>
<ul class="org-ul">
<li>GLR-парсер <a href="https://en.wikipedia.org/wiki/Earley_parser">https://en.wikipedia.org/wiki/Earley_parser</a></li>
<li>на идеи контекстно свободной грамматики
<a href="https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BD%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BD%D0%BE-%D1%81%D0%B2%D0%BE%D0%B1%D0%BE%D0%B4%D0%BD%D0%B0%D1%8F_%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0">https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BD%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BD%D0%BE-%D1%81%D0%B2%D0%BE%D0%B1%D0%BE%D0%B4%D0%BD%D0%B0%D1%8F_%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0%D1%82%D0%B8%D0%BA%D0%B0</a></li>
<li>Использует правила и словари, чтобы извлекать информацию из текста</li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-org8538997" class="outline-4">
<h4 id="org8538997"><span class="section-number-4">13.8.9.</span> UDPipe</h4>
<div class="outline-text-4" id="text-13-8-9">
<ul class="org-ul">
<li><a href="http://ufal.mff.cuni.cz/udpipe/users-manual#model_training">http://ufal.mff.cuni.cz/udpipe/users-manual#model_training</a></li>
</ul>
</div>
</div>
</div>



<div id="outline-container-org9b498b4" class="outline-3">
<h3 id="org9b498b4"><span class="section-number-3">13.9.</span> extracting features</h3>
<div class="outline-text-3" id="text-13-9">
</div>
<div id="outline-container-org3b22cae" class="outline-4">
<h4 id="org3b22cae"><span class="section-number-4">13.9.1.</span> bag-of-words bag of words</h4>
<div class="outline-text-4" id="text-13-9-1">
<ol class="org-ol">
<li>Managing Vocabulary

<ol class="org-ol">
<li>vocabulary of known words</li>
<li>measure of the presence of known words.</li>
</ol></li>
</ol>

<p>
can be as simple or complex - how to design the vocabulary of known words (or tokens) and how to score the presence
</p>

<ol class="org-ol">
<li>Scoring Words
<ul class="org-ul">
<li>Counts. Count the number of times each word appears in a document.</li>
<li>Frequencies. Calculate the frequency that each word appears in a document out of all the words in the document.</li>
</ul></li>

<li>Word Hashing ( “hash trick” or “feature hashing“.) - reduse vocabulary size.</li>

<li>TF-IDF see <a href="#org4b8425b">13.7.5.1.4</a> - approach to rescale the frequency of words by how often they appear in all documents,</li>
</ol>
</div>
</div>
</div>


<div id="outline-container-org5844dc5" class="outline-3">
<h3 id="org5844dc5"><span class="section-number-3">13.10.</span> preprocessing</h3>
<div class="outline-text-3" id="text-13-10">
<p>
Test: characters, <b>words</b>, Phrases and named entities, sentences, paragraphs
</p>

<p>
syntax can really help you to understand what is important to local context and what is not
</p>

<p>
Matrix factorization - measure of whether the words are similar.
</p>

<ul class="org-ul">
<li>GloVe - matrix factorization</li>
<li>skip-gram - Predict context words given a focus word
<ul class="org-ul">
<li>language modeling - probabilities of some words given
some other words</li>
</ul></li>
</ul>
</div>
<div id="outline-container-orgbd5fd8d" class="outline-4">
<h4 id="orgbd5fd8d"><span class="section-number-4">13.10.1.</span> Two existing strategies for applying pre-trained language representations to downstream tasks:</h4>
<div class="outline-text-4" id="text-13-10-1">
<ul class="org-ul">
<li>feature-based - (ELMo) - uses tasks-specific architectures that include the pre-trained representations as
additional features</li>
<li>fine-tuning - (OpenAI GPT) - Generative Pre-trained Transforme - minimal task-specific parameters, and is
trained on the downstream tasks by simply fine-tuning the pretrained parameters</li>
</ul>
</div>
</div>

<div id="outline-container-orgfc47e67" class="outline-4">
<h4 id="orgfc47e67"><span class="section-number-4">13.10.2.</span> <span class="todo TODO">TODO</span> singular-value decomposition (SVD) Сингулярное разложение</h4>
</div>

<div id="outline-container-org2ba741f" class="outline-4">
<h4 id="org2ba741f"><span class="section-number-4">13.10.3.</span> Word embedding</h4>
<div class="outline-text-4" id="text-13-10-3">
<ul class="org-ul">
<li><a href="https://habr.com/ru/company/ods/blog/329410/">https://habr.com/ru/company/ods/blog/329410/</a></li>
</ul>
<p>
techniques where words are mapped to vectors. (в Дистрибутивной семантике)
</p>
<ul class="org-ul">
<li>Embedding - one instance contained within another instance. by some injective and structure-preserving map
f:X-&gt;Y Например: целые числа в рациональных.</li>
<li>embedding from a space with one dimension per word to a continuous vector space with a much lower dimension</li>
<li>направленных на сопоставление словам (и, возможно, фразам) из некоторого словаря векторов из R , значительно
меньшего количества слов в словаре.</li>
<li>used as the underlying input representation, have been shown to boost the performance in NLP tasks such as
syntactic parsing[8] and sentiment analysis</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org50df61d" class="outline-3">
<h3 id="org50df61d"><span class="section-number-3">13.11.</span> n-gram</h3>
<div class="outline-text-3" id="text-13-11">
<p>
“The ball is blue”
</p>
<ul class="org-ul">
<li>1-gram (unigram): “The”, “ball”, “is”, “blue”</li>
<li>2-gram (bigram): “The ball”, “ball is”, “is blue”</li>
<li>3-gram (trigram): “The ball is”, “ball is blue”</li>
<li>4-gram: “The ball is blue”</li>
</ul>
</div>
</div>

<div id="outline-container-org2d70bcb" class="outline-3">
<h3 id="org2d70bcb"><span class="section-number-3">13.12.</span> Bleu Score and WER Metrics</h3>
<div class="outline-text-3" id="text-13-12">
<p>
Precision metric -
</p>

<p>
Bleu Score - [0;1]
</p>

<p>
WER = (num inserted + num deleted + num substituted) / num words in the reference (based on the Levenshtein distance)
</p>
<ul class="org-ul">
<li>can be larger than 1.0</li>
</ul>
</div>
</div>



<div id="outline-container-orgda4843c" class="outline-3">
<h3 id="orgda4843c"><span class="section-number-3">13.13.</span> Levels of analysis:</h3>
<div class="outline-text-3" id="text-13-13">
<p>
Increase Complexity of processing:
</p>
<ol class="org-ol">
<li>Morphology</li>
<li>POS tagging</li>
<li>Chunking</li>
<li>Parsing</li>
<li>Semantics</li>
<li>Discourse and Coreference</li>
</ol>
</div>

<div id="outline-container-org0d82a2e" class="outline-4">
<h4 id="org0d82a2e"><span class="section-number-4">13.13.1.</span> old</h4>
<div class="outline-text-4" id="text-13-13-1">
<ul class="org-ul">
<li>Speech - Phonetic/Phonological analysis</li>
<li>Text - OCR/Tokenization</li>
<li>Morphological analysis - слова - части речи</li>
<li>Syntactic an - словосочетания, типология высказывания</li>
<li>Semantic Interpretation - смысл слов и словосочетаний</li>
<li>Discourse Processing - Дискурсивный анализ - типы речи, языковоые сообщества, связи между предложениями</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orga89adc4" class="outline-3">
<h3 id="orga89adc4"><span class="section-number-3">13.14.</span> Universal grammar</h3>
<div class="outline-text-3" id="text-13-14">
<p>
Ideas:
</p>
<ul class="org-ul">
<li>all human languages are species of a common genus - limit in variations</li>
<li>Language structures is constrained by a universe cause - categories of language reflects categories of the
worlds</li>
<li>there is order in liguistic variations</li>
</ul>

<p>
Currently NLP relies heavily on linguistic annotation. But annotation scheme varies for different languages.
</p>
<ul class="org-ul">
<li>"In ins substance grammar in the same in all languages"</li>
</ul>

<p>
Категории языков:
</p>
<ul class="org-ul">
<li>left initial - most of the arrows go to right</li>
</ul>

<p>
<b>Cross-linguistically consistent standart for grammatical annotation</b> <a href="https://universaldependencies.org">https://universaldependencies.org</a>
</p>
<ul class="org-ul">
<li>Part-of-speech tags - NOUN, ADV，VERB  (Google)</li>
<li>Morphological of  morphosyntactic features - Number=Plur; Gender=Fem,Masc; Tense=Pres (UFAL?)</li>
<li>for syntax or dependency structure - modified Dependency relations (Stanfort) - Universal Dependencies</li>
</ul>

<p>
Goal: cross-linguistically consistent grammatical annotation
</p>

<p>
Principles:
</p>
<ul class="org-ul">
<li>available in threebans</li>
<li>Basic annotation units are words - syntactic or grammatical words (not phonological, or orphographical) - no attempts to segment words into a
morphems</li>
<li>Words have morphological properties</li>
<li>words enter into suntactic relations</li>
</ul>
</div>
</div>
<div id="outline-container-org569a0a5" class="outline-3">
<h3 id="org569a0a5"><span class="section-number-3">13.15.</span> Корпус языка</h3>
<div class="outline-text-3" id="text-13-15">
<ul class="org-ul">
<li>там ссылки <a href="https://tatianashavrina.github.io/2018/08/30/datasets/">https://tatianashavrina.github.io/2018/08/30/datasets/</a>
<a href="https://github.com/TatianaShavrina/tatianashavrina.github.io/blob/master/_posts/2018-08-30-datasets.md">https://github.com/TatianaShavrina/tatianashavrina.github.io/blob/master/_posts/2018-08-30-datasets.md</a></li>
<li>национальный используется <a href="http://ruscorpora.ru/corpora-usage.html">http://ruscorpora.ru/corpora-usage.html</a></li>
<li>русский проект используется в pymorphy2 <a href="http://opencorpora.org/">http://opencorpora.org/</a></li>
<li>treebank - syntactic or semantic sentence structure <a href="http://universaldependencies.org">http://universaldependencies.org</a>
<ul class="org-ul">
<li>SynTagRus - NC</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org8719f27" class="outline-3">
<h3 id="org8719f27"><span class="section-number-3">13.16.</span> seq2seq model</h3>
<div class="outline-text-3" id="text-13-16">
<ul class="org-ul">
<li>Introduced for the first time in 2014 by Google - aims to map a fixed length input with a fixed length
output where the length of the input and output may differ</li>
<li>arxiv.org/pdf/1406.1078.pdf</li>
<li>состоит из двух рекуррентных сетей (RNN):
<ul class="org-ul">
<li>encoder (кодер), которая обрабатывает входные данные</li>
<li>decoder (декодер), которая генерирует данные вывода</li>
</ul></li>
<li>For:
<ul class="org-ul">
<li>Machine Translation</li>
<li>Text Summarization</li>
<li>Conversational Modeling</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgbc00423" class="outline-3">
<h3 id="orgbc00423"><span class="section-number-3">13.17.</span> Рукописные цифры анализ</h3>
<div class="outline-text-3" id="text-13-17">
<p>
Сети:
</p>
<ul class="org-ul">
<li>LeNet 1988 - обычная CNN</li>
<li>ReNet(2015) - рекурентная для изображений - многонаправленная</li>
<li>PyraMiD-LSTM(2015) - для сегментации мозговых срезов</li>
<li>Grid LSTM(2016)</li>
</ul>
</div>
</div>
<div id="outline-container-orgd2b428e" class="outline-3">
<h3 id="orgd2b428e"><span class="section-number-3">13.18.</span> Fully-parallel text generation for neural machine translation</h3>
<div class="outline-text-3" id="text-13-18">
<ul class="org-ul">
<li><a href="https://blog.einstein.ai/fully-parallel-text-generation-for-neural-machine-translation/">https://blog.einstein.ai/fully-parallel-text-generation-for-neural-machine-translation/</a></li>
</ul>

<p>
Как Transformer, но Ускоряет генерацию, передавая все предложение целиком, а не по словам.
</p>
</div>
</div>
<div id="outline-container-orge2d7f3b" class="outline-3">
<h3 id="orge2d7f3b"><span class="section-number-3">13.19.</span> speaker diarization task</h3>
<div class="outline-text-3" id="text-13-19">
<ul class="org-ul">
<li>speaker has to talk for more than 30 seconds in order to accurately be detected by a Speaker Diarization model.</li>
<li>if the conversation is more energetic, with the speakers cutting each other off or speaking over one
another, or has significant background noise, the model’s accuracy will decrease.</li>
<li>if overtalk (aka crosstalk) , the model may even misidentify an imaginary third speaker, which includes the
portions of overtalk.</li>
</ul>
</div>
</div>
<div id="outline-container-org2651a2b" class="outline-3">
<h3 id="org2651a2b"><span class="section-number-3">13.20.</span> keyword extraction</h3>
<div class="outline-text-3" id="text-13-20">
<p>
<a href="https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c">https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c</a>
</p>
</div>
</div>

<div id="outline-container-org0720314" class="outline-3">
<h3 id="org0720314"><span class="section-number-3">13.21.</span> Approximate string matching or fuzzy string searching</h3>
<div class="outline-text-3" id="text-13-21">
<p>
approaches:
</p>
<ul class="org-ul">
<li>On-line: pattern can be processed before searching, but the text cannot.  searching without an index
<ul class="org-ul">
<li>Bitap algorithm - tells whether a given text contains a substring - distance k</li>
</ul></li>
<li>off-line:</li>
</ul>

<p>
tools:
</p>
<ul class="org-ul">
<li>agrep - bitap algorithm</li>
</ul>
</div>
<div id="outline-container-org099892d" class="outline-4">
<h4 id="org099892d"><span class="section-number-4">13.21.1.</span> steps</h4>
<div class="outline-text-4" id="text-13-21-1">
<ul class="org-ul">
<li>tokenize</li>
</ul>
</div>
</div>
<div id="outline-container-orga4d494a" class="outline-4">
<h4 id="orga4d494a"><span class="section-number-4">13.21.2.</span> agrep</h4>
<div class="outline-text-4" id="text-13-21-2">
<p>
-# - number of erros permitted. For insertions, deletions and substitutions (see -I -D and -S options)
</p>
</div>
</div>
<div id="outline-container-org17a4e13" class="outline-4">
<h4 id="org17a4e13"><span class="section-number-4">13.21.3.</span> links</h4>
<div class="outline-text-4" id="text-13-21-3">
<ul class="org-ul">
<li><a href="https://bart.degoe.de/building-a-full-text-search-engine-150-lines-of-code/">https://bart.degoe.de/building-a-full-text-search-engine-150-lines-of-code/</a></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org78f03a2" class="outline-3">
<h3 id="org78f03a2"><span class="section-number-3">13.22.</span> pre-training objective</h3>
<div class="outline-text-3" id="text-13-22">
<p>
<b>pre-training objective</b> is a task on which a model is trained before being fine-tuned for the end task
</p>

<p>
GPT models are trained on a Generative Pre-Training task (hence the name GPT) i.e. generating the next token
 given previous tokens
</p>

<p>
BERT uses MLM and NSP as its pre-training objectives.
</p>
<ul class="org-ul">
<li>Masked Language Model(MLM) - mask words from a sequence of input or sentences and the designed model needs
to predict the masked words to complete the sentence</li>
<li>Next Sentense Prediction (NSP)</li>
</ul>

<p>
Including coding tasks in pre-training dataset enhance reasinig and answers by 4-8%.
</p>
<ul class="org-ul">
<li>source: 2408.10914v1.pdf</li>
<li>adding a small amount of high-quality synthetic data can have an outsized impact on both natural(9%) and
code performance(44%)</li>
</ul>
</div>
</div>
<div id="outline-container-org868691c" class="outline-3">
<h3 id="org868691c"><span class="section-number-3">13.23.</span> Principle of compositionality or Frege's principle</h3>
<div class="outline-text-3" id="text-13-23">
<p>
meaning of a complex expression is determined by the meanings of its <b>constituent expressions</b> and the rules
used to combine them
</p>

<p>
Some theorists argue that the principle has to be revised to take into account linguistic and extralinguistic
context, which includes the tone of voice used, common ground between the speakers, the intentions of the
speaker, and so on.
</p>
</div>
</div>

<div id="outline-container-orgf864c21" class="outline-3">
<h3 id="orgf864c21"><span class="section-number-3">13.24.</span> 2023 major development</h3>
<div class="outline-text-3" id="text-13-24">
<p>
From RNNs to Transformers
</p>
<ul class="org-ul">
<li>Unrolled RNNs</li>
<li>Encoder-decoders</li>
<li>Attention mechanism with RNNs -  it suggests some way to prioritise which states the encoder is looking at.</li>
<li>First transformer architecture - self-attention</li>
<li>Transfer learning</li>
</ul>

<p>
Encoder-decoders - for mapping words in a language to another language. As new inputs are fed in, the encoder
updates the state until the final input, at which the last hidden state is taken into a numerical
representation. The decoder is fed this representation and uses it to generate the output sequence. The
decoder then “unpacks”, one output word at a time.
</p>

<p>
Problem: the information bottleneck caused by the use of only one hidden state was a problem.  the decoder
only has access to a very reduced representation of the sequence. As a result, practitioners began to give the
decoder access to all of the encoder’s hidden states. This is known as attention.
</p>

<p>
The clever solution is to assign learnable parameters (or weights, or attention) to each encoder state, at
each time step. During training, the decoder learns how much attention to pay to each output at each timestep.
</p>

<p>
Problem of attention - sequential computations, requiring inputs to be fed in one at a time, prevents
parallelisation across the input sequence. There are a few reasons why this is less than desirable, but one is
that it’s slow.
</p>

<p>
Transformer - it removed the recurrent network blocks, and allowed attention to engage with all states in the
same layer of the network. This is known as <b>self-attention</b> - faster than the previous attention mechanism
(in terms of training) and is the foundation for much of modern NLP practice.
</p>

<p>
Transfer learning is a huge deal in NLP (train the head on our task-specific data):
</p>
<ul class="org-ul">
<li>assembling a large text corpus to train on is often difficult</li>
<li>we don’t have powerful enough GPUs (unless we’re someone like OpenAI) to train these models anyway.</li>
</ul>

<p>
Key transfer learning method in NLP is ULMFiT (universal language model fine-tuning for text classification).
 Pretrain a model to predict the next word given a sequence of words, which as you may have noted doesn’t
 require labeled data. After this unsupervised pretraining, do the same training (predicting the next word) on
 your specific data. Finally, train the head of this new model on the classification task.
</p>

<p>
This breakthrough gestated two transformers that combined self-attention with transfer learning: GPT and
BERT. Both achieved state-of-the-art results on many NLP benchmark tasks.
</p>
</div>
</div>
<div id="outline-container-org7f45ac1" class="outline-3">
<h3 id="org7f45ac1"><span class="section-number-3">13.25.</span> IntellectDialog - автоматизации взаимодействия с клиентами в мессенджерах</h3>
<div class="outline-text-3" id="text-13-25">
<p>
Опыт работы в разработке NLP-приложений и знание инструментов по обработке естественного языка на Python,
 таких как SpaCy, NLTK, Gensim и т.д. Понимание основных приемов обработки естественного языка, включая
 способы извлечения ключевых слов, именованных сущностей, анализ синтаксиса, грамматические модели и обработку
 структурных данных.
</p>
</div>
</div>
<div id="outline-container-orgc2ddebb" class="outline-3">
<h3 id="orgc2ddebb"><span class="section-number-3">13.26.</span> Transformers applications for NLP</h3>
<div class="outline-text-3" id="text-13-26">
<p>
BERT/GPT/T5 и задач, которые они решают
</p>
</div>
<div id="outline-container-org44dd10e" class="outline-4">
<h4 id="org44dd10e"><span class="section-number-4">13.26.1.</span> BERT Bidirectional Encoder Representations  from Transformers</h4>
<div class="outline-text-4" id="text-13-26-1">
<p>
2019 <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>
</p>

<p>
Transformer which is composed of two parts, the Encoder and the Decoder.  BERT only uses the <b>Encoder</b>.
</p>

<p>
for each position in the input, the output at the same position is the same token (or the [MASK] token for
 masked tokens)
</p>

<p>
Models with only an encoder stack like BERT generate all its outputs at once.
</p>

<p>
Two steps:
</p>
<ul class="org-ul">
<li>pre-training (with  “masked  language  model”  (MLM) )
<ul class="org-ul">
<li>mask 15% of tokens [MASK]</li>
<li>predict the masked words</li>
</ul></li>
<li>fine-tuning</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org333f8fd" class="outline-3">
<h3 id="org333f8fd"><span class="section-number-3">13.27.</span> metrics</h3>
<div class="outline-text-3" id="text-13-27">
</div>
<div id="outline-container-org24811ad" class="outline-4">
<h4 id="org24811ad"><span class="section-number-4">13.27.1.</span> BLEU (bilingual evaluation understudy)</h4>
<div class="outline-text-4" id="text-13-27-1">
<p>
the quality of text which has been machine-translated from one natural language to another.
</p>
<ul class="org-ul">
<li>[0,1] - 1 is good, 0 is bad ( sometimes scale to [0,100])</li>
<li>how similar the <b>candidate text</b> is to the <b>reference texts</b></li>
<li>1 mean <b>candidate</b> is identical to one of the <b>reference</b> translations</li>
<li>used four-gram - The length which has the "highest correlation with monolingual human judgements was found to be 4.</li>
</ul>

<p>
pros: correlating well with human judgement
</p>

<p>
cons:
</p>
<ul class="org-ul">
<li>cannot, in its present form, deal with languages lacking word boundaries.</li>
<li>Designed to be used for several reference translation, in practice it's used with only the single one.</li>
<li>dependent on the tokenization technique (SacreBLEU variant was designed to solve it)</li>
<li></li>
</ul>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Candidate</td>
<td class="org-left">the</td>
<td class="org-left">the</td>
<td class="org-left">the</td>
<td class="org-left">the</td>
<td class="org-left">the</td>
<td class="org-left">the</td>
<td class="org-left">the</td>
</tr>

<tr>
<td class="org-left">Reference1</td>
<td class="org-left">the</td>
<td class="org-left">cat</td>
<td class="org-left">is</td>
<td class="org-left">on</td>
<td class="org-left">the</td>
<td class="org-left">mat</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Reference2</td>
<td class="org-left">there</td>
<td class="org-left">is</td>
<td class="org-left">a</td>
<td class="org-left">cat</td>
<td class="org-left">on</td>
<td class="org-left">the</td>
<td class="org-left">mat</td>
</tr>
</tbody>
</table>

<ol class="org-ol">
<li>for unigram: m/wt = 7/7 = 1, where</li>
<li>m - number of words from the candidate that are found in the reference (all "the" was found in reference)</li>
<li>wt - total number of words in candidate</li>
<li>the - 7, occure r1 = 2, r2 = 1, that is why we have 2/7 and 1/7</li>
<li>penalty if input&lt;output</li>
</ol>
</div>
</div>

<div id="outline-container-orgdafa00d" class="outline-4">
<h4 id="orgdafa00d"><span class="section-number-4">13.27.2.</span> Perplexity</h4>
<div class="outline-text-4" id="text-13-27-2">
<p>
measure of how well a probability model predicts a sample, It is used to evaluate language models, indicating
 how well they can predict the next word in a sequence of text. A lower perplexity score suggests that the
 model has a higher certainty in its predictions, meaning it is better at predicting the next word.
</p>
</div>
</div>
<div id="outline-container-org4cad351" class="outline-4">
<h4 id="org4cad351"><span class="section-number-4">13.27.3.</span> NIST - based on the BLEU</h4>
<div class="outline-text-4" id="text-13-27-3">
<p>
also calcuate: how informative a particular n-gram is.
</p>
</div>
</div>
<div id="outline-container-org04d428b" class="outline-4">
<h4 id="org04d428b"><span class="section-number-4">13.27.4.</span> Word error rate (WER) or word accuracy (WAcc)</h4>
<div class="outline-text-4" id="text-13-27-4">
<p>
performance of a speech recognition
</p>
<ul class="org-ul">
<li>derived from the Levenshtein distance</li>
<li>working at the word level</li>
<li>provides no details on the nature of translation errors</li>
</ul>

<p>
cons:  true understanding of spoken language relies on more than just high word recognition accuracy
</p>

<p>
WER = (S + D + I) / (S + D + C)
</p>
<ul class="org-ul">
<li>S - substitutions</li>
<li>D - deletions</li>
<li>I - insertions</li>
<li>C - correct words</li>
</ul>

<p>
WAcc = 1 - WER = (C - I) / N -  can be larger than 1.0
</p>


<p>
weighted WER = (S + 0.5*D + 0.5*I)/N (some errors may be more disruptive than others and some may be corrected more easily than others)
</p>
</div>
</div>
</div>
<div id="outline-container-org6a12d81" class="outline-3">
<h3 id="org6a12d81"><span class="section-number-3">13.28.</span> RLHF (Reinforcement Learning from Human Feedback) <a id="org01c35d9"></a></h3>
<div class="outline-text-3" id="text-13-28">
<p>
reinforce [riːɪnˈfɔːs] - укреплять
</p>
</div>

<div id="outline-container-orgba4effc" class="outline-4">
<h4 id="orgba4effc"><span class="section-number-4">13.28.1.</span> classic</h4>
<div class="outline-text-4" id="text-13-28-1">
<p>
The 5 Steps of RLHF:
</p>
<ol class="org-ol">
<li>Starting with a pre-trained model (to generate outputs for a specific task.)</li>
<li>Supervised fine-tuning <b>SFT</b> (trained on a specific task or domain with labeled data)</li>
<li>Reward model training <b>RM</b> (reward model is trained to recognize desirable outputs generated by the
generative model and assign a score) - auxiliary reward model</li>
<li><p>
Reinforcement learning <b>RL</b> via proximal policy optimization <b>PPO</b>: <img src="https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133_trl_peft/openai-diagram.png" alt="openai-diagram.png" />
</p>
<ul class="org-ul">
<li>allows the model to learn from experience and adapt to new situations in real-time.</li>
<li>It interacts with an environment and receives feedback in the form of rewards or penalties, allowing it</li>
</ul>
<p>
to learn which actions lead to desirable outcomes.
</p>
<ul class="org-ul">
<li>The goal is to learn a policy that maximizes the expected cumulative reward over a sequence of actions,</li>
</ul>
<p>
given a particular state, while also constraining the magnitude of updates to prevent large deviations.
</p></li>
<li>Red teaming: the system is stress-tested by a curated crowd to ensure it’s able to handle real-world
scenarios and make accurate and relevant predictions.</li>
</ol>

<p>
Note: add KL penalty - to the full reward maximisation objective via a reference model, which serves to
 prevent the model from learning to cheat or exploit the reward model.
</p>

<p>
PPO (schulman et at., 2017): <a href="https://arxiv.org/abs/1707.06347">https://arxiv.org/abs/1707.06347</a>
</p>

<p>
RL scheme (stiennon et al. 2020) <a href="https://arxiv.org/abs/2009.01325">https://arxiv.org/abs/2009.01325</a>
</p>
</div>
</div>
<div id="outline-container-orgaae11d3" class="outline-4">
<h4 id="orgaae11d3"><span class="section-number-4">13.28.2.</span> Direct Preference Optimization (DPO)</h4>
<div class="outline-text-4" id="text-13-28-2">
<p>
direct likelihood objective can be optimized without the need for a reward model or the need to perform the potentially fiddly RL based optimisation.
</p>

<p>
steps:
</p>
<ol class="org-ol">
<li>a supervised fine-tuning (SFT) step</li>
<li>the process of annotating data with preference labels</li>
<li>however the DPO training does away with the task of reward modeling and RL (steps 3 and 4) and directly
optimizes the DPO object on preference annotated data. (3. training a reward model on the preference
data 4. and the RL optmization step)</li>
</ol>
</div>
<ol class="org-ol">
<li><a id="org6e82151"></a>links<br />
<div class="outline-text-5" id="text-13-28-2-1">
<ul class="org-ul">
<li><a href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a></li>
<li><a href="https://huggingface.co/blog/dpo-trl">https://huggingface.co/blog/dpo-trl</a></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-orge7bc397" class="outline-4">
<h4 id="orge7bc397"><span class="section-number-4">13.28.3.</span> ChatGPT 3 steps</h4>
<div class="outline-text-4" id="text-13-28-3">
<ol class="org-ol">
<li><p>
Collect demonstration data and train a supervised policy.
</p>
<ul class="org-ul">
<li>pretrained transformer-based model is fine-tuned on this dataset combined with the old dataset, which is</li>
</ul>
<p>
transformed into a dialogue format.
</p></li>
<li>get a model that takes in a pair (prompt, text) and returns a scalar reward which should numerically
represent the human preference. <b>RM</b></li>
</ol>
</div>

<ol class="org-ol">
<li><a id="orge44a974"></a>links<br />
<div class="outline-text-5" id="text-13-28-3-1">
<ul class="org-ul">
<li><a href="https://dida.do/blog/chatgpt-reinforcement-learning">https://dida.do/blog/chatgpt-reinforcement-learning</a></li>
<li><a href="https://arxiv.org/pdf/2203.02155.pdf">https://arxiv.org/pdf/2203.02155.pdf</a></li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgc3fc4a7" class="outline-4">
<h4 id="orgc3fc4a7"><span class="section-number-4">13.28.4.</span> 2024 RLF with Mixture of Judges Experts</h4>
<div class="outline-text-4" id="text-13-28-4">
<p>
2409.20370v1
</p>

<p>
"Mixture-of-Agents-or-Judges" (MoA, MoJ) approach, multiple LLMs are used in a layered architecture to iteratively enhance the
 generation quality.
</p>
</div>
</div>

<div id="outline-container-orge9749cd" class="outline-4">
<h4 id="orge9749cd"><span class="section-number-4">13.28.5.</span> Multi-Armed Bandit problem <a id="org54a7d30"></a></h4>
<div class="outline-text-4" id="text-13-28-5">
<p>
type of <b>Stochastic scheduling</b> problem
</p>

<p>
<b>decision maker</b> iteratively selects one of multiple fixed choices (i.e., <b>arms</b>) when the
 properties of each choice are only partially known at the time of allocation, and may become better
 understood as time passes.
</p>
<ul class="org-ul">
<li>choosing an arm does not affect the properties of the arm or other arms</li>
</ul>

<p>
The objective is to maximize the sum of the collected rewards.
</p>

<p>
maker at trial choose between “exploitation" of the machine that has the highest expected payoff and
 "exploration" to get more information about the expected payoffs of the other machines.
</p>
</div>
<ol class="org-ol">
<li><a id="orgb9f644c"></a>term<br />
<div class="outline-text-5" id="text-13-28-5-1">
<ul class="org-ul">
<li>incentive <i>inˈsen(t)iv</i> - stimulus</li>
<li><b>decision maker</b> or <b>gambler</b></li>
<li><b>arms</b> or <b>actions</b> or <b>trials</b> or headlines or campaigns</li>
<li>optimal policy or action</li>
<li><b>optimal regret</b> - achieving the minimum possible difference between the cumulative rewards of the learning
algorithm and the optimal policy, often measured by sublinear regret bounds that reflect efficient learning
and adaptation.</li>
<li>regret lower bound -  min lim for regret, when T → ∞.</li>
<li>asymptotic regret lower bound - represents the best possible performance an algorithm can achieve in terms
of regret, when T → ∞.
<ul class="org-ul">
<li>asymptotic - math. the closer you get, the more you feel you can never make it.</li>
</ul></li>
</ul>
</div>
</li>
<li><a id="org642b8e2"></a>math model<br />
<div class="outline-text-5" id="text-13-28-5-2">
<ul class="org-ul">
<li>B = {R1, R2, R3} - unknown distributions</li>
<li>μ<sub>1</sub> &#x2026; μ<sub>n</sub> - mean values associated with these reward distributions.</li>
<li>rt - reward at step t.</li>
<li>μ' -  the mean reward of the best arm</li>
<li>K ∈ N+ - levels</li>
<li>H - number of rounds left</li>
<li>ρ - regret after T(or n) rounds - some difference between max possible reward and collected.
<ul class="org-ul">
<li>ρ = n*μ' - ∑rt , at step t</li>
</ul></li>
<li>Δ - mean rewards of the top two arms</li>
</ul>

<p>
The gambler iteratively plays one lever per round and observes the associated <b>reward</b>.
</p>

<p>
<b>optimal strategy</b> - minimizes regret by effectively balancing exploration and exploitation.
</p>

<p>
<b>zero-regret strategy</b> - a strategy whose average regret per round ρ/T → 0 with probability 1, when T →
 ∞. (Always getting max reward). Guaranteed to converge to a (not necessarily unique) optimal strategy if
 enough rounds are played.
</p>

<p>
<b>Instance Gap</b> The complexity of stochastic MAB problems is often characterized by the gap between the mean
 rewards of the top two arms. Algorithms like Upper Confidence Bound (UCB) adapt to this gap, achieving
 optimal O(log⁡ n) regret in instances with large gaps and near-optimal O(√(n*log ⁡n) minimax regret when the
 gap is small.
</p>

<p>
<b>logarithmic regret</b> - rate at which the cumulative regret grows over time, specifically growing at a
 logarithmic rate.
</p>
<ul class="org-ul">
<li>O(log⁡ n) - considered optimal for well-separated instances where the gap between the mean rewards of the top
two arms Δ is significant.</li>
</ul>

<p>
<b>Gap-Dependent Regret</b> - in O regret formula there is Δ
</p>

<p>
<b>Safety-Constrained MAB</b> - with safety constraints (e.g., not playing unsafe arms).
</p>

<p>
<b>General Lower Bound</b> Ω(√(NT)) for any MAB algorithm.
</p>
</div>
</li>
<li><a id="orgcb4e8f1"></a>types:<br />
<div class="outline-text-5" id="text-13-28-5-3">
<p>
<b>restless bandit problem</b> - the states of non-played arms can also evolve over time
</p>

<p>
<b>stochastic MAB problems</b> - rewards from each arm are drawn from a fixed but unknown probability
 distribution. strategy doubly optimistic index-based strategies can maintain logarithmic regret bounds while
 adhering to safety constraints.
</p>

<p>
<b>sationary/non-sationary case</b> - the distributions of the rewards do not change or change in time.
</p>

<p>
<b>Finite/Infinite Time Horizon</b> -  algorithm operates for a fixed/notfixed number of rounds.
</p>

<p>
<b>Incentive Compatibility</b> type - ensure that the arms have an incentive to report their true values.
</p>

<p>
<b>Adversarial Multi-Armed Bandits</b> - rewards are not drawn from a probability distribution but are instead
 chosen by an adversary at each time step. Adversary can adaptively decide the payoff structure for each arm
 at each iteration. The algorithm must perform well against any possible sequence of rewards chosen by the
 adversary.
</p>

<p>
<b>Combinatorial bandit</b> (CMAB) - instead of a single discrete variable to choose from, an agent needs to choose
 values for a set of variables. Assuming each variable is discrete, the number of possible choices per
 iteration is exponential in the number of variables.
</p>

<p>
<b>Dueling bandit</b> - gambler is allowed to pull two levers at the same time, they only get a binary feedback
 telling which lever provided the best reward. The relative feedback of dueling bandits can also lead to
 voting paradoxes.
</p>
<ul class="org-ul">
<li>Solutions: Relative Upper Confidence Bounds (RUCB),
Relative EXponential weighing (REX3), Copeland Confidence Bounds (CCB), Relative Minimum Empirical
Divergence (RMED), and Double Thompson Sampling (DTS).</li>
<li>voting paradoxes: in different voting rules different candidates can win (select one candidat or select two).</li>
</ul>
</div>
</li>


<li><a id="orgc54389f"></a>implementations<br />
<div class="outline-text-5" id="text-13-28-5-4">
<p>
EXP3 - solve Adversarial problem
</p>

<p>
Thompson Sampling and UCB have asymptotic regret lower bound:
</p>
<pre class="example">
O(√(N*T*log(T)))
</pre>

<p>
, where N is the number of arms and T is the number of time steps.
</p>
</div>
<ol class="org-ol">
<li><a id="org1687db7"></a>Upper Confidence Bound (UCB)<br />
<div class="outline-text-6" id="text-13-28-5-4-1">
<p>
Deterministic algorithm for RL. based on assigning a confidence interval to each variant/way based on each
 iteration.
</p>

<p>
The UCB algorithm is known to be nearly optimal in terms of regret.
</p>

<p>
UCB steps:
</p>
<ol class="org-ol">
<li>Assume a starting confidence band for all bandits</li>
<li>Try each bandit a few times (atleast once) In the first KK rounds, each arm is played once to gather
initial reward data.</li>
<li>calculate “UCB index” for each arm. UCBi(t)=μi' + √(α*log(t)/Ni(t))
<ul class="org-ul">
<li>where i is arm index, t is step, μi' - estimated mean reward</li>
<li>Ni(t) - number of times arm i has been played up to time t.</li>
<li>α is a parameter that controls the trade-off between exploration and exploitation.</li>
<li>μi' - exploitative part; √(α*log(t)/Ni(t)) - exploratory part, adds a bonus to arms that have been played
less frequently</li>
</ul></li>
<li>select arm with highest UCB index. If there are ties, the algorithm can break them arbitrarily.</li>
</ol>
</div>
</li>

<li><a id="orgae28b2a"></a>Thompson Sampling<br />
<div class="outline-text-6" id="text-13-28-5-4-2">
<p>
Uses the Beta Distribution to intelligently decide a range of values where the actual success percentage of a
 variant lies. It is a probabilistic algorithm (i.e. it includes some randomness).
</p>

<p>
Uses Bayesian probabilistic approach.
</p>

<p>
At the start of exploration, the ranges for each variant are wide, as we go the range gets smaller.
</p>

<p>
The variance of choosed distribution decreases as more data is collected, which means that the algorithm
 becomes more confident in its estimates over time and tends to exploit the best arm more frequently.
</p>

<p>
Steps:
</p>
<ul class="org-ul">
<li>For each arm, a prior distribution is defined. A common choice for binary rewards is the Beta distribution,
while for continuous rewards, other distributions like the Gaussian can be used.
<ul class="org-ul">
<li>Beta distribution is often initialized with parameters a=1 and b=1, representing a uniform prior.</li>
<li>for each two arrays with 0 create - number winds and loses.</li>
</ul></li>
<li>At each round t, the algorithm samples a <b>random variable</b> from the posterior distribution of each arm.
<ul class="org-ul">
<li>For binary rewards, this involves sampling from a Beta distribution with parameters a and b, where a is
the number of successful rewards and bb is the number of unsuccessful rewards for that arm.</li>
</ul></li>
</ul>


<p>
Suppose we have two ads (arms) and we  initialize their Beta distributions:
</p>
<ul class="org-ul">
<li>Ad 1: Beta(1, 1)</li>
<li>Ad 2: Beta(1, 1)</li>
</ul>

<p>
After some rounds, let's say we have the following observations:
</p>
<ul class="org-ul">
<li>Ad 1: 5 wins, 3 losses</li>
<li>Ad 2: 4 wins, 2 losses</li>
</ul>

<p>
The parameters of the Beta distributions are updated accordingly:
</p>
<ul class="org-ul">
<li>Ad 1: Beta(6, 4) because α=1+5=6 and β=1+3=4. Beta(6, 4).</li>
<li>Ad 2: Beta(5, 3) because α=1+4=5 and β=1+2=3. Beta(5, 3).</li>
</ul>

<p>
Let's say the sampled values are: 0.8&gt;0.7 - Ad 2 selected.
</p>
<ul class="org-ul">
<li>θ1​=0.7</li>
<li>θ2​=0.8</li>
</ul>

<p>
Beta distribution is used because it is conjugate to the Bernoulli distribution, making it easy to update the
 parameters using Bayes' rule.
</p>
</div>
</li>
<li><a id="org432caa7"></a><span class="todo TODO">TODO</span> Epsilon Greedy<br />
<div class="outline-text-6" id="text-13-28-5-4-3">
<p>
stochastic
</p>
</div>
</li>
<li><a id="org498fe18"></a>choosing<br />
<div class="outline-text-6" id="text-13-28-5-4-4">
<p>
UCB vs Thompson Sampling:
</p>
<ul class="org-ul">
<li>UCB also emphasizes exploration a lot more than TS: if best variant could very easily change over time, you
may want to use UCB.</li>
<li>Thompson Sampling updates can be delayed and it will still perform well: TS is easier to use in the real world</li>
</ul>

<p>
UCB pros:
</p>
<ul class="org-ul">
<li>Optimism in the Face of Uncertainty</li>
<li>Theoretical Guarantees:</li>
<li>reduces uncertainty overt time.</li>
</ul>
<p>
cons:
</p>
<ul class="org-ul">
<li>there are scenarios where it might not outperform other algorithms, such as when the number of arms is very
large or when the reward distributions are highly skewed.</li>
</ul>

<p>
Thompson Sampling pros:
</p>
<ul class="org-ul">
<li>useful when you have prior knowledge about the reward distributions.</li>
<li>performs well not just asymptotically but also in finite time</li>
</ul>

<p>
cons:
</p>
<ul class="org-ul">
<li>Computational Overhead</li>
<li>bad Initial Performance</li>
</ul>

<p>
Epsilon-Greedy Algorithm pros
</p>
<ul class="org-ul">
<li>simple, simple and fixed way to balance exploration and exploitation</li>
</ul>
<p>
cons:
</p>
<ul class="org-ul">
<li>does not take into account the uncertainty associated with each arm (in contrast with  UCB)</li>
</ul>

<p>
Thompson Sampling - good for long range for different scenaries
Epsilon-Greedy - simple with fixed tradoff.
Upper Confidence Bound  - when you need simple, fast and low range and theoretical guarantees on performance.
</p>
</div>
</li>
</ol>
</li>
<li><a id="org33e10aa"></a>Bandit strategies/policies<br />
<ol class="org-ol">
<li><a id="orgd94c898"></a>use cases<br />
<div class="outline-text-6" id="text-13-28-5-5-1">
<ul class="org-ul">
<li>Headlines and Short-Term Campaigns</li>
<li>Long-Term Dynamic Changes</li>
<li>Targeting: Used for targeting different types of users more effectively by learning targeting rules over time.</li>
</ul>

<p>
Theoretical Guarantees: The presence of mathematical guarantees on performance, such as regret bounds.
</p>

<p>
Contextual Information: Whether the algorithm considers additional context or features.
</p>
</div>
</li>
<li><a id="org664a33a"></a>tree from AI<br />
<div class="outline-text-6" id="text-13-28-5-5-2">
<ul class="org-ul">
<li>Optimal Solutions - strong theoretical guarantees, such as regret bounds. meaning it achieves the optimal
regret rate as the number of time steps (or rounds) increases.
<ul class="org-ul">
<li>Upper Confidence Bound (UCB) Algorithms
<ul class="org-ul">
<li>UCB1, UCB-Tuned</li>
</ul></li>
<li>Thompson Sampling
<ul class="org-ul">
<li>Bayesian approach with regret bounds</li>
</ul></li>
<li>Other Optimal Algorithms
<ul class="org-ul">
<li>Algorithms with proven regret bounds</li>
</ul></li>
</ul></li>

<li>Approximate Solutions - various heuristics and practical strategies that may not have optimal theoretical
guarantees but are effective in practice.
<ul class="org-ul">
<li>Semi-uniform Strategies
<ul class="org-ul">
<li>Epsilon-Greedy Algorithm</li>
<li>Epsilon-First Algorithm</li>
</ul></li>
<li>Probability Matching Strategies
<ul class="org-ul">
<li>Boltzmann Exploration</li>
<li>Pursuit Algorithms</li>
</ul></li>
<li>Pricing Strategies
<ul class="org-ul">
<li>Strategic Multi-Armed Bandits (when incentives are involved)</li>
</ul></li>
</ul></li>

<li>Real-World Applications and Variants - biased to special requirement
<ul class="org-ul">
<li>Dynamic Allocation
<ul class="org-ul">
<li>Real-time optimization of resources - allocate more traffic to the best-performing arms</li>
</ul></li>
<li>Continuous Optimization
<ul class="org-ul">
<li>Ongoing adaptation to changing conditions</li>
</ul></li>
<li>Specialized Use Cases
<ul class="org-ul">
<li>Headlines and Short-Term Campaigns</li>
<li>Long-Term Dynamic Changes</li>
<li>Targeting different types of users more effectively.</li>
</ul></li>
<li>Infinite-armed bandit - “arms" are a continuous variable.</li>
<li>Non-stationary bandit (in presence of <b>concept drift</b>) <a href="https://arxiv.org/abs/0805.3415">https://arxiv.org/abs/0805.3415</a></li>
<li>Contextual bandit</li>
<li>Adversarial bandit</li>
</ul></li>

<li>Contextual and Advanced Strategies
<ul class="org-ul">
<li>Contextual Bandit Algorithms
<ul class="org-ul">
<li>Contextual UCB (LinUCB)</li>
<li>Linear Bandits, Generalized Linear Bandits</li>
</ul></li>
<li>Personalized Multi-Armed Bandits - User Embeddings, Collaborative Filtering</li>
<li>Hybrid Strategies - Combining Multiple Algorithms (e.g., Contextual Bandits with Thompson Sampling or Epsilon-Greedy)</li>
<li>Hierarchical Multi-Armed Bandits
<ul class="org-ul">
<li>Decision-Tree based approaches (e.g., DT-TMP)</li>
</ul></li>
<li>Hybrid Strategies
<ul class="org-ul">
<li>Combining multiple basic strategies (e.g., combining UCB with Thompson Sampling)</li>
</ul></li>
</ul></li>
</ul>
</div>
</li>
</ol>
</li>
</ol>
</div>

<div id="outline-container-org0e736fe" class="outline-4">
<h4 id="org0e736fe"><span class="section-number-4">13.28.6.</span> links</h4>
<div class="outline-text-4" id="text-13-28-6">
<ul class="org-ul">
<li>RL : PPO course <a href="https://huggingface.co/learn/deep-rl-course/unit0/introduction">https://huggingface.co/learn/deep-rl-course/unit0/introduction</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org2715230" class="outline-3">
<h3 id="org2715230"><span class="section-number-3">13.29.</span> Language Server</h3>
<div class="outline-text-3" id="text-13-29">
<p>
Usually, the parser builds a concrete syntax tree (CST) before turning it into an abstract syntax tree (AST).
</p>
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Parse_tree">https://en.wikipedia.org/wiki/Parse_tree</a></li>
<li><a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">https://en.wikipedia.org/wiki/Abstract_syntax_tree</a></li>
</ul>


<p>
AST - data structure used in computer science to represent the structure of a program or code snippet
</p>
<ul class="org-ul">
<li>allow clone detection</li>
<li><p>
an edit action may result in the addition of a new AST node representing a function.
</p>

<p>
For example, take a simple expression 2 * (7 + 3):
</p></li>
</ul>
<div class="org-src-container">
<pre class="src src-text">           CST                    AST
          -----                  -----
          expr                     *
       /   |    \                /   \
  term     *   term             2     +
   |             |                   / \
factor         factor               7   3
   |         /   |    \
   2        (   expr   )
              /  |  \
          term   +  term
            |        |
          factor   factor
            |        |
            7        3
</pre>
</div>

<p>
<a href="https://supabase.com/blog/postgres-language-server-implementing-parser">https://supabase.com/blog/postgres-language-server-implementing-parser</a>
</p>
</div>
</div>
<div id="outline-container-org6c018bf" class="outline-3">
<h3 id="org6c018bf"><span class="section-number-3">13.30.</span> word2vec - Skip-gram and CBOW</h3>
<div class="outline-text-3" id="text-13-30">
<p>
Embeddings, NN-based, semantic relationships, two archs:
</p>
<ul class="org-ul">
<li>CBOW (Continuous Bag of Words) - capture meaning based on context</li>
<li>Skip-gram - predict context for word</li>
</ul>


<p>
Why?
</p>
<ul class="org-ul">
<li>relatively simpler.</li>
<li>don't require big datasets.</li>
</ul>

<p>
May be used in downstream natural language processing tasks: sentiment analysis, named entity recognition, and
 part-of-speech tagging.
</p>
</div>
</div>
<div id="outline-container-org530127e" class="outline-3">
<h3 id="org530127e"><span class="section-number-3">13.31.</span> GPT</h3>
<div class="outline-text-3" id="text-13-31">
<ul class="org-ul">
<li><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a></li>
<li><a href="https://github.com/openai/finetune-transformer-lm">https://github.com/openai/finetune-transformer-lm</a></li>
</ul>

<p>
steps:
</p>
<ol class="org-ol">
<li>first we train a transformer model on a very large amount of data in an unsupervised manner—using language
modeling as a training signal</li>
<li>we fine-tune this model on much smaller supervised datasets to help it solve specific tasks.</li>
</ol>
</div>
</div>
<div id="outline-container-org350a458" class="outline-3">
<h3 id="org350a458"><span class="section-number-3">13.32.</span> Text embeddings - neural retrival task</h3>
<div class="outline-text-3" id="text-13-32">
<p>
model BAAI/bge-reranker-base
</p>
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/2402.03216">https://arxiv.org/pdf/2402.03216</a></li>
<li><a href="https://huggingface.co/BAAI/bge-reranker-base">https://huggingface.co/BAAI/bge-reranker-base</a></li>
<li><a href="https://huggingface.co/BAAI/bge-small-en-v1.5/tree/main">https://huggingface.co/BAAI/bge-small-en-v1.5/tree/main</a></li>
</ul>

<p>
Modern approach: <b>dense</b> retrival -&gt; embedding similarity
</p>
</div>
<div id="outline-container-org3259068" class="outline-4">
<h4 id="org3259068"><span class="section-number-4">13.32.1.</span> history</h4>
<div class="outline-text-4" id="text-13-32-1">
<ul class="org-ul">
<li>2019 pretrained language models - semantic of data</li>
<li>2020 improvement of negative sampling with constrasting learning</li>
<li>2021 explatation of knowledge distillation</li>
</ul>
</div>
</div>

<div id="outline-container-orge1ac411" class="outline-4">
<h4 id="orge1ac411"><span class="section-number-4">13.32.2.</span> terms</h4>
<div class="outline-text-4" id="text-13-32-2">
<ul class="org-ul">
<li><b>Dense Retrieval (DR)</b> - encode to dense vector representations</li>
<li><p>
<b>multi-vector</b> retrival - just several vectors from one document (per token or passage segment). fine
grained interactions after output.
</p>
<ul class="org-ul">
<li>Improving the retrieval process by reducing the impact of dimensionality reduction and increasing the</li>
</ul>
<p>
robustness to noisy or irrelevant data.
</p></li>
<li>term weights - for sparse or <b>lexical retrival</b> - keyword relevance is crucial, and computational efficiency
is a consideration</li>
<li><b>Lexical</b> (Sparse) Retrieval (SR) <b>sparse embeddings</b> - count the frequency of words or keywords:
Bag-of-Words (BoW), TF-IDF, or BM25. modern: SPLADE.</li>
<li><b>dense embeddings</b> (like LLMs)</li>
<li>inverted indexes - [word] -&gt; [document1, document2, &#x2026;] - pros: fast query, Efficient Storage; cons: update slow</li>
<li>forward index - [document] -&gt; [word1, word2, word3 &#x2026;] - better for content management systems and document editing software, fast update; cons: slow search, memory hungry</li>
</ul>

<p>
see <a href="#org2698079">14.32</a>
</p>
</div>
</div>

<div id="outline-container-org394ae46" class="outline-4">
<h4 id="org394ae46"><span class="section-number-4">13.32.3.</span> banchmarks</h4>
<div class="outline-text-4" id="text-13-32-3">
<ul class="org-ul">
<li>BEIR benchmark <a href="https://github.com/beir-cellar/beir/wiki/Leaderboard">https://github.com/beir-cellar/beir/wiki/Leaderboard</a></li>
<li>(MTEB) Massive Text Embedding Benchmark  Leaderboard. <a href="https://huggingface.co/spaces/mteb/leaderboard">https://huggingface.co/spaces/mteb/leaderboard</a></li>
</ul>
</div>
</div>
<div id="outline-container-org6bc6be4" class="outline-4">
<h4 id="org6bc6be4"><span class="section-number-4">13.32.4.</span> text encoders</h4>
<div class="outline-text-4" id="text-13-32-4">
<ul class="org-ul">
<li>UL2 (Unifying Language Learning) “google/ul2” Text2Text Generation, Mixture-of-Denoisers (MoD) -
disentangling architectural archetypes with pre-training objectives. <a href="https://arxiv.org/pdf/2205.05131v1">https://arxiv.org/pdf/2205.05131v1</a></li>
<li>Clip-text (Contrastive Language-Image Pre-training) - model focuses on learning the relationship between text and images.</li>
<li>BT5 - "google/byt5-base", T5 architecture,  processing text at the byte level on raw UTF-8 bytes. No Tokenizer Needed.</li>
</ul>
</div>
</div>

<div id="outline-container-orgda31c86" class="outline-4">
<h4 id="orgda31c86"><span class="section-number-4">13.32.5.</span> SLADE</h4>
<div class="outline-text-4" id="text-13-32-5">
<ul class="org-ul">
<li>output sparse vector embeddings. (more efficient and interpretable than dense)</li>
<li>Based on BERT.</li>
<li>slower retrieval speed</li>
</ul>

<p>
<a href="https://github.com/naver/splade">https://github.com/naver/splade</a>
</p>
</div>
</div>
</div>

<div id="outline-container-orga12f3e2" class="outline-3">
<h3 id="orga12f3e2"><span class="section-number-3">13.33.</span> Text to speach</h3>
<div class="outline-text-3" id="text-13-33">
</div>
<div id="outline-container-org29b1703" class="outline-4">
<h4 id="org29b1703"><span class="section-number-4">13.33.1.</span> Yandex Alice - news</h4>
<div class="outline-text-4" id="text-13-33-1">
<div class="org-src-container">
<pre class="src src-text">&#1040;&#1083;&#1080;&#1089;&#1072; &#1085;&#1072; &#1082;&#1072;&#1079;&#1072;&#1093;&#1089;&#1082;&#1086;&#1084; &#1103;&#1079;&#1099;&#1082;&#1077;!

&#1053;&#1077;&#1076;&#1072;&#1074;&#1085;&#1086; &#1040;&#1083;&#1080;&#1089;&#1072; &#1079;&#1072;&#1075;&#1086;&#1074;&#1086;&#1088;&#1080;&#1083;&#1072; &#1085;&#1072; &#1082;&#1072;&#1079;&#1072;&#1093;&#1089;&#1082;&#1086;&#1084;. &#1069;&#1090;&#1086; &#1073;&#1086;&#1083;&#1100;&#1096;&#1086;&#1077; &#1089;&#1086;&#1073;&#1099;&#1090;&#1080;&#1077; &#1076;&#1083;&#1103; &#1085;&#1072;&#1089;,
 &#1087;&#1086;&#1101;&#1090;&#1086;&#1084;&#1091; &#1089;&#1077;&#1075;&#1086;&#1076;&#1085;&#1103; &#1088;&#1072;&#1089;&#1089;&#1082;&#1072;&#1078;&#1077;&#1084;, &#1082;&#1072;&#1082; &#1084;&#1099; &#1086;&#1073;&#1091;&#1095;&#1072;&#1083;&#1080; &#1040;&#1083;&#1080;&#1089;&#1091;, &#1089; &#1082;&#1072;&#1082;&#1080;&#1084;&#1080; &#1090;&#1088;&#1091;&#1076;&#1085;&#1086;&#1089;&#1090;&#1103;&#1084;&#1080;
 &#1089;&#1090;&#1086;&#1083;&#1082;&#1085;&#1091;&#1083;&#1080;&#1089;&#1100; &#1080; &#1082;&#1072;&#1082; &#1091;&#1089;&#1090;&#1088;&#1086;&#1077;&#1085; &#1089;&#1080;&#1085;&#1090;&#1077;&#1079; &#1088;&#1077;&#1095;&#1080; &#1091; &#1087;&#1086;&#1084;&#1086;&#1097;&#1085;&#1080;&#1082;&#1072;.

&#1044;&#1083;&#1103; &#1086;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1103; &#1080;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1086;&#1074;&#1072;&#1083;&#1080; &#1076;&#1072;&#1090;&#1072;&#1089;&#1077;&#1090; &#1080;&#1079; &#1079;&#1072;&#1087;&#1080;&#1089;&#1072;&#1085;&#1085;&#1099;&#1093; &#1072;&#1082;&#1090;&#1088;&#1080;&#1089;&#1086;&#1081; &#1089;&#1083;&#1086;&#1074; &#1080;
 &#1074;&#1099;&#1088;&#1072;&#1078;&#1077;&#1085;&#1080;&#1081; &#8212; &#1074;&#1089;&#1077;&#1075;&#1086; &#1086;&#1082;&#1086;&#1083;&#1086; 25 &#1095;&#1072;&#1089;&#1086;&#1074; &#1072;&#1091;&#1076;&#1080;&#1086;. &#1055;&#1086; &#1089;&#1091;&#1090;&#1080;, &#1090;&#1086;, &#1082;&#1072;&#1082; &#1040;&#1083;&#1080;&#1089;&#1072;
 &#1075;&#1086;&#1074;&#1086;&#1088;&#1080;&#1090; &#1085;&#1072; &#1088;&#1091;&#1089;&#1089;&#1082;&#1086;&#1084;, &#1084;&#1099; &#1074;&#1086;&#1089;&#1087;&#1088;&#1086;&#1080;&#1079;&#1074;&#1077;&#1083;&#1080; &#1085;&#1072; &#1082;&#1072;&#1079;&#1072;&#1093;&#1089;&#1082;&#1086;&#1084; &#8212; &#1090;&#1086; &#1077;&#1089;&#1090;&#1100;
 &#1072;&#1088;&#1093;&#1080;&#1090;&#1077;&#1082;&#1090;&#1091;&#1088;&#1072; &#1089;&#1080;&#1085;&#1090;&#1077;&#1079;&#1072; &#1103;&#1079;&#1099;&#1082;&#1072; &#1091; &#1087;&#1086;&#1084;&#1086;&#1097;&#1085;&#1080;&#1082;&#1086;&#1074; &#1089;&#1093;&#1086;&#1078;&#1072;&#1103;. &#1042;&#1087;&#1088;&#1086;&#1095;&#1077;&#1084;, &#1077;&#1089;&#1090;&#1100; &#1085;&#1102;&#1072;&#1085;&#1089;&#1099;,
 &#1089;&#1074;&#1103;&#1079;&#1072;&#1085;&#1085;&#1099;&#1077; &#1089; G2P &#8212; &#1072;&#1074;&#1090;&#1086;&#1084;&#1072;&#1090;&#1080;&#1095;&#1077;&#1089;&#1082;&#1086;&#1081; &#1090;&#1088;&#1072;&#1085;&#1089;&#1082;&#1088;&#1080;&#1073;&#1072;&#1094;&#1080;&#1077;&#1081; &#1073;&#1091;&#1082;&#1074; &#1074; &#1092;&#1086;&#1085;&#1077;&#1084;&#1099;. &#1054;&#1085;&#1072;
 &#1085;&#1091;&#1078;&#1085;&#1072;, &#1095;&#1090;&#1086;&#1073;&#1099; &#1040;&#1083;&#1080;&#1089;&#1072; &#1087;&#1088;&#1086;&#1080;&#1079;&#1085;&#1086;&#1089;&#1080;&#1083;&#1072; &#1089;&#1083;&#1086;&#1074;&#1072; &#1087;&#1088;&#1072;&#1074;&#1080;&#1083;&#1100;&#1085;&#1086;.

&#1056;&#1072;&#1073;&#1086;&#1090;&#1072; &#1089; &#1079;&#1072;&#1080;&#1084;&#1089;&#1090;&#1074;&#1086;&#1074;&#1072;&#1085;&#1080;&#1103;&#1084;&#1080;

&#1042; &#1082;&#1072;&#1079;&#1072;&#1093;&#1089;&#1082;&#1086;&#1084;, &#1082;&#1072;&#1082; &#1080; &#1074; &#1083;&#1102;&#1073;&#1086;&#1084; &#1076;&#1088;&#1091;&#1075;&#1086;&#1084; &#1103;&#1079;&#1099;&#1082;&#1077;, &#1077;&#1089;&#1090;&#1100; &#1079;&#1072;&#1080;&#1084;&#1089;&#1090;&#1074;&#1086;&#1074;&#1072;&#1085;&#1085;&#1099;&#1077; &#1089;&#1083;&#1086;&#1074;&#1072; &#8212; &#1074;
 &#1095;&#1072;&#1089;&#1090;&#1085;&#1086;&#1089;&#1090;&#1080;, &#1080;&#1079; &#1072;&#1088;&#1072;&#1073;&#1089;&#1082;&#1086;&#1075;&#1086;, &#1092;&#1072;&#1088;&#1089;&#1080;, &#1072;&#1085;&#1075;&#1083;&#1080;&#1081;&#1089;&#1082;&#1086;&#1075;&#1086;, &#1088;&#1091;&#1089;&#1089;&#1082;&#1086;&#1075;&#1086; &#1080; &#1080;&#1085;&#1099;&#1093;
 &#1103;&#1079;&#1099;&#1082;&#1086;&#1074;. &#1063;&#1072;&#1089;&#1090;&#1086; &#1090;&#1072;&#1082;&#1080;&#1077; &#1089;&#1083;&#1086;&#1074;&#1072; &#1087;&#1088;&#1086;&#1080;&#1079;&#1085;&#1086;&#1089;&#1103;&#1090;&#1089;&#1103; &#1089; &#1080;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1086;&#1074;&#1072;&#1085;&#1080;&#1077;&#1084; &#1079;&#1074;&#1091;&#1082;&#1086;&#1074;,
 &#1093;&#1072;&#1088;&#1072;&#1082;&#1090;&#1077;&#1088;&#1085;&#1099;&#1093; &#1076;&#1083;&#1103; &#171;&#1080;&#1089;&#1093;&#1086;&#1076;&#1085;&#1099;&#1093;&#187; &#1103;&#1079;&#1099;&#1082;&#1086;&#1074;. &#1053;&#1072;&#1087;&#1088;&#1080;&#1084;&#1077;&#1088;, &#1089;&#1083;&#1086;&#1074;&#1086; &#171;&#1092;&#1072;&#1082;&#1091;&#1083;&#1100;&#1090;&#1077;&#1090;&#187; &#1079;&#1074;&#1091;&#1095;&#1080;&#1090;
 &#1090;&#1072;&#1082; &#1078;&#1077;, &#1082;&#1072;&#1082; &#1074; &#1088;&#1091;&#1089;&#1089;&#1082;&#1086;&#1084;, &#1085;&#1086; &#1074; &#1092;&#1086;&#1085;&#1077;&#1090;&#1080;&#1095;&#1077;&#1089;&#1082;&#1086;&#1081; &#1089;&#1080;&#1089;&#1090;&#1077;&#1084;&#1077; &#1082;&#1072;&#1079;&#1072;&#1093;&#1089;&#1082;&#1086;&#1075;&#1086; &#1103;&#1079;&#1099;&#1082;&#1072; &#1085;&#1077;&#1090;
 &#1087;&#1088;&#1103;&#1084;&#1099;&#1093; &#1089;&#1086;&#1086;&#1090;&#1074;&#1077;&#1090;&#1089;&#1090;&#1074;&#1080;&#1081; &#1084;&#1085;&#1086;&#1075;&#1080;&#1084; &#1088;&#1091;&#1089;&#1089;&#1082;&#1080;&#1084; &#1079;&#1074;&#1091;&#1082;&#1072;&#1084;. &#1055;&#1086;&#1101;&#1090;&#1086;&#1084;&#1091; &#1084;&#1099; &#1076;&#1086;&#1087;&#1086;&#1083;&#1085;&#1080;&#1083;&#1080;
 &#1092;&#1086;&#1085;&#1077;&#1084;&#1085;&#1099;&#1081; &#1089;&#1083;&#1086;&#1074;&#1072;&#1088;&#1100; &#1079;&#1074;&#1091;&#1082;&#1072;&#1084;&#1080; &#1088;&#1091;&#1089;&#1089;&#1082;&#1086;&#1075;&#1086; &#1103;&#1079;&#1099;&#1082;&#1072;.

&#1040;&#1075;&#1075;&#1083;&#1102;&#1090;&#1080;&#1085;&#1072;&#1094;&#1080;&#1103;

&#1042;&#1072;&#1078;&#1085;&#1072;&#1103; &#1086;&#1089;&#1086;&#1073;&#1077;&#1085;&#1085;&#1086;&#1089;&#1090;&#1100; &#1082;&#1072;&#1079;&#1072;&#1093;&#1089;&#1082;&#1086;&#1075;&#1086; &#1080; &#1085;&#1077;&#1082;&#1086;&#1090;&#1086;&#1088;&#1099;&#1093; &#1076;&#1088;&#1091;&#1075;&#1080;&#1093; &#1103;&#1079;&#1099;&#1082;&#1086;&#1074; &#8212; &#1072;&#1075;&#1075;&#1083;&#1102;&#1090;&#1080;&#1085;&#1072;&#1094;&#1080;&#1103;
 &#8212; &#1090;&#1080;&#1087; &#1089;&#1083;&#1086;&#1074;&#1086;&#1080;&#1079;&#1084;&#1077;&#1085;&#1077;&#1085;&#1080;&#1103; &#1089; &#1087;&#1086;&#1084;&#1086;&#1097;&#1100;&#1102; &#1087;&#1086;&#1089;&#1083;&#1077;&#1076;&#1086;&#1074;&#1072;&#1090;&#1077;&#1083;&#1100;&#1085;&#1086;&#1075;&#1086; &#1087;&#1088;&#1080;&#1089;&#1086;&#1077;&#1076;&#1080;&#1085;&#1077;&#1085;&#1080;&#1103; &#1082;
 &#1085;&#1077;&#1080;&#1079;&#1084;&#1077;&#1085;&#1103;&#1077;&#1084;&#1099;&#1084; &#1082;&#1086;&#1088;&#1085;&#1102; &#1080;&#1083;&#1080; &#1086;&#1089;&#1085;&#1086;&#1074;&#1077; &#1075;&#1088;&#1072;&#1084;&#1084;&#1072;&#1090;&#1080;&#1095;&#1077;&#1089;&#1082;&#1080; &#1086;&#1076;&#1085;&#1086;&#1079;&#1085;&#1072;&#1095;&#1085;&#1099;&#1093;
 &#1072;&#1092;&#1092;&#1080;&#1082;&#1089;&#1086;&#1074;. &#1050;&#1083;&#1072;&#1089;&#1089;&#1080;&#1095;&#1077;&#1089;&#1082;&#1080;&#1081; &#1087;&#1088;&#1080;&#1084;&#1077;&#1088;: &#1092;&#1088;&#1072;&#1079;&#1072; &#171;&#1086;&#1090; &#1085;&#1072;&#1096;&#1080;&#1093; &#1087;&#1080;&#1089;&#1072;&#1090;&#1077;&#1083;&#1077;&#1081;&#187; &#1085;&#1072;
 &#1082;&#1072;&#1079;&#1072;&#1093;&#1089;&#1082;&#1086;&#1084; &#8212; &#171;&#1078;&#1072;&#1079;&#1091;&#1096;&#1099;&#1083;&#1072;&#1088;&#1099;&#1084;&#1099;&#1079;&#1076;&#1072;&#1085;&#187;. &#171;&#1046;&#1072;&#1079;&#1091;&#1096;&#1099;&#187; &#1079;&#1076;&#1077;&#1089;&#1100; &#8212; &#171;&#1087;&#1080;&#1089;&#1072;&#1090;&#1077;&#1083;&#1100;&#187;, &#171;&#1083;&#1072;&#1088;&#187; &#8212;
 &#1092;&#1086;&#1088;&#1084;&#1072;&#1085;&#1090; &#1084;&#1085;&#1086;&#1078;&#1077;&#1089;&#1090;&#1074;&#1077;&#1085;&#1085;&#1086;&#1075;&#1086; &#1095;&#1080;&#1089;&#1083;&#1072;, &#171;&#1099;&#1084;&#1099;&#1079;&#187; &#8212; &#171;&#1085;&#1072;&#1096;&#187; &#1080; &#1090;&#1072;&#1082; &#1076;&#1072;&#1083;&#1077;&#1077;.

&#1040;&#1075;&#1075;&#1083;&#1102;&#1090;&#1080;&#1085;&#1072;&#1094;&#1080;&#1103; &#1088;&#1072;&#1089;&#1087;&#1088;&#1086;&#1089;&#1090;&#1088;&#1072;&#1085;&#1103;&#1077;&#1090;&#1089;&#1103; &#1085;&#1077; &#1090;&#1086;&#1083;&#1100;&#1082;&#1086; &#1085;&#1072; &#1082;&#1072;&#1079;&#1072;&#1093;&#1089;&#1082;&#1080;&#1077; &#1089;&#1083;&#1086;&#1074;&#1072;, &#1085;&#1086; &#1080; &#1085;&#1072;
 &#1079;&#1072;&#1080;&#1084;&#1089;&#1090;&#1074;&#1086;&#1074;&#1072;&#1085;&#1080;&#1103;. &#1053;&#1072;&#1087;&#1088;&#1080;&#1084;&#1077;&#1088;, &#171;&#1082;&#1086;&#1084;&#1087;&#1100;&#1102;&#1090;&#1077;&#1088;&#1083;&#1077;&#1088;&#1110;&#1187;&#1110;&#1079;&#1076;&#1077;&#187; &#1079;&#1085;&#1072;&#1095;&#1080;&#1090; &#171;&#1085;&#1072; &#1074;&#1072;&#1096;&#1080;&#1093;
 &#1082;&#1086;&#1084;&#1087;&#1100;&#1102;&#1090;&#1077;&#1088;&#1072;&#1093;&#187;. &#1052;&#1099; &#1080;&#1089;&#1082;&#1072;&#1083;&#1080; &#1074; &#1089;&#1083;&#1086;&#1074;&#1072;&#1093; &#1080;&#1085;&#1090;&#1077;&#1088;&#1085;&#1072;&#1094;&#1080;&#1086;&#1085;&#1072;&#1083;&#1100;&#1085;&#1099;&#1077; &#1082;&#1086;&#1088;&#1085;&#1080; &#1080; &#1087;&#1099;&#1090;&#1072;&#1083;&#1080;&#1089;&#1100;
 &#1086;&#1090;&#1076;&#1077;&#1083;&#1080;&#1090;&#1100; &#1080;&#1093; &#1086;&#1090; &#1080;&#1089;&#1082;&#1086;&#1085;&#1085;&#1086; &#1082;&#1072;&#1079;&#1072;&#1093;&#1089;&#1082;&#1080;&#1093; &#1072;&#1092;&#1092;&#1080;&#1082;&#1089;&#1086;&#1074;, &#1087;&#1086;&#1090;&#1086;&#1084;&#1091; &#1095;&#1090;&#1086; &#1086;&#1085;&#1080;
 &#1087;&#1088;&#1086;&#1080;&#1079;&#1085;&#1086;&#1089;&#1103;&#1090;&#1089;&#1103; &#1087;&#1086; &#1088;&#1072;&#1079;&#1085;&#1099;&#1084; &#1087;&#1088;&#1072;&#1074;&#1080;&#1083;&#1072;&#1084;. &#1045;&#1089;&#1083;&#1080; &#1079;&#1072;&#1080;&#1084;&#1089;&#1090;&#1074;&#1086;&#1074;&#1072;&#1085;&#1085;&#1099;&#1077; &#1082;&#1086;&#1088;&#1085;&#1080; &#1073;&#1099;&#1083;&#1080;, &#1090;&#1086;
 &#1080;&#1093; &#1090;&#1088;&#1072;&#1085;&#1089;&#1082;&#1088;&#1080;&#1087;&#1094;&#1080;&#1103; &#1079;&#1072;&#1087;&#1080;&#1089;&#1099;&#1074;&#1072;&#1083;&#1072;&#1089;&#1100; &#1088;&#1091;&#1089;&#1089;&#1082;&#1080;&#1084;&#1080; &#1092;&#1086;&#1085;&#1077;&#1084;&#1072;&#1084;&#1080;, &#1072; &#1090;&#1088;&#1072;&#1085;&#1089;&#1082;&#1088;&#1080;&#1087;&#1094;&#1080;&#1103;
 &#1082;&#1072;&#1079;&#1072;&#1093;&#1089;&#1082;&#1080;&#1093; &#1089;&#1091;&#1092;&#1092;&#1080;&#1082;&#1089;&#1086;&#1074; &#8212; &#1082;&#1072;&#1079;&#1072;&#1093;&#1089;&#1082;&#1080;&#1084;&#1080;.

&#1053;&#1086;&#1088;&#1084;&#1072;&#1083;&#1080;&#1079;&#1072;&#1094;&#1080;&#1103;

&#1045;&#1097;&#1105; &#1086;&#1076;&#1080;&#1085; &#1101;&#1090;&#1072;&#1087; &#1085;&#1072; &#1087;&#1091;&#1090;&#1080; &#1082; &#1075;&#1086;&#1083;&#1086;&#1089;&#1086;&#1074;&#1086;&#1084;&#1091; &#1086;&#1090;&#1074;&#1077;&#1090;&#1091; &#8212; &#1085;&#1086;&#1088;&#1084;&#1072;&#1083;&#1080;&#1079;&#1072;&#1094;&#1080;&#1103; &#1090;&#1077;&#1082;&#1089;&#1090;&#1072;, &#1095;&#1090;&#1086;
 &#1086;&#1089;&#1086;&#1073;&#1077;&#1085;&#1085;&#1086; &#1074;&#1072;&#1078;&#1085;&#1086; &#1076;&#1083;&#1103; &#1082;&#1086;&#1088;&#1088;&#1077;&#1082;&#1090;&#1085;&#1086;&#1075;&#1086; &#1087;&#1088;&#1086;&#1080;&#1079;&#1085;&#1086;&#1096;&#1077;&#1085;&#1080;&#1103; &#1095;&#1080;&#1089;&#1083;&#1080;&#1090;&#1077;&#1083;&#1100;&#1085;&#1099;&#1093;. &#1040;&#1083;&#1080;&#1089;&#1072;
 &#1076;&#1086;&#1083;&#1078;&#1085;&#1072; &#1087;&#1086;&#1085;&#1080;&#1084;&#1072;&#1090;&#1100;, &#1095;&#1090;&#1086; &#1087;&#1077;&#1088;&#1077;&#1076; &#1085;&#1077;&#1081; &#1074;&#1088;&#1077;&#1084;&#1103; &#1080;&#1083;&#1080; &#1085;&#1086;&#1084;&#1077;&#1088; &#1076;&#1086;&#1084;&#1072; &#1080; &#1087;&#1088;&#1086;&#1080;&#1079;&#1085;&#1086;&#1089;&#1080;&#1090;&#1100;
 &#1094;&#1080;&#1092;&#1088;&#1099; &#1087;&#1088;&#1072;&#1074;&#1080;&#1083;&#1100;&#1085;&#1086; &#1074; &#1089;&#1086;&#1086;&#1090;&#1074;&#1077;&#1090;&#1089;&#1090;&#1074;&#1080;&#1080; &#1089; &#1082;&#1086;&#1085;&#1090;&#1077;&#1082;&#1089;&#1090;&#1086;&#1084;. &#1063;&#1090;&#1086;&#1073;&#1099; &#1076;&#1086;&#1089;&#1090;&#1080;&#1095;&#1100; &#1101;&#1090;&#1086;&#1075;&#1086;, &#1084;&#1099;
 &#1073;&#1088;&#1072;&#1083;&#1080; &#1090;&#1077;&#1082;&#1089;&#1090;&#1099; &#1089; &#1095;&#1080;&#1089;&#1083;&#1072;&#1084;&#1080;, &#1079;&#1072;&#1087;&#1080;&#1089;&#1072;&#1085;&#1085;&#1099;&#1084;&#1080; &#1089;&#1083;&#1086;&#1074;&#1072;&#1084;&#1080;, &#1087;&#1077;&#1088;&#1077;&#1074;&#1086;&#1076;&#1080;&#1083;&#1080; &#1080;&#1093; &#1074; &#1094;&#1080;&#1092;&#1088;&#1099;, &#1080;
 &#1091;&#1095;&#1080;&#1083;&#1080; &#1090;&#1088;&#1072;&#1085;&#1089;&#1092;&#1086;&#1088;&#1084;&#1077;&#1088; &#1087;&#1088;&#1077;&#1086;&#1073;&#1088;&#1072;&#1079;&#1086;&#1074;&#1099;&#1074;&#1072;&#1090;&#1100; &#1080;&#1093; &#1086;&#1073;&#1088;&#1072;&#1090;&#1085;&#1086; &#1074; &#1089;&#1083;&#1086;&#1074;&#1072;.

&#1050;&#1072;&#1082; &#1090;&#1077;&#1082;&#1089;&#1090; &#1087;&#1088;&#1077;&#1074;&#1088;&#1072;&#1097;&#1072;&#1077;&#1090;&#1089;&#1103; &#1074; &#1088;&#1077;&#1095;&#1100;

&#1050;&#1086;&#1075;&#1076;&#1072; &#1087;&#1088;&#1077;&#1076;&#1074;&#1072;&#1088;&#1080;&#1090;&#1077;&#1083;&#1100;&#1085;&#1099;&#1077; &#1101;&#1090;&#1072;&#1087;&#1099; &#1079;&#1072;&#1074;&#1077;&#1088;&#1096;&#1077;&#1085;&#1099; &#1080; &#1090;&#1077;&#1082;&#1089;&#1090; &#1087;&#1077;&#1088;&#1077;&#1074;&#1077;&#1076;&#1105;&#1085; &#1074; &#1092;&#1086;&#1085;&#1077;&#1084;&#1099;,
 &#1089;&#1087;&#1077;&#1094;&#1080;&#1072;&#1083;&#1100;&#1085;&#1072;&#1103; &#1084;&#1086;&#1076;&#1077;&#1083;&#1100; &#1087;&#1088;&#1077;&#1074;&#1088;&#1072;&#1097;&#1072;&#1077;&#1090; &#1077;&#1075;&#1086; &#1074; &#1089;&#1087;&#1077;&#1082;&#1090;&#1088;&#1086;&#1075;&#1088;&#1072;&#1084;&#1084;&#1091; &#8212; &#1074;&#1080;&#1079;&#1091;&#1072;&#1083;&#1100;&#1085;&#1086;&#1077;
 &#1087;&#1088;&#1077;&#1076;&#1089;&#1090;&#1072;&#1074;&#1083;&#1077;&#1085;&#1080;&#1077; &#1079;&#1074;&#1091;&#1082;&#1072;. &#1055;&#1086;&#1090;&#1086;&#1084; &#1074; &#1076;&#1077;&#1083;&#1086; &#1074;&#1089;&#1090;&#1091;&#1087;&#1072;&#1077;&#1090; &#1077;&#1097;&#1105; &#1086;&#1076;&#1085;&#1072; &#1084;&#1086;&#1076;&#1077;&#1083;&#1100;, &#1082;&#1086;&#1090;&#1086;&#1088;&#1072;&#1103;
 &#1087;&#1088;&#1077;&#1086;&#1073;&#1088;&#1072;&#1079;&#1091;&#1077;&#1090; &#1089;&#1087;&#1077;&#1082;&#1090;&#1088;&#1086;&#1075;&#1088;&#1072;&#1084;&#1084;&#1091; &#1074; wav-&#1092;&#1072;&#1081;&#1083;. &#1055;&#1086;&#1089;&#1083;&#1077;&#1076;&#1085;&#1080;&#1077; &#1076;&#1074;&#1072; &#1101;&#1090;&#1072;&#1087;&#1072; &#1086;&#1076;&#1080;&#1085;&#1072;&#1082;&#1086;&#1074;&#1099;
 &#1076;&#1083;&#1103; &#1074;&#1089;&#1077;&#1093; &#1103;&#1079;&#1099;&#1082;&#1086;&#1074;.
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org4ea3b77" class="outline-3">
<h3 id="org4ea3b77"><span class="section-number-3">13.34.</span> negative sampleing</h3>
<div class="outline-text-3" id="text-13-34">
<p>
Negative sampling used in NLP, RecSys, retrival and classification tasks to address the computational
 challenges associated with large vocabularies or item sets. It modifies the training objective: Instead of
 computing the softmax over the entire vocabulary, it focuses on distinguishing the target word from a few
 randomly selected "noise" or "negative" words.
</p>
<ul class="org-ul">
<li>Instead of using the softmax function, negative sampling uses the sigmoid function to learn to differentiate
between positive and negative samples.</li>
<li>Negative sampling transforms the problem into a series of independent binary classification tasks. For each
positive sample and its corresponding negative samples, the model predicts whether the pair belongs together
or not.</li>
</ul>

<p>
Training on pair (w,c) where w is the target word and c is a context word or word in the same class.
</p>

<p>
Instead of loss:
</p>
<pre class="example">
softmax(x_i) = e^(x_i) / (sum of e^(x_j) for all j from 1 to n)
L = -log(p(w | c)) = -log(softmax(x_i))
</pre>


<p>
We use:
</p>
<pre class="example">
L = log(sigmoid(v_w * v_c)) + sum(log(sigmoid(-v_w * v_neg_i))) for i in range(k)
</pre>

<p>
where:
</p>
<ul class="org-ul">
<li>vw - vector representation of the target word</li>
<li>vc - vector representation of context word</li>
<li>vneg - vector representations of the k negative sample.</li>
<li>k - number of negative samples</li>
<li>log(sigmoid(v<sub>w</sub> * v<sub>c</sub>)) - positive term with dot product or cosine simularity.</li>
<li>1∑k(​logσ(−vw​⋅vnegi​​)) - negative term - minimize the similarity between the target word w and the negative samples</li>
</ul>

<p>
For binary Classification: Negative sampling transforms the problem into a series of binary classification
 tasks, where the model learns to distinguish between positive and negative samples.
</p>

<p>
Selecting more frequent words from noise distribution. P(w) = (f(w)<sup>3</sup>/4)/Z, where f(w) frequency of word, Z -
 normalization constant.
</p>
</div>
<div id="outline-container-orga353f24" class="outline-4">
<h4 id="orga353f24"><span class="section-number-4">13.34.1.</span> selecting negative samples strategies</h4>
<div class="outline-text-4" id="text-13-34-1">
<p>
High-quality negative should be both informativeness and unbiasedness.
</p>

<ul class="org-ul">
<li>Random</li>
<li>Popularity-Based</li>
<li>Hard - selecting negative samples that are likely to be confused with positive samples. HNS can be
implemented using dynamic negative sampling or softmax-based sampling methods.</li>
</ul>
</div>
</div>

<div id="outline-container-orgdc6a614" class="outline-4">
<h4 id="orgdc6a614"><span class="section-number-4">13.34.2.</span> example</h4>
<div class="outline-text-4" id="text-13-34-2">
<p>
For sentence "The dog is playing with a bone," and assume a window size of 2 positive samples for
 the target word "dog" would include:
</p>
<ul class="org-ul">
<li>("dog", "The")</li>
<li>("dog", "is")</li>
<li>("dog", "playing")</li>
<li>("dog", "with")</li>
<li>("dog", "a")</li>
<li>("dog", "bone")</li>
</ul>

<p>
Negative Samples:
</p>
<ul class="org-ul">
<li>("dog", "car"), ("dog", "apple"), ("dog", "house"), ("dog", "tree")</li>
</ul>

<p>
calc: logσ(vdog​⋅vbone​)+logσ(−vdog​⋅vcar​)+logσ(−vdog​⋅vapple​)+logσ(−vdog​⋅vhouse​)
</p>
</div>
</div>
<div id="outline-container-orgf362a90" class="outline-4">
<h4 id="orgf362a90"><span class="section-number-4">13.34.3.</span> improved performance</h4>
<div class="outline-text-4" id="text-13-34-3">
<p>
ability to improve model performance through the selection of negatives, particularly those that are closely
 aligned with positive samples in embedding space. By focusing on a subset of more informative negative
 samples, the model can better capture the subtle distinctions between different samples. As highlighted in
 studies [53, 6, 54, 14, 13], negative samples, particularly more informative or “hard” negatives, contribute
 significantly to the gradients during the training process. Hard negatives refer to samples that closely
 resemble positive samples in feature space, making the difficult for the model to distinguish from the
 positives. Training with these hard negative samples forces the model to learn finer distinctions because
 these negatives contribute more significantly to the gradients, leading to a more effective optimization
 process and improvement in the model’s ability to distinguish between positive and negative samples.
</p>

<p>
<a href="https://arxiv.org/html/2402.17238v1">https://arxiv.org/html/2402.17238v1</a>
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgd4f8e8e" class="outline-2">
<h2 id="orgd4f8e8e"><span class="section-number-2">14.</span> LLM, chat bots,  conversational AI, intelligent virtual agents (IVAs)</h2>
<div class="outline-text-2" id="text-14">
<p>
LLM intro <a href="https://www.youtube.com/watch?v=zjkBMFhNj_g">https://www.youtube.com/watch?v=zjkBMFhNj_g</a>
</p>
<ul class="org-ul">
<li>Slides as PDF: <a href="https://drive.google.com/file/d/1pxx_ZI7O-Nwl7ZLNk5hI3WzAsTLwvNU7/view?usp=share_link">https://drive.google.com/file/d/1pxx_ZI7O-Nwl7ZLNk5hI3WzAsTLwvNU7/view?usp=share_link</a> (42MB)</li>
<li>Slides. as Keynote:  <a href="https://drive.google.com/file/d/1FPUpFMiCkMRKPFjhi9MAhby68MHVqe8u/view?usp=share_link">https://drive.google.com/file/d/1FPUpFMiCkMRKPFjhi9MAhby68MHVqe8u/view?usp=share_link</a></li>
</ul>

<p>
positively impacted by AI bot solutions as below:
</p>
<ul class="org-ul">
<li>Eliminate wait times: Customers today look for faster response times across all aspects of their daily lives. But, during peak times, agents can become overburdened responding to multiple inbound requests, requiring incoming customer calls or chats to be in a queue. As the queue increases and waiting times prolong, customers might abandon or get frustrated, leading to poor experience and potential business loss.</li>
<li>Reduce Missed Chats or Abandon Rate: Live chat abandon rates can represent missed business opportunities and poor experience. Most of the time, the connection to the live chat agent breaks down, requiring the customer to start from scratch and launch a new chat window. Chatbots operate in an asynchronous mode where customers can start, pause, or continue a conversation hours later without having to start everything from scratch.</li>
<li>Shortens Average Agent Handling Time: A bot can assist an agent by providing them with suggested responses or information and automating the underlying tasks that better support the agent in responding faster. Since the bot can also detect customer intent, it can speed up access to the correct information and automate the live chat interaction. This is key to making agents more productive and resolving customer issues faster.</li>
<li>Increases accuracy and consistency: Although a customer gets through an agent, there are still chances of not obtaining the right or complete information. This can lead to serious consequences for businesses as well as their customers. AI bots alongside virtual agents can often bring the best results, where the former responds to routine requests and automates underlying workflows while the latter can tackle more complex issues with emotional intelligence.</li>
<li>Improves customer experience and retention: The application of AI within customer care centers is not just confined to handling simple customer requests and workflows. They also have the capability to automate complex customer journeys such as customer onboarding, subscription renewals, and claims management, all of which lead to increased sales conversion, higher retention, faster resolution, and more.</li>
<li>Enhances productivity and satisfaction: Chatbots working alongside agents can help automate routine workflows, allowing agents to free up from mundane tasks and focus on areas&#x2026;</li>
</ul>


<p>
byte-pair encoding
</p>

<p>
GPT4 -&gt; AutoGPT -&gt; ChatDev MetaGPT -&gt; AutoGen
</p>
</div>
<div id="outline-container-orge83ddcc" class="outline-3">
<h3 id="orge83ddcc"><span class="section-number-3">14.1.</span> terms</h3>
<div class="outline-text-3" id="text-14-1">
<dl class="org-dl">
<dt>LLM</dt><dd>large-scale unsupervised language model</dd>
<dt>the context length</dt><dd></dd>

<dt>context window</dt><dd>is a range of tokens the model can consider when generating responses to
prompts. GPT-3=2k, GPT-4=32k - cost increase quadratically or at least linear. Measured in count of tokens.
<ul class="org-ul">
<li>can be fixed or variable size - input have context window and target token position.</li>
<li>during training used to learn, during prediction the context window generates predictions.</li>
</ul></dd>
<dt>(no term)</dt><dd>key-value head see <a href="#org10d2176">12.15.6.5</a></dd>
<dt>autoregressive</dt><dd>refers to the fact that the model generates its output one step at a time, based on the
previous steps.</dd>
<dt>Self-supervised data</dt><dd>labels or annotations are generated automatically from the data itself.</dd>
</dl>
<p>
o
</p>
<ul class="org-ul">
<li>Supervised Fine-tuning step (SFT)</li>
<li>Reward Modeling step (RM)</li>
<li>Proximal Policy Optimization (PPO) step - 2017 Proximal Policy Optimization Algorithms <a href="https://arxiv.org/pdf/1707.06347.pdf">https://arxiv.org/pdf/1707.06347.pdf</a></li>
<li>Chain-of-Thought (COT)</li>
<li>multiple choice format - ask to limit LLM answer as a classification : 1,2 or 3.</li>
<li id="non-parametric knowledge">i.e., retrieved text chunks</li>
<li id="parametric knowledge">knowledge stored in their parameters.</li>
<li id="user utterance"></li>

<li id="grounding data or source content">critical reference point for evaluating LLM outputs to mitigate hallucinations</li>
<li id="factual accuracy">metrics for detecting accuracy for facts in answer</li>
<li id="intent">the goal or purpose that a user has within the context of a conversation with a customer service
chatbot. <b>Intent Classification</b> is used.</li>
<li id="sentence transformer">transformer model optimized for generate dense vector representations (embeddings)
for sentences, paragraphs, or even images. For Semantic Search, Clustering, Similarity Comparison. SBERT was
first.</li>
</ul>
</div>
</div>
<div id="outline-container-org0275c13" class="outline-3">
<h3 id="org0275c13"><span class="section-number-3">14.2.</span> complexity</h3>
<div class="outline-text-3" id="text-14-2">
<p>
квадратичной сложности механизма внимания
</p>


<div id="org579dead" class="figure">
<p><img src="file:///home/u/docsmy_short/modified/imgs/comparison_conv_rnn.svg" alt="comparison_conv_rnn.svg" class="org-svg" />
</p>
</div>
</div>
</div>
<div id="outline-container-org571525d" class="outline-3">
<h3 id="org571525d"><span class="section-number-3">14.3.</span> Context window problem</h3>
<div class="outline-text-3" id="text-14-3">
<p>
SSM
</p>

<p>
S4 <a href="https://arxiv.org/abs/2111.00396">https://arxiv.org/abs/2111.00396</a>
</p>

<p>
Jamba LLM - SSM + Mamba + MoE (Mixture-of-Experts)
</p>
<ul class="org-ul">
<li><a href="https://arxiv.org/abs/2408.12570">https://arxiv.org/abs/2408.12570</a></li>
<li><a href="https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251">https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251</a></li>
</ul>
</div>

<div id="outline-container-orgdbc8afe" class="outline-4">
<h4 id="orgdbc8afe"><span class="section-number-4">14.3.1.</span> SSM</h4>
<div class="outline-text-4" id="text-14-3-1">
<p>
x′(t)=Ax(t)+Bu(t)
y(t)=Cx(t)+Du(t)
​где u(t) &#x2013; входной сигнал, x(t) &#x2013; n-мерное латентное представление, y(t) выходной сигнал, а A,B,C и D &#x2013; обучаемые матрицы.
</p>

<p>
В работе D считается равным нулю, потому что это аналог skip-connection и его легко вычислить.
</p>
</div>
</div>
</div>

<div id="outline-container-orga9b2154" class="outline-3">
<h3 id="orga9b2154"><span class="section-number-3">14.4.</span> types</h3>
<div class="outline-text-3" id="text-14-4">
<ul class="org-ul">
<li>Transformer-like LLMs (e.g., Llama)</li>
<li>Mixture-of-Expert LLMs (e.g., Mixtral)</li>
<li>Multi-modal LLMs (e.g., LLaVA)</li>
</ul>
</div>
</div>

<div id="outline-container-orgda090bc" class="outline-3">
<h3 id="orgda090bc"><span class="section-number-3">14.5.</span> tools</h3>
<div class="outline-text-3" id="text-14-5">
<p>
scripting
</p>
<ul class="org-ul">
<li><a href="https://github.com/gptscript-ai/gptscript">https://github.com/gptscript-ai/gptscript</a></li>
<li><a href="https://github.com/microsoft/guidance/">https://github.com/microsoft/guidance/</a></li>
</ul>


<p>
inference - ollama, vllm <a href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a>
</p>

<p>
models - GPT, LLama, mixtral
</p>

<p>
Vector databases - Postgres+pgvector / Mivus / Qdrant / Faiss
</p>
<ul class="org-ul">
<li><a href="https://github.com/ssahgal/embeddinghub">https://github.com/ssahgal/embeddinghub</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgf9ce279" class="outline-3">
<h3 id="orgf9ce279"><span class="section-number-3">14.6.</span> history</h3>
<div class="outline-text-3" id="text-14-6">

<div id="org697e8bf" class="figure">
<p><img src="imgs/llm-hist.jpg" alt="llm-hist.jpg" />
</p>
</div>
<ul class="org-ul">
<li>halutination problem - before 2023</li>
</ul>

<p>
major papers:
</p>
<ul class="org-ul">
<li>Seq2Seq</li>
<li>Attention is all you need</li>
<li>BERT</li>
<li>GPT-1</li>
<li>Scalling Laws for Neural Language Models</li>
<li>T5</li>
<li>GPT-2: Language Models are Unsupervied Multi-Task Learners</li>
<li>InstructGPT:Training Language Models to Follow instructions</li>
<li>GPT-3: Language Models are Few-Shot Learners</li>
</ul>
</div>
</div>
<div id="outline-container-orgae96c1a" class="outline-3">
<h3 id="orgae96c1a"><span class="section-number-3">14.7.</span> theory</h3>
<div class="outline-text-3" id="text-14-7">
<p>
training:
</p>
<ol class="org-ol">
<li><p>
unsupervised - train to predict next or missing word -&gt; <b>basic model</b> or <b>foundational model</b>
</p>
<ul class="org-ul">
<li>not yet particularly capable of answering questions or conducting dialogues. Rather, the model always</li>
</ul>
<p>
tries to continue a text that it receives as input.
</p></li>
<li>RHLF ( Reinforcement Learning with human feedback ) - human feedbacks + ratings of independent evaluation
model -&gt; chatbot
<ul class="org-ul">
<li>s</li>
</ul></li>
</ol>
</div>
</div>
<div id="outline-container-orgd4aa89c" class="outline-3">
<h3 id="orgd4aa89c"><span class="section-number-3">14.8.</span> calculation or RAM required</h3>
<div class="outline-text-3" id="text-14-8">
<p>
<a href="https://www.anyscale.com/blog/num-every-llm-developer-should-know">https://www.anyscale.com/blog/num-every-llm-developer-should-know</a>
</p>

<p>
<b>Tokens</b> - “eating” might be broken into two tokens “eat” and “ing”. A 750 word document will be about 1000
 tokens.
</p>

<p>
~1GB: Typical GPU memory requirements of an embedding model - sentence transformers.
</p>
</div>

<div id="outline-container-orgefde4a6" class="outline-4">
<h4 id="orgefde4a6"><span class="section-number-4">14.8.1.</span> estimation of memory by parameters</h4>
<div class="outline-text-4" id="text-14-8-1">
<p>
For a trained model: paramets * type * 3/4
</p>
<ul class="org-ul">
<li>types:
<ul class="org-ul">
<li>float: 32-bit floating point, 4 bytes</li>
<li>half/BF16: 16-bit floating point, 2 bytes</li>
<li>int8: 8-bit integer, 1 byte</li>
<li>int4: 4-bit integer, 0.5 bytes</li>
</ul></li>
<li>3 to 4 times - back-propagation, Adam optimization, and Transformer architecture</li>
</ul>

<p>
for Inference: parameters * type
</p>
<ul class="org-ul">
<li></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org7fe44cd" class="outline-3">
<h3 id="org7fe44cd"><span class="section-number-3">14.9.</span> Adaptation to task - ICL vs Fine-tuning</h3>
<div class="outline-text-3" id="text-14-9">
<ul class="org-ul">
<li>In-Context Learning (ICL) or prompt engineering - learns a new task from a small set of examples presented within the context (the
prompt) at inference time.</li>
<li>Parameter-Efficient Fine-Tuning</li>
</ul>
</div>

<div id="outline-container-orgac8e948" class="outline-4">
<h4 id="orgac8e948"><span class="section-number-4">14.9.1.</span> when not enough?</h4>
<div class="outline-text-4" id="text-14-9-1">
<p>
ICL bad in out-of-domain accuracy “Few-shot Fine-tuning vs In-context Learning: A Fair Comparision and
 Evaluation 2023” <a href="https://arxiv.org/abs/2305.16938">https://arxiv.org/abs/2305.16938</a>
</p>
<ul class="org-ul">
<li>Fine-Tuning don't break generalization.</li>
</ul>

<p>
We can measure failures if we change INPUT and ICL examples: On the relation between sensitivity and accuracy
 in In-context learning 2023 <a href="https://arxiv.org/pdf/2209.07661">https://arxiv.org/pdf/2209.07661</a>
</p>

<p>
In-Context Learning (ICL) or prompt engineering
</p>
<ul class="org-ul">
<li>tasks that have complicated and extensive task specifications.</li>
<li>instruction following tasks, particularly those that require a high level of precision and adherence to
specific styles.</li>
<li>tasks that require strong piors:  like emotion recognition</li>
</ul>

<p>
Parameter-Efficient Fine-Tuning
</p>
<ul class="org-ul">
<li>highly specialized domains that require domain-specific data and deep domain knowledge</li>
<li>require a very deep understanding of context and long sequences, fine-tuning alone might not be
enough. These tasks often demand not just parameter adjustments but also architectural changes or additional
techniques to handle the complexity of the input data</li>
<li>Ethical and Fairness Considerations, to fight biases</li>
<li>Tasks that require several hours for humans to master, such as certain information extraction or complex
decision-making tasks, may still pose significant challenges even with fine-tuning</li>
</ul>
</div>
</div>

<div id="outline-container-orge46d29e" class="outline-4">
<h4 id="orge46d29e"><span class="section-number-4">14.9.2.</span> enhancing ICL</h4>
<div class="outline-text-4" id="text-14-9-2">
<ul class="org-ul">
<li>index “INPUT” by embadder clusterize and select one per cluster.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org41a8569" class="outline-3">
<h3 id="org41a8569"><span class="section-number-3">14.10.</span> Prompt engineering: цепочки и деревья команд к LLMs</h3>
<div class="outline-text-3" id="text-14-10">
<ul class="org-ul">
<li>Zero-Shot: Single prompt with an implicit multi-step reasoning instruction.</li>
<li>Few-Shot: Multiple examples with multiple reasoning steps each.</li>
<li>Manual: Multiple handcrafted examples with detailed reasoning steps.</li>
<li>Auto: Multiple automatically generated examples with reasoning steps.</li>
<li>Self-Consistency with CoT: Multiple reasoning paths with multiple steps each, followed by evaluation.</li>
</ul>

<p>
The Prompt Report: A Systematic Survey of Prompting Techniq <a href="https://arxiv.org/html/2406.06608v3">https://arxiv.org/html/2406.06608v3</a>
</p>
</div>
<div id="outline-container-org0a40694" class="outline-4">
<h4 id="org0a40694"><span class="section-number-4">14.10.1.</span> terms</h4>
<div class="outline-text-4" id="text-14-10-1">
<dl class="org-dl">
<dt>decoding</dt><dd>the process of transforming the model’s output into a human-understandable format, typically a
sequence of words or tokens.</dd>
<dt>Greedy decoding path or Greedy Search Decoding</dt><dd>At each step, it selects the word (token) with the
highest. Probability and adds it to the sequence.</dd>
<dt>Beam Search Decoding</dt><dd>keeps track of multiple potential sequences at each step, then selects the top ‘k’
most probable sequences from these new sequences.</dd>
<dt>contstrains</dt><dd>.</dd>
</dl>
</div>
</div>
<div id="outline-container-org6b8a369" class="outline-4">
<h4 id="org6b8a369"><span class="section-number-4">14.10.2.</span> general findings</h4>
<div class="outline-text-4" id="text-14-10-2">
<ul class="org-ul">
<li>LLMs can reason if we consider the alternative decoding paths.</li>
<li>Model is predisposed to immediate problem-solving, by answering shortly. Ex. “Yes. No. Idk”. Models are
highly influenced by the distribution they have been trained on.</li>
<li>Model starts to struggle with generating the correct CoT-paths when the steps become 3 or</li>
</ul>
<p>
more.
</p>
<ul class="org-ul">
<li>CoT prompting constrains the model to follow an artificial strategy curated through human knowledge and
intervention which could be biased by the prompt designers.</li>
</ul>
</div>
</div>
<div id="outline-container-org77cc8c1" class="outline-4">
<h4 id="org77cc8c1"><span class="section-number-4">14.10.3.</span> Chain of Thoughts (CoT) and Variants</h4>
<div class="outline-text-4" id="text-14-10-3">
<p>
guide to reason step-by-step by providing intermediate reasoning steps.
</p>
<ul class="org-ul">
<li>Chain of Thoughts (CoT) (single)</li>
<li>Tree of Thoughts (ToT) (Manual) - experts or tree of steps. - to explore multiple reasoning paths
simultaneously</li>
<li>Algorithm-of-Thoughts (AoT) (Auto)</li>
<li>Clue And Reasoning Prompting (CARP) - Text Classification via Large Language Models
<a href="https://arxiv.org/abs/2305.08377">https://arxiv.org/abs/2305.08377</a></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org963125f"></a>ex. Clue And Reasoning Prompting (CARP)<br />
<div class="outline-text-5" id="text-14-10-3-1">
<ol class="org-ol">
<li>first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc),</li>
<li>clues and input as premises and induce a diagnostic reasoning process for final decisions</li>
<li>finally determine the final label considering the above two steps.</li>
</ol>

<p>
Work better if we provide example.
</p>
<div class="org-src-container">
<pre class="src src-text">This is an overall sentiment classifier for movie reviews.
First, list CLUES (i.e., keywords, phrases, contextual information, semantic relations, semantic meaning,
 tones, references) that supports the sentiment determination (Limit the number of words to 130).
Third, determine the overall SENTIMENT of INPUT as Positive or Negative considering CLUES, the REASONING
 process and the INPUT.
INPUT: press the delete key.
CLUES: delete key
REASONING: The phrase "delete key" implies an action of removing something, which could be interpreted as a
negative sentiment.
SENTIMENT: Negative
</pre>
</div>
</div>
</li>

<li><a id="orgdc2b3c3"></a>ex. CoT. Let's think step by step:<br />
<div class="outline-text-5" id="text-14-10-3-2">
<ol class="org-ol">
<li>Define quantum physics.</li>
<li>Explain wave-particle duality.</li>
<li>Describe the principles of superposition and entanglement.</li>
</ol>
</div>
</li>

<li><a id="orgea92e57"></a>ex. ToT.<br />
<div class="outline-text-5" id="text-14-10-3-3">
<div class="org-src-container">
<pre class="src src-text">Imagine three different experts are answering this question.
All experts will write down 1 step of their thinking,
then share it with the group.
Then all experts will go on to the next step, etc.
If any expert realizes they're wrong at any point then they leave.
The question is: Solve the equation to get 24 using the numbers 3, 4, 5, and 6.
</pre>
</div>

<p>
ex. ToT
</p>
<div class="org-src-container">
<pre class="src src-text">Main Prompt (Trunk): Plan a vacation to a destination that meets the following criteria: warm weather, beach access, and cultural attractions.

Branch 1: Identify potential destinations with warm weather.
  - Thought 1.1: Research tropical islands in the Caribbean.
    - Potential destinations: Jamaica, Barbados, St. Lucia.
  - Thought 1.2: Consider Mediterranean coastal cities.
    - Potential destinations: Barcelona, Nice, Athens.

Branch 2: Evaluate destinations with beach access.
  - Thought 2.1: List beaches in the identified tropical islands.
    - Jamaica: Seven Mile Beach, Doctor's Cave Beach.
    - Barbados: Carlisle Bay, Crane Beach.
  - Thought 2.2: Assess the quality of beaches in Mediterranean coastal cities.
    - Barcelona: Barceloneta Beach, Nova Ic&#224;ria Beach.
    - Nice: Promenade des Anglais, Plage de la Croisette
</pre>
</div>
</div>
</li>

<li><a id="org5efc914"></a>AoT Prompt Template:<br />
<div class="outline-text-5" id="text-14-10-3-4">
<ol class="org-ol">
<li>Clearly state the problem: "Use numbers and basic arithmetic operations (+, -, *, /) to obtain 24."</li>
<li>Identify the numbers and operations available: "Given numbers: 3, 4, 5, 6."</li>
<li>Break down the numbers and identify potential combinations: "Possible combinations include adding,
subtracting, multiplying, or dividing these numbers."</li>
<li>Formulate a Hypothesis: "One possible combination is (3 + 5) * 4."</li>
<li>Test the Hypothesis: "Calculate (3 + 5) * 4 = 8 * 4 = 32, which is not correct."</li>
<li>Draw Conclusions: "Try another combination: (6 - 2) * 4 = ?"</li>
<li>Reflect: "If (6 - 2) * 4 does not yield 24, explore other combinations systematically."</li>
</ol>
<div class="org-src-container">
<pre class="src src-text">Example:
(6 - 2) * 4 = 4 * 4 = 16, which is not correct.
Next, try: (6 / 2) * 4 = 3 * 4 = 12, which is not correct.
Finally, try: (6 * 4) / 2 = 24 / 2 = 12, which is not correct.
However, (6 * 4) / 2 is not the correct path; instead, try: (6 * 4) / (3 + 1) = 24 / 4 = 6, which is not correct.
But (6 * 4) / (3 + 1) is not the correct path; instead, try: (6 * 3) / (4 - 1) = 18 / 3 = 6, which is not correct.
</pre>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-org7d5a4ab" class="outline-4">
<h4 id="org7d5a4ab"><span class="section-number-4">14.10.4.</span> others techs</h4>
<div class="outline-text-4" id="text-14-10-4">
<ul class="org-ul">
<li>Reasoning and Action
<ul class="org-ul">
<li>ReAct (Multi-Step)</li>
</ul></li>
<li>Consistency and Evaluation
<ul class="org-ul">
<li>Self-Consistency (Multi-Step)</li>
</ul></li>
<li>Iterative and Feedback-Based Techniques
<ul class="org-ul">
<li>Iterative Prompting (Multi-Step)</li>
<li>Feedback Loops (Multi-Step)</li>
</ul></li>
<li>Context and Format Specification
<ul class="org-ul">
<li>Context Amplification (Single-Step)</li>
<li>Format Specification (Single-Step)</li>
</ul></li>
<li>Creative and Generative Techniques
<ul class="org-ul">
<li>Creative Writing (Single-Step)</li>
<li>Code Generation (Single-Step)</li>
</ul></li>
</ul>

<p>
Reasoning and Action - combines internal reasoning with external actions, enabling interaction with
  environments.
</p>

<p>
ReAct - Feedback Loops. refine outputs through iterative interactions and evaluations.
</p>

<p>
Consistency and Evaluation - ensures coherence and consistency across different reasoning paths.
</p>

<p>
Self-Consistency - Ensures the model's output is consistent with the input and context, using checks and
    multiple evaluations to maintain coherence.
</p>

<p>
Iterative and Feedback-Based Techniques - Involve refining outputs through multiple interactions and evaluations.
</p>

<p>
Context and Format Specification - Provide additional context and specify the desired output format to guide
  the model effectively.
</p>

<p>
Creative and Generative Techniques - Guide the model in generating creative content or code.
</p>
</div>
</div>
<div id="outline-container-org5faa6ec" class="outline-4">
<h4 id="org5faa6ec"><span class="section-number-4">14.10.5.</span> <span class="todo TODO">TODO</span> Automated Prompt Engineering (APE)</h4>
</div>
</div>

<div id="outline-container-orga616f9a" class="outline-3">
<h3 id="orga616f9a"><span class="section-number-3">14.11.</span> Fine-tuning</h3>
<div class="outline-text-3" id="text-14-11">
<p>
<a href="https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters">https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters</a>
</p>
<ul class="org-ul">
<li>Feature-based approach - frozen all transformer + output embedding - train only classifier.
<ul class="org-ul">
<li>pre-training real-valued embeddings vectors.</li>
</ul></li>
<li>Finetuning 1 - keep frozen all except 1 or more fully connected layers - PEFT</li>
<li>Finetuning 2 - update all layers</li>
<li>Adapter mudules - bottleneck architecture - PEFT</li>
</ul>

<p>
<b>proximal policy optimization</b> PPO - online policy gradient method
</p>

<p>
steps of training:
</p>
<ol class="org-ol">
<li>Pretraining on unlabeled text corpus - unsupervised pretraining</li>
<li>finetune all model or PEFT (with frozen layers and new ones)</li>
</ol>

<p>
<b>Supervised Instruction Finetuning</b> - finetune (via next-token prediction) at dataset of pairs: instrinction,
 input (Optional), output.
</p>

<p>
LLM-generated dataset: seed instructions -llm&gt; more instructions -llm&gt; output. Approaches:
</p>
<ul class="org-ul">
<li>Self-Instruct <a href="https://arxiv.org/abs/2212.10560">https://arxiv.org/abs/2212.10560</a></li>
<li>Backtranslation <a href="https://arxiv.org/abs/2308.06259">https://arxiv.org/abs/2308.06259</a></li>
<li>Danger: “imitation models” primarily replicated the style of the upstream LLMs they were trained on rather
than their factual accuracy.</li>
</ul>
</div>

<div id="outline-container-orge67dad0" class="outline-4">
<h4 id="orge67dad0"><span class="section-number-4">14.11.1.</span> <b>Parameter-Efficient Finetuning</b> techniques (PEFT)</h4>
<div class="outline-text-4" id="text-14-11-1">
<p>
finetune LLM while require the training of only a small number of parameters
</p>
<ul class="org-ul">
<li>subset of the existing model parameters - or set of newly added parameters</li>
<li>does the method aim to minimize memory footprint or only storage efficiency</li>
</ul>

<p>
types:
</p>
<ul class="org-ul">
<li><p>
<b>additive</b> - augmenting the existing pre-trained model with extra parameters or layers and training only the
newly added
</p>
<ul class="org-ul">
<li>adapters - add additional parameters to each transformer block.</li>
<li>prompt tuning or modifications - hard or soft or prefix tuning (as LLaMa adapter) - appends a tensor to</li>
</ul>
<p>
the embedded inputs of a pretrained LLM
</p>
<ul class="org-ul">
<li>soft prompts - consists of a task description accompanied by a few in-context examples</li>
</ul></li>
<li><b>selective</b> - fine-tuning only selected layers/biases/rows</li>
<li><p>
<b>reparametrization-based</b> (kind of additive) - leverage low-rank representations to minim the number of
trainable parameters. Low-rank subspace finetuning. Part of the model's input embeddings is fine-tuned via gradient descent.
</p>
<ul class="org-ul">
<li>Fastfood transform to reparametrize the update to NN params.</li>
<li>LoRa - simple low-rank matrix decomposition(or Kronecker product decomposition) to parametrize the weight</li>
</ul>
<p>
update
</p></li>
</ul>


<p>
In case of Adam, for every byte of trainable parameter, one extra byte is needed for its gradient, and two
 more bytes are needed to store the optimizer state: the first and second moments of the gradient.
</p>
<ul class="org-ul">
<li>= 3x</li>
<li>training a model re quires 12-20 times more GPU memory than the model weights</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org51962c8"></a>Adapters - additive type<br />
<div class="outline-text-5" id="text-14-11-1-1">
<ul class="org-ul">
<li>2019 <a href="https://arxiv.org/pdf/1902.00751.pdf">https://arxiv.org/pdf/1902.00751.pdf</a> Parameter-Efficient Transfer Learning for NLP</li>
</ul>

<p>
fully connected layers of the adapters are usually relatively small and have a bottleneck structure similar to
 autoencoders.
</p>

<p>
ex. input 1024, first layer 24 -&gt; 1,024 x 24 + 24 x 1,024 = 49,152 weight parameters.
</p>
<ul class="org-ul">
<li>1,024 x 1024 = 1,048,576 # if first layers would have 1024 - it would be too many parameters</li>
</ul>

<p>
Performance compatible with full fine-tuning by tuning less than 4% of the totam model params.
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">transformer_block_with_adapter</span>(x):
    <span style="color: #cae682;">residual</span> = x
    <span style="color: #cae682;">x</span> = self_attention(x)
    <span style="color: #cae682;">x</span> = AdapterLayers(x) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">adpater</span>
    <span style="color: #cae682;">x</span> = LayerNorm(x + residual)
    <span style="color: #cae682;">residual</span> = x
    <span style="color: #cae682;">x</span> = FullyConnectedLayer(x)
    <span style="color: #cae682;">x</span> = AdapterLayers(x) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">adpater</span>
    <span style="color: #cae682;">x</span> = LayerNorm(x + residual)
    <span style="color: #8ac6f2; font-weight: bold;">return</span> x

<span style="color: #8ac6f2; font-weight: bold;">def</span> <span style="color: #cae682; font-weight: bold;">AdapterLayers</span>(x):
    <span style="color: #cae682;">residual</span> = x
    <span style="color: #cae682;">x</span> = SmallFullyConnectedLayer(x) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">to a low-dimensional representation</span>
    <span style="color: #cae682;">x</span> = ReLU(x) <span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">NonlinearActivation</span>
    <span style="color: #cae682;">x</span> = FullyConnectedLayer(x) + residual <span style="color: #fa8072;">#  </span><span style="color: #99968b; font-style: italic;">back into the input dimension</span>
    <span style="color: #8ac6f2; font-weight: bold;">return</span> x
</pre>
</div>
</div>
</li>

<li><a id="org9d23888"></a>LoRA - Low rank adaptation (LoRA) - reparameterization type<br />
<div class="outline-text-5" id="text-14-11-1-2">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/2106.09685.pdf">https://arxiv.org/pdf/2106.09685.pdf</a> LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</li>
<li>article <a href="https://habr.com/ru/articles/747534/">https://habr.com/ru/articles/747534/</a></li>
</ul>


<p>
LoRA - freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer
 of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.
</p>

<p>
LoRA - add smaal number of trainable parameters to the model while the original model parameters remain frozen.
</p>
</div>
</li>
<li><a id="org5762666"></a><span class="todo TODO">TODO</span> BitFit - selective type<br /></li>
<li><a id="orgd6bdc03"></a>links<br />
<div class="outline-text-5" id="text-14-11-1-4">
<p>
<a href="https://arxiv.org/abs/2303.15647">https://arxiv.org/abs/2303.15647</a> Comparision of PEFT methods
</p>
<ul class="org-ul">
<li>Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. ArXiv,
abs/2106.10199.</li>
<li>Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685</li>
<li>Rabeeh Karimi Mahabadi, James Henderson, and</li>
</ul>
<p>
Sebastian Ruder. 2021. Compacter: Efficient low-rank hypercomplex adapter layers. In Ad- vances in Neural
 Information Processing Sys- tems, volume 34, pages 1022–1035. Curran As- sociates, Inc
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org93861f5" class="outline-4">
<h4 id="org93861f5"><span class="section-number-4">14.11.2.</span> multi-task learning</h4>
<div class="outline-text-4" id="text-14-11-2">
<p>
Sharing network parameters (weights) across tasks (in lower layers) exploits task regularities, yielding
 improved performance.
</p>

<p>
A single model to solve all problems.
</p>
</div>
</div>
<div id="outline-container-org34df80c" class="outline-4">
<h4 id="org34df80c"><span class="section-number-4">14.11.3.</span> links</h4>
<div class="outline-text-4" id="text-14-11-3">
<p>
<a href="https://lightning.ai/pages/category/community/tutorial/">https://lightning.ai/pages/category/community/tutorial/</a>
</p>

<p>
Instruction Tuning for Large Language Models paper <a href="https://arxiv.org/abs/2308.10792">https://arxiv.org/abs/2308.10792</a>
</p>
</div>
</div>
</div>

<div id="outline-container-orgfcaac83" class="outline-3">
<h3 id="orgfcaac83"><span class="section-number-3">14.12.</span> Hallucinations and checking of reasoning</h3>
<div class="outline-text-3" id="text-14-12">
<p>
Hallucinations occur when LLMs are asked to answer prompts whose task, input or output were not present in the
 training set.
</p>

<p>
Naive approach of directly asking the LLM to check a step is typically ineffective [1]
</p>


<p>
for knowledge intensive tasks (Lewis et al., 2020), users need to painstakingly verify any information they
 receive with external sources lest they be misled
</p>

<p>
Еще Проблема - научить говорить "я не знаю".
</p>
</div>
<div id="outline-container-orgc62d29e" class="outline-4">
<h4 id="orgc62d29e"><span class="section-number-4">14.12.1.</span> survey</h4>
<div class="outline-text-4" id="text-14-12-1">
<p>
<a href="https://amatria.in/blog/images/Mitigating_Hallucinations.pdf">https://amatria.in/blog/images/Mitigating_Hallucinations.pdf</a>
</p>

<p>
diff modalities <a href="https://arxiv.org/pdf/2309.05922">https://arxiv.org/pdf/2309.05922</a>
</p>

<p>
<a href="https://arxiv.org/pdf/2303.08896">https://arxiv.org/pdf/2303.08896</a>
</p>

<p>
aproaches
</p>
<ul class="org-ul">
<li>gray-box methods - require output token-level probabilities.</li>
<li>black-box methods</li>
</ul>
</div>
</div>
<div id="outline-container-orgaa41ead" class="outline-4">
<h4 id="orgaa41ead"><span class="section-number-4">14.12.2.</span> selfcheckgpt - black-box</h4>
<div class="outline-text-4" id="text-14-12-2">
<p>
several samples of the same prompt are drawn, and used to detect inconsistencies among them. The higher the
 inconsistencies, the more likely the LLM is hallucinating.
</p>

<p>
<a href="https://huggingface.co/blog/dhuynh95/automatic-hallucination-detection">https://huggingface.co/blog/dhuynh95/automatic-hallucination-detection</a>
<a href="https://arxiv.org/abs/2303.08896">https://arxiv.org/abs/2303.08896</a>
</p>
</div>
</div>
<div id="outline-container-org6581972" class="outline-4">
<h4 id="org6581972"><span class="section-number-4">14.12.3.</span> detection of hallucinations</h4>
<div class="outline-text-4" id="text-14-12-3">
<p>
Banchmarks with datasets - what types of content and to which extent
</p>
<ul class="org-ul">
<li>HELMA, HaluEval, HALTT4LLM, HALTT4LLM, возможно что и LIAR</li>
</ul>

<p>
HaluEval - Hallucination Benchmark and dataset with QA of tens of thousands sampels <a href="https://arxiv.org/pdf/2305.11747v3">https://arxiv.org/pdf/2305.11747v3</a>
</p>

<p>
”Grounding Defect Rate” being a common measure. This metric quantifies the proportion of responses from the
 LLM that lack proper grounding in the reference data, offering a clear indicator of the model’s propensity
 for hallucination.
</p>

<p>
disadvantages of classical metrics ROUGE and BLEU, PARENT, PARENT-T, Knowledge F1:
</p>
<ul class="org-ul">
<li>capacity to fully grasp the syntactic and semantic subtleties inherent in natural language</li>
</ul>

<p>
model-based metrics:
</p>
<ul class="org-ul">
<li>Information Extraction (IE) - answer to relational tuple for comparision with source content</li>
<li>Complementing IE-based -</li>
<li>NLI-based metrics employ Natural Language Inference datasets to scrutinize the ve-</li>
</ul>
<p>
racity of hypotheses generated by LLMs, grounded on specific premises drawn from the source.
</p>
</div>
</div>
<div id="outline-container-orgc956784" class="outline-4">
<h4 id="orgc956784"><span class="section-number-4">14.12.4.</span> checking by LLM problems:</h4>
<div class="outline-text-4" id="text-14-12-4">
<ol class="org-ol">
<li>hard: multiple aspects to the checking problem that the checker must deal with simultaneously: it needs to</li>
</ol>
<p>
understand the key content in the step and then collect all related information from the context, before
actually checking for its correctness
</p>
<ol class="org-ol">
<li>‘checking’ is a less common task in the training corpus</li>
<li>strong correlations between the errors such a checker will make with the errors made in the original
generation</li>
</ol>
</div>
</div>

<div id="outline-container-orgf9d041f" class="outline-4">
<h4 id="orgf9d041f"><span class="section-number-4">14.12.5.</span> stopping hallucinations or mitigation of hallucinations</h4>
<div class="outline-text-4" id="text-14-12-5">
<p>
interface to an external knowledge source
</p>
</div>
</div>
<div id="outline-container-org3817564" class="outline-4">
<h4 id="org3817564"><span class="section-number-4">14.12.6.</span> WikiChat stops the hallucination</h4>
<div class="outline-text-4" id="text-14-12-6">
<p>
by Few-Shot Grounding on Wikipedia <a href="https://github.com/stanford-oval/WikiChat">https://github.com/stanford-oval/WikiChat</a>
</p>
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/2305.14292">https://arxiv.org/pdf/2305.14292</a></li>
</ul>

<p>
steps:
</p>
<ol class="org-ol">
<li>Query - GET - Retrive from Wiki pages - generates a search query that captures the user’s interest with a
prompt</li>
<li>Summarize &amp; Filter - SUM - form retrived several pages.</li>
<li>Generate - SUM - answer from 2) and query</li>
<li>Extract Claims - SUM - use 3 to extract informaton from 2) pages again</li>
<li>Fact-Check - GET -</li>
<li>Draft - SUM - from 5)</li>
<li>Refine - SUM - check with other model?</li>
</ol>

<p>
models are available on the HuggingFace Hub.
</p>

<p>
state-of-the-art neural search <b>retrieval index</b> - Wikipedia index build for <a href="https://github.com/stanford-futuredata/ColBERT/">https://github.com/stanford-futuredata/ColBERT/</a>
</p>
<ul class="org-ul">
<li>100GB or 35GB RAM required</li>
<li><a href="https://github.com/stanford-futuredata/ColBERT/">https://github.com/stanford-futuredata/ColBERT/</a></li>
<li>BERT-based search over large text collections</li>
<li>CPU-based</li>
</ul>



<p>
metrics: factuality, conversationality, and latency
</p>

<p>
information retrieval (IR)
</p>
<ul class="org-ul">
<li>retrieve-then-generate approach - generates a response from data retrieved with the user query</li>
<li>fact-check a system’s outputs and remove errors</li>
<li>expensive changes to the pre-training process of language model</li>
</ul>

<p>
Bing Chat: 58.7% of the facts generated are grounded in what it retrieves
</p>
</div>
</div>

<div id="outline-container-org82c3d8c" class="outline-4">
<h4 id="org82c3d8c"><span class="section-number-4">14.12.7.</span> SelfCheck - prompt engineering for enhance <b>correctness of reasoning step</b></h4>
<div class="outline-text-4" id="text-14-12-7">
<ol class="org-ol">
<li>correctness of reasoning step based on the preceding steps -&gt; confidence score [0,1]</li>
<li>weighted voting on multiple solutions to the same question</li>
</ol>

<p>
to address "hard" problem in 1)
</p>
<dl class="org-dl">
<dt>target extraction</dt><dd>prompting the LLM to figure out the target of the current step</dd>
<dt>information collection</dt><dd>what information it uses to achieve the targe</dd>
<dt>step regeneration</dt><dd>ask the LLM to re-achieve the target using only the collected information, providing an
alternative to the original step that maintains the same purpose in the overall reasoning process.</dd>
<dt>result comparison</dt><dd>ask to compare the original step with the regenerated output - match/mismatch</dd>
</dl>
</div>
</div>

<div id="outline-container-org233e0b5" class="outline-4">
<h4 id="org233e0b5"><span class="section-number-4">14.12.8.</span> banchmarks</h4>
<div class="outline-text-4" id="text-14-12-8">
<p>
Banchmarks работают путем проверки языковой модели опрашивая ее. Нам нужно найти нужно найти галюцинации в данных я думаю а не в модели.
</p>
</div>
</div>
<div id="outline-container-org8f39080" class="outline-4">
<h4 id="org8f39080"><span class="section-number-4">14.12.9.</span> Fact Checking</h4>
<div class="outline-text-4" id="text-14-12-9">
<p>
<a href="https://toloka.ai/blog/fact-checking-llm-generated-content">https://toloka.ai/blog/fact-checking-llm-generated-content</a>
</p>
<ul class="org-ul">
<li>AI isn’t a reliable fact-checking tool</li>
</ul>

<p>
<b>guardrails</b> are defenses — policies, strategies, mechanisms — that are put in place to ensure the ethical and
 responsible application of AI-based technologies
</p>
</div>
</div>
<div id="outline-container-org1e6fe77" class="outline-4">
<h4 id="org1e6fe77"><span class="section-number-4">14.12.10.</span> citates</h4>
<div class="outline-text-4" id="text-14-12-10">
<ol class="org-ol">
<li>Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su.</li>
</ol>
<p>
Deductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872, 2023.
</p>
</div>
</div>
<div id="outline-container-orgf71a736" class="outline-4">
<h4 id="orgf71a736"><span class="section-number-4">14.12.11.</span> 2024 Self-Correct via Reinforcement Learning (google)</h4>
<div class="outline-text-4" id="text-14-12-11">
<p>
<a href="https://arxiv.org/pdf/2409.12917">https://arxiv.org/pdf/2409.12917</a>
</p>
</div>
</div>
</div>
<div id="outline-container-org50daf95" class="outline-3">
<h3 id="org50daf95"><span class="section-number-3">14.13.</span> choosing LLM model and architecture</h3>
<div class="outline-text-3" id="text-14-13">
<ul class="org-ul">
<li>proprietary and open source models, cloud vs in-house/On<sub>Premise</sub></li>
<li>Model Types: Chat(conversations), QA (retrieving specific information from text), summarization, etc.</li>
<li>language specialization of model</li>
</ul>
</div>
</div>
<div id="outline-container-org7c2bfbb" class="outline-3">
<h3 id="org7c2bfbb"><span class="section-number-3">14.14.</span> free chatgpt api, cloud models, LLM Providers</h3>
<div class="outline-text-3" id="text-14-14">
<ul class="org-ul">
<li><a href="https://zzzcode.ai">https://zzzcode.ai</a></li>
<li><a href="https://deepai.org/">https://deepai.org/</a></li>

<li>Anthropic</li>

<li>Claude</li>
<li>Aleph Alpha - German company</li>
</ul>

<p>
1
</p>
<ul class="org-ul">
<li>OpenAI</li>
<li>A? OpenAI - microsoft?</li>
<li>Anthrop\c</li>
<li>HuggingFace</li>
<li>Vertex AI</li>
<li>fireworks.ai</li>
<li>ollama</li>
<li>amazon Bedrock</li>
</ul>

<p>
2 OSS Model providers
</p>
<ul class="org-ul">
<li>Huggingface</li>
<li>fireworks.ai</li>
<li>ollama</li>
<li>LLAMA.CPP</li>
<li>replicate</li>
<li>GPT4ALL</li>
<li>together.ai</li>
<li>anyscale</li>
</ul>
</div>
</div>

<div id="outline-container-org9269488" class="outline-3">
<h3 id="org9269488"><span class="section-number-3">14.15.</span> instruction-following LLMs</h3>
<div class="outline-text-3" id="text-14-15">
<p>
Training language models to follow instructions with human feedback <a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a>
</p>
</div>
</div>
<div id="outline-container-org8cf9ad9" class="outline-3">
<h3 id="org8cf9ad9"><span class="section-number-3">14.16.</span> DISADVANTAGES AND PROBLEMS</h3>
<div class="outline-text-3" id="text-14-16">
<ul class="org-ul">
<li>pop - their knowledge is limited to concepts and facts that they explicitly encountered in the training data</li>
<li>not deep</li>
<li>not answer close and dont explain topic - it is to logic</li>
<li>temporal degradation - recency of the information - can not work with current weather, stock prices or even today’s date.</li>
</ul>

<p>
When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories
 <a href="https://arxiv.org/pdf/2212.10511">https://arxiv.org/pdf/2212.10511</a>
</p>
<ul class="org-ul">
<li>Adaptive Retrieval method - Using Retrieval Only Where It Helps - popularity threshold - how?</li>
<li>GenRead</li>
<li>BM25</li>
<li>Contriever</li>
</ul>

<p>
Economic riscs:
</p>
<ul class="org-ul">
<li>плохое упроавление данными в компании: фрагментированность данных</li>
<li>адаптация к переменным вычислительым требованиям</li>
</ul>
</div>
</div>
<div id="outline-container-org0b60d87" class="outline-3">
<h3 id="org0b60d87"><span class="section-number-3">14.17.</span> Advantages for programming</h3>
<div class="outline-text-3" id="text-14-17">
<ul class="org-ul">
<li>faster read code</li>
<li>faster choose solutions</li>
</ul>
</div>
</div>
<div id="outline-container-org69c9836" class="outline-3">
<h3 id="org69c9836"><span class="section-number-3">14.18.</span> ability to use context from previous interactions to inform their responses to subsequent questions</h3>
<div class="outline-text-3" id="text-14-18">
<ul class="org-ul">
<li>tech "dialogue context" to maintain a conversation's state</li>
<li>tech "teacher forcing,"</li>
<li>tech "prompt engineering" - does not have memory or knowledge, instead: converstation history is
concatenated into a single text prompt, with each message or response separated by a special delimiter.</li>
</ul>

<p>
reinforcement learning used for fine-tuning.
</p>
</div>
</div>
<div id="outline-container-orga62002f" class="outline-3">
<h3 id="orga62002f"><span class="section-number-3">14.19.</span> GigaChat Sber</h3>
<div class="outline-text-3" id="text-14-19">
<p>
GigaChat работает на
</p>
<ul class="org-ul">
<li>языковых моделях ruGPT-3 и FRED-TP</li>
<li>нейросетевой ансамбль NeONKA (NEural Omnimodal Network with Knowledge-Awareness)</li>
<li><a href="https://habr.com/ru/companies/sberdevices/articles/730088/">https://habr.com/ru/companies/sberdevices/articles/730088/</a></li>
<li><a href="https://habr.com/ru/companies/sberdevices/articles/564440/">https://habr.com/ru/companies/sberdevices/articles/564440/</a></li>
<li><a href="https://habr.com/ru/companies/sberbank/articles/730108/">https://habr.com/ru/companies/sberbank/articles/730108/</a></li>
</ul>
<p>
18 миллиардах параметров
</p>

<p>
картинки uCLIP и Kandinsky 2.1
</p>

<p>
API: <a href="https://developers.sber.ru/docs/ru/gigachat/models">https://developers.sber.ru/docs/ru/gigachat/models</a>
</p>

<p>
examples <a href="https://github.com/ai-forever/gigachat/blob/dev/examples/README.md">https://github.com/ai-forever/gigachat/blob/dev/examples/README.md</a>
</p>
</div>
</div>
<div id="outline-container-org4f67f66" class="outline-3">
<h3 id="org4f67f66"><span class="section-number-3">14.20.</span> GPT - Generative Pre-trained Transformer</h3>
<div class="outline-text-3" id="text-14-20">
<p>
<a href="https://github.com/karpathy/nanoGPT">https://github.com/karpathy/nanoGPT</a>
Decoder-Only
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> transformers <span style="color: #8ac6f2; font-weight: bold;">import</span> GPT2Tokenizer, GPT2Model

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Load pre-trained model and tokenizer</span>
<span style="color: #cae682;">tokenizer</span> = GPT2Tokenizer.from_pretrained(<span style="color: #95e454;">'gpt2'</span>)
<span style="color: #cae682;">model</span> = GPT2Model.from_pretrained(<span style="color: #95e454;">'gpt2'</span>)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Prepare input</span>
<span style="color: #cae682;">input_text</span> = <span style="color: #95e454;">"This is an example input."</span>
<span style="color: #cae682;">inputs</span> = tokenizer(input_text, return_tensors=<span style="color: #95e454;">"pt"</span>)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Generate output</span>
<span style="color: #cae682;">outputs</span> = model(**inputs)

<span style="color: #fa8072;"># </span><span style="color: #99968b; font-style: italic;">Process output</span>
<span style="color: #cae682;">last_hidden_state</span> = outputs.last_hidden_state
</pre>
</div>
</div>
</div>

<div id="outline-container-org78fad9e" class="outline-3">
<h3 id="org78fad9e"><span class="section-number-3">14.21.</span> llama2 <a id="org505f0e4"></a></h3>
<div class="outline-text-3" id="text-14-21">
<p>
<a href="python-ds#MissingReference">python-ds#MissingReference</a>
</p>
</div>
<div id="outline-container-org09cd4aa" class="outline-4">
<h4 id="org09cd4aa"><span class="section-number-4">14.21.1.</span> theory</h4>
<div class="outline-text-4" id="text-14-21-1">
<ul class="org-ul">
<li>Meta's Llama 1</li>
<li>Llama2 product of an uncommon alliance between Meta and Microsoft,</li>
<li>Llama 2 was trained with 40% more data than its predecessor</li>
</ul>

<p>
LLama1 - based on transformer architecture - 65B trained on 2048 x 80GB RAM GPUs - dataset 1.4T tokens - 21 days
</p>
<ul class="org-ul">
<li>Pre-normalization [GPT-3] - RMSNorm</li>
<li>SwiGLU activation [PALM] - replace the ReLU - for performance</li>
<li>Rotary Embeddings [GPTNeo] - replace absolute embeddings with RoPE at each layer of the nerwork.</li>
<li>optimizer - AdamW with cosing learning rate schedule - final learning rate is 10% of the max lr.</li>
<li>optimizations:
<ul class="org-ul">
<li><b>causal multi-head attention</b> - to reduse memory usage</li>
<li>reduce amount of activations with checkpointing: replace PyTorch autograd with custom.</li>
<li>overlap comps between GPUs over the network (due to all<sub>reduce</sub> operations)</li>
</ul></li>
<li>Context length 2k</li>
</ul>

<p>
<b>Warmup steps</b> are just a few updates with low learning rate before / at the beginning of training. After this
 warmup, you use the regular learning rate (schedule) to train your model to convergence.
</p>

<p>
LLama2 - is auto-regressive transformer pretrained on an corpus of self-supervised data, followed by alignment
 with human preferences via RLHF.
</p>
<ul class="org-ul">
<li>Supervised fine-tuning used an autoregressive loss function with token loss on user prompts zeroed out. (wiki)</li>
<li>Batch size was 64 (wiki)</li>
<li>2T tokens dataset</li>
<li>Context length 4k</li>
<li>Grouped Query Attention (GQA) - main difference from LLama1 - speed up decoder inference (hf.com)</li>
<li><p>
steps:
</p>
<ol class="org-ol">
<li>supervised learning (LLama2) - chat backpropageted только ответы, 27540 анотоций, 2 epochs, cosine</li>
</ol>
<p>
learning rate, init. lr=2e-05, w. decay=0.1 batch=64.
</p>
<ol class="org-ol">
<li>supervised fine-tuning (LLama-2-chat)</li>
<li>Rejection Sampling -&gt; Proximal Policy Optimization PPO (cycle)</li>
<li>Human feedback</li>
</ol></li>
<li>lateralization logic framework, literalization pathways ?</li>
</ul>
</div>
</div>

<div id="outline-container-org0398bf7" class="outline-4">
<h4 id="org0398bf7"><span class="section-number-4">14.21.2.</span> quantization libraries</h4>
<div class="outline-text-4" id="text-14-21-2">
<p>
HF - Hugging Face pytorch pickle file. file format
</p>
<ul class="org-ul">
<li>GPTQ
<ul class="org-ul">
<li><a href="https://huggingface.co/docs/transformers/main_classes/quantization">https://huggingface.co/docs/transformers/main_classes/quantization</a></li>
<li><a href="https://pypi.org/project/gptq/">https://pypi.org/project/gptq/</a></li>
<li>Torch</li>
<li>2/3/4/8-bit quantized matrix full-precision vector product CUDA kernel</li>
</ul></li>
<li>ggml <a href="https://github.com/ggerganov/ggml">https://github.com/ggerganov/ggml</a></li>
<li>bitsandbytes <a href="https://pypi.org/project/bitsandbytes/">https://pypi.org/project/bitsandbytes/</a>
<ul class="org-ul">
<li>Torch ?</li>
</ul></li>

<li>Quantization allows PostgresML to fit larger models in less RAM.</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="orga899dd3"></a>comparizaion<br />
<div class="outline-text-5" id="text-14-21-2-1">
<p>
<a href="https://github.com/ggerganov/llama.cpp/discussions/2424">https://github.com/ggerganov/llama.cpp/discussions/2424</a>
</p>

<p>
I've run GPTQ and bitsandbytes NF4 on a T4 GPU and found:
</p>

<p>
fLlama-7B (2GB shards) nf4 bitsandbytes quantisation:
</p>
<ul class="org-ul">
<li>PPL: 8.8, GPU Mem: 4.7 GB, 12.2 toks.</li>
</ul>

<p>
Llama-7B-GPTQ-4bit-128:
</p>
<ul class="org-ul">
<li>PPL: 9.3, GPU Mem: 4.8 GB, 21.4 toks.</li>
</ul>

<p>
fLlama-13B (4GB shards) nf4 bitsandbytes quantisation:
</p>
<ul class="org-ul">
<li>PPL: 8.0, GPU Mem: 8.2 GB, 7.9 toks.</li>
</ul>

<p>
Llama-13B-GPTQ-4bit-128:
</p>
<ul class="org-ul">
<li>PPL: 7.8, GPU Mem: 8.5 GB, 15 toks.</li>
</ul>

<p>
I've also run ggml on T4 and got 2.2 toks, so it seems much slower - whether I do 3 or 5bit quantisation.
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgd00ef14" class="outline-4">
<h4 id="orgd00ef14"><span class="section-number-4">14.21.3.</span> jailbreak</h4>
<div class="outline-text-4" id="text-14-21-3">
<p>
<a href="https://github.com/facebookresearch/llama-recipes">https://github.com/facebookresearch/llama-recipes</a>
</p>

<p>
<a href="https://simonwillison.net/2023/Mar/11/llama/?ref=thisismeteor.com">https://simonwillison.net/2023/Mar/11/llama/?ref=thisismeteor.com</a>
</p>
</div>
</div>
<div id="outline-container-org2d07113" class="outline-4">
<h4 id="org2d07113"><span class="section-number-4">14.21.4.</span> gpt vs llama</h4>
<div class="outline-text-4" id="text-14-21-4">
<p>
AI2 Reasoning Challenge (25-shot) - a set of grade-school science questions.
</p>
<ul class="org-ul">
<li>Llama 1 (llama-65b): 57.6</li>
<li>LLama 2 (llama-2-70b-chat-hf): 64.6</li>
<li>GPT-3.5: 85.2</li>
<li>GPT-4: 96.3</li>
</ul>

<p>
HellaSwag (10-shot) - a test of commonsense inference, which is easy for humans (~95%) but challenging for SOTA models.
</p>
<ul class="org-ul">
<li>Llama 1: 84.3</li>
<li>LLama 2: 85.9</li>
<li>GPT-3.5: 85.3</li>
<li>GPT-4: 95.3</li>
</ul>

<p>
MMLU (5-shot) - a test to measure a text model’s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more.
</p>
<ul class="org-ul">
<li>Llama 1: 63.4</li>
<li>LLama 2: 63.9</li>
<li>GPT-3.5: 70.0</li>
<li>GPT-4: 86.4</li>
</ul>

<p>
TruthfulQA (0-shot) - a test to measure a model’s propensity to reproduce falsehoods commonly found online. Note: TruthfulQA in the Harness is actually a minima a 6-shots task, as it is prepended by 6 examples systematically, even when launched using 0 for the number of few-shot examples.
</p>
<ul class="org-ul">
<li>Llama 1: 43.0</li>
<li>LLama 2: 52.8</li>
<li>GPT-3.5: 47.0</li>
<li>GPT-4: 59.0</li>
</ul>
</div>
</div>
<div id="outline-container-orgf877eb8" class="outline-4">
<h4 id="orgf877eb8"><span class="section-number-4">14.21.5.</span> fine tuning</h4>
<div class="outline-text-4" id="text-14-21-5">
<p>
see <a href="#org01c35d9">13.28</a>
</p>

<ul class="org-ul">
<li>TRL + PEFT : <a href="https://huggingface.co/docs/trl/index">https://huggingface.co/docs/trl/index</a>
<ul class="org-ul">
<li>trl.SFTTrainer  (QLoRA) <a href="https://www.philschmid.de/instruction-tune-llama-2">https://www.philschmid.de/instruction-tune-llama-2</a></li>
<li>QLoRA steps:
<ol class="org-ol">
<li>Quantize the pre-trained model to 4 bits and freeze it.</li>
<li>Attach small, trainable adapter layers. (LoRA)</li>
<li>Finetune only the adapter layers while using the frozen quantized model for context.</li>
</ol></li>
<li>Flash Attention - see <a href="#orge459cf5">14.21.5.1</a></li>
</ul></li>
<li>PEFT
<ul class="org-ul">
<li>huggingface/autotrain-advanced</li>
</ul></li>
<li>DPO <a href="https://huggingface.co/blog/dpo-trl">https://huggingface.co/blog/dpo-trl</a></li>
</ul>


<p>
Original paper:
</p>
</div>

<ol class="org-ol">
<li><a id="orge3032e2"></a>Flash Attention - accelerates training up to 3x <a id="orge459cf5"></a><br />
<div class="outline-text-5" id="text-14-21-5-1">
<ul class="org-ul">
<li><a href="https://github.com/Dao-AILab/flash-attention/tree/main">https://github.com/Dao-AILab/flash-attention/tree/main</a></li>
<li>"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness" <a href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a></li>
</ul>
<pre class="example">
python -c "import torch; assert torch.cuda.get_device_capability()[0] &gt;= 8, 'Hardware not supported for Flash Attention'"
</pre>


<pre class="example">
pip install ninja packaging
MAX_JOBS=4 pip install flash-attn --no-build-isolation
</pre>


<p>
usage examples:
</p>
<ul class="org-ul">
<li><a href="https://www.philschmid.de/instruction-tune-llama-2#3-instruction-tune-llama-2-using-trl-and-the-sfttrainer">https://www.philschmid.de/instruction-tune-llama-2#3-instruction-tune-llama-2-using-trl-and-the-sfttrainer</a></li>
</ul>
</div>
</li>
<li><a id="org87e5999"></a>DPO<br />
<div class="outline-text-5" id="text-14-21-5-2">
<ul class="org-ul">
<li>DPO - Direct Preference Optimization</li>
</ul>

<p>
cast the RL-based objective used by existing methods to an objective which can be directly optimized via a
 simple binary cross-entropy loss which simplifies this process of refining LLMs greatly.
</p>

<p>
DPO bypasses the reward modeling step and directly optimises the language model on preference data via a key insight
</p>

<p>
no need need for a reward model.
</p>

<p>
see <a href="python-ds#MissingReference">python-ds#MissingReference</a>
</p>

<p>
DPO <a href="https://arxiv.org/abs/2305.18290">https://arxiv.org/abs/2305.18290</a>
</p>
</div>
<ol class="org-ol">
<li><a id="orgdb37d81"></a>DPO vs PPO<br />
<div class="outline-text-6" id="text-14-21-5-2-1">
<ul class="org-ul">
<li>PPO - Proximal Policy Optimization</li>

<li><a href="https://huggingface.co/blog/dpo-trl">https://huggingface.co/blog/dpo-trl</a></li>
</ul>
</div>
</li>
</ol>
</li>

<li><a id="orga0bf7fd"></a>links<br />
<div class="outline-text-5" id="text-14-21-5-3">
<ul class="org-ul">
<li>TODO: <a href="https://github.com/OpenGVLab/LLaMA-Adaptern">https://github.com/OpenGVLab/LLaMA-Adaptern</a></li>
<li>Inference TODO: <a href="https://brev.dev/blog/fine-tuning-llama-2">https://brev.dev/blog/fine-tuning-llama-2</a></li>
<li><a href="https://github.com/huggingface/trl/tree/main/examples/research_projects/stack_llama_2/scripts">https://github.com/huggingface/trl/tree/main/examples/research_projects/stack_llama_2/scripts</a></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org8aa0eab" class="outline-4">
<h4 id="org8aa0eab"><span class="section-number-4">14.21.6.</span> stackllama</h4>
<div class="outline-text-4" id="text-14-21-6">
<p>
LlaMa model to answer questions on Stack Exchange
</p>

<p>
<a href="https://huggingface.co/blog/stackllama">https://huggingface.co/blog/stackllama</a>
</p>
</div>
</div>
<div id="outline-container-orgdae5276" class="outline-4">
<h4 id="orgdae5276"><span class="section-number-4">14.21.7.</span> distribute</h4>
<div class="outline-text-4" id="text-14-21-7">
<p>
problems: <a id="orgcc85b7e"></a>
</p>
<ul class="org-ul">
<li>Data parallelism does not help reduce memory footprint per device</li>
<li>Model parallelism does not scale efficiently beyond a single node due to fine-grained computation and
expensive communication. ex. NVIDIA Megatron-LM - at multi-node performance degrades.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orga679e00"></a>links<br />
<div class="outline-text-5" id="text-14-21-7-1">
<p>
<a href="https://huggingface.co/blog/accelerate-large-models">https://huggingface.co/blog/accelerate-large-models</a>
</p>
</div>
</li>
<li><a id="org0d6a6c2"></a>DeepSpeed - supported by huggingface.<br />
<div class="outline-text-5" id="text-14-21-7-2">
<p>
ZeRO - The Zero Redundancy Optimizer - solution for problems - microsoft: "ZeRO-powered data parallelism".
 see <a href="#orgcc85b7e">14.21.7</a>,
</p>
<ul class="org-ul">
<li>partitioning the model states: parameters, gradients, and optimizer state - (not replicating!)</li>
<li>dynamic communication schedule during training to share the necessary state across distributed devices to
retain the computational granularity and communication volume of data parallelism.</li>
<li>ZeRO eliminates memory redundancies and makes the full aggregate memory capacity of a cluster available.</li>
</ul>

<p>
Zero <a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/</a>
</p>
<ul class="org-ul">
<li><img src="https://www.microsoft.com/en-us/research/uploads/prod/2020/02/DeepSpeed-Image-1.png" alt="DeepSpeed-Image-1.png" /></li>
<li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/02/Turing-Animation.mp4">https://www.microsoft.com/en-us/research/uploads/prod/2020/02/Turing-Animation.mp4</a></li>
</ul>
<p>
Turing Natural Language Generation (T-NLG) - Microsoft LModel for NLP task (17B parameters)
<a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/</a>
</p>

<p>
DeepSpeed Chat <a href="https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat">https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat</a>
</p>

<p>
<a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/</a>
</p>
</div>
<ol class="org-ol">
<li><a id="orgb14f555"></a><span class="todo TODO">TODO</span> Mixture of Experts (MoE)<br />
<div class="outline-text-6" id="text-14-21-7-2-1">
<p>
DeepSpeed v0.5 introduces new support
</p>

<p>
DeepSpeed MoE supports five different forms of parallelism:
</p>
<ul class="org-ul">
<li>E 	Expert 	Scales the model size by increasing the number of experts</li>
<li>E + D 	Expert + Data 	Accelerates training throughput by scaling to multiple data parallel groups</li>
<li>E + Z 	Expert + ZeRO-powered data 	Partitions the nonexpert parameters to support larger base models</li>
<li>E + D + M 	Expert + Data + Model 	Supports massive hidden sizes and even larger base models than E+Z</li>
<li>E + D + Z 	Expert + Data + ZeRO-powered data 	Supports massive hidden sizes and even larger base models than E+Z</li>
<li>E + Z-Off + M 	Expert + ZeRO-Offload + Model 	Leverages both GPU and CPU memory for large MoE models on limited # of GPUs</li>
</ul>

<p>
Random token selection addresses the limitation of biased selection problem in MoE model training.
<a href="https://www.deepspeed.ai/tutorials/mixture-of-experts/">https://www.deepspeed.ai/tutorials/mixture-of-experts/</a>
</p>
</div>
</li>
</ol>
</li>
<li><a id="orgff4fc2b"></a><span class="todo TODO">TODO</span> torchx<br />
<div class="outline-text-5" id="text-14-21-7-3">
<p>
Not all available out-of-the-box.
</p>
<ul class="org-ul">
<li>Model Parallel</li>
<li>DDP</li>
</ul>

<p>
<a href="https://pytorch.org/torchx/main/components/overview.html">https://pytorch.org/torchx/main/components/overview.html</a>
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org0581319" class="outline-4">
<h4 id="org0581319"><span class="section-number-4">14.21.8.</span> schema trl+deepspeed</h4>
<div class="outline-text-4" id="text-14-21-8">
<p>
SFTTrainer: A light and friendly wrapper around transformers Trainer to easily fine-tune language models or
 adapters on a custom dataset.
</p>


<p>
trl is a wraper around huggingface/transformers
</p>
</div>
</div>

<div id="outline-container-org2a08d0f" class="outline-4">
<h4 id="org2a08d0f"><span class="section-number-4">14.21.9.</span> wiki at work</h4>
<div class="outline-text-4" id="text-14-21-9">
<div class="org-src-container">
<pre class="src src-text">
&#1048;&#1085;&#1090;&#1077;&#1088;&#1092;&#1077;&#1081;&#1089; &#1082; &#1082;&#1083;&#1080;&#1077;&#1085;&#1090;&#1091;, &#1095;&#1090;&#1086; &#1086;&#1085; &#1085;&#1072;&#1084; &#1076;&#1072;&#1077;&#1090;?

SFT - &#1074;&#1086;&#1087;&#1088;&#1086;&#1089;, &#1086;&#1090;&#1074;&#1077;&#1090;?

PPO - human-provided rankings of multiple answers to the same query?

DPO - ?
&#1058;&#1077;&#1088;&#1084;&#1080;&#1085;&#1099;

    LLaMa2 Chat - LLaMa2 &#1084;&#1086;&#1076;&#1077;&#1083;&#1100; &#1087;&#1088;&#1086;&#1096;&#1077;&#1076;&#1096;&#1072;&#1103; SFT &#1080; PPO, &#1074;&#1077;&#1089;&#1072; &#1087;&#1086;&#1089;&#1090;&#1072;&#1074;&#1083;&#1103;&#1102;&#1090;&#1089;&#1103; &#1082;&#1072;&#1082; &#1086;&#1090;&#1076;&#1077;&#1083;&#1100;&#1085;&#1072;&#1103; &#1084;&#1086;&#1076;&#1077;&#1083;&#1100;, &#1085;&#1072; &#1088;&#1072;&#1074;&#1085;&#1077; &#1089; LLaMa2.
    Proximal Policy Optimization (PPO)
    Direct Preference Optimization (DPO)
    offloading - &#1088;&#1072;&#1079;&#1075;&#1088;&#1091;&#1079;&#1082;&#1072; GPU &#1080; &#1087;&#1077;&#1088;&#1077;&#1085;&#1086;&#1089; &#1074;&#1099;&#1095;&#1080;&#1089;&#1083;&#1077;&#1085;&#1080;&#1081; &#1080; &#1087;&#1072;&#1084;&#1103;&#1090;&#1080; &#1085;&#1072; CPU.
    Automatic Mixed Precision (AMP) - &#1040;&#1074;&#1090;&#1086;&#1084;&#1072;&#1090;&#1080;&#1095;&#1077;&#1089;&#1082;&#1072;&#1103; &#1082;&#1086;&#1085;&#1074;&#1077;&#1088;&#1090;&#1072;&#1094;&#1080;&#1103; &#1087;&#1072;&#1088;&#1072;&#1084;&#1077;&#1090;&#1088;&#1086;&#1074; &#1074; float16 &#1076;&#1083;&#1103; &#1091;&#1089;&#1082;&#1086;&#1088;&#1077;&#1085;&#1080;&#1103;. Some ops, like linear layers and convolutions, are much faster in float16 or bfloat16. (PyTorch + Nvidia)
    Automatic loss scaling (ALS) - &#1090;&#1077;&#1093;&#1085;&#1080;&#1082;&#1072; &#1080;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1091;&#1077;&#1084;&#1072;&#1103; &#1087;&#1088;&#1080; mixed precision &#1076;&#1083;&#1103; &#1091;&#1083;&#1091;&#1095;&#1096;&#1077;&#1085;&#1080;&#1103; &#1089;&#1090;&#1072;&#1073;&#1080;&#1083;&#1100;&#1085;&#1086;&#1089;&#1090;&#1080; &#1080; &#1090;&#1086;&#1095;&#1085;&#1086;&#1089;&#1090;&#1080;. (DeepSpeed + Nvidia)
    Distributed Data Parallel (DDP) - &#1085;&#1072; &#1082;&#1072;&#1078;&#1076;&#1086;&#1084; GPU/&#1084;&#1072;&#1096;&#1080;&#1085;&#1072;&#1084; &#1093;&#1088;&#1072;&#1085;&#1080;&#1090;&#1089;&#1103; &#1082;&#1086;&#1087;&#1080;&#1103; &#1087;&#1072;&#1088;&#1072;&#1084;&#1077;&#1090;&#1088;&#1086;&#1074; &#1080; states. (PyTorch)
    Fully Sharded Data Parallel (FSDP) - &#1088;&#1072;&#1079;&#1076;&#1077;&#1083;&#1077;&#1085;&#1080;&#1077; &#1087;&#1072;&#1088;&#1072;&#1084;&#1077;&#1090;&#1088;&#1086;&#1074; &#1080; states &#1087;&#1086; GPU/&#1084;&#1072;&#1096;&#1080;&#1085;&#1072;&#1084; &#1080; &#1086;&#1073;&#1077;&#1089;&#1087;&#1077;&#1095;&#1077;&#1085;&#1080;&#1077; &#1074;&#1086;&#1079;&#1084;&#1086;&#1078;&#1085;&#1086;&#1089;&#1090;&#1080; offload &#1074; CPU. (PyTorch)
    Gradient Clipping -


&#1044;&#1086;&#1086;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077;

&#1069;&#1090;&#1072;&#1087;&#1099; &#1076;&#1086;&#1086;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1103; (RLHF):

    supervised fine-tuning (SFT) - &#1074; llama2 chat backpropageted &#1090;&#1086;&#1083;&#1100;&#1082;&#1086; &#1086;&#1090;&#1074;&#1077;&#1090;&#1099;, 27540 &#1072;&#1085;&#1086;&#1090;&#1086;&#1094;&#1080;&#1081;, 2 epochs, cosine learning rate, init. lr=2e-05, w. decay=0.1 batch=64.
    PPO (&#1082;&#1083;&#1072;&#1089;&#1089;&#1080;&#1095;&#1077;&#1089;&#1082;&#1072;&#1103;) &#1080;&#1083;&#1080; DPO (&#1085;&#1086;&#1074;&#1072;&#1103;) &#1076;&#1086;&#1086;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077;. PPO - &#1086;&#1073;&#1091;&#1095;&#1072;&#1077;&#1090;&#1089;&#1103; ranking model, &#1082;&#1086;&#1090;&#1086;&#1088;&#1072;&#1103; &#1079;&#1072;&#1090;&#1077;&#1084; &#1080;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1091;&#1077;&#1090;&#1089;&#1103; &#1076;&#1083;&#1103; &#1076;&#1086;&#1086;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1103;, DPO - &#1073;&#1077;&#1079; ranking model.



&#1041;&#1080;&#1073;&#1083;&#1080;&#1086;&#1090;&#1077;&#1082;&#1080;:

    huggingface/autotrain-advanced with peft (sft training)
    huggingface/transformers - &#1084;&#1086;&#1078;&#1077;&#1090; &#1080;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1086;&#1074;&#1072;&#1090;&#1100;: DeepSpeed
    huggingface/trl - &#1084;&#1086;&#1078;&#1077;&#1090; &#1080;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1086;&#1074;&#1072;&#1090;&#1100;: transformers, PEFT, accelerate
    huggingface/peft - Parameter-Efficient Fine-Tuning (PEFT) - State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning.
    huggingface/accelerate - &#1088;&#1072;&#1089;&#1087;&#1088;&#1077;&#1076;&#1077;&#1083;&#1077;&#1085;&#1085;&#1086;&#1077; &#1086;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077;, &#1084;&#1086;&#1078;&#1077;&#1090; &#1080;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1086;&#1074;&#1072;&#1090;&#1100;: DeepSpeed, Megatron-LM
    DeepSpeed - Pipeline-parallelism (kind of model-parallelism), Tensor-parallelism

&#1041;&#1080;&#1073;&#1083;&#1080;&#1086;&#1090;&#1077;&#1082;&#1080; (&#1082; &#1089;&#1074;&#1077;&#1076;&#1077;&#1085;&#1080;&#1102;):

    PyTorch Lightening - &#1074;&#1099;&#1089;&#1086;&#1082;&#1086;&#1091;&#1088;&#1086;&#1074;&#1085;&#1077;&#1074;&#1099;&#1081; &#1080;&#1085;&#1090;&#1077;&#1088;&#1092;&#1077;&#1081;&#1089; &#1082; PyTorch, &#1087;&#1086;&#1076;&#1076;&#1077;&#1088;&#1078;&#1080;&#1074;&#1072;&#1077;&#1090; &#1088;&#1072;&#1089;&#1087;&#1088;&#1077;&#1076;&#1077;&#1083;&#1077;&#1085;&#1085;&#1086;&#1077; &#1086;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077;: DDP, FSDP, DeepSpeed

&#1057;&#1089;&#1099;&#1083;&#1082;&#1080; &#1087;&#1086; &#1087;&#1088;&#1080;&#1086;&#1088;&#1080;&#1090;&#1077;&#1090;&#1091; &#1080;&#1085;&#1092;&#1086;&#1088;&#1084;&#1072;&#1090;&#1080;&#1074;&#1085;&#1086;&#1089;&#1090;&#1100;+&#1087;&#1086;&#1085;&#1103;&#1090;&#1085;&#1086;&#1089;&#1090;&#1100;:

    https://en.wikipedia.org/wiki/LLaMA
    https://huggingface.co/docs/transformers/model_doc/llama2
    LLama 1 (Touvron et al. 2023) https://arxiv.org/abs/2302.13971
    LLama 2 https://arxiv.org/abs/2307.09288
    official inference code https://github.com/facebookresearch/llama
    models https://huggingface.co/models?search=llama2
    Code LLama https://arxiv.org/abs/2308.12950

&#1058;&#1088;&#1072;&#1085;&#1089;&#1092;&#1086;&#1088;&#1084;&#1077;&#1088;

    https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
    https://machinelearningmastery.com/the-transformer-model/
    Improving Language Understanding by Generative Pre-Training https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf
    Multi Query Attention (MQA) - &#1080;&#1089;&#1087;&#1086;&#1083;&#1100;&#1079;&#1091;&#1077;&#1090;&#1089;&#1103; LLaMa2 &#1076;&#1083;&#1103; &#1091;&#1089;&#1082;&#1086;&#1088;&#1077;&#1085;&#1080;&#1103; https://arxiv.org/pdf/2305.13245.pdf
    https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/

&#1044;&#1086;&#1086;&#1073;&#1091;&#1095;&#1077;&#1085;&#1080;&#1077;

1. https://huggingface.co/blog/dpo-trl

2. trl + accelerate https://huggingface.co/blog/trl-peft
Like
Be the first to like this
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc5a3fb8" class="outline-4">
<h4 id="orgc5a3fb8"><span class="section-number-4">14.21.10.</span> links</h4>
<div class="outline-text-4" id="text-14-21-10">
<ul class="org-ul">
<li>doc First llama (Touvron et al. 2023) <a href="https://arxiv.org/abs/2302.13971">https://arxiv.org/abs/2302.13971</a></li>
<li>llama2 <a href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a>
<ul class="org-ul">
<li><a href="file:///home/ff/Downloads/2307.09288.pdf">file:///home/ff/Downloads/2307.09288.pdf</a></li>
<li>? code llama2  <a href="https://arxiv.org/abs/2308.12950">https://arxiv.org/abs/2308.12950</a></li>
</ul></li>
<li>? <a href="https://arxiv.org/pdf/2305.13245.pdf">https://arxiv.org/pdf/2305.13245.pdf</a>
<ul class="org-ul">
<li><a href="file:///home/ff2/Downloads/2305.13245.pdf">file:///home/ff2/Downloads/2305.13245.pdf</a></li>
</ul></li>
<li>code llama <a href="https://arxiv.org/abs/2308.12950">https://arxiv.org/abs/2308.12950</a></li>
<li>huggungface model description <a href="https://huggingface.co/docs/transformers/model_doc/llama2">https://huggingface.co/docs/transformers/model_doc/llama2</a></li>
<li>official inference code <a href="https://github.com/facebookresearch/llama">https://github.com/facebookresearch/llama</a></li>
<li>models <a href="https://huggingface.co/models?search=llama2">https://huggingface.co/models?search=llama2</a></li>
<li>doc <a href="https://huggingface.co/docs/transformers/main/model_doc/llama2">https://huggingface.co/docs/transformers/main/model_doc/llama2</a></li>
<li>doc <a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/?_fb_noscript=1">https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/?_fb_noscript=1</a>
<ul class="org-ul">
<li><a href="https://scontent-iev1-1.xx.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=04ReMOti9ikAX9WxYJw&amp;_nc_ht=scontent-iev1-1.xx&amp;oh=00_AfCzbf3jU5lAs6PLGJH0eFZXj_uaSXnKUDFxzgTd2Y-iBw&amp;oe=64E3F9BF">https://scontent-iev1-1.xx.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&amp;ccb=1-7&amp;_nc_sid=3c67a6&amp;_nc_ohc=04ReMOti9ikAX9WxYJw&amp;_nc_ht=scontent-iev1-1.xx&amp;oh=00_AfCzbf3jU5lAs6PLGJH0eFZXj_uaSXnKUDFxzgTd2Y-iBw&amp;oe=64E3F9BF</a></li>
<li>/home/ff/Downloads/10000000<sub>662098952474184</sub><sub>2584067087619170692</sub><sub>n.pdf</sub></li>
</ul></li>
<li>sub models <a href="https://www.reddit.com/r/LocalLLaMA/wiki/models/">https://www.reddit.com/r/LocalLLaMA/wiki/models/</a></li>
<li>download <a href="https://easywithai.com/resources/llama-2/">https://easywithai.com/resources/llama-2/</a></li>
<li>Source – HF – GPTQ – ggml - file formats, not equal to original.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orga2ad51c" class="outline-3">
<h3 id="orga2ad51c"><span class="section-number-3">14.22.</span> frameworks to control control LLM</h3>
<div class="outline-text-3" id="text-14-22">
<ul class="org-ul">
<li><a href="https://github.com/microsoft/guidance/">https://github.com/microsoft/guidance/</a></li>
<li><a href="https://github.com/langchain-ai/langchain">https://github.com/langchain-ai/langchain</a></li>
</ul>
</div>
</div>
<div id="outline-container-org0a58d4d" class="outline-3">
<h3 id="org0a58d4d"><span class="section-number-3">14.23.</span> size optimization</h3>
<div class="outline-text-3" id="text-14-23">
<p>
NVIDIA  bfloat16 keeps the full exponential range of float32, but gives up a 2/3rs of the precision
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Format</th>
<th scope="col" class="org-left">Significand</th>
<th scope="col" class="org-left">Exponent</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">bfloat16</td>
<td class="org-left">8 bits</td>
<td class="org-left">8 bits</td>
</tr>

<tr>
<td class="org-left">float16</td>
<td class="org-left">11 bits</td>
<td class="org-left">5 bits</td>
</tr>

<tr>
<td class="org-left">float32</td>
<td class="org-left">24 bits</td>
<td class="org-left">8 bits</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-orge675c0e" class="outline-3">
<h3 id="orge675c0e"><span class="section-number-3">14.24.</span> distribute training - choose framework</h3>
<div class="outline-text-3" id="text-14-24">
<p>
sharded data parallelism - multiple GPU at single machine
</p>

<p>
model parallelism
</p>
<ul class="org-ul">
<li>torch.distributed.rpc - This package allows you to perform a model-parallelism strategy. It is very efficient if your model is large and does not fit in a single GPU.</li>
<li>DeepSpeed - model-parallelism on PyTorch <a href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li>
<li>Mesh TensorFlow - model-parallelism on Tensorflow</li>
</ul>

<p>
Asychronous Data-parallelism
</p>
<ul class="org-ul">
<li>parameter server strategy in Tensorflow and Torch</li>
<li>torch.nn.DistributedDataParallel</li>
</ul>

<p>
Pipeline Parallelism <a href="https://people.eecs.berkeley.edu/~matei/papers/2019/sosp_pipedream.pdf">https://people.eecs.berkeley.edu/~matei/papers/2019/sosp_pipedream.pdf</a>
</p>
<ul class="org-ul">
<li>DeepSpeed</li>
<li>PyTorch TODO: <a href="https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html">https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html</a></li>
</ul>

<p>
Tensor Parallelism: Model parallelism and Pipeline parallelism split model vertically to slices from input to
 output. Tensor parallelism split horizontally - every tensor.
</p>

<p>
Mixture-of-Experts(MoE) -
</p>

<p>
TensorFlowOnSpark - <a href="https://github.com/yahoo/TensorFlowOnSpark">https://github.com/yahoo/TensorFlowOnSpark</a>
</p>


<p>
huggingface/accelerate support
</p>
<ul class="org-ul">
<li>DeepSpeed - Current integration doesn’t support Pipeline Parallelism of DeepSpeed, doesn’t support multiple models</li>
<li>Megatron-LM</li>
</ul>

<p>
BigDL Intel for Apache Spark - ?
</p>
<ul class="org-ul">
<li><a href="https://github.com/intel-analytics/BigDL">https://github.com/intel-analytics/BigDL</a></li>
<li><a href="https://bigdl.readthedocs.io/">https://bigdl.readthedocs.io/</a></li>
<li><a href="https://bigdl.readthedocs.io/en/latest/doc/UserGuide/notebooks.html">https://bigdl.readthedocs.io/en/latest/doc/UserGuide/notebooks.html</a></li>
</ul>

<p>
Horovod Uber -  data parallelism only
</p>
<ul class="org-ul">
<li><a href="https://github.com/horovod/horovod">https://github.com/horovod/horovod</a></li>
<li><a href="https://github.com/horovod/horovod#documentation">https://github.com/horovod/horovod#documentation</a></li>
<li><a href="https://github.com/horovod/horovod/tree/master/examples">https://github.com/horovod/horovod/tree/master/examples</a></li>
</ul>

<p>
Ray - data parallelism, Model parallelism
</p>
<ul class="org-ul">
<li><a href="https://github.com/ray-project/ray">https://github.com/ray-project/ray</a></li>
<li><a href="https://www.ray.io/docs">https://www.ray.io/docs</a></li>
<li><a href="https://github.com/ray-project/tutorial">https://github.com/ray-project/tutorial</a></li>
</ul>

<p>
Megatron-LM Nvidia (used in NeMo Megatron) - tensor, pipeline and sequence based model parallelism for
 pre-training transformer based Language Models - Transformers
</p>
<ul class="org-ul">
<li>Nvidia and Apache License 2.0 for Facebook, huggingface and Google Research code</li>
<li><a href="https://github.com/NVIDIA/Megatron-LM">https://github.com/NVIDIA/Megatron-LM</a></li>
<li>Model parallelism <a href="https://arxiv.org/abs/1909.08053">https://arxiv.org/abs/1909.08053</a></li>
<li>GPU Clusters <a href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf</a></li>
<li><a href="https://huggingface.co/docs/accelerate/usage_guides/megatron_lm">https://huggingface.co/docs/accelerate/usage_guides/megatron_lm</a></li>
</ul>

<p>
DeepSpeed Microsoft - empowers ChatGPT-like model training
</p>
<ul class="org-ul">
<li>Apache License 2.0</li>
<li>deepspeed.ai</li>
<li><a href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li>
</ul>

<p>
ColossalAI - Data Parallelism, Tensor Parallelism - single machine?
</p>
<ul class="org-ul">
<li>llama2 supported</li>
<li><a href="https://arxiv.org/abs/2110.14883">https://arxiv.org/abs/2110.14883</a></li>
<li><a href="https://github.com/hpcaitech/ColossalAI">https://github.com/hpcaitech/ColossalAI</a></li>
</ul>

<p>
Yandex - decetralized - LLama, Falcon
<a href="https://github.com/bigscience-workshop/petals">https://github.com/bigscience-workshop/petals</a>
</p>
</div>
<div id="outline-container-orgf96598a" class="outline-4">
<h4 id="orgf96598a"><span class="section-number-4">14.24.1.</span> wiki work</h4>
<div class="outline-text-4" id="text-14-24-1">
<p>
Термины
</p>

<ul class="org-ul">
<li>microbatches - используется в PyTorch Pipeline Parallelism как разбиение батчей, для обеспечения data parallelism. В TF Mirrored Strategy называется "batch per replica".</li>
</ul>
<p>
Парадигмы
</p>

<p>
Model parallelism
</p>

<ul class="org-ul">
<li>Не используется в отдельности без pipeline parallelism, так как в одинмомент времени задействована только 1 машина.</li>
<li>torch.distributed.rpc - This package allows you to perform a model-parallelism strategy. It is very efficient if your model is large and does not fit in a single GPU.

<ul class="org-ul">
<li><a href="https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html">https://pytorch.org/tutorials/intermediate/dist_pipeline_parallel_tutorial.html</a></li>

<li>ResNet50 <a href="https://github.com/pytorch/examples/blob/main/distributed/rpc/pipeline/main.py">https://github.com/pytorch/examples/blob/main/distributed/rpc/pipeline/main.py</a></li>
</ul></li>
<li>DeepSpeed - model-parallelism on PyTorch <a href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li>
<li>Mesh TensorFlow - model-parallelism on Tensorflow</li>

<li>Pytorch TorchX - model-parallelism, DDP, may not work out-of-the-box. Универсальный запускатель задач, использует distributed.elastic.  Fault-tolerance ориентирован.</li>
</ul>

<p>
Asychronous Data-parallelism
</p>
<ul class="org-ul">
<li>parameter server strategy in Tensorflow and Torch</li>
<li>torch.nn.DistributedDataParallel</li>
</ul>

<p>
Pipeline Parallelism
</p>

<ul class="org-ul">
<li><a href="https://people.eecs.berkeley.edu/~matei/papers/2019/sosp_pipedream.pdf">https://people.eecs.berkeley.edu/~matei/papers/2019/sosp_pipedream.pdf</a></li>

<li>DeepSpeed</li>

<li><p>
PyTorch -  torch.distributed.pipeline - Pipe only supports intra-node pipelining currently!
</p>

<ul class="org-ul">
<li>Transformers <a href="https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html">https://pytorch.org/tutorials/intermediate/pipeline_tutorial.html</a></li>

<li><a href="https://pytorch.org/docs/stable/pipeline.html">https://pytorch.org/docs/stable/pipeline.html</a></li>
</ul>
<ul class="org-ul">
<li>2020 "torchgpipe" <a href="https://arxiv.org/abs/2004.09910">https://arxiv.org/abs/2004.09910</a></li>

<li>2019 <a href="https://arxiv.org/abs/1811.06965">https://arxiv.org/abs/1811.06965</a></li>
</ul></li>
</ul>

<p>
Tensor Parallelism - в отличии от pipeline, model parallelism - горизонтальный, разделяет каждый тензон. Используется для Inference?
</p>

<ul class="org-ul">
<li>PyTorch - experimental! <a href="https://pytorch.org/docs/stable/distributed.tensor.parallel.html">https://pytorch.org/docs/stable/distributed.tensor.parallel.html</a></li>
</ul>


<p>
PyTorch - native
</p>

<ul class="org-ul">
<li>DistributedDataParallel (DDP) + Model parallelism - необходимо вручную разбить модель на части.

<ul class="org-ul">
<li><a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a></li>
</ul></li>

<li>DDP + torch.distributed.rpc - hybrid parallelism - часть модели разделяется между воркерами, другая часть дублируется, вручную.

<ul class="org-ul">
<li><a href="https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html">https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html</a></li>
</ul></li>

<li>FSDP - (как расширенный DDP) можно автоматически разделить слои по машинам по размеру или другим признакам. 4x larder models compared to DDP and 20x larger with activation checkpointing and activation offloading.

<ul class="org-ul">
<li><a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/</a></li>

<li><a href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html">https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html</a></li>

<li><a href="https://www.youtube.com/watch?v=HQeKwCsnH4k&amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT">https://www.youtube.com/watch?v=HQeKwCsnH4k&amp;list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT</a></li>
</ul></li>
</ul>


<p>
Список высокоуровневых библиотек
</p>

<p>
Huggingface/accelerate
</p>

<ul class="org-ul">
<li>DeepSpeed в режиме Pipeline Parallelism не поддерживается в huggingspace сейчас  <a href="https://huggingface.co/docs/accelerate/usage_guides/deepspeed#few-caveats-to-be-aware-of">https://huggingface.co/docs/accelerate/usage_guides/deepspeed#few-caveats-to-be-aware-of</a></li>

<li>Megatron-LM</li>

<li>TRL - просто обертка для Transformers и Accelerate</li>
</ul>


<p>
FairScale by Meta, facebook. FSDP oriented. автоматический mixed precision и шардирование данных, Масштабированная оптимизация
</p>

<ul class="org-ul">
<li>BSD-3-Clause</li>

<li><a href="https://github.com/facebookresearch/fairscale/">https://github.com/facebookresearch/fairscale/</a></li>

<li><a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/">https://engineering.fb.com/2021/07/15/open-source/fsdp/</a></li>
</ul>


<p>
Megatron-LM by Nvidia (used in  NeMo Megatron) - "pipeline model parallelism"? model-parallel (tensor, sequence, and pipeline) for Transformers
</p>
<ul class="org-ul">
<li><a href="https://github.com/NVIDIA/Megatron-LM">https://github.com/NVIDIA/Megatron-LM</a></li>
<li>Model parallelism <a href="https://arxiv.org/abs/1909.08053">https://arxiv.org/abs/1909.08053</a></li>

<li>GPU Clusters <a href="https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf">https://people.eecs.berkeley.edu/~matei/papers/2021/sc_megatron_lm.pdf</a></li>
<li><a href="https://huggingface.co/docs/accelerate/usage_guides/megatron_lm">https://huggingface.co/docs/accelerate/usage_guides/megatron_lm</a></li>
</ul>


<p>
DeepSpeed by Microsoft - pipeline parallelism
</p>

<ul class="org-ul">
<li>Apache License 2.0</li>
<li>deepspeed.ai</li>
<li><a href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></li>

<li><img src="https://www.microsoft.com/en-us/research/uploads/prod/2020/02/DeepSpeed-Image-1.png" alt="DeepSpeed-Image-1.png" /></li>
<li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2020/02/Turing-Animation.mp4">https://www.microsoft.com/en-us/research/uploads/prod/2020/02/Turing-Animation.mp4</a></li>
</ul>


<p>
PyTorch Lightning - Apache 2.0
</p>


<p>
TensorFlowOnSpark - <a href="https://github.com/yahoo/TensorFlowOnSpark">https://github.com/yahoo/TensorFlowOnSpark</a>
</p>

<p>
BigDL Intel for Apache Spark - ?
</p>
<ul class="org-ul">
<li><a href="https://github.com/intel-analytics/BigDL">https://github.com/intel-analytics/BigDL</a></li>
<li><a href="https://bigdl.readthedocs.io/">https://bigdl.readthedocs.io/</a></li>
<li><a href="https://bigdl.readthedocs.io/en/latest/doc/UserGuide/notebooks.html">https://bigdl.readthedocs.io/en/latest/doc/UserGuide/notebooks.html</a></li>
</ul>

<p>
Horovod Uber -  data parallelism only
</p>
<ul class="org-ul">
<li><a href="https://github.com/horovod/horovod">https://github.com/horovod/horovod</a></li>
<li><a href="https://github.com/horovod/horovod#documentation">https://github.com/horovod/horovod#documentation</a></li>
<li><a href="https://github.com/horovod/horovod/tree/master/examples">https://github.com/horovod/horovod/tree/master/examples</a></li>
</ul>

<p>
Ray - data parallelism, Model parallelism
</p>
<ul class="org-ul">
<li><a href="https://github.com/ray-project/ray">https://github.com/ray-project/ray</a></li>
<li><a href="https://www.ray.io/docs">https://www.ray.io/docs</a></li>
<li><a href="https://github.com/ray-project/tutorial">https://github.com/ray-project/tutorial</a></li>
</ul>

<p>
ColossalAI - Data Parallelism, Tensor Parallelism - single machine?
</p>
<ul class="org-ul">
<li>llama2 supported</li>
<li><a href="https://arxiv.org/abs/2110.14883">https://arxiv.org/abs/2110.14883</a></li>
<li><a href="https://github.com/hpcaitech/ColossalAI">https://github.com/hpcaitech/ColossalAI</a></li>
</ul>



<p>
Ссылки
</p>

<p>
Лучшие статьи о парадигмах:
</p>

<ol class="org-ol">
<li><a href="https://huggingface.co/docs/transformers/v4.17.0/en/parallelism">https://huggingface.co/docs/transformers/v4.17.0/en/parallelism</a></li>

<li><a href="https://lilianweng.github.io/posts/2021-09-25-train-large/">https://lilianweng.github.io/posts/2021-09-25-train-large/</a></li>

<li>comparision of distributed ml systems <a href="https://arxiv.org/pdf/1909.02061.pdf">https://arxiv.org/pdf/1909.02061.pdf</a></li>
</ol>

<p>
Ссылки
</p>

<ul class="org-ul">
<li><a href="https://neptune.ai/blog/distributed-training-frameworks-and-tools">https://neptune.ai/blog/distributed-training-frameworks-and-tools</a></li>
<li><a href="https://www.libhunt.com/r/Megatron-LM">https://www.libhunt.com/r/Megatron-LM</a></li>
</ul>
<p>
Like
Be the first to like this
</p>
</div>
</div>
<div id="outline-container-orgcbf1ce1" class="outline-4">
<h4 id="orgcbf1ce1"><span class="section-number-4">14.24.2.</span> links</h4>
<div class="outline-text-4" id="text-14-24-2">
<ul class="org-ul">
<li><a href="https://neptune.ai/blog/distributed-training-frameworks-and-tools">https://neptune.ai/blog/distributed-training-frameworks-and-tools</a></li>
<li><a href="https://www.libhunt.com/r/Megatron-LM">https://www.libhunt.com/r/Megatron-LM</a></li>
<li><a href="https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214">https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214</a></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org0b09a0e" class="outline-3">
<h3 id="org0b09a0e"><span class="section-number-3">14.25.</span> <span class="todo TODO">TODO</span> bots</h3>
<div class="outline-text-3" id="text-14-25">
<p>
Pyrogram или AIOGram
</p>
</div>
</div>
<div id="outline-container-orgb6fa3b1" class="outline-3">
<h3 id="orgb6fa3b1"><span class="section-number-3">14.26.</span> Inference optimization</h3>
<div class="outline-text-3" id="text-14-26">
<ul class="org-ul">
<li>Sparsifying - silent neuron selection</li>
<li>CUDA graph</li>
<li>Speculation decoding - trade off higher compute for lower latency</li>
<li>Quantization (GPTQ, AWQ, FP8)
<ul class="org-ul">
<li>FP8 - trade off accuracy for lower latency</li>
</ul></li>
<li>Automatic prefix caching
<ul class="org-ul">
<li>trade off higher memory for lower latency</li>
</ul></li>
<li>Chuned prefils (a.k.a. Dynamic SplitFuse)
<ul class="org-ul">
<li>trade off time-to-first-token for inter-token-latency</li>
</ul></li>
<li>Multi-LoRA serving</li>
<li>Constraind decoding</li>
<li>FlashAttention &amp; FlashDecoding</li>
<li>reduced per-step scheduling overhead</li>
<li>kernel tuning</li>
</ul>

<p>
<a href="https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit#slide=id.g272083aca97_0_46">https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit#slide=id.g272083aca97_0_46</a>
</p>

<p>
SPIN: Sparsifying and Integrating Internal Neurons in Large Language Models for Text Classification
</p>
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/2311.15983">https://arxiv.org/pdf/2311.15983</a></li>
</ul>

<p>
<a href="https://pypi.org/project/torchao/">https://pypi.org/project/torchao/</a>
</p>

<p>
<a href="https://www.marqo.ai/course/introduction-to-sentence-transformers">https://www.marqo.ai/course/introduction-to-sentence-transformers</a>
</p>
</div>
</div>
<div id="outline-container-org3e72010" class="outline-3">
<h3 id="org3e72010"><span class="section-number-3">14.27.</span> pipeline</h3>
<div class="outline-text-3" id="text-14-27">
</div>
<div id="outline-container-org419d34c" class="outline-4">
<h4 id="org419d34c"><span class="section-number-4">14.27.1.</span> types:</h4>
<div class="outline-text-4" id="text-14-27-1">
<ul class="org-ul">
<li>Advantages:
<ul class="org-ul">
<li>simple</li>
<li>modular</li>
<li>Efficient</li>
</ul></li>
<li>compose your own</li>
<li>Off-the-shelf</li>
<li>legacy class</li>
<li>LCEL
<ul class="org-ul">
<li>streaming</li>
<li>Async (and sync) support</li>
<li>Optimized parallel execution</li>
<li>Integrated with LangSmith and LangServe</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgb854af3" class="outline-4">
<h4 id="orgb854af3"><span class="section-number-4">14.27.2.</span> use cases</h4>
<div class="outline-text-4" id="text-14-27-2">
<dl class="org-dl">
<dt>QA over structured data</dt><dd>Qustion -&gt; SQL Query -&gt; Query result -&gt; additional context -&gt; answer</dd>
<dt>Extraction</dt><dd>Unstructured Text + JSON Schema ➞ Compiled JSON</dd>
<dt>Summarization</dt><dd>MOAR text ➞ LESS text</dd>
<dt>Synthetic data generation</dt><dd>JSON Schema ➞ [Unstructured Text, Unstructured Text, Unstructured Text, Unstructured Text …]</dd>
<dt>Agents</dt><dd>let LLM takes actions</dd>
</dl>
</div>
</div>
</div>
<div id="outline-container-org8dbe94d" class="outline-3">
<h3 id="org8dbe94d"><span class="section-number-3">14.28.</span> Knowledge Graph (KG)</h3>
<div class="outline-text-3" id="text-14-28">
<ul class="org-ul">
<li><a href="https://neo4j.com/blog/what-is-knowledge-graph/">https://neo4j.com/blog/what-is-knowledge-graph/</a></li>
<li>papers <a href="https://github.com/zjukg/KG-LLM-Papers">https://github.com/zjukg/KG-LLM-Papers</a></li>
</ul>

<p>
вводит некий inductive bias и может помочь ML-модели в выучивании закономерностей.
</p>
</div>
<div id="outline-container-orgeff988c" class="outline-4">
<h4 id="orgeff988c"><span class="section-number-4">14.28.1.</span> terms</h4>
<div class="outline-text-4" id="text-14-28-1">
<dl class="org-dl">
<dt>Semantic networks</dt><dd>semantic relations between concepts.</dd>

<dt>viewing conceptually</dt><dd>accomplished by transferring the data into nodes and its relationships into edges</dd>
<dt>product taxonomy</dt><dd>hierarchical system used to categorize producs. Alternative to KG.
<ul class="org-ul">
<li>Categories: Broad groups that organize products (e.g., Clothing, Accessories).</li>
<li>Subcategories: More specific groups within categories (e.g., Men’s Clothing, Women’s Clothing).</li>
<li>Product Types: Specific types of products within subcategories (e.g., Shirts, Pants).</li>
<li>Attributes: Detailed features of products (e.g., Material, Size, Color).</li>
</ul></dd>
<dt>information extraction pipeline</dt><dd>the process of extracting structured information from unstructured text,
often in the form of entities and relationships.</dd>
<dt>Knowledge Graph Question Answering (KGQA)</dt><dd>task</dd>
<dt>recall</dt><dd>meaning "how many of the relevant documents are we retrieving".</dd>
<dt>LLM recall</dt><dd>the ability of an LLM to find information from the text placed within its context
window. degrades as we put more tokens in the context window.</dd>
<dt>reranking model or cross-encoder</dt><dd>input: query and document pair, output: a similarity score. Used for A
two-stage retrieval system. Why? Rerankers are much more accurate than embedding models.</dd>
<dt>(no term)</dt><dd></dd>
</dl>
</div>
</div>
<div id="outline-container-orga8f3553" class="outline-4">
<h4 id="orga8f3553"><span class="section-number-4">14.28.2.</span> types</h4>
<div class="outline-text-4" id="text-14-28-2">
<p>
<a href="https://neo4j.com/blog/rdf-vs-property-graphs-knowledge-graphs/">https://neo4j.com/blog/rdf-vs-property-graphs-knowledge-graphs/</a>
</p>
<ul class="org-ul">
<li>Property Graphs - nodes, relationships, and properties. Nodes and relationships can store any number of
properties as key-value pairs.</li>
<li>RDF (Resource Description Framework) - W3C standard - triple record - (node/subject, relationship/predicate, node/object)
<ul class="org-ul">
<li>W3C standard for data exchange on the Web.</li>
<li>“Triplestore databases”, with SPARQL as their query language.</li>
</ul></li>
</ul>

<p>
drawbacks of RDF:
</p>
<ul class="org-ul">
<li>can not create multiple relationships of the same type between nodes</li>
<li>if complexity will increase and it will require to store the properties of each relationship, it will be a
problem.</li>
</ul>
</div>
</div>

<div id="outline-container-org6a3a741" class="outline-4">
<h4 id="org6a3a741"><span class="section-number-4">14.28.3.</span> levels:</h4>
<div class="outline-text-4" id="text-14-28-3">
<ul class="org-ul">
<li>data - rows</li>
<li>relationships - rows (node relationship node) - graph database</li>
<li>Organizing principles - additional nodes which is rules or categories around the data that provide a
flexible, conceptual structure to drive deeper data insights.</li>
<li>ontologies - several additional nodes or rules, semantic networks are a common way to represent ontologies.
<ul class="org-ul">
<li>product taxonomy as the organizing principle is sufficient for a product recommendation use case.</li>
</ul></li>
</ul>

<p>
Ontologies are available in the OBO and OWL formats.
</p>
<ul class="org-ul">
<li>OBO <a href="https://obofoundry.org/">https://obofoundry.org/</a>
<ul class="org-ul">
<li><a href="https://obofoundry.org/ontology/cido.html">https://obofoundry.org/ontology/cido.html</a></li>
</ul></li>
<li><a href="https://www.w3.org/OWL/">https://www.w3.org/OWL/</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgcbfbae4" class="outline-4">
<h4 id="orgcbfbae4"><span class="section-number-4">14.28.4.</span> building</h4>
<div class="outline-text-4" id="text-14-28-4">
<p>
The usefulness of a knowledge graph lies in the way it organizes the principles, data, and relationships to
 surface new knowledge for your user or business.
</p>

<p>
The key challenge is to use the meaning of your query, rather than a simple keyword match. (embeddings?)
</p>

<p>
How to manage ontologies:
</p>
<ul class="org-ul">
<li>the problems you’re trying to solve with a knowledge graph</li>
</ul>

<p>
steps:
</p>
<ol class="org-ol">
<li>conceptually mapping the graph data model</li>
<li>implementing it in a database</li>
</ol>

<p>
Implementations of RDF:
</p>
<ul class="org-ul">
<li>RDBMS: a table with three columns</li>
<li>NoSQL Redis implementation:
<ul class="org-ul">
<li>Nodes - Hashes - key is the unique identifier for the node and the fields - properties of the node.</li>
<li>Edges - Sets - key is the edge identifier. Each edge is stored as a set of related nodes.</li>
<li>Indexing - Sorted Sets - index on specific node properties</li>
</ul></li>
<li>Triplestores</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="orgd4ee394"></a>PT (Product taxonomy) vs KG (Knowledge graph)<br />
<div class="outline-text-5" id="text-14-28-4-1">
<ul class="org-ul">
<li>PT - hierarchical structure
<ul class="org-ul">
<li>good as a starting point to just split data.</li>
</ul></li>
<li>KG - complex relationships and connections between entities. add
related concepts. for advanced data analysis, semantic search, and
recommendation systems.
<ul class="org-ul">
<li>good for source code understanding and summarization of big texts</li>
</ul></li>
</ul>
</div>
</li>

<li><a id="orgfafb4c3"></a>Vector/Embeddings vs KG/Graph vs hybrid. Why KG required?<br />
<div class="outline-text-5" id="text-14-28-4-2">
<ul class="org-ul">
<li>Embeddings good for unsupervised way.</li>
<li>KG require expert knowledge</li>
</ul>
<p>
<a href="https://memgraph.com/blog/introduction-to-node-embedding">https://memgraph.com/blog/introduction-to-node-embedding</a>
</p>

<p>
graph embedding is superior to alternatives in many supervised
 learning tasks, such as
</p>
<ul class="org-ul">
<li>node classification - determine the label of nodes based on other
labeled nodes and the topology of the network</li>
<li>link prediction -  predicting missing links or links that are likely to occur in the future</li>
<li>Clustering - graph reconstruction</li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-org665642f" class="outline-4">
<h4 id="org665642f"><span class="section-number-4">14.28.5.</span> <span class="todo TODO">TODO</span> problems</h4>
<div class="outline-text-4" id="text-14-28-5">
<dl class="org-dl">
<dt>Distribution shift</dt><dd>scenario where the data distribution in the training phase differs significantly from
the data distribution in the inference or test phase.
<ul class="org-ul">
<li>For example, a KG trained on a specific domain may not perform well when applied to a different domain due
to these distribution shifts.</li>
</ul></dd>
<dt>Cold start</dt><dd>New item, new relationship, Limited Training Data</dd>
<dt>Dynamic vocabulary</dt><dd>the ability of the model to handle and adapt to changes in the vocabulary or the set
of entities and relationships over time.
<ul class="org-ul">
<li>Evolution of Knowledge</li>
<li>This process involves translating natural language queries into graph queries and updating the KG
accordingly</li>
</ul></dd>
</dl>
</div>
</div>
<div id="outline-container-orga1d60d2" class="outline-4">
<h4 id="orga1d60d2"><span class="section-number-4">14.28.6.</span> usage patterns:</h4>
<div class="outline-text-4" id="text-14-28-6">
<ul class="org-ul">
<li>real-time applications</li>
<li>search and discovery</li>
<li>grounding generative AI for question-answering.</li>
</ul>

<p>
Cases:
</p>
<ul class="org-ul">
<li>Generative AI for Enterprise Search Applications
<ul class="org-ul">
<li>organize key domain-specific or proprietary company information</li>
</ul></li>
<li>Fraud Detection and Analytics in Financial Services, Banking, and Insurance
<ul class="org-ul">
<li>a network of transactions, their participants, and relevant information about them.</li>
</ul></li>
<li>Master Data Management - when multiple divisions or applications interacting with customers
<ul class="org-ul">
<li>customers and the interactions with them.</li>
</ul></li>
<li>Supply Chain Management
<ul class="org-ul">
<li>network of suppliers, raw materials, products, and logistics that work together to supply a company’s
operations and customers</li>
</ul></li>
<li>Investigative Journalism
<ul class="org-ul">
<li>key entities (companies, people, bank accounts, etc.) and activities under investigation.</li>
</ul></li>
<li>Drug Discovery in Healthcare Research
<ul class="org-ul">
<li>research subject: protein and genome sequences together with environmental and chemical data, revealing intricate patterns
and expanding our knowledge of proteins</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgc36ae8b" class="outline-4">
<h4 id="orgc36ae8b"><span class="section-number-4">14.28.7.</span> Naive RAG, problems and Advanced technique</h4>
<div class="outline-text-4" id="text-14-28-7">
<p>
Before RAG: sparce methods based on term-frequency, using TF-IDF or BM25 weighting. This is near-exact
 matches. (lexical gap and do not generalize well)
</p>

<p>
Naive
</p>
<ol class="org-ol">
<li>Indexing of chunked Documents</li>
<li>retrival by query</li>
<li>LLM with query and retrived result</li>
</ol>

<p>
ways:
</p>
<ul class="org-ul">
<li>remove irrelevant information, ambiguity in entities and terms, confirming factual accuracy, maintaining
context, and updating outdated information.</li>
<li>optimize chunk sizes to capture relevant context or add information from graph structure to capture
relationships between entities.</li>
<li>add dates, chapters, subsections, purposes or any other relevant information into chunks as metadata to
improve the data filtering</li>
</ul>

<p>
“Pre-retrival” - query modification.
</p>
<ul class="org-ul">
<li>Sliding Window — chunking method that uses overlap between the chunks.</li>
<li>Auto-Merging Retrieval — utilizes small text blocks during the initial search phase and subsequently
provides larger related text blocks to the language model for processing.</li>
<li>Abstract Embedding — prioritizes Top-K retrieval based on document abstracts (or summaries), offering a
comprehensive understanding of the entire document context.</li>
<li>Metadata Filtering — leverages document metadata to enhance the filtering process.</li>
<li>Graph Indexing — transforms entities and relationships into nodes and connections, significantly improving
relevance.</li>
<li>summary of chunks stored in VS instead of actual chunks</li>
<li>create graph at ingestion step with LLM or custom text domain models to perform the information extraction
pipeline.</li>
</ul>

<p>
Retrieval:
</p>
<ul class="org-ul">
<li>Embedding model fine-tuning with dataset: queries, a corpus and relevant documents.</li>
<li>Choosing Similarity Metric is also optimization problem: Cosine Similarity, dot product .. etc.</li>
</ul>

<p>
“Post-retrival” - rerank and relocate relevant chinks to the edges of the prompt.
</p>
<ul class="org-ul">
<li>Reranking — rerank the retrieved information to prioritize the most relevant content first.</li>
<li>Prompt Compression - compress irrelevant context and reduce context length before presenting to LLM. Use
Small Language Models to calculate prompt mutual information or perplexity to estimate element
importance. Use summarization techniques when the context is long.</li>
</ul>

<p>
RAG-Fusion: multi-query of diverse perspectives and re-rank after.
</p>

<p>
Routing — query routing decides the subsequent action to a user’s query for example summarization, searching
 specific databases, etc.
</p>

<p>
Memory Module - use history of queries.
</p>

<p>
Search Module - use not only vector database.
</p>
</div>
</div>
<div id="outline-container-org590a6c6" class="outline-4">
<h4 id="org590a6c6"><span class="section-number-4">14.28.8.</span> <span class="todo TODO">TODO</span> RAG loss</h4>
<div class="outline-text-4" id="text-14-28-8">
<p>
InfoNCE
</p>
</div>
</div>

<div id="outline-container-org8833f12" class="outline-4">
<h4 id="org8833f12"><span class="section-number-4">14.28.9.</span> RAG - indexing</h4>
<div class="outline-text-4" id="text-14-28-9">
<p>
methods like ANNOY, Faiss, or Pinecone
</p>
</div>
</div>
<div id="outline-container-orge7068cd" class="outline-4">
<h4 id="orge7068cd"><span class="section-number-4">14.28.10.</span> RAG - graph-based database</h4>
<div class="outline-text-4" id="text-14-28-10">
<p>
Noe4j - extract and map <b>entities</b> and <b>relationships</b>
</p>

<p>
LangGraph - framework for agentic apps. Cycles into chain that are DAGs
</p>
<ul class="org-ul">
<li>chain - a sequence of components to process user's input and output of LLM.</li>
</ul>

<p>
<b>Hybrid search</b> - vector + keyword for retrival. Neo4j AuraDB as a graph database
</p>
<ul class="org-ul">
<li>neo4j - Java</li>
<li>Aerospike</li>
</ul>

<p>
search for vectors:
</p>
<ul class="org-ul">
<li>Faiss is a library developed by Facebook for similarity search in vector databases with large datasets</li>
<li>Annoy is a C++ library with Python bindings optimized for efficient Approximate Nearest Neighbors search.</li>
</ul>

<p>
vector databases:
</p>
<ul class="org-ul">
<li>Milvus</li>
<li>Pinecone</li>
</ul>

<p>
graph databases:
</p>
<ul class="org-ul">
<li>ArangoDB</li>
<li>Memgraph</li>
<li>neo4j</li>
</ul>
</div>
</div>
<div id="outline-container-org5c4990d" class="outline-4">
<h4 id="org5c4990d"><span class="section-number-4">14.28.11.</span> RAG - types of graphs</h4>
<div class="outline-text-4" id="text-14-28-11">
<ul class="org-ul">
<li>keywords - create integer communities or clusters</li>
<li>embeddings - create float communities</li>
<li>KG knowledge graph - greate directed relationships between nodes</li>
<li>set of all nodes with edges (nodes is unique edges is not)</li>
<li>list of all edges
<ul class="org-ul">
<li>adjacency matrix between all its nodes</li>
</ul></li>
<li></li>
</ul>
</div>
</div>
<div id="outline-container-org32524a7" class="outline-4">
<h4 id="org32524a7"><span class="section-number-4">14.28.12.</span> GRAG - alternatives</h4>
<div class="outline-text-4" id="text-14-28-12">
<p>
<a href="https://superlinked.com/vectorhub/articles/improving-rag-performance-knowledge-graphs">https://superlinked.com/vectorhub/articles/improving-rag-performance-knowledge-graphs</a>
</p>
</div>
<ol class="org-ol">
<li><a id="org107f22c"></a>Multi-Head RAG<br />
<div class="outline-text-5" id="text-14-28-12-1">
<p>
Каждая голова в Multi-Head RAG может быть настроена для работы с определенными типами информации или
 контекстами: Тексты научных статей, Данные из социальных сетей, Коммерческая информация
</p>
<ul class="org-ul">
<li>Результаты от каждой головы комбинируются для формирования финального ответа</li>
</ul>

<p>
<a href="https://habr.com/ru/companies/otus/articles/821943/">https://habr.com/ru/companies/otus/articles/821943/</a>
</p>
</div>
</li>
<li><a id="org66dada0"></a>Text2Cypher<br />
<div class="outline-text-5" id="text-14-28-12-2">
<p>
retrival mechanism: translates tasks or questions into an answer-oriented graph query
</p>
<ul class="org-ul">
<li>generates graph pattern queries based on the knowledge graph schema and the given task, while</li>
<li></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgfaaacbd" class="outline-4">
<h4 id="orgfaaacbd"><span class="section-number-4">14.28.13.</span> GRAG - graph-based RAG</h4>
<div class="outline-text-4" id="text-14-28-13">
<p>
Challenges:
</p>
<ul class="org-ul">
<li>leverage the semantic inside and across documents.</li>
<li>aggregate text and topological info in prompt (knowledge graphs)</li>
</ul>

<p>
Advantages:
</p>
<ul class="org-ul">
<li>Multi-Hop, Joint and knowledge-based Reasoning - Entity Resolution can identify and link references that
pertain to the same real-world object, enabling collective analysis.</li>
<li>Explainable Relevance - Graph topology lets us transparently analyze the connections and relationships that
determine why certain facts are retrieved as relevant.</li>
<li>Personalization — KGs capture and tailor query results according to user attributes, context, and historical
interactions.</li>
</ul>

<p>
types:
</p>
<ul class="org-ul">
<li>two steps - reasoning chains - not consider topological information in retrival process</li>
<li>one step -
<ul class="org-ul">
<li>subgraph <a href="#org6b9489d">14.28.13.2</a></li>
<li>community detection - partition graph into communities.</li>
</ul></li>
</ul>

<p>
terms
</p>
<ul class="org-ul">
<li><b>k-hop ego-graphs</b> - a type of graph representation that focuses on the ego node (the central node) and its
k-hop neighborhood.
<ul class="org-ul">
<li>k-Hop Neighborhood: The nodes reachable within k edges (hops) from the ego node.</li>
</ul></li>
<li>KG - knowledge graph</li>
<li>node embeddings - semantic meaning of each node and its relationships.</li>
<li>graph embeddings</li>
<li>Vector Indexing - for rapid similarity search. methods like ANNOY, Faiss, or Pinecone</li>
</ul>

<p>
(Sub)Graph RAG obtains relevant subgraphs to provide context - rely on entity subgraph retrieval
</p>

<p>
steps:
</p>
<ol class="org-ol">
<li>retrieval of relevant textual subraphs on large-scale textual graphs - NP-hard
<ol class="org-ol">
<li>embeddings of query</li>
<li>Vector search in KG - Top K related chunks.</li>
<li>Query GraphDB in KG - query knowledge from related entities - relations?</li>
<li>Graph Query response In KG - chunks + subgraph/knowledge</li>
</ol></li>
<li>LLM generation with joint text and topological info</li>
</ol>
</div>


<ol class="org-ol">
<li><a id="org1edc691"></a>types of graphs<br />
<div class="outline-text-5" id="text-14-28-13-1">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Graph as a:</th>
<th scope="col" class="org-left">Semantic metadata</th>
<th scope="col" class="org-left">Domain knowledge</th>
<th scope="col" class="org-left">Factial data</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Metadata Store</td>
<td class="org-left">+</td>
<td class="org-left">-</td>
<td class="org-left">-</td>
</tr>

<tr>
<td class="org-left">Expert</td>
<td class="org-left">+</td>
<td class="org-left">+</td>
<td class="org-left">-</td>
</tr>

<tr>
<td class="org-left">Database</td>
<td class="org-left">+</td>
<td class="org-left">+</td>
<td class="org-left">+</td>
</tr>
</tbody>
</table>
</div>
</li>

<li><a id="orgde3f2c7"></a>SubGrapRAG - original paper <a id="org6b9489d"></a><br />
<div class="outline-text-5" id="text-14-28-13-2">
<p>
GRAG 2405.16506v1.pdf
</p>

<p>
terms:
</p>
<ul class="org-ul">
<li>topoligical information - graph</li>
<li>textual information - text</li>
<li>hierarchical description - textual graph + graph embedding (Graph token generator)</li>
<li>textual graph - text of graph - text attributed nodes and edges presented as a single text.</li>
<li>graph of text - embeddings</li>
<li>Graph embedding - embeddings for every k-hop ego-graph stored in database and retrived by calculating
simularity with the question embedding.</li>
</ul>

<p>
divide-and-conquer strategy
</p>

<p>
indexes k-hop ego-graphs
</p>

<p>
a soft pruning mechanism - to reduce irrelevant entities
</p>

<p>
trade-off between the number of candidates and computational intensity
</p>

<p>
steps:
</p>
<ol class="org-ol">
<li>k-hop ego-graphs within the main textual graph are indexed and converted into graph embeddings (pre-rained
LLM)
<ul class="org-ul">
<li>offline - encode the neighborhood surrounding each node (testual graph).</li>
<li>text attibutes of node and edges to embeddings</li>
<li>mean pooling operation on embeddings to calc graph embedding. [textual subgraph indexing]</li>
</ul></li>
<li>retrive topN textual subgraphs most relevant to query - cosine simularity - [textual subgraph ranking]</li>
<li>soft pruning by learning scaling factors based on their relevance to the query.
<ol class="org-ol">
<li>LLM process node and edge attibute in graph (not central node) to embedding</li>
<li>compare embeddings - element-wide distance to node attribute scaling factor? and edge attribute scaling factor?</li>
<li>prune graph - assign weights nodes and edges. - [textual subgraph pruning]</li>
</ol></li>
<li>prompt to LLM: pruned textual subgraphs + query
<ul class="org-ul">
<li>GNN - align graph embeddings with LLM text vectors</li>
<li>LLM1 text embedder - input: textual graph + question</li>
<li>LLM2 attanetion layers - LLM1 hidden state + Graph Token Generator (Graph Encoder of prunned graphs)</li>
<li>graph embedding (Graph token generator)</li>
</ul></li>
</ol>

<p>
prompt two parts:
</p>
<dl class="org-dl">
<dt>hard prompts</dt><dd>hierchical text descriptions + query</dd>
<dt>soft prompts</dt><dd>graph's topological information - embeddings (by graph encoder model)</dd>
</dl>
</div>
</li>

<li><a id="org42ff6aa"></a>MetaAI G-Retriever<br />
<div class="outline-text-5" id="text-14-28-13-3">
<p>
2402.07630v3.pdf
</p>

<p>
Graph pruning - Prize Collecting Steiner Tree (PCST)
</p>
<ul class="org-ul">
<li>higher weight for nodes and lower for edges</li>
</ul>

<p>
Textualized graph:
node<sub>id,node</sub><sub>attr</sub>
2, name: computer
3, name: person
15, name: woman
src, edge<sub>attr</sub>, dst
15, to the right of, 3
2, in front of, 3
</p>

<p>
steps:
</p>
<ol class="org-ol">
<li>Indexer:
<ul class="org-ul">
<li>text embeddings of <b>text attibutes</b> of edges and nodes kept separately in database: SentenceBert encoder</li>
<li>Full text of node kept separately.</li>
</ul></li>
<li>Retriever:
<ul class="org-ul">
<li>embedding of question - k-nearest neighbords cosine simularity - V nodes, E edges</li>
</ul></li>
<li>Subgraph construction - select top endges and nodes - limit count of them to limit size of graph.</li>
<li>Textualized</li>
<li>LLM self attention inputs: graph encoder + LLM embeddings (textualized graph + query) (LLama2, AdamW
lr=1e-5 wdecay=0.05, batch size 4, 10 epochs, early stopping with patience of 2 epochs)</li>
</ol>

<p>
Prompt optimization techniques:
</p>
<ul class="org-ul">
<li>Zero-CoT: appending the “Let's think step by step.” to the end of a question.</li>
<li>CoT-BAG: “Let's construct a graph with the nodes and edges first.” after textual graph given.</li>
<li>PrompTing (KAPING): two steps: first retrieves the relevant facts in the knowledge graph from the entities in
the question and then qugment them to the prompt. Texutal graph: Below are the facts that might be relevant
to answer the question: (head entitiy, relation, tail entity) &#x2026; Question: &#x2026;
<ul class="org-ul">
<li><a href="https://aclanthology.org/2023.nlrse-1.7/">https://aclanthology.org/2023.nlrse-1.7/</a></li>
</ul></li>
</ul>
</div>
</li>
<li><a id="orgb8144fb"></a>microsoft Graph RAG for summarization <a id="org79bfe4a"></a><br />
<div class="outline-text-5" id="text-14-28-13-4">
<p>
2404.16130v1.pdf
<a href="https://github.com/microsoft/graphrag">https://github.com/microsoft/graphrag</a>
</p>

<p>
RAG used for query-focused summarization (QFS) and can not answer to question: “What are the main themes in
 the dataset?”
</p>

<p>
tradeoff - one main community description in prompt or many prunned communities.
</p>

<p>
Solution:
</p>
<ol class="org-ol">
<li>entity KG from documents - knowledge triples (subject, predicate, object)</li>
<li>pregenerate community summaries for all groups of closely-related entities</li>
</ol>

<p>
steps:
</p>
<ol class="org-ol">
<li>Indexing
<ul class="org-ul">
<li>text extraction and chinking -&gt; Text chunks</li>
<li>domain-tailored summarization -&gt; Element Instances
<ul class="org-ul">
<li>graph nodes for each chunk and relationships - single list of tuples</li>
<li>prompt - 1) identify “entities” in text with name, type and description. 2) identify all relationships
between clearly-related entities (source, target, desciption of relationship) 3) output to a single list of tuples.</li>
</ul></li>
<li>element summaries -  all collected tuples we use as prompt to LLM-&gt; tuple + description (“named entities”)
<ul class="org-ul">
<li>also extract “claims” - subject, object, type, description, source text span, start and end dates.</li>
</ul></li>
</ul></li>
<li>Query
<ul class="org-ul">
<li>community detection - search in database -&gt; Graph communities
<ul class="org-ul">
<li>(homogenneous undirected weighted graph, edge weights representing the normalized counts of detected
relationship instances)</li>
<li>(get large communities “Leiden”)</li>
</ul></li>
<li>domain-tailored summarization - prompt: descriptions of the source node, target node, linked covariates
[-&gt; Community summaries]
<ul class="org-ul">
<li>leaf-level communities than higher-level communities. shorted first.</li>
</ul></li>
<li>query-focused summarization -&gt; Community Answers
<ol class="org-ol">
<li>random community summaries in chunks</li>
<li>generate intermediate answers in parallel, one for each chunk + asked for score 0-100 indicated how
helpfull the generated answer is in answered the target question. 0 scored filtered.</li>
<li>intermediate community answers in descending order and added into a new context window.</li>
</ol></li>
<li>Global answer</li>
</ul></li>
</ol>


<p>
“Leiden” community detection algorithm to partition the graph into communities of nodes
</p>
<ul class="org-ul">
<li><a href="https://arxiv.org/abs/1810.08473">https://arxiv.org/abs/1810.08473</a></li>
<li><a href="https://github.com/vtraag/louvain-igraph">https://github.com/vtraag/louvain-igraph</a></li>
<li><a href="https://github.com/taynaud/python-louvain">https://github.com/taynaud/python-louvain</a></li>
<li><a href="https://packages.gentoo.org/packages/dev-python/graph-tool">https://packages.gentoo.org/packages/dev-python/graph-tool</a></li>
<li><a href="https://packages.gentoo.org/packages/sci-libs/metis">https://packages.gentoo.org/packages/sci-libs/metis</a></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org9a98e21" class="outline-4">
<h4 id="org9a98e21"><span class="section-number-4">14.28.14.</span> Contriver - contrastive retriver</h4>
<div class="outline-text-4" id="text-14-28-14">
<p>
contrastive learning - Unsupervised training on unaligned documents.
</p>
<ul class="org-ul">
<li>A contrastive loss is used to learn by discriminating between documents.</li>
<li>loss compares either positive (from the same document) or negative (from diﬀerent documents) pairs of
document representations.</li>
</ul>

<p>
bi-encoders - representations of queries and documents are computed independently. - USED HERE.
</p>
<ul class="org-ul">
<li>cons: because they are different vectors prevent the model to capture ﬁne-grained interactions between terms.</li>
</ul>

<p>
cross-encoder -  jointly encodes queries and document.
</p>
<ul class="org-ul">
<li>cons: requrie re-encoding every document for any new query and hence does not scale to large collections of
documents.</li>
<li>used for re-ranked retrieved documents.</li>
</ul>

<p>
transformer network embed both queries and documents independently. Revelvance score is dot product of embeds..
</p>


<p>
Links:
</p>
<ul class="org-ul">
<li><a href="https://github.com/facebookresearch/contriever">https://github.com/facebookresearch/contriever</a></li>
<li><a href="https://arxiv.org/pdf/2112.09118">https://arxiv.org/pdf/2112.09118</a></li>
<li>438 MB pyTorch model <a href="https://huggingface.co/facebook/contriever/tree/main">https://huggingface.co/facebook/contriever/tree/main</a></li>
</ul>

<p>
AriGraph agent with Contriver.
</p>
<ul class="org-ul">
<li><a href="https://github.com/AIRI-Institute/AriGraph">https://github.com/AIRI-Institute/AriGraph</a></li>
<li><a href="https://arxiv.org/pdf/2407.04363">https://arxiv.org/pdf/2407.04363</a></li>
</ul>
</div>
</div>

<div id="outline-container-org87117cc" class="outline-4">
<h4 id="org87117cc"><span class="section-number-4">14.28.15.</span> SBERT and sentence transformers</h4>
<div class="outline-text-4" id="text-14-28-15">
<p>
For Semantic Search, Clustering, Similarity Comparison.
</p>

<p>
SBERT - BERT-like model that is adapted to matching sentence embedding
</p>
<ul class="org-ul">
<li>requires aligned pairs of sentences to form positive pairs while we propose to use data augmentation to
leverage large unaligned textual corpora.</li>
<li>siamese architecture. Every sentences goes in parallet to BERT transformer. They tested (mean, max and
[CLS]) pooling and mean pooling performed best.</li>
</ul>

<p>
Before was <b>mean pooling</b> of  BERT word vectors (as it produce word vectors) - poor quality.
</p>

<p>
<a href="https://www.sbert.net/index.html">https://www.sbert.net/index.html</a>
</p>
<ul class="org-ul">
<li>Python module for accessing, using, and training state-of-the-art text and image embedding models.</li>
</ul>

<p>
<a href="https://huggingface.co/models?library=sentence-transformers">https://huggingface.co/models?library=sentence-transformers</a>
</p>

<p>
article <a href="https://www.marqo.ai/course/introduction-to-sentence-transformers">https://www.marqo.ai/course/introduction-to-sentence-transformers</a>
</p>

<p>
SBERT <a href="https://arxiv.org/abs/1908.10084">https://arxiv.org/abs/1908.10084</a>
</p>
</div>
</div>

<div id="outline-container-orgea59377" class="outline-4">
<h4 id="orgea59377"><span class="section-number-4">14.28.16.</span> Reasoning on KG</h4>
<div class="outline-text-4" id="text-14-28-16">
<p>
Decoding on Graphs: Faithful and Sound Reasoning on Knowledge Graphs through Generation of Well-Formed Chains
 <a href="https://arxiv.org/pdf/2410.18415">https://arxiv.org/pdf/2410.18415</a>
</p>
<ul class="org-ul">
<li>Existing research on the utiliza tion of KG for large language models (LLMs) prevalently relies on subgraph
retriever or iterative prompting.</li>
</ul>

<p>
s
</p>
<ul class="org-ul">
<li>DoG (Decoding on Graphs) - a trainingfree approach, is able to provide faithful and sound reasoning
trajectories grounded on the KGs.  by enforcing graphaware constrained decodingand beam search execution.</li>
<li id="well-formed chain">a sequence of interrelated fact triplets on the KGs, starting from question entities and
leading to answers.</li>
<li id="graph-aware constrained decoding">constraint derived from the topology of the KG regulates the decoding
process of the LLMs. Same to beam decoding <a href="https://arxiv.org/pdf/2402.10200">https://arxiv.org/pdf/2402.10200</a> but on graph.</li>
</ul>
</div>
</div>

<div id="outline-container-org45ba7bd" class="outline-4">
<h4 id="org45ba7bd"><span class="section-number-4">14.28.17.</span> GNN-RAG - GNN + LLM for RAG-based KGQA</h4>
<div class="outline-text-4" id="text-14-28-17">
<p>
Steps:
</p>
<ol class="org-ol">
<li>GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question.
<ul class="org-ul">
<li>as a dense subgraph reasoner.</li>
<li>ability to handle complex graph interactions and answer multi-hop questions.</li>
</ul></li>
<li>Second, the shortest paths in the KG that connect question entities and GNN-based answers are extracted to represent useful KG reasoning paths.</li>
<li>The extracted paths are verbalized and given as input for LLM reasoning with RAG.</li>
</ol>

<p>
existing KGQA methods.
LLM-based ToG
LLM-based RoG
GNN-based KGQA
</p>


<p>
<a href="https://arxiv.org/pdf/2405.20139">https://arxiv.org/pdf/2405.20139</a>
</p>
</div>
</div>



<div id="outline-container-org56fbb09" class="outline-4">
<h4 id="org56fbb09"><span class="section-number-4">14.28.18.</span> reranking model or cross-encoder - A two-stage retrieval system.</h4>
<div class="outline-text-4" id="text-14-28-18">
<p>
Why?
</p>
<ol class="org-ol">
<li>Rerankers are much more accurate than embedding models.</li>
<li>embedding models don't have context to better understand query.</li>
</ol>

<p>
input: query and document pair, output: a similarity score.
</p>

<p>
<a href="https://www.pinecone.io/learn/series/rag/rerankers/">https://www.pinecone.io/learn/series/rag/rerankers/</a>
</p>
</div>
</div>
<div id="outline-container-orgaf78970" class="outline-4">
<h4 id="orgaf78970"><span class="section-number-4">14.28.19.</span> prompt</h4>
<div class="outline-text-4" id="text-14-28-19">
<div class="org-src-container">
<pre class="src src-perl">You are a helpful assistant that can analyse the knowledge graphs in the contexts and then answer the questions based on the knowledge
graphs.
The answers should give the grounded reasoning chains and think step by step, and the reasoning chains should be logically complete but
have as fewer steps as possible. Do not include information irrelvant to the question.
**<span style="color: #cae682;">Example</span> 1:**
<span style="color: #e5786d; font-weight: bold;">Context</span>: [ Bahamas -&gt; location.country.first_level_divisions -&gt; Grand Cay | Grand Bahama -&gt; location.location.containedby -&gt; Ba-
hamas | Bahamas -&gt; location.location.contains -&gt; Grand Cay | Bahamas -&gt; location.location.contains -&gt; Grand Bahama | Grand
Cay -&gt; location.location.containedby -&gt; Bahamas | Bahamas -&gt; location.country.first_level_divisions -&gt; East Grand Bahama | Ba-
hamas -&gt; location.country.first_level_divisions -&gt; West Grand Bahama | Grand Bahama -&gt; location.location.contains -&gt; Grand Ba-
hama International Airport | Bahamas -&gt; location.location.contains -&gt; East Grand Bahama | Bahamas -&gt; location.location.contains
-&gt; West Grand Bahama | East Grand Bahama -&gt; location.location.containedby -&gt; Bahamas | Bahamas -&gt; location.location.contains
-&gt; Grand Bahama International Airport | Grand Bahama -&gt; location.location.people_born_here -&gt; Hubert Ingraham | Grand Cay -&gt;
location.administrative_division.first_level_division_of -&gt; Bahamas | Bahamas -&gt; location.country.administrative_divisions -&gt; Cat Island,
Bahamas | Bahamas -&gt; location.country.administrative_divisions -&gt; Long Island | West Grand Bahama -&gt; location.location.containedby -&gt;
Bahamas | Bahamas -&gt; location.country.capital -&gt; Nassau | Bahamas -&gt; location.country.administrative_divisions -&gt; Inagua | Bahamas -&gt;
location.country.administrative_divisions -&gt; Exuma | Grand Bahama International Airport -&gt; location.location.containedby -&gt; Bahamas |
Grand Bahama -&gt; location.location.people_born_here -&gt; Juan Lewis | Grand Bahama -&gt; location.location.contains -&gt; West End Airport ]
<span style="color: #e5786d; font-weight: bold;">Question</span>: What country is the grand bahama island in?
<span style="color: #e5786d; font-weight: bold;">Answer</span>: Let&#8217;s break down the steps to find the answer to the question.
1. &lt; Grand Bahama -&gt; location.location.containedby -&gt; Bahamas &gt; This tells us Grand Bahama is located in Bahamas.
Grand Bahama is in Bahamas. Therefore, the answer is * Bahamas.
**<span style="color: #cae682;">Example</span> 1:**
<span style="color: #e5786d; font-weight: bold;">Context</span>: [ William Shakespeare -&gt; people.person.profession -&gt; Playwright | William Shakespeare -&gt; people.person.profession -&gt; Poet |
William Shakespeare -&gt; base.kwebbase.kwtopic.has_sentences -&gt; By the time these works were published in 1609, Shakespeare was
an acknowledged master of drama and an established country gentleman. | William Shakespeare -&gt; people.person.profession -&gt; Actor |
William Shakespeare -&gt; people.person.profession -&gt; Author | William Shakespeare -&gt; people.person.profession -&gt; Lyricist | In the 21
years between 1592 and 1613, Shakespeare produced more than 30 plays. -&gt; base.kwebbase.kwsentence.previous_sentence -&gt; Above all,
his humanity spanned all classes and circumstances ]
<span style="color: #e5786d; font-weight: bold;">Question</span>: What did William Shakespeare <span style="color: #8ac6f2; font-weight: bold;">do</span> <span style="color: #8ac6f2; font-weight: bold;">for</span> a living?
<span style="color: #e5786d; font-weight: bold;">Answer</span>: Let&#8217;s break down the steps to find the answer to the question.
1. &lt; William Shakespeare -&gt; people.person.profession -&gt; Playwright &gt; This tells us William Shakespeare is was playwright.
2. &lt; William Shakespeare -&gt; people.person.profession -&gt; Poet &gt; This tells us William Shakespeare was a poet.
William Shakespeare was a playwright, and poet. Therefore, the answer is * playwright, and * poet.
**<span style="color: #cae682;">Example</span> 3:**
<span style="color: #e5786d; font-weight: bold;">Context</span>: [ Carlton the Bear -&gt; sports.mascot.team -&gt; Toronto Maple Leafs | Toronto Maple Leafs -&gt; sports.sports_team.team_mascot -&gt;
Carlton the Bear | Carlton the Bear -&gt; common.topic.notable_types -&gt; Mascot | Mascot -&gt; type.type.properties -&gt; Team | Toronto Maple
Leafs -&gt; sports.sports_team.previously_known_as -&gt; Toronto St. Patricks | Team -&gt; type.property.master_property -&gt; Team Mascot |
Toronto Maple Leafs -&gt; sports.sports_team.previously_known_as -&gt; Toronto Arenas | m<span style="color: #95e454;">.0crt465 -&gt; sports.</span>sports_league_participation.team
-&gt; Toronto Maple Leafs | Toronto St. Patricks -&gt; sports.defunct_sports_team.later_known_as -&gt; Toronto Maple Leafs | Toronto Maple
Leafs -&gt; sports.sports_team.sport -&gt; Ice Hockey | Toronto St. Patricks -&gt; sports.sports_team.sport -&gt; Ice Hockey | Toronto Arenas -&gt;
sports.defunct_sports_team.later_known_as -&gt; Toronto Maple Leafs | Toronto -&gt; sports.sports_team_location.teams -&gt; Toronto Maple
Leafs | Toronto Maple Leafs -&gt; sports.sports_team.location -&gt; Toronto ]
<span style="color: #e5786d; font-weight: bold;">Question</span>: What is the sport played by the team with a mascot known as Carlton the Bear?
<span style="color: #e5786d; font-weight: bold;">Answer</span>: Let&#8217;s break down the steps to find the answer to the question.
1. &lt; Carlton the Bear -&gt; sports.mascot.team -&gt; Toronto Maple Leafs &gt; This tells us Carlton the Bear is the mascot of the team Toronto
Maple Leafs.
2. &lt; Toronto Maple Leafs -&gt; sports.sports_team.sport -&gt; Ice Hockey &gt; This tells us Toronto Maple Leafs plays Ice Hockey.
Carlton the Bear is the mascot of the team Toronto Maple Leafs which plays Ice Hockey. Therefore, the answer is * Ice Hockey.
**<span style="color: #cae682;">Example</span> 4:**
<span style="color: #e5786d; font-weight: bold;">Context</span>: [ {graph} ]
<span style="color: #e5786d; font-weight: bold;">Question</span>: {question}
<span style="color: #e5786d; font-weight: bold;">Answer</span>: Let&#8217;s break down the steps to find the answer to the question.
</pre>
</div>
</div>
</div>
<div id="outline-container-org42e07a7" class="outline-4">
<h4 id="org42e07a7"><span class="section-number-4">14.28.20.</span> open source RAGs</h4>
<div class="outline-text-4" id="text-14-28-20">
<ul class="org-ul">
<li><a href="https://github.com/Cinnamon/kotaemon">https://github.com/Cinnamon/kotaemon</a></li>
<li><a href="https://github.com/khoj-ai/khoj">https://github.com/khoj-ai/khoj</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org20bab2d" class="outline-3">
<h3 id="org20bab2d"><span class="section-number-3">14.29.</span> Articles and Research automation</h3>
<div class="outline-text-3" id="text-14-29">
</div>
<div id="outline-container-orgec0ff6a" class="outline-4">
<h4 id="orgec0ff6a"><span class="section-number-4">14.29.1.</span> <a href="https://github.com/SakanaAI/AI-Scientist">https://github.com/SakanaAI/AI-Scientist</a></h4>
<div class="outline-text-4" id="text-14-29-1">
<p>
<a href="https://arxiv.org/pdf/2408.06292">https://arxiv.org/pdf/2408.06292</a>
</p>

<p>
Steps:
</p>
<ul class="org-ul">
<li>Idea Generation</li>
<li>Experiment Iteration</li>
<li>Paper Write-Up</li>
</ul>
</div>
</div>
<div id="outline-container-org0e47ed7" class="outline-4">
<h4 id="org0e47ed7"><span class="section-number-4">14.29.2.</span> <a href="https://github.com/stanford-oval/storm">https://github.com/stanford-oval/storm</a></h4>
<div class="outline-text-4" id="text-14-29-2">
<p>
Roles: Storm - Outlines + simulated conversaton + seeking articles
</p>
<ul class="org-ul">
<li>stanford-oval <a href="https://github.com/stanford-oval">https://github.com/stanford-oval</a>
Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking</li>
<li><a href="https://github.com/stanford-oval/storm">https://github.com/stanford-oval/storm</a></li>
<li>challenges at the <b>pre-writing</b> stage <a href="https://arxiv.org/abs/2402.14207">https://arxiv.org/abs/2402.14207</a></li>
</ul>

<p>
Prompt: 30 questins about the topic.
</p>

<p>
Perspective-Guided Question Asking -
</p>

<p>
stages:
</p>
<ol class="org-ol">
<li>Ask 30 questions about the given topic for one who question.</li>
<li></li>

<li>survey related articles by question and discovering diverse <b>perspectives</b> in researching the given topic
<ol class="org-ol">
<li>perspective is the table of content of related topics stored in context for prompt.</li>
</ol></li>
<li>simulating conversations where writers carrying different perspectives pose questions to a topic expert
grounded on trusted Internet sources. (M rounds)</li>
<li>collect conversations from different perspectives + draft outline (only by topic)
<ul class="org-ul">
<li>we add link to source in answer of every convers. question.</li>
</ul></li>
<li>curating the collected information to create an outline
<ul class="org-ul">
<li>prompt: topic + draft outline + simulated conversations - to refine outline.</li>
</ul></li>
<li>full article composed section by section. Prompt: section title and the headings of its all-level
subsections to retrive relevant documents from links based on simularity calculated Sentence-BERT embeddings.</li>
</ol>


<p>
steps of pre-writing stage
</p>
<ol class="org-ol">
<li>Related articles</li>
<li>Identify perspectives (P - points of view)</li>
<li>Read &amp; Ask: Wiki writed (ask) -&gt; Expert</li>
<li>Split queries (EXPERT)</li>
<li>Search &amp; Sift (EXPERT)</li>
<li>Synthesize (EXPERT)</li>
<li>Direct generate &#x2026;&#x2026; converstations</li>
<li>Outline</li>
</ol>
<p>
<a href="https://arxiv.org/pdf/2402.14207">https://arxiv.org/pdf/2402.14207</a>
</p>
</div>
</div>
</div>

<div id="outline-container-org13fe032" class="outline-3">
<h3 id="org13fe032"><span class="section-number-3">14.30.</span> RAG-пайплайн or framework</h3>
<div class="outline-text-3" id="text-14-30">
<p>
Retrieval-Augmented Generation (RAG)
</p>
<ul class="org-ul">
<li><a href="https://arxiv.org/abs/2005.11401">https://arxiv.org/abs/2005.11401</a> 2020 Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</li>
<li>rag <a href="https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7">https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7</a></li>
<li><a href="https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2">https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2</a></li>
<li>Personalized - open source <a href="https://github.com/mem0ai/mem0">https://github.com/mem0ai/mem0</a></li>
</ul>

<p>
RAG - reduce factual errors. relevant candidates retiving on text similarity.
</p>
<ul class="org-ul">
<li>LLM input: user question prompt + data retrived from external knowledge sources.</li>
</ul>

<p>
Problem:
</p>
<ul class="org-ul">
<li>sematically similar but irrelevant documents. LLM is parametric, we need non-parametric algorithm.</li>
<li>RAG used for query-focused summarization (QFS) and can not answer to question: “What are the main themes in
the dataset?”</li>
<li>trade-off between the number of candidates and computational intensity</li>
<li>indexing and question steps can be very different.</li>
<li>After vectorizing document there is little analysis of what information is lost versus preserved, and how it
affects downstream tasks.</li>
<li>do not transfer well to new applications with no training data, and are outperformed by unsupervised
term-frequency methods such as BM25.</li>
</ul>

<p>
LLM should have some knowledge in KG domain and KG should provide knowledge.
</p>

<p>
*Enhance Large Language Models (LLMs) enabling the
</p>

<p>
RAG - It combines a <b>retriever system</b>, which fetches relevant document snippets from a large corpus, and an
 LLM, which produces answers using the information from those snippets.
</p>
</div>
<div id="outline-container-orga2147ce" class="outline-4">
<h4 id="orga2147ce"><span class="section-number-4">14.30.1.</span> Steps:</h4>
<div class="outline-text-4" id="text-14-30-1">
<ol class="org-ol">
<li>Text Encoding, llm models (task feature extraction)</li>
<li>Indexing: ingesting data from a source and indexing it. This usually happens offline.</li>
<li>Query Encoding - query is encoded as a vector (model of 1)</li>
<li>Similarity Retrieval</li>
<li>ReRange - pruning of retrival</li>
<li>Retrieval</li>
<li>Generation - LLM gets query + found context to finetune.</li>
</ol>
</div>
</div>

<div id="outline-container-org313975a" class="outline-4">
<h4 id="org313975a"><span class="section-number-4">14.30.2.</span> Original paper</h4>
<div class="outline-text-4" id="text-14-30-2">
<p>
ﬁne-tune end-to-end:
</p>
<ul class="org-ul">
<li>Pre-trained <b>Retriever</b> (Query Encoder + Document Index)
<ul class="org-ul">
<li>ﬁnd the top-K documents</li>
<li>non-parametric memory - <b>nonparametric</b> - use collected data to make assumption about new data.</li>
</ul></li>
<li>Pre-trained <b>LLM</b>
<ul class="org-ul">
<li>parametric memory - change internal weights</li>
<li>simply concatenate documents and input</li>
</ul></li>
</ul>

<p>
Parametric Machine Learning Algorithms - linear regression: b0 + b1*x1 + b2*x2 = 0
</p>

<p>
Nonparametric - k-Nearest Neighbors
</p>

<p>
Dataset:
</p>
<ul class="org-ul">
<li>ExplaGraphs <a href="https://aclanthology.org/2021.emnlp-main.609.pdf">https://aclanthology.org/2021.emnlp-main.609.pdf</a>
<ul class="org-ul">
<li><a href="https://github.com/swarnaHub/ExplaGraphs">https://github.com/swarnaHub/ExplaGraphs</a></li>
<li><a href="https://explagraphs.github.io/">https://explagraphs.github.io/</a></li>
</ul></li>
<li>WebQSP</li>
</ul>

<p>
steps:
</p>
<ol class="org-ol">
<li></li>
</ol>
</div>
</div>

<div id="outline-container-orgb71421d" class="outline-4">
<h4 id="orgb71421d"><span class="section-number-4">14.30.3.</span> RAG python inference</h4>
<div class="outline-text-4" id="text-14-30-3">
<pre class="example">
docs = retriver.get_relevant_documents(question)
</pre>

<pre class="example">
context = "\n\n".join(doc.page_content for doc in docs)
</pre>

<pre class="example">
prompt_val = prompt.invoke({"context": context, "question": questions})
</pre>

<pre class="example">
result = llm(prompt_val.to_message())
</pre>
</div>
</div>
</div>

<div id="outline-container-org153ec6f" class="outline-3">
<h3 id="org153ec6f"><span class="section-number-3">14.31.</span> tools</h3>
<div class="outline-text-3" id="text-14-31">
<dl class="org-dl">
<dt>pipeline orchestration</dt><dd>LangChain, LlamaIndex, Langraph, LangSmith</dd>
<dt>vector databases</dt><dd>Weaviate, Pinecone, Milvus, Postgres, Elasticsearch</dd>
<dt>monitoring of LLMs</dt><dd>OpenTelemetry используется под капотом, OpenLLMetry</dd>
</dl>
</div>
</div>
<div id="outline-container-org45dd0e0" class="outline-3">
<h3 id="org45dd0e0"><span class="section-number-3">14.32.</span> vector database <a id="org2698079"></a></h3>
<div class="outline-text-3" id="text-14-32">
<p>
see <a href="bi#MissingReference">bi#MissingReference</a>
</p>

<p>
links collection <a href="https://github.com/currentslab/awesome-vector-search">https://github.com/currentslab/awesome-vector-search</a>
</p>

<p>
article <a href="https://medium.com/codex/a-brief-comparison-of-vector-databases-e194dedb0a80">https://medium.com/codex/a-brief-comparison-of-vector-databases-e194dedb0a80</a>
</p>
<ul class="org-ul">
<li>self-host</li>
<li>Aggregations - filter by various criteria, average distances</li>
<li>support for distances</li>
<li>Clustering: Ability to group similar vectors automatically, useful for data exploration and analysis.</li>
</ul>

<p>
some
</p>
<dl class="org-dl">
<dt>Weaviate</dt><dd>vector database (<a href="https://weaviate.io/">https://weaviate.io/</a>) - Open Source, Go,  GraphQL query language for more complex
queries and data retrieved (data attached to the vectors). supports the Hierarchical Navigable Small Worlds
indexing method. Knewledge graph creation - good for recommendations or inf. retrival systems.</dd>
<dt>Pinecone</dt><dd>vector database. ( cloud-native only, commercial) “hybrid search” based on <b>sparse embeddings</b>
(such as TF/IDF from keywords) and <b>dense embeddings</b> (like LLMs). does not offer Exact Nearest Neighbor
search. Support parallel upsert. General purpose not only NLP. no Exact Nearest Neighbor search</dd>
<dt>Milvus</dt><dd>vector database. indexes using L2 not cosine distance. the fine-grained scalability.</dd>
<dt>Elasticsearch</dt><dd>Search engine</dd>
<dt>Postgres</dt><dd>Exact Nearest Neighbor search. cheapest.</dd>
<dt>Chroma</dt><dd>AI-native open-source embedding database</dd>
<dt>Qdrant</dt><dd>vector similarity engine</dd>
<dt>LlamaIndex</dt><dd>data framework</dd>
</dl>

<p>
vector database:
</p>
<ul class="org-ul">
<li>pgvector SQL extension and <a href="https://github.com/pgvector/pgvector-python">https://github.com/pgvector/pgvector-python</a></li>
<li>Elasticsearch vs. Pinecone vs. Weaviate  <a href="https://db-engines.com/en/system/Elasticsearch%3BPinecone%3BWeaviate">https://db-engines.com/en/system/Elasticsearch%3BPinecone%3BWeaviate</a></li>
<li>LanceDB <a href="https://github.com/lancedb/lancedb">https://github.com/lancedb/lancedb</a></li>
</ul>
<p>
upsert is a portmanteau – a combination of the words “update” and “insert.”
</p>
<ul class="org-ul">
<li>marqo - for both text and images <a href="https://github.com/marqo-ai/marqo">https://github.com/marqo-ai/marqo</a></li>
</ul>

<p>
Pinecone supports namespaced data, Weaviate uses classes for different types of data.
</p>

<p>
FAISS, short for Facebook AI Similarity Search - efficient similarity search and clustering of dense vectors.
</p>
<ul class="org-ul">
<li>Indexing - most simular</li>
<li>Similarity Search - the most similar vectors</li>
<li>Scoring and Ranking - calculate similarity scores between the query vector and the stored vectors. Very
important, because LLM accept only a certain number of tokens as input.</li>
<li>Efficient Storage - quantization and compression</li>
<li>scalling - distributed systems and parallel processing</li>
</ul>

<p>
"FAISS vector store": FAISS + vector store = robust foundation
</p>

<p>
SCaNN google fast simularity search
<a href="https://github.com/google-research/google-research/tree/master/scann">https://github.com/google-research/google-research/tree/master/scann</a>
</p>

<p>
steps:
</p>
<ul class="org-ul">
<li>determining context using vector memory (most relevant context using FAISS)</li>
<li>using a language model to generate a response (context along with our question)</li>
</ul>

<p>
Columnar storage is better for training than CSV files - better compression
</p>

<p>
<a href="https://blog-b1--systems-de.translate.goog/mit-proprietaren-daten-trainierte-ki-chat-anwendung?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US&amp;_x_tr_pto=wapp">https://blog-b1--systems-de.translate.goog/mit-proprietaren-daten-trainierte-ki-chat-anwendung?_x_tr_sl=auto&amp;_x_tr_tl=en&amp;_x_tr_hl=en-US&amp;_x_tr_pto=wapp</a>
</p>
</div>
<div id="outline-container-org354a8a3" class="outline-4">
<h4 id="org354a8a3"><span class="section-number-4">14.32.1.</span> internal implementation</h4>
<div class="outline-text-4" id="text-14-32-1">
<ul class="org-ul">
<li>Approximate Nearest Neighbor (ANN) indexing
<ul class="org-ul">
<li>locality-sensitive hashing (LSH), product quantization (PQ), or hierarchical navigable small world graphs (HNSW)</li>
</ul></li>
<li>Tree-based Indexing - k-d trees or ball trees to partition the vector space and facilitate efficient nearest neighbor search.</li>
<li>Compression and Quantization - Reducing the dimensionality and precision of vector embeddings, sparce vectors</li>
<li>Parallelization and Distributed Processing</li>
</ul>
</div>
</div>
<div id="outline-container-orgace62e2" class="outline-4">
<h4 id="orgace62e2"><span class="section-number-4">14.32.2.</span> lmdb vs redis vs redict</h4>
<div class="outline-text-4" id="text-14-32-2">
<p>
<a href="https://git.openldap.org/openldap/openldap/">https://git.openldap.org/openldap/openldap/</a>
</p>
<ul class="org-ul">
<li>LMDB provides optional disk persistence, while Redis is an in-memory database with no built-in persistence
mechanism.</li>
<li>LMDB is a simple key-value store, while Redis supports a variety of data structures (strings, hashes, lists,
sets, maps).</li>
<li>LMDB’s design is more focused on concurrent access and transactional consistency.</li>
</ul>

<p>
Redict is an independent fork of Redis®1 OSS 7.2.4 licensed under the Lesser GNU General Public license
 (LGPL-3.0-only).
</p>

<p>
Qdrant, Redis, and Weaviate
</p>
<ul class="org-ul">
<li>Weaviate - fast vector search - so graph database will not be required</li>
</ul>
</div>
</div>
<div id="outline-container-org5226218" class="outline-4">
<h4 id="org5226218"><span class="section-number-4">14.32.3.</span> sqlite vs Redis vs Clickhouse</h4>
<div class="outline-text-4" id="text-14-32-3">
<p>
<a href="https://github.com/asg017/sqlite-vec">https://github.com/asg017/sqlite-vec</a>
</p>
<ul class="org-ul">
<li>in-memory - create new database.</li>
<li>may backup to file, or use other mechanic for in-memory</li>
<li>Manual handling of vector storage and search logic might get complex with evolving requirements.</li>
</ul>

<p>
Clickhouse <a href="https://clickhouse.com/docs/knowledgebase/vector-search">https://clickhouse.com/docs/knowledgebase/vector-search</a>
</p>
<ul class="org-ul">
<li>uses all available system resources to process queries efficiently.</li>
</ul>

<p>
Redis <a href="https://redis.io/docs/latest/develop/get-started/vector-database/">https://redis.io/docs/latest/develop/get-started/vector-database/</a>
</p>
<ul class="org-ul">
<li>uses advanced algorithms like HNSW and IVFFLat for fast vector searches.</li>
<li><a href="https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/vectors/">https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/vectors/</a></li>
<li>banchmark <a href="https://redis.io/blog/benchmarking-results-for-vector-databases/">https://redis.io/blog/benchmarking-results-for-vector-databases/</a>
<ul class="org-ul">
<li>weaviate second</li>
</ul></li>
</ul>
</div>
</div>
<div id="outline-container-orgf982c62" class="outline-4">
<h4 id="orgf982c62"><span class="section-number-4">14.32.4.</span> Elasticsearch vs edgedb vs taxi vs Chroma vs pgvector vs VQLite vs Weaviate</h4>
<div class="outline-text-4" id="text-14-32-4">
<p>
app-misc/elasticsearch
</p>
<ul class="org-ul">
<li>PROS
<ul class="org-ul">
<li>Hybrid search with text+vector</li>
<li>popular choice</li>
</ul></li>
<li>may be too exhaustive when storing vast numbers of vectors.</li>
<li>Also only supports dot product, L2norm and cosine distance algorithms. ie: no hamming distance, unlike milvus, OpenSearch.</li>
<li>require Java JRE.</li>
<li>vector search up to 1024 dimensions, which may not be sufficient for some use cases</li>
</ul>

<p>
EdgeDB - Built on top of PostgreSQL
</p>
<ul class="org-ul">
<li><a href="https://github.com/edgedb/edgedb">https://github.com/edgedb/edgedb</a></li>
<li>Optimized for write-heavy scenarios,</li>
</ul>

<p>
<a href="https://github.com/neuml/txtai">https://github.com/neuml/txtai</a>
</p>
<ul class="org-ul">
<li>Python only</li>
<li>slow</li>
</ul>

<p>
Chroma - Rust + Python
</p>

<p>
pgvector - 5 times slower than Milvus (open source) and Zilliz (managed) in a benchmark.
</p>

<p>
China <a href="https://github.com/VQLite/VQLite">https://github.com/VQLite/VQLite</a>
</p>
<ul class="org-ul">
<li>many dependencies</li>
</ul>

<p>
Weaviate <a href="https://github.com/weaviate/weaviate">https://github.com/weaviate/weaviate</a>
</p>
<ul class="org-ul">
<li>hell of Go dependencies</li>
</ul>
</div>
</div>
<div id="outline-container-org220fb95" class="outline-4">
<h4 id="org220fb95"><span class="section-number-4">14.32.5.</span> Faiss</h4>
<div class="outline-text-4" id="text-14-32-5">
<p>
<a href="https://github.com/facebookresearch/faiss">https://github.com/facebookresearch/faiss</a>
</p>
<ul class="org-ul">
<li>L2 and/or dot product vector comparison.</li>
</ul>
</div>
</div>

<div id="outline-container-orga1c441c" class="outline-4">
<h4 id="orga1c441c"><span class="section-number-4">14.32.6.</span> fastest Qdrant vs Epsilla vs Chroma</h4>
<div class="outline-text-4" id="text-14-32-6">
<p>
<a href="https://github.com/epsilla-cloud/vectordb">https://github.com/epsilla-cloud/vectordb</a>
</p>

<p>
Chroma - Rust + Python
</p>
<ul class="org-ul">
<li><a href="https://github.com/chroma-core/chroma">https://github.com/chroma-core/chroma</a></li>
<li>cons: hell of dependencies, raw state</li>
</ul>
</div>
</div>
<div id="outline-container-orgb33d6d9" class="outline-4">
<h4 id="orgb33d6d9"><span class="section-number-4">14.32.7.</span> Most Used Vectorstores</h4>
<div class="outline-text-4" id="text-14-32-7">
<ol class="org-ol">
<li>Chroma</li>
<li>FAISS</li>
<li>Pinecone</li>
<li>drant</li>
<li>docarray</li>
<li>weaviate</li>
<li>PostrgreSQL</li>
<li>supabase</li>
<li>neo4j</li>
<li>redis</li>
<li>Azure Cognitive Search</li>
<li>Astra DB</li>
</ol>
</div>
</div>

<div id="outline-container-orga4135e3" class="outline-4">
<h4 id="orga4135e3"><span class="section-number-4">14.32.8.</span> Milvus</h4>
<div class="outline-text-4" id="text-14-32-8">
<p>
<a href="https://github.com/milvus-io/milvus">https://github.com/milvus-io/milvus</a>
</p>
<ul class="org-ul">
<li>Milvus 2.0 vs. 1.x: Cloud-native, distributed architecture, highly scalable, and more</li>
</ul>

<p>
<a href="https://github.com/milvus-io/milvus-lite">https://github.com/milvus-io/milvus-lite</a>
</p>
<ul class="org-ul">
<li>Python 70% + C++ 30%</li>
<li>many hidden dependencies <a href="https://github.com/milvus-io/milvus-lite/tree/main/thirdparty">https://github.com/milvus-io/milvus-lite/tree/main/thirdparty</a></li>
</ul>
</div>
</div>
<div id="outline-container-org0482372" class="outline-4">
<h4 id="org0482372"><span class="section-number-4">14.32.9.</span> vector database vs hash tables vs tree based</h4>
<div class="outline-text-4" id="text-14-32-9">
<ul class="org-ul">
<li>V - array-based data structure, where elements are stored in a contiguous block of memory
<ul class="org-ul">
<li>Accessing (O(1)) - because the elements are stored in a contiguous block of memory.</li>
<li>Insertion and Deletion - expensive, collisions not handles and very rare.</li>
<li>retrieving slower</li>
</ul></li>
<li>H - associative array data structure that uses a hash function to map keys to their corresponding values.
<ul class="org-ul">
<li>Accessing O(1) - O(n)</li>
<li>Insertion and Deletion - cheep</li>
<li>retriving faster</li>
</ul></li>
<li>T - kd-trees, R-trees
<ul class="org-ul">
<li>Accessing O(log n) - O (n)</li>
<li>Insertion and Deletion - ?</li>
</ul></li>
</ul>

<p>
Approximate Nearest Neighbor (ANN) structures - designed to efficiently manage and query large vector datasets.
</p>
<ul class="org-ul">
<li>preprocess vector data into efficient indexes to speed up the search process.</li>
<li>designed to trade some accuracy for significant performance gains, making them suitable for large-scale
applications where exact nearest neighbor searches are impractical due to high dimensionality and dataset
size.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org54e3b22" class="outline-3">
<h3 id="org54e3b22"><span class="section-number-3">14.33.</span> LangChain</h3>
<div class="outline-text-3" id="text-14-33">
<p>
Tools, Models, Example selectors, Text splitters, Promts, Output Parsers, Vector Stores
</p>

<p>
pros:
</p>
<ul class="org-ul">
<li>Python (also JS/TS) framework</li>
<li>Building blocks</li>
<li>Swappable components</li>
<li>Examples</li>
<li>From PoC to Production</li>
<li>Speed of improvement</li>
</ul>

<p>
Text Splitters: 5 levels of text splitting:
</p>
<ul class="org-ul">
<li>Characters / Tokens</li>
<li>Recursive Character</li>
<li>Document structure</li>
<li>Semantic Chunker</li>
<li>Agent-like Splitting</li>
</ul>
</div>

<div id="outline-container-org2e64bf0" class="outline-4">
<h4 id="org2e64bf0"><span class="section-number-4">14.33.1.</span> cons:</h4>
<div class="outline-text-4" id="text-14-33-1">
<p>
Astractions is not universal and getting old.
</p>

<p>
Typically all you need is:
</p>
<ul class="org-ul">
<li>Client to LLM
<ul class="org-ul">
<li>managing files and application state through data persistence and caching.</li>
</ul></li>
<li>Functions/Tools for calling</li>
<li>Vector database for RAG
<ul class="org-ul">
<li>chunking and embeddings for vector databases</li>
</ul></li>
<li>An observability platform for tracing, evaluation etc.</li>
</ul>

<p>
Even if using agents, it’s unlikely you’ll be doing much beyond simple agent to agent communication in a
 predetermined sequential flow with business logic for handling agent state and their responses. You don’t
 need a framework to implement this.
</p>
</div>
</div>
</div>

<div id="outline-container-orgc608dc3" class="outline-3">
<h3 id="orgc608dc3"><span class="section-number-3">14.34.</span> Promt Engineering vs Train Foundation Models vs Adapters</h3>
<div class="outline-text-3" id="text-14-34">
<p>
Promt Engineering
</p>
<ul class="org-ul">
<li>pros
<ul class="org-ul">
<li>Do not require GPUs or vast amount of data</li>
<li>Very practical for fast, iterative problem solving</li>
</ul></li>
<li>cons: Limited capabilities, highly dependent on foundation model capabilities.</li>
</ul>

<p>
Train Foundation Models
</p>
<ul class="org-ul">
<li>pros: Very good bragging material</li>
<li>cons:
<ul class="org-ul">
<li>Require amounts of data and GPUs - inaccessible to most</li>
<li>Very risky: no guarantee that it will solve the actual problem you may want it for</li>
</ul></li>
</ul>

<p>
Adapters - "plugged in” between the LLM layers to learn the new knowledge
</p>
<ul class="org-ul">
<li>two steps: memorization (LLM is frozen, adapter learn new facts) and utilisation (adapter frozen)</li>
</ul>
</div>
</div>

<div id="outline-container-orga7a75c6" class="outline-3">
<h3 id="orga7a75c6"><span class="section-number-3">14.35.</span> <span class="todo TODO">TODO</span> Named tensor notation.</h3>
<div class="outline-text-3" id="text-14-35">
<ul class="org-ul">
<li>ArXiv, abs/2102.13196</li>
<li>ArXiv 2303.15647</li>
</ul>
</div>
</div>

<div id="outline-container-orgfcdee08" class="outline-3">
<h3 id="orgfcdee08"><span class="section-number-3">14.36.</span> Agents, auto-programming</h3>
<div class="outline-text-3" id="text-14-36">
<ul class="org-ul">
<li>give away PC <a href="https://github.com/OpenInterpreter/open-interpreter">https://github.com/OpenInterpreter/open-interpreter</a></li>
<li><a href="https://github.com/OpenDevin/OpenDevin">https://github.com/OpenDevin/OpenDevin</a></li>
<li><a href="https://github.com/semanser/codel">https://github.com/semanser/codel</a></li>
</ul>

<p>
Target: full replication of production-grade applications
</p>

<p>
tools
</p>
<ul class="org-ul">
<li>Task Planning: Developing capabilities for bug detection, codebase management, and optimization.</li>
</ul>
</div>
<div id="outline-container-orgbf0d526" class="outline-4">
<h4 id="orgbf0d526"><span class="section-number-4">14.36.1.</span> terms</h4>
<div class="outline-text-4" id="text-14-36-1">
<p>
SOP (Standard Operation Process). An SOP defines subgoals/subtasks for the overall task and allows users to
 customize a fine-grained workflow for the language agents.
</p>
</div>
</div>
<div id="outline-container-orgd314395" class="outline-4">
<h4 id="orgd314395"><span class="section-number-4">14.36.2.</span> theory</h4>
<div class="outline-text-4" id="text-14-36-2">
<p>
For:
</p>
<ul class="org-ul">
<li>hard questions where required several RAG requests and more intelligence steps to answer.</li>
</ul>

<p>
Types of LLM Agents
</p>
<ul class="org-ul">
<li>Single-Agent: for specific task or set of tasks.</li>
<li>Swarm Agents</li>
</ul>


<p>
Agent consist of:
</p>
<ul class="org-ul">
<li>Agent core</li>
<li>Memory: Short/Long-term memory, Hybrid Memory. Composite score is made up of semantic similarity, importance, recency, and
other application-specific metrics. It is used for retrieving specific information.</li>
<li>Tools: third-party APIs, RAG pipeline, etc.</li>
<li><p>
Planning module:
</p>
<ul class="org-ul">
<li>Task and question decomposition</li>
<li>Reflection or critic - ReAct, Reflexion, Chain of Thought, and Graph of thought - critic– or</li>
</ul>
<p>
evidence-based prompting frameworks.
</p></li>
</ul>
</div>
</div>
<div id="outline-container-org82efb4a" class="outline-4">
<h4 id="org82efb4a"><span class="section-number-4">14.36.3.</span> <span class="todo TODO">TODO</span> Development steps</h4>
</div>

<div id="outline-container-orgfc1136f" class="outline-4">
<h4 id="orgfc1136f"><span class="section-number-4">14.36.4.</span> Developer tools</h4>
<div class="outline-text-4" id="text-14-36-4">
<ul class="org-ul">
<li>LangChain</li>
<li>AutoGPT</li>
<li>Langroid</li>
<li>AutoGen</li>
<li>LMQL - A query language for programming (large) language models.</li>
<li>AgentVerse - is designed to facilitate the deployment of multiple LLM-based agents in various applications</li>
<li>Gorilla - An API store for LLMs. Large Language Model Connected with Massive APIs</li>
<li>modelscope-agent - An agent framework connecting LLM models to tools: code<sub>interpreter</sub>, web<sub>browser</sub>,
wordart<sub>texture</sub><sub>generation</sub>, image recognition</li>
<li>agent - long-short term memory, tool usage, web navigation, multi-agent communication, human-agent interaction and symbolic control. <a href="https://github.com/aiwaves-cn/agents">https://github.com/aiwaves-cn/agents</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgcd8ffda" class="outline-4">
<h4 id="orgcd8ffda"><span class="section-number-4">14.36.5.</span> links</h4>
<div class="outline-text-4" id="text-14-36-5">
<ul class="org-ul">
<li>2023 <a href="https://developer.nvidia.com/blog/introduction-to-llm-agents/">https://developer.nvidia.com/blog/introduction-to-llm-agents/</a></li>
<li>overview 2024 <a href="https://www.promptingguide.ai/research/llm-agents">https://www.promptingguide.ai/research/llm-agents</a></li>
</ul>

<p>
<a href="https://github.com/ScarletPan/awesome-autonomous-gpt?tab=readme-ov-file">https://github.com/ScarletPan/awesome-autonomous-gpt?tab=readme-ov-file</a>
</p>
</div>
</div>
</div>
<div id="outline-container-org56ef4b6" class="outline-3">
<h3 id="org56ef4b6"><span class="section-number-3">14.37.</span> Jailbreaks</h3>
<div class="outline-text-3" id="text-14-37">
<p>
2024 Many-shot Jailbreaking
</p>
<ul class="org-ul">
<li><a href="https://www.anthropic.com/research/many-shot-jailbreaking">https://www.anthropic.com/research/many-shot-jailbreaking</a></li>
<li><a href="https://cdn.sanity.io/files/4zrzovbb/website/af5633c94ed2beb282f6a53c595eb437e8e7b630.pdf">https://cdn.sanity.io/files/4zrzovbb/website/af5633c94ed2beb282f6a53c595eb437e8e7b630.pdf</a></li>
</ul>
</div>
</div>
<div id="outline-container-org468eebe" class="outline-3">
<h3 id="org468eebe"><span class="section-number-3">14.38.</span> Spreadsheets</h3>
<div class="outline-text-3" id="text-14-38">
<p>
<a href="https://huggingface.co/papers/2407.09025">https://huggingface.co/papers/2407.09025</a>
<a href="https://arxiv.org/pdf/2407.09025">https://arxiv.org/pdf/2407.09025</a>
</p>
</div>
</div>
<div id="outline-container-org012bf1e" class="outline-3">
<h3 id="org012bf1e"><span class="section-number-3">14.39.</span> USECASES</h3>
<div class="outline-text-3" id="text-14-39">
<p>
How to hide question from later survilance.
</p>
<ol class="org-ol">
<li>Find state that always true</li>
<li>Change state in additional text for current session only.</li>
</ol>

<p>
Global summarization request will be lack of intelligence to handle it, maybe.
</p>
</div>
</div>
<div id="outline-container-orgc899a92" class="outline-3">
<h3 id="orgc899a92"><span class="section-number-3">14.40.</span> <span class="todo TODO">TODO</span> Alpha telega bot</h3>
<div class="outline-text-3" id="text-14-40">
<p>
Подключенный к telegram-боту, работающий python-код (напр. aiogram, telegram.ext),
 использующий любую локальную LLM модель (без общеизвестных API, напр.
ChatGPT, YandexGPT), способный вести диалог на русском языке.
</p>

<p>
Реализация должна включать finetuning model, например через парсинг общедоступной информации с сайта <a href="https://alfabank.by/">https://alfabank.by/</a>
</p>



<p>
PS: полезные ссылки для выполнения ТЗ
</p>



<p>
<a href="https://betterprogramming.pub/private-llms-on-local-and-in-the-cloud-with-langchain-gpt4all-and-cerebrium-6dade79f45f6">https://betterprogramming.pub/private-llms-on-local-and-in-the-cloud-with-langchain-gpt4all-and-cerebrium-6dade79f45f6</a>
<a href="https://github.com/imartinez/privateGPT">https://github.com/imartinez/privateGPT</a>
<a href="https://pub.towardsai.net/meta-releases-llama-2-free-for-commercial-use-e4662757e7d1">https://pub.towardsai.net/meta-releases-llama-2-free-for-commercial-use-e4662757e7d1</a>
<a href="https://huggingface.co/datasets/">https://huggingface.co/datasets/</a>
<a href="https://github.com/nomic-ai/gpt4all">https://github.com/nomic-ai/gpt4all</a>
<a href="https://the-eye.eu/public/AI/">https://the-eye.eu/public/AI/</a>
<a href="https://gpt4all.io/index.html">https://gpt4all.io/index.html</a>
<a href="https://medium.com/@kennethleungty/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8?source=email-b514a69ac1c5-1690070070505-digest.reader-7f60cf5620c9-3d636037a3d8----0-73------------------83e886a6_8f10_4ba6_82f6_da6c4318e741-1">https://medium.com/@kennethleungty/running-llama-2-on-cpu-inference-for-document-q-a-3d636037a3d8?source=email-b514a69ac1c5-1690070070505-digest.reader-7f60cf5620c9-3d636037a3d8----0-73------------------83e886a6_8f10_4ba6_82f6_da6c4318e741-1</a>
</p>
</div>
</div>

<div id="outline-container-org0551b8c" class="outline-3">
<h3 id="org0551b8c"><span class="section-number-3">14.41.</span> personal IDE, PC helpers</h3>
<div class="outline-text-3" id="text-14-41">
<p>
<a href="https://github.com/khoj-ai/khoj">https://github.com/khoj-ai/khoj</a>
</p>
</div>
</div>
<div id="outline-container-orge27ff74" class="outline-3">
<h3 id="orge27ff74"><span class="section-number-3">14.42.</span> private data</h3>
<div class="outline-text-3" id="text-14-42">
<p>
see <a href="file:///home/u/docsmy_short/modified/bi#MissingReference">Data masking</a>
</p>
</div>
</div>
<div id="outline-container-org40c52d5" class="outline-3">
<h3 id="org40c52d5"><span class="section-number-3">14.43.</span> standardization</h3>
<div class="outline-text-3" id="text-14-43">
<p>
Model Cards for Model Reporting <a href="https://arxiv.org/pdf/1810.03993">https://arxiv.org/pdf/1810.03993</a>
</p>
</div>
</div>
<div id="outline-container-orgde9e868" class="outline-3">
<h3 id="orgde9e868"><span class="section-number-3">14.44.</span> NLP metrics</h3>
<div class="outline-text-3" id="text-14-44">
<p>
Транскрибация
</p>
<ul class="org-ul">
<li>Word Error Rate (WER)</li>
<li>Character Error Rate (CER)</li>
</ul>

<p>
Summarization
</p>
<ul class="org-ul">
<li>ROUGE-N</li>
</ul>

<p>
Классификция
</p>
<ul class="org-ul">
<li>F-score, accuracy, precision, recall</li>
</ul>
</div>
</div>
<div id="outline-container-org37c2984" class="outline-3">
<h3 id="org37c2984"><span class="section-number-3">14.45.</span> interpretation</h3>
<div class="outline-text-3" id="text-14-45">
<ul class="org-ul">
<li>супер-активации, критичные для качества <a href="https://arxiv.org/abs/2402.17762">https://arxiv.org/abs/2402.17762</a></li>
<li>The Super Weight in Large Language Models <a href="https://arxiv.org/abs/2411.07191">https://arxiv.org/abs/2411.07191</a></li>
</ul>
</div>
</div>
<div id="outline-container-orgf02bb86" class="outline-3">
<h3 id="orgf02bb86"><span class="section-number-3">14.46.</span> links</h3>
<div class="outline-text-3" id="text-14-46">
<ol class="org-ol">
<li><a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</a></li>
</ol>

<p>
<a href="https://lightning.ai/pages/category/community/tutorial/">https://lightning.ai/pages/category/community/tutorial/</a>
</p>

<p>
learning
</p>
<ul class="org-ul">
<li><a href="https://microsoft.github.io/generative-ai-for-beginners/#/https://ibm.github.io">https://microsoft.github.io/generative-ai-for-beginners/#/https://ibm.github.io</a></li>
<li>IMB Watsonx <a href="https://ibm.github.io/watsonx-workshop-genai/prompt-engineering-exercises/#create-a-watsonxai-project">https://ibm.github.io/watsonx-workshop-genai/prompt-engineering-exercises/#create-a-watsonxai-project</a></li>
<li><a href="https://github.com/Hannibal046/Awesome-LLM">https://github.com/Hannibal046/Awesome-LLM</a></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org669987d" class="outline-2">
<h2 id="org669987d"><span class="section-number-2">15.</span> Adversarial machine learning</h2>
<div class="outline-text-2" id="text-15">
<ul class="org-ul">
<li>GAN <a href="#org3ee3e47">12.20</a></li>
</ul>
<p>
Attacks
</p>
<dl class="org-dl">
<dt>evasion attacks</dt><dd>уклонение. spam, biometric verification systems.</dd>
<dt>data poisoning attacks</dt><dd>contaminating the training dataset ??????</dd>
<dt>Byzantine attacks</dt><dd>.</dd>
<dt>model extraction</dt><dd></dd>
</dl>

<p>
GAN models are known for potentially unstable training and less diversity in generation due to their adversarial training nature
</p>
</div>
<div id="outline-container-org5ed42db" class="outline-3">
<h3 id="org5ed42db"><span class="section-number-3">15.1.</span> linear classifiers - spam - evasion attacks</h3>
</div>
</div>
<div id="outline-container-org16a6808" class="outline-2">
<h2 id="org16a6808"><span class="section-number-2">16.</span> Diffusion NN (DNN)</h2>
<div class="outline-text-2" id="text-16">
<p>
<img src="./imgs/generativeneural-networs-architectures-overview.png" alt="generativeneural-networs-architectures-overview.png" />
are generative model.
</p>
</div>
<div id="outline-container-orgdc984ea" class="outline-3">
<h3 id="orgdc984ea"><span class="section-number-3">16.1.</span> history</h3>
<div class="outline-text-3" id="text-16-1">
<ul class="org-ul">
<li>EBMs - not diffustion, text-to-image</li>
<li>Flow-based models - not diffustion, text-to-image</li>
<li>2021 DALL-E by OpenAI - not diffustion, text-to-image</li>
<li>2021 GLIDE by OpenAI</li>
<li>2022 DALL-E 2 by OpenAI</li>
<li>2022 Imagen</li>
</ul>
</div>
</div>
<div id="outline-container-org8a445d7" class="outline-3">
<h3 id="org8a445d7"><span class="section-number-3">16.2.</span> DALL-E</h3>
<div class="outline-text-3" id="text-16-2">
<p>
<b>Autoregressive approach</b>
</p>
<ul class="org-ul">
<li>train a discrete VAE model to compress images into image tokens,</li>
<li>concatenate the encoded text snippet with the image tokens and train the autoregressive transformer to learn
the joint distribution over text and images.</li>
</ul>

<p>
<b>CLIP</b>
2021 Zero-Shot Text-to-Image Generation <a href="https://arxiv.org/pdf/2102.12092">https://arxiv.org/pdf/2102.12092</a>
</p>
</div>
</div>
<div id="outline-container-orgc260bbb" class="outline-3">
<h3 id="orgc260bbb"><span class="section-number-3">16.3.</span> Basics: forward and reverse diffusion processes and sampling procedure</h3>
<div class="outline-text-3" id="text-16-3">
<ul class="org-ul">
<li><p>
<b>forward</b> or destructive process - gradually applying Gaussian noise to the image until it becomes entirely unrecognizable.
</p>
<ul class="org-ul">
<li>noise (typically an isotropic Gaussian distributio) is usually added according to a fixed variance</li>
</ul>
<p>
schedule, which can be learned or fixed.
</p></li>
<li><b>Reverse</b> image diffusion -</li>
<li><b>the sampling procedure</b> - generating new samples from the learned model by progressively refining random
noise until it converges to the target data distribution.</li>
</ul>

<p>
Markov chain - where each step depends only on the previous step.
</p>

<p>
The reparameterization trick
</p>

<p>
evidence lower bound (ELBO) of the log likelihood of the data
</p>

<p>
After training, we can pass randomly sampled noise through the learned denoising process.
</p>

<p>
We can condition the diffusion models on image labels or text embeddings in order to “guide” the diffusion process.
</p>

<p>
The diffusion process can be formulated as an SDE (stochastic differential equation). Solving the reverse SDE
 allows us to generate new samples.
</p>


<div id="orgdf51838" class="figure">
<p><img src="file:///home/u/docsmy_short/modified/imgs/score-based-sde-overview.png" alt="score-based-sde-overview.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-orgfdc4d1a" class="outline-3">
<h3 id="orgfdc4d1a"><span class="section-number-3">16.4.</span> optimizations</h3>
<div class="outline-text-3" id="text-16-4">
<p>
Cascade and Latent diffusion are two approaches to scale up models to high-resolutions.
</p>

<p>
Cascade diffusion models are sequential diffusion models that generate images of increasing resolution.
</p>

<p>
Latent diffusion models (like stable diffusion) apply the diffusion process on a smaller latent space for computational efficiency using a variational autoencoder for the up and downsampling.
</p>
</div>
</div>

<div id="outline-container-orge981576" class="outline-3">
<h3 id="orge981576"><span class="section-number-3">16.5.</span> links</h3>
<div class="outline-text-3" id="text-16-5">
<ul class="org-ul">
<li>2015 first work <a href="https://arxiv.org/pdf/1503.03585v8">https://arxiv.org/pdf/1503.03585v8</a></li>
<li>article <a href="https://deepsense.ai/the-recent-rise-of-diffusion-based-models">https://deepsense.ai/the-recent-rise-of-diffusion-based-models</a></li>
<li>article DNN in depth <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org8643923" class="outline-2">
<h2 id="org8643923"><span class="section-number-2">17.</span> OLD deploy tf keras</h2>
<div class="outline-text-2" id="text-17">
<ul class="org-ul">
<li>keras <a href="https://medium.com/analytics-vidhya/deploy-your-first-deep-learning-model-on-kubernetes-with-python-keras-flask-and-docker-575dc07d9e76">https://medium.com/analytics-vidhya/deploy-your-first-deep-learning-model-on-kubernetes-with-python-keras-flask-and-docker-575dc07d9e76</a></li>
<li>tensorflow-serving<sub>sidecar</sub>  <a href="https://towardsdatascience.com/deploy-your-machine-learning-models-with-tensorflow-serving-and-kubernetes-9d9e78e569db">https://towardsdatascience.com/deploy-your-machine-learning-models-with-tensorflow-serving-and-kubernetes-9d9e78e569db</a></li>
<li>flask docker kibernetes <a href="https://mikulskibartosz.name/a-comprehensive-guide-to-putting-a-machine-learning-model-in-production-using-flask-docker-and-e3176aa8d1ce">https://mikulskibartosz.name/a-comprehensive-guide-to-putting-a-machine-learning-model-in-production-using-flask-docker-and-e3176aa8d1ce</a></li>
</ul>
</div>
</div>
<div id="outline-container-org993fea1" class="outline-2">
<h2 id="org993fea1"><span class="section-number-2">18.</span> deeppavlov lections</h2>
<div class="outline-text-2" id="text-18">
<ul class="org-ul">
<li>Seminar 1. Part 1 <a href="https://www.youtube.com/watch?v=3nKhzlfaOTE">https://www.youtube.com/watch?v=3nKhzlfaOTE</a>
<ul class="org-ul">
<li>Conversional AI</li>
<li>2015 messengers &gt; social networks</li>
<li>request -&gt; Modular Dialog system - &gt; NLU (domain detection, intent detection, Entities detection) -&gt;
Dialogue manager (dialogue state, policy) -&gt; Natural Language Generator (Generative models, Templates) -&gt;
answer</li>
<li>Encoder LSTMs -&gt; attention -&gt; Decoder LSTMs -&gt; softmax</li>
<li>Embedding or Encoder -&gt; memory -&gt;Attention (current input and state) -&gt;Decoder or Action generator</li>
<li>Нейросеть работает быстрее правил</li>
<li>Language models на огромной выборке данных и использовать для решения NLP задач
<ul class="org-ul">
<li>BERT</li>
<li>OpenAI</li>
<li>ESIM+ELMO</li>
<li>ESIM</li>
<li>LSTM+GloVe</li>
<li>FastText</li>
</ul></li>
<li>Алиса - Yandex, AliMe Assis - wechat ( если не может дать ответ, переключает на оператора), Xiaolce -
Microsoft in China, Google Assistent, Amazon - Aleksa
<ul class="org-ul">
<li>Chit-chat  - Seq2seq -&gt; seq2seq with conv context -&gt;knowledge-grounded seq2seq</li>
<li>Task-oriented - Single-domain sytem-initiative -&gt; Multi-domain, contextual, multi-initiative -&gt;
End-to-end learning, massively multi-domain</li>
</ul></li>
<li>Hype cycle of Gartner -  Hype Cycle for Emerging Technologies, 2018</li>
<li>Значительную часть интеллекта в Алексу добляют третьи стороны</li>
<li>Minsky's 'Society of mind' - мозг - общество когнитивных агентов</li>
<li>МФТИ(исследования) -&gt; DeepPavlov &lt;- DeepReplay (сбербанк)(платформенные решения в виле сервисов) ( потребности рынка)</li>
</ul></li>
<li>Seminar 1. Part 2. <a href="https://www.youtube.com/watch?v=U_1xdGUQZ5o">https://www.youtube.com/watch?v=U_1xdGUQZ5o</a></li>
<li>Seminar 1. Part 3. skipgram cbow <a href="https://www.youtube.com/watch?v=juDdkybtTv0">https://www.youtube.com/watch?v=juDdkybtTv0</a>
<ul class="org-ul">
<li>есть какой-то стендфордский курс</li>
<li>простейшая модель классификации: x - вектор, U - matrix p(y(x) = k) = softmax(U*x)=&gt; Pk = exp(Uxk)/∑k(exp(Ux))</li>
</ul></li>
<li>Stanford Lecture 4: Word Window Classification and Neural Networks <a href="https://www.youtube.com/watch?v=uc2_iwVqrRI">https://www.youtube.com/watch?v=uc2_iwVqrRI</a></li>
<li>Seminar 2. Part 1 <a href="https://www.youtube.com/watch?v=92Ctk9OzlDg">https://www.youtube.com/watch?v=92Ctk9OzlDg</a>
<ul class="org-ul">
<li>слова в word2vec без дополнительного обучения плохо работают для sentiment</li>
</ul></li>
<li>Seminar 2. Part 2 <a href="https://www.youtube.com/watch?v=1zv1IJAS9r4">https://www.youtube.com/watch?v=1zv1IJAS9r4</a>
<ul class="org-ul">
<li>elu лучше, но медленее считается</li>
<li>градиентный спуск</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org9fc68cc" class="outline-2">
<h2 id="org9fc68cc"><span class="section-number-2">19.</span> passport</h2>
<div class="outline-text-2" id="text-19">
<p>
rec:
</p>
<ul class="org-ul">
<li><a href="https://www.pyimagesearch.com/2015/11/30/detecting-machine-readable-zones-in-passport-images/">https://www.pyimagesearch.com/2015/11/30/detecting-machine-readable-zones-in-passport-images/</a></li>
<li><a href="https://habr.com/ru/post/208090/">https://habr.com/ru/post/208090/</a></li>
</ul>


<p>
colour:
</p>
<ul class="org-ul">
<li><a href="http://www.compvision.ru/forum/index.php?/topic/1568-%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0-%D0%BF%D0%B0%D1%81%D0%BF%D0%BE%D1%80%D1%82%D0%B0-opencv/">http://www.compvision.ru/forum/index.php?/topic/1568-%D1%80%D0%B0%D1%81%D0%BF%D0%BE%D0%B7%D0%BD%D0%B0%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5-%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0-%D0%BF%D0%B0%D1%81%D0%BF%D0%BE%D1%80%D1%82%D0%B0-opencv/</a></li>

<li>automatic adj for OCR <a href="https://stackoverflow.com/questions/56905592/automatic-contrast-and-brightness-adjustment-of-a-color-photo-of-a-sheet-of-pape">https://stackoverflow.com/questions/56905592/automatic-contrast-and-brightness-adjustment-of-a-color-photo-of-a-sheet-of-pape</a></li>
</ul>

<p>
rectangle:
</p>
<ul class="org-ul">
<li><a href="https://stackoverflow.com/questions/26583649/opencv-c-rectangle-detection-which-has-irregular-side">https://stackoverflow.com/questions/26583649/opencv-c-rectangle-detection-which-has-irregular-side</a></li>
<li>OpenCV shape detection <a href="https://www.pyimagesearch.com/2016/02/08/opencv-shape-detection/">https://www.pyimagesearch.com/2016/02/08/opencv-shape-detection/</a></li>
<li><a href="https://robotclass.ru/tutorials/opencv-detect-rectangle-angle/">https://robotclass.ru/tutorials/opencv-detect-rectangle-angle/</a></li>
<li><a href="https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491">https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491</a></li>
<li><a href="https://github.com/jrieke/shape-detection">https://github.com/jrieke/shape-detection</a></li>

<li>MRZ <a href="https://web.archive.org/web/20140801191250/http://www.fms.gov.ru/upload/iblock/fe3/prikaz_mchz279.pdf">https://web.archive.org/web/20140801191250/http://www.fms.gov.ru/upload/iblock/fe3/prikaz_mchz279.pdf</a></li>
<li><a href="https://github.com/Shreeshrii/tessdata_ocrb">https://github.com/Shreeshrii/tessdata_ocrb</a></li>
<li>Машиночитаемая запись МЧЗ</li>
<li>двух строк по 44 символа</li>
<li>Шрифт  OCR-B type 1 (Стандарт ИСО 1073/II).</li>
<li>верхняя строка 6-44 модернизированный клер</li>
<li>10, 20, 28, 43, 44 нижней строки МЧЗ содержат контрольные цифры.</li>

<li>top
<ul class="org-ul">
<li>1-2 PN Passport National - Тип документа</li>
<li>3-5 RUS - код ИКАО</li>
<li>6-44 BAQRAMOV&lt;&lt;AMIR&lt;IL9GAM&lt;&lt;0GLY&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; - БАЙРАМОВ \n АМИР \n ИЛЬГАМ - ОГЛЫ
<ul class="org-ul">
<li>дефис = '&lt;' - знак заполнитель</li>
<li>имя сокращается на букве, которая является 42 знаком строки, знака-заполнителя вносится первая буква отчества.</li>
<li>фамилия сокращается на букве, которая является 39 знаком строки, после двух знаков-заполнителей
последовательно вносятся первая буква имени, знак-заполнитель, первая буква отчества</li>
</ul></li>
</ul></li>
<li>bottom
<ul class="org-ul">
<li>1-9 460123456 - серии 4601 № 123456</li>
<li>10 Контрольная цифра - по 1-9</li>
<li>11-13 RUS</li>
<li>14-19 YYMMDD 931207 - 1993.12.07 - Дата рождения</li>
<li>20 Контрольная цифра - по 14-19</li>
<li>21 F or M - женский, мужской</li>
<li>22-27 Дата истечения срока действия или все &lt;, вместе с контрольной ццифрой</li>
<li>28 Контрольная цифра or &lt;</li>
<li>29 Последняя цифра серии</li>
<li>30-35 YYMMDD Дата выдачи паспорта</li>
<li>36-41 Код подразделения</li>
<li>42 &lt; в контрольной сумме учитывается как 0</li>
<li>43 Контрольная цифра 29-42</li>
<li>44 Контрольная цифра 1-10, 14-20, 22-28, 29-43</li>
</ul></li>
</ul>
</div>

<div id="outline-container-org144f8e3" class="outline-3">
<h3 id="org144f8e3"><span class="section-number-3">19.1.</span> error</h3>
<div class="outline-text-3" id="text-19-1">
<p>
<a href="rq.worker:opencv-tasks">rq.worker:opencv-tasks</a>: file (7120f9a5-7fde-41ba-96f4-ef1da72c5c1d)
</p>

<p>
Traceback (most recent call last):
return method<sub>number</sub><sub>list</sub>[method<sub>number</sub>](obj).OUTPUT<sub>OBJ</sub>
File "/code/parsers/multiparser.py", line 22, in passport<sub>and</sub><sub>drivelicense</sub>
aop = passport<sub>main</sub><sub>page</sub>(img<sub>cropped</sub>)
File "/code/parsers/passport.py", line 162, in passport<sub>main</sub><sub>page</sub>
res<sub>i</sub> = fio<sub>checker.double</sub><sub>query</sub><sub>name</sub>(anonymous<sub>return.OUTPUT</sub><sub>OBJ</sub>['MRZ']['mrz<sub>i</sub>'], i<sub>pass</sub>)
File "/code/groonga.py", line 248, in double<sub>query</sub><sub>name</sub>
return FIOChecker.<sub>get</sub><sub>appropriate</sub>(items1, word1)
File "/code/groonga.py", line 236, in _double<sub>query</sub>
equal = [x for x in items if x[2] <code>= 4]  # score
File "/code/groonga.py", line 129, in &lt;listcomp&gt;
ERROR:root:Uncatched exception in ParserClass
return self._double_query(word1, word2, self.names_table)
File "/code/groonga.py", line 129, in _get_appropriate
equal = [x for x in items if x[2] =</code> 4]  # score
KeyError: 2
File "/code/MainOpenCV.py", line 40, in parser<sub>call</sub>
</p>
</div>
</div>
<div id="outline-container-org0f5e93f" class="outline-3">
<h3 id="org0f5e93f"><span class="section-number-3">19.2.</span> Расчет контрольной суммы</h3>
<div class="outline-text-3" id="text-19-2">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">data</td>
<td class="org-left">5 1 0 5 0 9</td>
</tr>

<tr>
<td class="org-left">weight</td>
<td class="org-left">7 3 1 7 3 1</td>
</tr>

<tr>
<td class="org-left">after multiply</td>
<td class="org-left">35 3 0 35 0 9</td>
</tr>
</tbody>
</table>
<ul class="org-ul">
<li>Сумма результатов 35 + 3 + 0 + 35 +0 +9 = 82</li>
<li>82 / 10 =8, остаток деления 2</li>
<li>2</li>
</ul>


<ul class="org-ul">
<li>361753650</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">import</span> numpy <span style="color: #8ac6f2; font-weight: bold;">as</span> np
<span style="color: #cae682;">a</span>=np.array([3,6,1,7,5,3,6,5,0])
<span style="color: #cae682;">b</span>=np.array([7,3,1,7,3,1,7,3,1])
np.<span style="color: #e5786d;">sum</span>(a*b)%10
</pre>
</div>
</div>
</div>
<div id="outline-container-orgc21432d" class="outline-3">
<h3 id="orgc21432d"><span class="section-number-3">19.3.</span> passport serial number</h3>
<div class="outline-text-3" id="text-19-3">
<ul class="org-ul">
<li><a href="http://ukrainian-passport.com/blog/everything-you-have-to-know-about-the-russian-passport/">http://ukrainian-passport.com/blog/everything-you-have-to-know-about-the-russian-passport/</a></li>
<li>consists of two signs and refers to the code assigned to the appropriate area (region) of the Russian Federation.</li>
<li>indicates the year of passport issue</li>
<li>passport serial number - six signs</li>
</ul>
</div>
</div>
<div id="outline-container-org95ba083" class="outline-3">
<h3 id="org95ba083"><span class="section-number-3">19.4.</span> string metric for measuring the difference between two sequences</h3>
<div class="outline-text-3" id="text-19-4">
<ul class="org-ul">
<li>коэффициент Танимото <a href="https://grishaev.me/2012/10/05/1/">https://grishaev.me/2012/10/05/1/</a></li>
<li><a href="https://habr.com/en/post/341148/">https://habr.com/en/post/341148/</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org97326b1" class="outline-2">
<h2 id="org97326b1"><span class="section-number-2">20.</span> captcha</h2>
<div class="outline-text-2" id="text-20">
</div>
<div id="outline-container-orgcfa3ec5" class="outline-3">
<h3 id="orgcfa3ec5"><span class="section-number-3">20.1.</span> audio capcha</h3>
<div class="outline-text-3" id="text-20-1">
<p>
speech recognition model
</p>
<ul class="org-ul">
<li><a href="https://github.com/openai/whisper">https://github.com/openai/whisper</a>
<ul class="org-ul">
<li>download models: <a href="https://github.com/openai/whisper/blob/main/whisper/__init__.py">https://github.com/openai/whisper/blob/main/whisper/__init__.py</a></li>
</ul></li>
<li><a href="https://github.com/chussong/pocketsphinx">https://github.com/chussong/pocketsphinx</a></li>
</ul>
</div>
</div>

<div id="outline-container-org298446a" class="outline-3">
<h3 id="org298446a"><span class="section-number-3">20.2.</span> <span class="todo TODO">TODO</span> split file by worlds</h3>
</div>
<div id="outline-container-orge34b1e2" class="outline-3">
<h3 id="orge34b1e2"><span class="section-number-3">20.3.</span> reCAPTCHA google</h3>
<div class="outline-text-3" id="text-20-3">
<ul class="org-ul">
<li>Version 2 ~2013, also asked users to decipher text or match images if the analysis of cookies and canvas
rendering suggested the page was being downloaded automatically.
<ul class="org-ul">
<li>behavioral analysis of the browser's interactions to predict whether the user was a human or a bot</li>
</ul></li>
<li>version 3, at the end of 2019,  reCAPTCHA will never interrupt users and is intended to run automatically when users load pages
or click buttons.</li>
</ul>

<p>
On May 26, 2012, Adam, C-P and Jeffball - accuracy rate of 99.1% analyse the audio version of reCAPTCHA
</p>
<ul class="org-ul">
<li>after: the audio version was increased in length from 8 seconds to 30 seconds, and is much more difficult to
understand, both for humans as well as bots.</li>
<li>after: 60.95% and 59.4% respectively</li>
</ul>
</div>
</div>
<div id="outline-container-orgcb5b99f" class="outline-3">
<h3 id="orgcb5b99f"><span class="section-number-3">20.4.</span> image captcha</h3>
<div class="outline-text-3" id="text-20-4">
<ul class="org-ul">
<li><a href="https://arxiv.org/pdf/2006.08296.pdf">https://arxiv.org/pdf/2006.08296.pdf</a></li>
<li><a href="https://arxiv.org/pdf/1409.0925.pdf">https://arxiv.org/pdf/1409.0925.pdf</a></li>

<li><a href="https://github.com/Gregwar/Captcha">https://github.com/Gregwar/Captcha</a></li>
<li>wifi all fonts <a href="https://commons.wikimedia.org/wiki/Category:MediaWiki_SVG_font_lists">https://commons.wikimedia.org/wiki/Category:MediaWiki_SVG_font_lists</a></li>
</ul>
</div>
<div id="outline-container-orgdb03097" class="outline-4">
<h4 id="orgdb03097"><span class="section-number-4">20.4.1.</span> <span class="todo TODO">TODO</span> remove colour</h4>
<div class="outline-text-4" id="text-20-4-1">
<div class="org-src-container">
<pre class="src src-python">
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org5e2b6ea" class="outline-3">
<h3 id="org5e2b6ea"><span class="section-number-3">20.5.</span> tesseract fine-tuning</h3>
<div class="outline-text-3" id="text-20-5">
<p>
<a href="https://www.statworx.com/en/content-hub/blog/fine-tuning-tesseract-ocr-for-german-invoices/">https://www.statworx.com/en/content-hub/blog/fine-tuning-tesseract-ocr-for-german-invoices/</a>
</p>
</div>
</div>
<div id="outline-container-org375d583" class="outline-3">
<h3 id="org375d583"><span class="section-number-3">20.6.</span> links</h3>
<div class="outline-text-3" id="text-20-6">
<p>
<a href="https://habr.com/en/post/241145/">https://habr.com/en/post/241145/</a>
<a href="https://habr.com/en/post/241263/">https://habr.com/en/post/241263/</a>
cnn <a href="https://medium.com/swlh/captcha-recognition-44522c2fe9c">https://medium.com/swlh/captcha-recognition-44522c2fe9c</a>
</p>
</div>
</div>
</div>
<div id="outline-container-orgd144ae7" class="outline-2">
<h2 id="orgd144ae7"><span class="section-number-2">21.</span> kaggle</h2>
<div class="outline-text-2" id="text-21">
<ul class="org-ul">
<li>Using News to Predict Stock Movements <a href="https://www.kaggle.com/c/two-sigma-financial-news">https://www.kaggle.com/c/two-sigma-financial-news</a>
<ul class="org-ul">
<li><a href="https://www.kaggle.com/artgor/eda-feature-engineering-and-everything">https://www.kaggle.com/artgor/eda-feature-engineering-and-everything</a></li>
<li><a href="https://www.kaggle.com/gracewan/plot-model">https://www.kaggle.com/gracewan/plot-model</a></li>
</ul></li>
</ul>
</div>
<div id="outline-container-org7d5e4ab" class="outline-3">
<h3 id="org7d5e4ab"><span class="section-number-3">21.1.</span> 1C forecast</h3>
<div class="outline-text-3" id="text-21-1">
<p>
<a href="https://www.kaggle.com/c/competitive-data-science-predict-future-sales/overview">https://www.kaggle.com/c/competitive-data-science-predict-future-sales/overview</a>
</p>

<ul class="org-ul">
<li>sales<sub>train.csv</sub> - the training set. Daily historical data from January 2013 to October 2015.</li>
<li>test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.</li>
<li>sample<sub>submission.csv</sub> - a sample submission file in the correct format.</li>
<li>items.csv - supplemental information about the items/products.</li>
<li>item<sub>categories.csv</sub>  - supplemental information about the items categories.</li>
<li>shops.csv- supplemental information about the shops.</li>
</ul>


<p>
test - November 2015
</p>
<ul class="org-ul">
<li>id</li>
<li>shop<sub>id</sub> - unique identifier of a shop # 42</li>
<li>item<sub>id</sub> - unique identifier of a product</li>
</ul>
</div>
</div>
<div id="outline-container-orgfd4434f" class="outline-3">
<h3 id="orgfd4434f"><span class="section-number-3">21.2.</span> Keras measure of intelligence</h3>
<div class="outline-text-3" id="text-21-2">
<ul class="org-ul">
<li><a href="https://www.kaggle.com/c/competitive-data-science-predict-future-sales/overview">https://www.kaggle.com/c/competitive-data-science-predict-future-sales/overview</a></li>
<li><a href="https://arxiv.org/abs/1911.01547">https://arxiv.org/abs/1911.01547</a></li>
<li>APP files:/mnt/hit4/hit4user/git<sub>projects</sub>/ARC/apps/</li>
</ul>

<p>
Abstractionand Reasoning Corpus (ARC)
</p>

<p>
<a href="https://www.kaggle.com/c/abstraction-and-reasoning-challenge">https://www.kaggle.com/c/abstraction-and-reasoning-challenge</a>
</p>
</div>
<div id="outline-container-orged3fdf2" class="outline-4">
<h4 id="orged3fdf2"><span class="section-number-4">21.2.1.</span> teory</h4>
<div class="outline-text-4" id="text-21-2-1">
<p>
skill-acquisition efficiency
</p>
<ul class="org-ul">
<li>scope</li>
<li>generalization difficulty</li>
<li>priors - about ourselves, about the world, and about how to learn</li>
<li>experience</li>
</ul>

<p>
Turing Test - such tests completely opt out of objectively defining and measuring intelligence, and instead
outsource thetask to unreliable human judges who themselves do not have clear definitions or
evaluationprotocols.
</p>

<p>
two divergent visions:
</p>
<ul class="org-ul">
<li><b>Intelligence measures an agent’s ability to achieve goals in a wide range of environments</b>
<ul class="org-ul">
<li>task-specific skill</li>
<li>generality and adaptation - able to learn to handle new task</li>
</ul></li>
</ul>

<p>
crystallized skill on one hand, skill-acquisition ability on the other.
</p>

<p>
principles of psychometrics:
</p>
<ul class="org-ul">
<li>skill-acquisition efficiency</li>
<li>batteries of tasks - never knewn</li>
<li>standards regarding reliability, validity, standardization, andfreedomfrom bias
<ul class="org-ul">
<li>test results for a given system should be reproducible</li>
<li>successful result of test must be clear</li>
<li>no uniquely human acquired knowledge, or should not involve constraints un-related to intelligence within
which machines have unfair advantages</li>
</ul></li>
</ul>

<p>
learning machine certainlymaybe intelligent: learning is a necessary condition to adapt to new information
and acquire new skills
</p>

<p>
Для алгоритма нужно контролировать:
</p>
<ul class="org-ul">
<li>priors - инженерно запрограммированные - именно то что определяет мощные позновательные способности</li>
<li>experience - ?</li>
<li>generalization difficulty</li>
</ul>

<p>
<b>general  intelligence is</b> a spectrum, tied to:
</p>
<ul class="org-ul">
<li>a scope of application, which may be more or less broad</li>
<li>efficiency with which the system translate its priors and experience into new skills over the scope considered</li>
<li>generalization difficulty represented by different points in the scope considered</li>
</ul>

<p>
Main deffinition: <b>The intelligence of a system is a measure of its skill-acquisition efficiency over a scope of
tasks, with respect to priors, experience, and generalization difficulty</b>
</p>
<ul class="org-ul">
<li>во время практики - более эффективно превращает приоры в навыки</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="orgcacbc9b"></a>priors<br />
<div class="outline-text-5" id="text-21-2-1-1">
<p>
We need a clear understanding of <b>human cognitive priors</b> in order to fairly <b>evaluate</b> general intelligence
between humans and machines.
</p>
<dl class="org-dl">
<dt>low-level</dt><dd>sensorimotor space - reflexes</dd>
<dt>Meta-learning priors</dt><dd>governing our learning strategies and capabilities for knowledge acquisition
<ul class="org-ul">
<li>information in the universe follows a modular-hierarchical structure</li>
<li>assumptions regarding causality and spatio-temporal continuity</li>
</ul></dd>
<dt>High-level</dt><dd>knowledge priors</dd>
</dl>


<p>
science theory of CoreKnowledge, priors: ( hard-coded)
</p>
<ul class="org-ul">
<li>Objectness and elementary physics - environment shouldbe parsed into “objects” characterized by principles
of:
<dl class="org-dl">
<dt>cohesion</dt><dd>objects  move  ascontinuous, connected, bounded wholes</dd>
<dt>persistence</dt><dd>objects do not suddenly ceaseto exist and do not suddenly materialize</dd>
<dt>contact</dt><dd>objects do not act at a distanceand cannot interpenetrate</dd>
</dl></li>
<li>Agentness and goal-directedness - some objects are inanimate, some other are “agents”. We expect that these
agentsmay act contingently and reciprocally.</li>
<li>Natural numbers and elementary arithmetic. These number representations may be added or subtracted, and may
be compared to each other, or sorted.</li>
<li>Elementary  geometry  and  topology - distance, orientation, in/out relationships</li>
</ul>
</div>
</li>

<li><a id="org08da022"></a>MY<br />
<div class="outline-text-5" id="text-21-2-1-2">
<p>
Мое наблюдение - неинтеллектуально:
</p>
<ul class="org-ul">
<li>Cубъективно - незнание:
<ul class="org-ul">
<li>нераспознание объектов</li>
<li>незнания как поступить</li>
<li>незнание собственным ментальных и физических способностей</li>
</ul></li>
<li>Объективно - сложные движения высокоприспособленные - <b>fitnes</b></li>
</ul>

<p>
<b>30 million training situations is not enough for a Deep Learningmodel to learn to drive a car in a plain supervised settin</b>
</p>
<ul class="org-ul">
<li>rules</li>
<li>training</li>
<li>crash situations</li>
</ul>

<p>
<b>pretraining and aftertraining</b>
</p>
<ul class="org-ul">
<li>универсальня стратегия и тактика</li>
<li>адаптация к непредсказуемым? изменениям</li>
</ul>

<p>
<b>unlimited data or unlimited engineering</b> - то что нужно для универсального алгоритма
</p>

<p>
cognitive adaptability or sensorimotor adaptability
</p>
</div>
</li>
<li><a id="org3c8acae"></a>Generalization - he ability to handle situations (or tasks) that differ from previously encountered situations<br />
<div class="outline-text-5" id="text-21-2-1-3">
<ul class="org-ul">
<li>System-centric generalization - test accuracy - prior knowledge isignored by this measure of generalization</li>
<li>Developer-aware generalization - developer of the system as part of the system</li>
</ul>

<p>
degrees:
</p>
<ul class="org-ul">
<li>Absense (algorithm)</li>
<li>Local generalization, or “robustness” - preadaptation to known unknowns within a single task or well-defined
set of tasks (common NN)</li>
<li>Broad generalization, or “flexibility” - выполнение заранее неизвестных тасок но в общей категории (никто не знает что это)</li>
<li>Extreme generalization - типа пусть выполнит что-то невиданное но чтобы мы поняли что в этом есть смысл</li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgc2ea517" class="outline-4">
<h4 id="orgc2ea517"><span class="section-number-4">21.2.2.</span> new in AI since 2017</h4>
<div class="outline-text-4" id="text-21-2-2">
<ul class="org-ul">
<li>Reinforcement Learning (RL) algorithms</li>

<li>StarCraft [93] for DeepMind</li>
<li>DotA2  [89]  for  OpenAI)</li>
</ul>

<p>
два вида программирования:
</p>
<ul class="org-ul">
<li>инженером</li>
<li>вход/выход данными</li>
</ul>
</div>
</div>
<div id="outline-container-org49cbf7c" class="outline-4">
<h4 id="org49cbf7c"><span class="section-number-4">21.2.3.</span> automatic programming</h4>
<div class="outline-text-4" id="text-21-2-3">
<ul class="org-ul">
<li>Inductive programming - from incomplete specifications, such as input/output examples or constraints
<ul class="org-ul">
<li>Inductive functional programming - based on Lisp, Haskell</li>
<li>inductive logic programming - based on Prolog</li>
</ul></li>
<li>constraint programming - declarative - users declaratively state the constraints on the feasible solutions
for a set of decision variables</li>
<li>probabilistic programming - probabilistic models are specified and inference for these models is performed
automatically</li>
</ul>
</div>
</div>
<div id="outline-container-org82aded6" class="outline-4">
<h4 id="org82aded6"><span class="section-number-4">21.2.4.</span> Data</h4>
<div class="outline-text-4" id="text-21-2-4">
<p>
colour = 0..9, where 0 - black
max<sub>input</sub> = 30x30
train pairs max = 10
train pairs min = 2
</p>
</div>
</div>


<div id="outline-container-orgdc27d37" class="outline-4">
<h4 id="orgdc27d37"><span class="section-number-4">21.2.5.</span> MY programming</h4>
<div class="outline-text-4" id="text-21-2-5">
<p>
augumn:
</p>
<ul class="org-ul">
<li>more colours</li>
</ul>
</div>

<ol class="org-ol">
<li><a id="orgdd15a7e"></a>exploring<br />
<div class="outline-text-5" id="text-21-2-5-1">
<p>
<a href="https://www.kaggle.com/boliu0/visualizing-all-task-pairs-with-gridlines">https://www.kaggle.com/boliu0/visualizing-all-task-pairs-with-gridlines</a>
op
</p>
<ul class="org-ul">
<li>object segregation by colour</li>
<li>moving</li>
<li>rotation</li>
</ul>

<p>
Имеют смысл только в контексте задачи:
</p>
<ul class="org-ul">
<li>Object - small or equal to gs</li>
<li>gs - large objects - abstract</li>
<li>orientation - objects or gs</li>
<li>mv - movement or copy one direction ( exception 4 - many directions)</li>
</ul>
<p>
ww
</p>
<ol class="org-ol">
<li>objects by colour, and groups of objects of same colour (320)
<ul class="org-ul">
<li>position to each other, to contour (170)</li>
<li>shape - square or not 101</li>
<li>overlapped or not (23)</li>
<li>groups of small objects</li>
</ul></li>
<li>compare two images and calc changed per object:
<ul class="org-ul">
<li>zoom to object 35</li>
<li>moved</li>
<li>rotated</li>
<li>mirrored</li>
<li>colored 282, 22</li>
<li>replaced</li>
<li>transformed</li>
<li>new objects? 75, 101, 330, 14</li>
<li>rescaled</li>
<li>mixed together 320</li>
<li>repeat</li>
</ul></li>
</ol>


<p>
158 moved, rescaled
</p>


<p>
huita 62, 170
</p>
</div>
</li>
<li><a id="org3f08e12"></a>plan<br />
<div class="outline-text-5" id="text-21-2-5-2">
<p>
train small<sub>CNN</sub>:
</p>
<ul class="org-ul">
<li>count solid objects by colour, and groups of objects of same colour (separated by another solid object not black), groups of small objects in dark
<ul class="org-ul">
<li>10x2 int - colour + probability</li>
<li>9x2 int - colour + probability - groups of same colour</li>
<li>9x2 int - count + probabolity - groups of diff colour in dark</li>
</ul></li>
<li>shape - square or not
<ul class="org-ul">
<li>10 int - probability</li>
</ul></li>
<li>horizontal/vertical orientation, 0 0 - cube 1,1 - horizontal, -1,-1 - vertical, (1, -1) - \, (-1, 1) - /
<ul class="org-ul">
<li>9x2 int</li>
</ul></li>
</ul>


<p>
demo
</p>
<ul class="org-ul">
<li>2 картинки -&gt; small<sub>CNN</sub> -&gt;small<sub>v</sub><sub>1</sub> для первой</li>
<li>2 картинки -&gt; small<sub>CNN</sub> -&gt;small<sub>v</sub><sub>2</sub> для второй (чтобы обнаружить повторения 66)</li>
<li>small<sub>v</sub> + 2 images+2 sizes-&gt; CNN сравнивает -&gt; вектор (программа)</li>
</ul>
<p>
test
</p>
<ul class="org-ul">
<li>вектор + input изобр +size -&gt; CNN которая encode-decode в итоговое изображение</li>
<li>из encode выбирается ?x? которые обрежут изображение из центра</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org557bd57" class="outline-2">
<h2 id="org557bd57"><span class="section-number-2">22.</span> ИИ в банках</h2>
<div class="outline-text-2" id="text-22">
</div>
<div id="outline-container-orgd360038" class="outline-3">
<h3 id="orgd360038"><span class="section-number-3">22.1.</span> 2020 Ассоция российских банков обсудила <a href="https://banks.cnews.ru/news/line/2020-01-24_v_assotsiatsii_rossijski">https://banks.cnews.ru/news/line/2020-01-24_v_assotsiatsii_rossijski</a></h3>
<div class="outline-text-3" id="text-22-1">
<ul class="org-ul">
<li>51% кредитных организаций задействовали ИИ для точечных решений и индивидуальных задач</li>
<li>27% тестировали его в пилотных проектах</li>
<li>19% использовали компьютерный интеллект во всем банке в целом.</li>
</ul>

<p>
Блоки
</p>
<ul class="org-ul">
<li>Распознавание образов</li>
<li>Роботизация бизнес-процессов</li>
<li>Чат-боты, голосовые роботы</li>
<li>Большие данные, машинное обучение, нейронные сети</li>
</ul>

<p>
научные круги
</p>
<ul class="org-ul">
<li>обучении на прецедентах, задачах по экстраполяции и алгоритмизации решения конкретных бизнес-задач</li>
<li>не ии - а “прецедентный анализ”</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orga80d842" class="outline-2">
<h2 id="orga80d842"><span class="section-number-2">23.</span> MLOps and ModelOps (Machine Learning Operations)</h2>
<div class="outline-text-2" id="text-23">
</div>
<div id="outline-container-orgbfeaff9" class="outline-3">
<h3 id="orgbfeaff9"><span class="section-number-3">23.1.</span> terms</h3>
<div class="outline-text-3" id="text-23-1">
<ul class="org-ul">
<li>Reliability - What happen when the signal is not available? would you know?</li>
<li>Versioning - Does system that computes this signal ever change? How often? What would happen?</li>
</ul>

<p>
<a href="https://ml-ops.org/content/mlops-principles#monitoring">https://ml-ops.org/content/mlops-principles#monitoring</a>
</p>

<p>
ModelOps (model operations) - life cycle management of a wide range of operationalized artificial intelligence
(AI) and decision models. Skill set needed to scale analitical practices.
</p>
<ul class="org-ul">
<li>technical and business KPI's.</li>
<li>evaluate AI models in production, independent of data scientists</li>
<li>puts ModelOps in the center, connecting both DataOps and DevOps</li>
<li>MDLC (model development lifecycle)</li>
<li>versioning both for models and data.</li>
<li>continuously monitoring the performance of the model</li>
<li>Continuous Training (CT) is unique to MLOps, where the framework has mechanisms in place for retraining and
calibrating models periodically.</li>
<li>data Ingestion [ɪnˈʤesʧən]</li>
<li><p>
production Testing methods:
</p>
<ul class="org-ul">
<li><b>Batch testing</b> - just test model in test envirtonment on metrics.</li>
<li><b>A/B testing</b> - for assessing marketing campaigns
<ul class="org-ul">
<li>Real-time or live data is fragmented or split into two sets, Set A and Set B.</li>
<li>Set A data is routed to the old model, and Set B data is routed to the new model.</li>
<li>In order to evaluate whether the new model (model B) performs better than the old model (model A), various statistical techniques can be used to evaluate model performance (for example, accuracy, precision, etc), depending on the business use case or operations.</li>
<li>Then, we use statistical hypothesis testing: The null hypothesis asserts that the new model does not increase the average value of the monitoring business metrics. The alternate hypothesis asserts that the new model improves the average value of the monitoring business metrics.</li>
<li>Ultimately, we evaluate whether the new model drives a significant boost in specific business metrics.</li>
</ul></li>
<li><b>Stage test or shadow test</b> - tested in a replicated production-like environment (staging</li>
</ul>
<p>
environment). for robustness and assessing its performance on real-time data.
</p></li>
</ul>

<p>
<b>Model decay metric</b> - used in monitoring can consist of several tests.
</p>


<p>
tools:
</p>
<dl class="org-dl">
<dt>Model Registry</dt><dd>is a central repository that allows model developers to publish production-ready models
for ease of access.</dd>
<dt>Store the metadata</dt><dd>for your trained models, as well as their runtime dependencies so the deployment
process is eased.</dd>
<dt>Build automated pipelines</dt><dd>that make continuous integration, delivery, and training of your production model possible.</dd>
<dt>Compare models running</dt><dd>in production (champion models) to freshly trained models (or challenger models) in the staging environment.</dd>
</dl>

<p>
<b>Data lineage</b> ['lɪnɪɪʤ] (проиcхождение) - data origin, what happens to it, and where it moves over
 time. greatly simplifying the ability to trace errors back to the root cause in a <b>data analytics process</b>.
</p>
<ul class="org-ul">
<li>data provenance ['prɔv(ə)nəns]</li>
</ul>

<p>
<b>Model serving</b> - the way trained models are made available for others to use.
</p>

<p>
Multi Model Server (MMS) - serving deep learning models trained using any ML/DL framework. The tool can be
 used for many types of inference in production settings. It provides an easy-to-use command line interface
 and utilizes REST-based APIs handle state prediction requests.
</p>

<p>
The fundamental feature of having a CI/CD pipeline is to ensure that data scientists and software engineering
 teams are able to create and deploy error-free code as quickly as possible.
</p>

<p>
ML Process: <a id="org561c8af"></a>
</p>
<ul class="org-ul">
<li>idea</li>
<li>Research: NLP, DL</li>
<li>Opportunity Analysis</li>
<li>Offline experiment: feature, label/target, algortithm, model -&gt; model training -&gt; offline evaluation</li>
<li>Imporve offline metrics?</li>
<li>Productionalization</li>
<li>Verification</li>
<li>Deployment</li>
<li>Online A/B test</li>
<li>improve online metrics?</li>
</ul>

<p>
execution of ML Process:
</p>
<ul class="org-ul">
<li>Feature engineering</li>
<li>Trainging, and tuning</li>
<li>serving: offline, inference, online</li>
</ul>

<p>
management of ML Process:
</p>
<ul class="org-ul">
<li>Tracking: Data, Code, Configurations</li>
<li>Reproducing Results</li>
<li>Deployment in variety of environments</li>
</ul>

<p>
ML Model lifecycle:
</p>
</div>
</div>

<div id="outline-container-orgf599d6e" class="outline-3">
<h3 id="orgf599d6e"><span class="section-number-3">23.2.</span> Deployment Types:</h3>
<div class="outline-text-3" id="text-23-2">
<ul class="org-ul">
<li>Batch (offline) - prediction made at night at big dataset (most stupid)</li>
<li>Online serving - prediction made on new data
<ul class="org-ul">
<li>web services - forecasting(hourly, daily, monthly) - 1 client -&gt; 1 model</li>
<li>streaming - 1 client -&gt; stream -&gt; N models</li>
</ul></li>
<li>Online serving + learning</li>
</ul>
</div>
</div>

<div id="outline-container-org3884e82" class="outline-3">
<h3 id="org3884e82"><span class="section-number-3">23.3.</span> DevOps</h3>
<div class="outline-text-3" id="text-23-3">
<p>
Free book - “DS for DevOps”: <a href="https://do4ds.com">https://do4ds.com</a>
</p>
</div>
<div id="outline-container-org0184096" class="outline-4">
<h4 id="org0184096"><span class="section-number-4">23.3.1.</span> DevOps strategies</h4>
<div class="outline-text-4" id="text-23-3-1">
<p>
creating several instances of a live inferencing application for scalability and progressively switching from
 an older to a newer model.
</p>

<p>
Blue-Green Deployment - the newer version of the model is brought into the staging environment that is almost
 identical to the production environment. In some cases, the environment is the same as the production
 environment but the traffic is routed differently. If we utilize Kubernetes, it is possible to have a single
 k8s cluster to route the traffic to a separate (new k8s cluster) - the ‘blue’ deployment while the production
 traffic is going to older - ‘green’ deployment. This is to allow further testing of the newer model in a
 production environment before complete adoption. Once enough confidence is established in the newer model the
 older version is then moved to ‘green’ status and the process will repeat with any further improvements.
</p>

<p>
Canary deployment is a bit more involved and usually a lot riskier but it is gaining popularity among the
 DevOps community. It follows a similar deployment model as the blue-green discussed above but provides the
 ability to progressively change configuration based on constraints depending on the level of confidence in
 the newer model. In this case, traffic is routed progressively to the newer model at the same time the
 previous model is serving predictions. So the two versions are live and processing requests simultaneously,
 but doing them in different ratios. The reason for this percentage-based rollout is that you can enable
 metrics and other checks to capture problems in real-time, allowing you to roll back immediately if
 conditions are unfavorable.
</p>

<p>
Both of these strategies can be applied by Kubeflow as it natively relies on the Kubernetes environment.
</p>
</div>
</div>
</div>
<div id="outline-container-orgb38fa55" class="outline-3">
<h3 id="orgb38fa55"><span class="section-number-3">23.4.</span> CRISP-ML. The ML Lifecycle Process. <a id="orgf7b4d45"></a></h3>
<div class="outline-text-3" id="text-23-4">
<p>
Cross-Industry Standard Process for the development of Machine Learning applications with Quality assurance
 methodology
</p>
<ul class="org-ul">
<li><a href="https://ml-ops.org/content/crisp-ml">https://ml-ops.org/content/crisp-ml</a></li>
<li><a href="https://arxiv.org/pdf/2003.05155.pdf">https://arxiv.org/pdf/2003.05155.pdf</a></li>
</ul>

<p>
CRISP-DM focuses on data mining and does not cover the application
  scenario of ML models inferring real-time decisions over a long period
  of time.
</p>
</div>

<div id="outline-container-orgc0c4cc3" class="outline-4">
<h4 id="orgc0c4cc3"><span class="section-number-4">23.4.1.</span> CRISP-ML(Q) states main characteristics of mode choose: ⚿</h4>
<div class="outline-text-4" id="text-23-4-1">
<ul class="org-ul">
<li>Performance - on unseen data</li>
<li>Rebustness - model resiliency to inconsistent inputs and to failures in the env.</li>
<li>Scalability - to high data valume</li>
<li>Explainabilty - direct or post-hoc</li>
<li>Model Complexity - should suit the data complexity</li>
<li>Resorce Demand</li>
</ul>
</div>
</div>

<div id="outline-container-org13f05ac" class="outline-4">
<h4 id="org13f05ac"><span class="section-number-4">23.4.2.</span> phases</h4>
<div class="outline-text-4" id="text-23-4-2">
<ul class="org-ul">
<li>Business and Data Understanding</li>
<li>Data Engineering (Data Preparation)</li>
<li>Machine Learning Model Engineering</li>
<li>Quality Assurance for Machine Learning Applications</li>
<li>Deployment</li>
<li>Monitoring and Maintenance.</li>
</ul>



<p>
Business and Data Understanding
</p>
<ul class="org-ul">
<li>Define business objectives</li>
<li>Translate business objectives into ML objectives</li>
<li>Collect and verify data</li>
<li>Assess the project feasibility</li>
<li>Create POC</li>
</ul>
<p>
Data Engineering
</p>
<ul class="org-ul">
<li>Feature selection</li>
<li>Data selection</li>
<li>Class balancing</li>
<li>Cleaning data (noise reduction, data imputation)</li>
<li>Feature engineering (data construction)</li>
<li>Data augmentation</li>
<li>Data standartization</li>
</ul>
<p>
ML Model Engineering
</p>
<ul class="org-ul">
<li>Define quality measure of the model</li>
<li>ML algorithm selection (baseline selection)</li>
<li>Adding domain knowledge to specialize the model</li>
<li>Model training</li>
<li>Optional: applying trainsfer learning (using pre-trained models)</li>
<li>Model compression</li>
<li>Ensemble learning</li>
<li>Documenting the ML model and experiments</li>
</ul>
<p>
ML Model Evaluation
</p>
<ul class="org-ul">
<li>Validate model's performance</li>
<li>Determine robustess</li>
<li>Increase model's explainability</li>
<li>Make a decision whether to deploy the model</li>
<li>Document the evaluation phase</li>
</ul>
<p>
Model Deployment
</p>
<ul class="org-ul">
<li>Evaluate model under production condition</li>
<li>Assure user acceptance and usability</li>
<li>Model governance</li>
<li>Deploy according to the selected strategy (A/B testing, multi-armed bandits)</li>
</ul>
<p>
Model Monitoring and Maintenance
</p>
<ul class="org-ul">
<li>Monitor the efficiency and efficacy of the model prediction serving</li>
<li>Compare to the previously specified success criteria (thresholds)</li>
<li>Retrain model if required</li>
<li>Collect new data</li>
<li>Perform labelling of the new data points</li>
<li>Repeat tasks from the <b>Model Engineering</b> and <b>Model Evaluation</b> phases</li>
<li>Continuous, integration, training, and deployment of the model</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgbd878c1" class="outline-3">
<h3 id="orgbd878c1"><span class="section-number-3">23.5.</span> Challenges with the ML Process:</h3>
<div class="outline-text-3" id="text-23-5">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">data</th>
<th scope="col" class="org-left">model</th>
<th scope="col" class="org-left">Production</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Data/Research</td>
<td class="org-left">preparation</td>
<td class="org-left">ML Experties</td>
<td class="org-left">A/B testing</td>
</tr>

<tr>
<td class="org-left">scientist/</td>
<td class="org-left">analysis</td>
<td class="org-left">implement SOTA ML Research</td>
<td class="org-left">Model Evaluation</td>
</tr>

<tr>
<td class="org-left">ML Platform</td>
<td class="org-left">f. engineering</td>
<td class="org-left">Experimentation</td>
<td class="org-left">Analysis of Predictions</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Software/Data/</td>
<td class="org-left">Pipeline</td>
<td class="org-left">Manage GPU infrastructure</td>
<td class="org-left">deploy in variety of env.</td>
</tr>

<tr>
<td class="org-left">ML Engineer/</td>
<td class="org-left">Management,Feature Store</td>
<td class="org-left">Scalable training &amp;</td>
<td class="org-left">CI/CD, Highly available</td>
</tr>

<tr>
<td class="org-left">Abstraction</td>
<td class="org-left">Manages big data clusters</td>
<td class="org-left">hyperparameter tuning</td>
<td class="org-left">prod services</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org3f7c8ca" class="outline-3">
<h3 id="org3f7c8ca"><span class="section-number-3">23.6.</span> implemetation steps:</h3>
<div class="outline-text-3" id="text-23-6">
<ul class="org-ul">
<li>capture data from your business processes (ETL)
<ul class="org-ul">
<li>Hadoop to store and MapReduce to process</li>
<li>Apache Spark solved this problem by holding all the data in system memory</li>
</ul></li>
<li>combine this big data with massive processing to create machine learning models
<ul class="org-ul">
<li>create a machine learning data pipeline</li>
</ul></li>
<li>validate the models for accuracy and deploy them</li>
</ul>
</div>
</div>

<div id="outline-container-org1f85a56" class="outline-3">
<h3 id="org1f85a56"><span class="section-number-3">23.7.</span> pipeline services or workflow management software (WMS)</h3>
<div class="outline-text-3" id="text-23-7">
<ul class="org-ul">
<li>cron</li>
<li>Airbyte</li>
<li>Airflow</li>
<li>Dagster</li>
<li>Fivetran</li>
<li>Glue</li>
<li>Fifi</li>
<li>Luigi</li>
</ul>
</div>
</div>
<div id="outline-container-org1d84f62" class="outline-3">
<h3 id="org1d84f62"><span class="section-number-3">23.8.</span> tasks and tools</h3>
<div class="outline-text-3" id="text-23-8">
</div>
<div id="outline-container-org34bb6a7" class="outline-4">
<h4 id="org34bb6a7"><span class="section-number-4">23.8.1.</span> tasks</h4>
<div class="outline-text-4" id="text-23-8-1">
<p>
tasks
</p>
<ul class="org-ul">
<li>Model
<ul class="org-ul">
<li>model version management</li>
<li>model monitoring</li>
<li>model serving</li>
</ul></li>
<li>Data
<ul class="org-ul">
<li>хранение данных ML pipeline - входных, промежуточных, результирующих</li>
<li>data lineage</li>
</ul></li>
<li>Experiment
<ul class="org-ul">
<li>transparency</li>
<li>reproducibility</li>
<li>versioning</li>
<li>centralizes metadata</li>
</ul></li>
<li>Pipeline ML/ETL</li>
<li>experiment tracking and model registry.</li>
<li>верисонирование данных, моделей, экспериментов, pipeleine</li>
<li>data scientists collaborations</li>
<li>software repository is usually used to store artifacts - ex. JFrog Artifactory and Nexus repository.</li>
<li>reproducibility</li>
</ul>

<p>
Feature Store is to process data from various data sources at the same time and turn it into features.
</p>
<ul class="org-ul">
<li>Offline Stores - Store composed of preprocessed features of Batch Data, used for building a historical
source of features - focus on data lake, HDFS, etc.  including meta-repository</li>
<li>Online Stores - from the Offline Store combined with real-time preprocessed features from streaming data
sources. databases for rapid access, like MySQL, Cassandra, Redis. online part (I considered creating an API
layer and using storage such as Cassandra, MongoDB, Redis, etc.)</li>
</ul>

<p>
Feature Stores:
</p>
<ul class="org-ul">
<li>Metaflow - Proprietary - Netflix</li>
<li>Michelangelo 	Proprietary 	Uber</li>
<li>Feast 	Open-source 	Feast-dev, Tecton</li>
<li>Hopsworks 	Open-source 	LogicalClocks</li>
<li>Butterfree 	Open-source 	QuintoAndar</li>
<li></li>
</ul>
</div>
</div>
<div id="outline-container-orgd0f399a" class="outline-4">
<h4 id="orgd0f399a"><span class="section-number-4">23.8.2.</span> tools</h4>
<div class="outline-text-4" id="text-23-8-2">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">task</th>
<th scope="col" class="org-left">tools</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">IT Infrastructure</td>
<td class="org-left">Selectel, VMware, on-prem, hybrid clouds</td>
</tr>

<tr>
<td class="org-left">Data Labelling</td>
<td class="org-left">Label Studio</td>
</tr>

<tr>
<td class="org-left">Data Versioning &amp; Management</td>
<td class="org-left">DVC, Pachyderm, W&amp;B, Feast</td>
</tr>

<tr>
<td class="org-left">Exploratory Data Analysis (EDA)</td>
<td class="org-left">Jupyter Lab</td>
</tr>

<tr>
<td class="org-left">Code Management</td>
<td class="org-left">Git (external)</td>
</tr>

<tr>
<td class="org-left">Model Development</td>
<td class="org-left">Jupyter Lab, VS Code, PyCharm Pro</td>
</tr>

<tr>
<td class="org-left">Distributed Training</td>
<td class="org-left">Horovod, PyTorch</td>
</tr>

<tr>
<td class="org-left">Hyperparameter Tuning</td>
<td class="org-left">NNI, W&amp;B</td>
</tr>

<tr>
<td class="org-left">Experiment Tracking &amp; Metadata Store</td>
<td class="org-left">TensorBoard, MLflow, Kubeflow, ClearML</td>
</tr>

<tr>
<td class="org-left">Model Repository</td>
<td class="org-left">MLflow, Kubeflow, ClearML, W&amp;B</td>
</tr>

<tr>
<td class="org-left">Model Inference</td>
<td class="org-left">Seldon Core, Nvidia Triton, Nvidia TensorRT, MLflow, Kubeflow, ClearML, TensorFlowServing, /Tencent/ncnn</td>
</tr>

<tr>
<td class="org-left">Model Optimization (quantization)</td>
<td class="org-left">AIMET, torchao, HF load<sub>in</sub><sub>8</sub>/4bit=True</td>
</tr>

<tr>
<td class="org-left">Model Deployment</td>
<td class="org-left">Seldon Core, Seldon Deploy, TorchServe</td>
</tr>

<tr>
<td class="org-left">Model Testing / Validation</td>
<td class="org-left">Locust</td>
</tr>

<tr>
<td class="org-left">Model Monitoring</td>
<td class="org-left">evidently, MongoDB/Grafana, <a href="https://cloud.google.com/looker">https://cloud.google.com/looker</a></td>
</tr>

<tr>
<td class="org-left">Monitoring / Observability</td>
<td class="org-left">Prometheus + Grafana</td>
</tr>

<tr>
<td class="org-left">Experiment tracking</td>
<td class="org-left">MLFlow (small/medium), ClearML (big, include learning workflows and pipelines)</td>
</tr>

<tr>
<td class="org-left">Interpretation / Explainability</td>
<td class="org-left">SHAP, Seldon Alib</td>
</tr>

<tr>
<td class="org-left">интерфейс, оптимизация моделей</td>
<td class="org-left">OpenVino, ONNX Runtime, TensorRT, CoreML, ONNX, TorchScript, ORT</td>
</tr>

<tr>
<td class="org-left">Storage format</td>
<td class="org-left">HDF5, Apache arrow</td>
</tr>

<tr>
<td class="org-left">hyperparameter tuning</td>
<td class="org-left">grid search, random search, bayesian optimization,</td>
</tr>

<tr>
<td class="org-left">optimization</td>
<td class="org-left">dropout, batch normalization</td>
</tr>

<tr>
<td class="org-left">LLM orchistration</td>
<td class="org-left">LangChain, LlamaIndex, Langraph, LangSmith, guidance-ai/guidance</td>
</tr>

<tr>
<td class="org-left">High-Level Abstractions for NN</td>
<td class="org-left">fast.ai</td>
</tr>

<tr>
<td class="org-left">Computer vision</td>
<td class="org-left">OpenCV, fast.ai, TorchVision, TensorFlow, scikit-image, Dlib, MMCV</td>
</tr>

<tr>
<td class="org-left">gradient boosting, table data</td>
<td class="org-left">LightGBM(Microsoft), CatBoost(Yandex), XGBoost</td>
</tr>

<tr>
<td class="org-left">LLM clouds</td>
<td class="org-left">OpenAI, Anthropic</td>
</tr>

<tr>
<td class="org-left">LLM Models</td>
<td class="org-left">Llama2, Falcon, nomic-ai/gpt4all, YaLM, Claude</td>
</tr>

<tr>
<td class="org-left">LLM monitoring</td>
<td class="org-left">openlit</td>
</tr>

<tr>
<td class="org-left">data mining, presentation</td>
<td class="org-left">matplotlib, seaborn, plotly, IPyWidgets</td>
</tr>

<tr>
<td class="org-left">cloud of 3d points, 3d points cloud</td>
<td class="org-left"><a href="https://cloudcompare.org/">https://cloudcompare.org/</a></td>
</tr>

<tr>
<td class="org-left">Research</td>
<td class="org-left"><a href="https://notebooklm.google.com/">https://notebooklm.google.com/</a></td>
</tr>

<tr>
<td class="org-left">forecast</td>
<td class="org-left"><a href="https://pypi.org/project/prophet/">https://pypi.org/project/prophet/</a></td>
</tr>

<tr>
<td class="org-left">database hub</td>
<td class="org-left">Apache Calcite, trino, GraphQL</td>
</tr>

<tr>
<td class="org-left">Boolean satisfiability problem</td>
<td class="org-left">python-sat</td>
</tr>
</tbody>
</table>

<p>
search parameters: <a href="#orgdbc6eff">24.4</a>
</p>

<p>
Reinforcement learning tools: KerasRL, Pyqlearning, Tensorforce, RL<sub>Coach</sub>, TFAgents, Stable Baselines,
 mushroomRL, RLlib, Dopamine, SpinningUp, garage, Acme, coax, SURREAL
</p>

<p>
LightAutoML
</p>
<ul class="org-ul">
<li>LightAutoML на GitHub</li>
<li>Курс «Автоматическое машинное обучение с помощью LightAutoML»</li>
</ul>

<p>
Intel 2020  AI Infrastructure Stack <a href="https://intelcapital.file.force.com/sfc/dist/version/renditionDownload?rendition=ORIGINAL_Png&amp;versionId=0681I00000JFdtt&amp;operationContext=DELIVERY&amp;contentId=05T1I00000zZq3f&amp;page=0&amp;d=/a/1I000000Pii3/mlo1oVubic9_kTpSI5uTdrgR_T5RsBz3xNMXcobw9lM&amp;oid=00D1I000003pf77&amp;dpt=null&amp;viewId">https://intelcapital.file.force.com/sfc/dist/version/renditionDownload?rendition=ORIGINAL_Png&amp;versionId=0681I00000JFdtt&amp;operationContext=DELIVERY&amp;contentId=05T1I00000zZq3f&amp;page=0&amp;d=/a/1I000000Pii3/mlo1oVubic9_kTpSI5uTdrgR_T5RsBz3xNMXcobw9lM&amp;oid=00D1I000003pf77&amp;dpt=null&amp;viewId</a>=
</p>

<p>
TensorRT is a platform for high-performance deep learning inference.  inference throughput increased by up to
 2x to 3x over native Tensorflow depending on the batch size and precision used for TensorRT conversion.
</p>

<p>
notebooklm
</p>
<ul class="org-ul">
<li><a href="https://blog.google/technology/ai/notebooklm-beginner-tips/">https://blog.google/technology/ai/notebooklm-beginner-tips/</a></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org4531006"></a>data hub<br />
<div class="outline-text-5" id="text-23-8-2-1">
<ul class="org-ul">
<li>Apache Calcite: Ideal for building complex database systems, data integration, and federated queries.</li>
<li>Trino: Best suited for real-time analytics and big data querying.</li>
<li>GraphQL: Suitable for building flexible and efficient APIs where clients need to specify exact data
requirements.</li>
</ul>
</div>
</li>

<li><a id="org043f04b"></a>Distributed Training<br />
<div class="outline-text-5" id="text-23-8-2-2">
<p>
Ray is a unified framework for scaling AI and Python applications. Apache License 2.0 <a href="https://github.com/ray-project/ray">https://github.com/ray-project/ray</a>
</p>
</div>
</li>
<li><a id="orgc90d852"></a>ClearML<br />
<div class="outline-text-5" id="text-23-8-2-3">
<ul class="org-ul">
<li>Experiment Manager - Automagical experiment tracking, environments and results</li>
<li>MLOps / LLMOps - Orchestration, Automation &amp; Pipelines solution for ML/DL/GenAI jobs</li>
<li>Data-Management - Fully differentiable data management &amp; version control solution on top of object-storage (S3 / GS / Azure / NAS)</li>
<li>Model-Serving - (cloud-ready) - Deploy model endpoints, Nvidia-Triton, Model Monitoring</li>
<li>Reports</li>
<li>Orchestration Dashboard - Live rich dashboard for your entire compute cluster (Cloud / Kubernetes / On-Prem)</li>
<li>support Jupyter and Visual Studio</li>
</ul>
</div>
</li>
<li><a id="orgf903dd3"></a>ONNX - Open Neural Network Exchange<br />
<div class="outline-text-5" id="text-23-8-2-4">
<p>
was developed by the PyTorch team at Facebook, Common platform, Algorithm training, inference focused
</p>
<ul class="org-ul">
<li>open source format for AI model</li>
<li>compatible with TensorFlow, Keras, Caffe, Torch,</li>
</ul>


<p>
intent:
</p>
<ul class="org-ul">
<li>Framework interoperability</li>
<li>Allow hardware vendors - multiple frameoworks</li>
</ul>

<p>
Includes:  extensible computation graph model, built-in operators and standard data types
</p>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-org3fec1f2" class="outline-3">
<h3 id="org3fec1f2"><span class="section-number-3">23.9.</span> principles</h3>
<div class="outline-text-3" id="text-23-9">
<ul class="org-ul">
<li>CI/CD</li>
<li>Workflow orchestration</li>
<li>Reproducibility</li>
<li>Versioning of data, code, model</li>
<li>Collaboration</li>
<li>Continuous ML training &amp; evaluation</li>
<li>ML metadata tracking</li>
<li>Continuous monitoring</li>
<li>Feedback loops</li>
</ul>
</div>
</div>
<div id="outline-container-org5558078" class="outline-3">
<h3 id="org5558078"><span class="section-number-3">23.10.</span> standard</h3>
<div class="outline-text-3" id="text-23-10">
<p>
ISO/IEC 23053 Machine learning framework
</p>
<ul class="org-ul">
<li>ИСО/МЭК 23053:2022</li>
<li>Дата введения в действие: 20.06.2022</li>
<li>Платформа разработки систем искусственного интеллекта (AI) с использованием машинного обучения (ML)</li>
<li>Заглавие на английском языке Framework for Artificial Intelligence (AI) Systems Using Machine Learning (ML)</li>
<li>Количество страниц оригинала 44</li>
</ul>
</div>

<div id="outline-container-org2502763" class="outline-4">
<h4 id="org2502763"><span class="section-number-4">23.10.1.</span> ISO/IEC DIS 5259-1 Artificial intelligence — Data quality for analytics and machine learning (ML) — Part 1: Overview, terminology, and examples</h4>
<div class="outline-text-4" id="text-23-10-1">
<ul class="org-ul">
<li>ISO/IEC WD 5259 Качество данных для аналитики и машинного обучения. Инструменты для мониторинга качества данных.</li>
<li>Роли
<ul class="org-ul">
<li>аннотатор - маркировка</li>
<li>инспектор - проверяет макрировку</li>
<li>менеджер - распред работ по маркировке и назначает инспекторов и ответств лица</li>
</ul></li>
<li>DLC - data life cycle - модель DLC</li>
<li>DQPF - data quality process framework</li>
</ul>
<p>
Дедентиикация
</p>
<ul class="org-ul">
<li>анонимизация</li>
<li>псевдоанонимизация</li>
<li>удаление записей</li>
<li>агрегация</li>
<li>дифференциальная конфиденциальность.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org6a60dbb" class="outline-3">
<h3 id="org6a60dbb"><span class="section-number-3">23.11.</span> TFX - Tensorflow Extended</h3>
<div class="outline-text-3" id="text-23-11">
<p>
open-source version of the data science and initial phases of the MLOps solution developed by Google.
</p>

<p>
TFX emphasizes the importance of validating datasets and asserting the schema, calculating the statistics and
 distribution of the features, etc.
</p>

<p>
TFDV gives us the ability to compare two datasets that can be used to determine if our train/eval splits are
 having similar characteristics, etc.
</p>
</div>
</div>
<div id="outline-container-orge77c217" class="outline-3">
<h3 id="orge77c217"><span class="section-number-3">23.12.</span> <span class="todo TODO">TODO</span> Kubeflow</h3>
</div>
<div id="outline-container-org5d332a1" class="outline-3">
<h3 id="org5d332a1"><span class="section-number-3">23.13.</span> <span class="todo TODO">TODO</span> Airflow</h3>
<div class="outline-text-3" id="text-23-13">
<p>
<a href="https://towardsdatascience.com/machine-learning-in-production-using-apache-airflow-91d25a4d8152">https://towardsdatascience.com/machine-learning-in-production-using-apache-airflow-91d25a4d8152</a>
</p>
</div>
</div>
<div id="outline-container-orgcdb86de" class="outline-3">
<h3 id="orgcdb86de"><span class="section-number-3">23.14.</span> <span class="todo TODO">TODO</span> - mlmodel service</h3>
</div>
<div id="outline-container-org73d2c5e" class="outline-3">
<h3 id="org73d2c5e"><span class="section-number-3">23.15.</span> <span class="todo TODO">TODO</span> continuous training</h3>
<div class="outline-text-3" id="text-23-15">
<p>
see <a href="#org9def081">11.5.9.2</a>
</p>
</div>
</div>
<div id="outline-container-orgcd19c52" class="outline-3">
<h3 id="orgcd19c52"><span class="section-number-3">23.16.</span> <span class="todo TODO">TODO</span> Feature attribution or feature importance</h3>
<div class="outline-text-3" id="text-23-16">
<p>
is a function that will accept model inputs and give a per-feature attribution score based on the feature's
 contribution to the model's output
</p>

<p>
used in continuous monitoring?
</p>
</div>
</div>
<div id="outline-container-orga39a4f1" class="outline-3">
<h3 id="orga39a4f1"><span class="section-number-3">23.17.</span> Monitoring</h3>
<div class="outline-text-3" id="text-23-17">
<p>
tools <a href="https://github.com/awesome-mlops/awesome-ml-monitoring">https://github.com/awesome-mlops/awesome-ml-monitoring</a>
archive prediction logs + processing pipeline that assigns the ground truth labels
</p>
</div>
<div id="outline-container-org775f0a0" class="outline-4">
<h4 id="org775f0a0"><span class="section-number-4">23.17.1.</span> metrics</h4>
<div class="outline-text-4" id="text-23-17-1">
<ol class="org-ol">
<li>Service health:
<ul class="org-ul">
<li>uptime</li>
<li>memory</li>
<li>latency</li>
</ul></li>
<li><p>
Model performance
</p>
<ul class="org-ul">
<li>model accuracy</li>
<li>Prediction drift - distributions of model outputs (if your model behaves similarly to how it acted in</li>
</ul>
<p>
training, it’s probably of a similar quality. )
</p></li>
<li>Data quality and integrity
<ul class="org-ul">
<li>broken piplelines</li>
<li>schema change</li>
<li>data outage - some failure in transfer</li>
<li>count of missing values</li>
</ul></li>
<li>Data and concept drift
<ul class="org-ul">
<li>concept drift - change in real-world patterns, lead to model degrade</li>
<li>data drift -  shift in input data distributions.</li>
</ul></li>
<li>Advanced metrics
<ul class="org-ul">
<li>performance by segments - check for underperforming</li>
<li>model bias /fairness</li>
<li>outliers</li>
<li>explainability</li>
</ul></li>
</ol>

<p>
or
</p>
<ul class="org-ul">
<li>ML model decay</li>
<li>Numerical stability</li>
<li>Computational performance of the ML model</li>
</ul>
</div>
</div>
<div id="outline-container-org7c60644" class="outline-4">
<h4 id="org7c60644"><span class="section-number-4">23.17.2.</span> batch vs online</h4>
<div class="outline-text-4" id="text-23-17-2">
<p>
for batch:
</p>
<ul class="org-ul">
<li>we have historical dataset and new batch of data.</li>
<li>expected data queality - 80% non-constant</li>
<li>data distribution type (e.g. normality)</li>
<li>descriptive statistics: averages, median, quantiles, min-max, individual features</li>
</ul>

<p>
for online:
</p>
<ul class="org-ul">
<li>sliding windows with parameters: window size and step size</li>
</ul>

<p>
for both:
</p>
<ul class="org-ul">
<li>with or without moving reference window of testing("compare") data.</li>
<li>database for (I/O model logs) for prediction logs</li>
<li>database for monitoring evaluation result store</li>
<li>report or dashboard</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org6fd739e" class="outline-3">
<h3 id="org6fd739e"><span class="section-number-3">23.18.</span> Principles</h3>
<div class="outline-text-3" id="text-23-18">
<p>
<a href="https://ml-ops.org/content/mlops-principles#summary-of-mlops-principles-and-best-practices">https://ml-ops.org/content/mlops-principles#summary-of-mlops-principles-and-best-practices</a>
</p>
</div>
<div id="outline-container-orga6e47c4" class="outline-4">
<h4 id="orga6e47c4"><span class="section-number-4">23.18.1.</span> effectivenes metrics</h4>
<div class="outline-text-4" id="text-23-18-1">
</div>
<ol class="org-ol">
<li><a id="org8f8b5a3"></a>Deployment Frequency<br />
<div class="outline-text-5" id="text-23-18-1-1">
<p>
How often does your organization deploy code to production or release it to end-users?
</p>

<p>
ML Model Deployment Frequency depends on
</p>
<ol class="org-ol">
<li>Model retraining requirements (ranging from less frequent to online training). Two aspects are crucial for model retraining</li>
</ol>
<p>
1.1) Model decay metric.
1.2) New data availability.
</p>
<ol class="org-ol">
<li>The level of automation of the deployment process, which might range between <b>manual deployment</b> and <b>fully automated CI/CD pipeline</b>.</li>
</ol>
</div>
</li>
<li><a id="org9ee35dc"></a>Lead Time for Changes<br />
<div class="outline-text-5" id="text-23-18-1-2">
<p>
How long does it take to go from code committed to code successfully running in production?
</p>

<p>
ML Model Lead Time for Changes depends on
</p>
<ol class="org-ol">
<li>Duration of the explorative phase in Data Science in order to finalize the ML model for deployment/serving.</li>
<li>Duration of the ML model training.</li>
<li>The number and duration of manual steps during the deployment process.</li>
</ol>
</div>
</li>
<li><a id="org532f9cf"></a>Mean Time To Restore (MTTR)<br />
<div class="outline-text-5" id="text-23-18-1-3">
<p>
How long does it generally take to restore service when a service incident or a defect that impacts users
 occurs (e.g., unplanned outage or service impairment)?
</p>

<p>
ML Model MTTR depends on the number and duration of manually performed model debugging, and model deployment
 steps. In case, when the ML model should be retrained, then MTTR also depends on the duration of the ML model
 training. Alternatively, MTTR refers to the duration of the rollback of the ML model to the previous version.
</p>
</div>
</li>
<li><a id="org76b8f32"></a>Change Failure Rate<br />
<div class="outline-text-5" id="text-23-18-1-4">
<p>
What percentage of changes to production or released to users result in degraded service (e.g., lead to
 service impairment or service outage) and subsequently require remediation (e.g., require a hotfix, rollback,
 fix forward, patch)?
</p>

<p>
ML Model Change Failure Rate can be expressed in the difference of the currently deployed ML model performance
 metrics to the previous model's metrics, such as Precision, Recall, F-1, accuracy, AUC, ROC, false positives,
 etc. ML Model Change Failure Rate is also related to A/B testing.
</p>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-orgb308b19" class="outline-3">
<h3 id="orgb308b19"><span class="section-number-3">23.19.</span> links</h3>
<div class="outline-text-3" id="text-23-19">
<p>
<a href="https://en.wikipedia.org/wiki/ModelOps">https://en.wikipedia.org/wiki/ModelOps</a>
</p>
<ul class="org-ul">
<li>arxiv.org 2205.02302 Machine Learning Operations (MLOps): Overview, Definition, Architecture</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org77eb9ed" class="outline-2">
<h2 id="org77eb9ed"><span class="section-number-2">24.</span> Automated machine learning (AutoML)</h2>
<div class="outline-text-2" id="text-24">
<p>
AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model.
</p>

<p>
Open Source
</p>
<ul class="org-ul">
<li>Seldom Core</li>
<li>Mlflow - popular</li>
</ul>

<p>
ML platforms RUS
</p>
<ul class="org-ul">
<li>selectel.ru</li>
<li>ML Space - Сбер</li>
</ul>

<p>
LLMOps: Auto-GPT, vectorDBs
</p>
</div>
<div id="outline-container-org18bc08d" class="outline-3">
<h3 id="org18bc08d"><span class="section-number-3">24.1.</span> major papers</h3>
<div class="outline-text-3" id="text-24-1">
<ul class="org-ul">
<li>Automated Machine Learning - Methods, Systems, Challenges. Springer, 2019 <a href="https://www.automl.org/wp-content/uploads/2019/05/AutoML_Book.pdf">https://www.automl.org/wp-content/uploads/2019/05/AutoML_Book.pdf</a></li>
</ul>
</div>
</div>
<div id="outline-container-orge584993" class="outline-3">
<h3 id="orge584993"><span class="section-number-3">24.2.</span> history</h3>
<div class="outline-text-3" id="text-24-2">
<ul class="org-ul">
<li>AUTO-WEKA (Thornton et al., 2013) - Bayesian optimization to select and tune the algorithm</li>
</ul>
</div>
</div>
<div id="outline-container-org3d287ca" class="outline-3">
<h3 id="org3d287ca"><span class="section-number-3">24.3.</span> tasks</h3>
<div class="outline-text-3" id="text-24-3">
<ul class="org-ul">
<li>Neural Architecture Search (NAS)</li>
<li>Hyperparameter Optimization</li>
<li>Meta-Learning - 1) collect meta-data: prior learning tasks and previously learned models 2) learn from
meta-data to extract and transfer knowledge that guides the search for optimal models for</li>
</ul>
<p>
new tasks
</p>
<ul class="org-ul">
<li>meta-features - measurable properties of the task itself</li>
</ul>
</div>
</div>
<div id="outline-container-orgee0a05a" class="outline-3">
<h3 id="orgee0a05a"><span class="section-number-3">24.4.</span> approaches</h3>
<div class="outline-text-3" id="text-24-4">
<ul class="org-ul">
<li>sequential model-based optimization (Hutter et al., 2011; Snoek et al., 2012),</li>
<li>hierarchical task planning (Erol et al., 1994)</li>
<li>genetic programming (Koza, 1992)</li>
</ul>

<p>
optimization techniques <a id="orgdbc6eff"></a>:
</p>
<ul class="org-ul">
<li>Bayesian optimization (BO)</li>
<li>evolutionary optimization (EO)</li>
<li>random search (RS)</li>
<li>cost frugal optimization (CFO)</li>
</ul>
</div>
</div>
<div id="outline-container-org42992a1" class="outline-3">
<h3 id="org42992a1"><span class="section-number-3">24.5.</span> banchmark</h3>
<div class="outline-text-3" id="text-24-5">
<p>
2023 <a href="https://arxiv.org/pdf/2207.12560.pdf">https://arxiv.org/pdf/2207.12560.pdf</a>
</p>
</div>
</div>
<div id="outline-container-org87daa64" class="outline-3">
<h3 id="org87daa64"><span class="section-number-3">24.6.</span> opensource frameworks</h3>
<div class="outline-text-3" id="text-24-6">
<ul class="org-ul">
<li>AUTOGLUON		Stacked ensembles of preset pipelines	Erickson et al. (2020)</li>
<li>AUTO-SKLEARN		BO of SCIKIT-LEARN pipelines 		Feurer et al. (2015a)</li>
<li>AUTO-SKLEARN 2	BO of iterative algorithms 		Feurer et al. (2020)</li>
<li>FLAML 		CFO of iterative algorithms 		Wang et al. (2021)</li>
<li>GAMA 		EO of SCIKIT-LEARN pipelines 		Gijsbers and Vanschoren (2021)</li>
<li>H2O AUTOML 		Iterative mix of RS and ensembling 	LeDell and Poirier (2020)</li>
<li>LIGHTAUTOML 		BO of linear models and GBM 		Vakhrushev et al. (2021)</li>
<li>MLJAR 		Custom data science pipeline 		Plónska and Plónski (2021)</li>
<li>NAIVEAUTOML 		Custom data science pipeline 		Mohr and Wever (2023)</li>
<li>TPOT 		EO of SCIKIT-LEARN pipelines 		Olson and Moore (2016)</li>
</ul>
<p>
GPU based
</p>
<ul class="org-ul">
<li>AUTO-KERAS 		(Jin et al.,2019)</li>
<li>AUTOPYTORCH 		(Zimmer et al., 2021)</li>
</ul>
</div>
</div>

<div id="outline-container-org21dee2b" class="outline-3">
<h3 id="org21dee2b"><span class="section-number-3">24.7.</span> popular web</h3>
<div class="outline-text-3" id="text-24-7">
<ul class="org-ul">
<li><a href="https://huggingface.co/autotrain">https://huggingface.co/autotrain</a>
<ul class="org-ul">
<li><a href="https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/colabs/AutoTrain_Dreambooth.ipynb#scrollTo=JvMRbVLEJlZT">https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/colabs/AutoTrain_Dreambooth.ipynb#scrollTo=JvMRbVLEJlZT</a></li>
<li><a href="https://github.com/huggingface/autotrain-advanced">https://github.com/huggingface/autotrain-advanced</a></li>
</ul></li>
<li><a href="https://cloud.ru/ru/aicloud/mlspace">https://cloud.ru/ru/aicloud/mlspace</a></li>
<li><a href="https://aws.amazon.com/sagemaker/autopilot/">https://aws.amazon.com/sagemaker/autopilot/</a></li>
<li><a href="https://azure.microsoft.com/ru-ru/products/machine-learning/automatedml/">https://azure.microsoft.com/ru-ru/products/machine-learning/automatedml/</a></li>
<li><a href="https://h2o.ai/platform/h2o-automl/#web">https://h2o.ai/platform/h2o-automl/#web</a></li>
<li>ML Space</li>
<li>oracle
<ul class="org-ul">
<li>ui <a href="https://docs.oracle.com/en/database/oracle/machine-learning/oml-automl-ui/books.html">https://docs.oracle.com/en/database/oracle/machine-learning/oml-automl-ui/books.html</a>
<ul class="org-ul">
<li>python <a href="https://docs.oracle.com/en/database/oracle/machine-learning/oml4py/2/mlpug/automated-machine-learning1.html#GUID-9F514C2B-1772-4073-807F-3E829D5D558C">https://docs.oracle.com/en/database/oracle/machine-learning/oml4py/2/mlpug/automated-machine-learning1.html#GUID-9F514C2B-1772-4073-807F-3E829D5D558C</a></li>
</ul></li>
</ul></li>
</ul>
</div>

<div id="outline-container-org50c91c4" class="outline-4">
<h4 id="org50c91c4"><span class="section-number-4">24.7.1.</span> ml space horovod + tensorflow</h4>
<div class="outline-text-4" id="text-24-7-1">
<p>
<a href="https://cloud.ru/ru/docs/aicloud/mlspace/concepts/guides/guides__mt/environments__model-training__training-with-horovod-example.html">https://cloud.ru/ru/docs/aicloud/mlspace/concepts/guides/guides__mt/environments__model-training__training-with-horovod-example.html</a>
</p>
</div>
</div>
</div>
<div id="outline-container-org6e58b8c" class="outline-3">
<h3 id="org6e58b8c"><span class="section-number-3">24.8.</span> classification of tasks</h3>
<div class="outline-text-3" id="text-24-8">
<p>
<a href="https://learn.microsoft.com/en-us/azure/machine-learning/concept-automated-ml?view=azureml-api-2">https://learn.microsoft.com/en-us/azure/machine-learning/concept-automated-ml?view=azureml-api-2</a>
</p>
</div>
</div>

<div id="outline-container-org9f73318" class="outline-3">
<h3 id="org9f73318"><span class="section-number-3">24.9.</span> automl &amp; blockchain</h3>
<div class="outline-text-3" id="text-24-9">
<p>
<a href="https://analyticsindiamag.com/how-machine-learning-can-be-used-with-blockchain-technology/">https://analyticsindiamag.com/how-machine-learning-can-be-used-with-blockchain-technology/</a>
</p>

<p>
A Blockchain and AutoML Approach for Open and Automated Customer Service
</p>
<ul class="org-ul">
<li>Authors: Zhi Li</li>
<li>GuangDong University of Technology</li>
</ul>

<p>
Combining Blockchain and Artificial Intelligence - Literature Review and State of the Art
</p>
<ul class="org-ul">
<li>Nov 2020</li>
<li>Erik Karger</li>
</ul>

<p>
Artificial Intelligence and Blockchain Integration in Business: Trends from a Bibliometric-Content Analysis
</p>
<ul class="org-ul">
<li>Apr 2022</li>
<li>Satish Kumar Weng Marc LimUthayasankar Sivarajah    Jaspreet Kaur</li>
</ul>

<p>
A Blockchain and AutoML Approach for Open and Automated Customer Service
</p>
<ul class="org-ul">
<li>2019)</li>
<li>Zhi Li; Hanyang Guo; Wai Ming Wang; Yijiang Guan; Ali Vatankhah Barenji</li>
</ul>

<p>
BACS: blockchain and AutoML-based technology for efficient credit scoring classification
</p>
<ul class="org-ul">
<li>Fan Yang, Yanan Qiao, Yong Qi, Junge Bo &amp; Xiao Wang</li>
<li>2022</li>
</ul>

<p>
Towards Open and Automated Customer Service: A Blockchain-based AutoML Framework
</p>
<ul class="org-ul">
<li>22 October 2018</li>
<li>W. Wang, Hanyang Guo, A. V. Barenj</li>
</ul>
</div>
</div>

<div id="outline-container-org16ac881" class="outline-3">
<h3 id="org16ac881"><span class="section-number-3">24.10.</span> books</h3>
<div class="outline-text-3" id="text-24-10">
<ul class="org-ul">
<li><a href="https://library.oapen.org/handle/20.500.12657/23012">https://library.oapen.org/handle/20.500.12657/23012</a></li>
<li><a href="https://www.automl.org/book/">https://www.automl.org/book/</a></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org26e72e9" class="outline-2">
<h2 id="org26e72e9"><span class="section-number-2">25.</span> Big Data</h2>
<div class="outline-text-2" id="text-25">
<p>
Large and complex data sets. To extract value from data and seldom to a particular size of data set. опред
размера. =&gt; Advanced data analytics methods.
</p>
<ul class="org-ul">
<li>offer greater <b>statistical power</b></li>
<li>may lead to a higher <b>false discovery rate</b></li>
<li>concepts:
<ul class="org-ul">
<li>volume[ˈvɒljuːm]</li>
<li>variety[vəˈraɪɪtɪ]
<ul class="org-ul">
<li>Transactions - database records</li>
<li>Files - documents, log files</li>
<li>Events - Messages, Data streams.</li>
</ul></li>
<li>velocity[vɪˈlɒsɪtɪ] (noise, value) - batch, peruiduc, near Real Time, Real Time or Hot, Warm, Cold</li>
<li>veracity [vɛˈræsɪtɪ]</li>
</ul></li>
</ul>
<p>
For:
</p>
<ul class="org-ul">
<li>spot business trends</li>
<li>prevent diseases, combat crime</li>
<li>Internet search, fintech, urban informatics, and business informatics</li>
<li>e-Science - meteorolgy, genomics, connectomics, complex physics simulations</li>
</ul>
<p>
Sources:
</p>
<ul class="org-ul">
<li>Internet of things devices such as mobile devices</li>
<li>aerial (remote sensing)</li>
<li>software logs</li>
<li>cameras, microphones, radio-frequency identification (RFID) readers</li>
<li>wireless sensor networks</li>
</ul>


<p>
<b>Architecture:</b> require massively parallel software running on clusters or more.
</p>
<ul class="org-ul">
<li>Commercial vendors historically offered parallel database management systems.</li>
<li>physics experiment - high performance computing (supercomputers)</li>
<li>Google - MapReduce 1. queries are split and distributed across parallel nodes and processed in parallel (the
Map step). 2. results are then gathered and delivered. Adopted by an Apache project <b>Hadoop</b> and <b>Spark</b></li>
<li>MIKE2.0 methodology -  pilot project for a "framework"</li>
<li>multiple-layer architecture - inserts data into a parallel DBMS, which implements the use of MapReduce and
Hadoop frameworks</li>
<li>data lake - способ управления большими данными, когда все сбрасывается в один репозиторий файлов или blob
объейктов, а потом уже анализируется.</li>
</ul>

<p>
<b>Store</b>
</p>
<ul class="org-ul">
<li>Records - database</li>
<li>documents - search?</li>
<li>files - file store</li>
<li>messages - Amazon SQS</li>
<li>streams - Apache Kafka, Amazon shit.</li>
</ul>

<p>
Why stream storage?
</p>
<ul class="org-ul">
<li>Decouple producers &amp; consumenrs</li>
<li>Persistent buffer</li>
<li>Collect multiple streams</li>
<li>Preserve client ordering</li>
<li>Parallel consumption</li>
<li>Streaming MapReduce</li>
</ul>

<p>
Delivery (deduping - data deduplication) guarantees
</p>
<ul class="org-ul">
<li>at-most-once delivery - message may be lost - hight perfomance</li>
<li>at-least-once delivery - may be duplicated but not lost</li>
<li>exactly-once delivery - not lost and not duplicated</li>
</ul>
</div>
</div>

<div id="outline-container-org9f74c6e" class="outline-2">
<h2 id="org9f74c6e"><span class="section-number-2">26.</span> hard questions</h2>
<div class="outline-text-2" id="text-26">
<p>
при убирании старых записей (Stratified kfold cross validation)
</p>
<ul class="org-ul">
<li>точность на кросс вал увелививается,</li>
<li>точность на тестовой выборке уменьшается</li>
<li>при увеличении сплитов точность падает</li>
<li>гипотеза - разница увеличивается со временем.
<ul class="org-ul">
<li>не объясняет уменьшение точности на тестовой выборке</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgeca2b55" class="outline-2">
<h2 id="orgeca2b55"><span class="section-number-2">27.</span> cloud, clusters</h2>
<div class="outline-text-2" id="text-27">
<ul class="org-ul">
<li>Desk - paralled computing for sklearn, numpy, pandas</li>
</ul>
</div>

<div id="outline-container-orgb36cbd9" class="outline-3">
<h3 id="orgb36cbd9"><span class="section-number-3">27.1.</span> Data Anonymization, Dataset Privacy, Scrubbing Techniques</h3>
<div class="outline-text-3" id="text-27-1">
</div>
<div id="outline-container-orgf85bf92" class="outline-4">
<h4 id="orgf85bf92"><span class="section-number-4">27.1.1.</span> terms</h4>
<div class="outline-text-4" id="text-27-1-1">
<ul class="org-ul">
<li>direct identifiers - any unique code, Names, dates, phone numbrs, account numbers, biometric identifiers,
face photo</li>
<li>indirect identifiers - age, geo-location, service provider, race,</li>
</ul>
</div>
</div>

<div id="outline-container-org5a1ea77" class="outline-4">
<h4 id="org5a1ea77"><span class="section-number-4">27.1.2.</span> Scrubbing Techniques</h4>
<div class="outline-text-4" id="text-27-1-2">
<ul class="org-ul">
<li>Scrubbing Techniques - just delete columns with phone numbers (for direct)
<ul class="org-ul">
<li>important information may be mistaken for personal information and deleted accidentally.</li>
</ul></li>
<li>Pseudonymization - label encoding or hash (for direct)
<ul class="org-ul">
<li>If you have a list of students and you release their grades using an anonymous ID, it is probably a good
idea not to do it in alphabetical order as it makes it fairly easy to reidentify people!</li>
<li>if a deterministic algorithm is used to perform the pseudonymization, and the nature of the algorithm used
is uncovered, it then compromises the anonymity of the individuals.</li>
<li>direct identifiers can be difficult to identify and replace, and indirect identifiers are inadvertently
left in the dataset.</li>
</ul></li>
<li>Statistical Noise (for indirect)
<ul class="org-ul">
<li>Generalization: Specific values can be reported as a range</li>
<li>Perturbation: Specific values can be randomly adjusted for all patients in a dataset. For example,
systematically adding or subtracting the same number of days from when a patient was admitted for care, or
adding noise from a normal distribution.</li>
<li>Swapping: Data can be exchanged between individual records within a dataset.</li>
</ul></li>
<li>Aggregation - the dataset is aggregated and only a summary statistic or subset is released.</li>
</ul>

<p>
University of Waterloo
</p>
<ul class="org-ul">
<li>Removal – eliminating the variable from the data set</li>
<li>Bracketing – combining the categories of a variable</li>
<li>Top-coding – restricting the upper range of a variable</li>
<li>Collapsing and/or combining variables – merging the concepts embodied in two or more variables by creating a
new summary variable</li>
<li>Sampling – rather than providing all of the original data, releasing a random sample of sufficient size to
yield reasonable inferences</li>
<li>Swapping – matching unique cases on the indirect identifier, then exchanging the values of key variables
between the cases. Swapping is a service that archives may offer to limit disclosure risk</li>
<li>Disturbing – adding random variation or stochastic error to the variable.</li>
</ul>

<p>
Additional tips for minimizing disclosure risk:
 Use weighted data; disclosure risk is reduced when weights are used to generate
output
 Avoid submitting tables with small cell sizes (i.e., cells with fewer than 5
respondents)
 Restrict cross-tabular analysis to two or three dimensions
 Be cautious when using small subgroups or small areas
 Avoid listings of cases with outliers
</p>

<p>
<b>Federated Learning</b>, also known as collaborative learning, is a deep learning technique where the training
  takes place across multiple decentralized edge devices (clients) or servers on their personal data, without
  sharing the data with other clients, thus keeping the data private. It aims at training a machine learning
  algorithm, say, deep neural networks on multiple devices (clients) having local datasets without explicitly
  exchanging the data samples.
</p>
</div>
</div>
<div id="outline-container-org49daad9" class="outline-4">
<h4 id="org49daad9"><span class="section-number-4">27.1.3.</span> tools</h4>
<div class="outline-text-4" id="text-27-1-3">
<p>
<a href="https://github.com/microsoft/presidio">https://github.com/microsoft/presidio</a>
</p>
</div>
</div>
<div id="outline-container-org5ea6deb" class="outline-4">
<h4 id="org5ea6deb"><span class="section-number-4">27.1.4.</span> links</h4>
<div class="outline-text-4" id="text-27-1-4">
<p>
<a href="https://towardsdatascience.com/data-privacy-in-the-age-of-big-data-c28405e15508">https://towardsdatascience.com/data-privacy-in-the-age-of-big-data-c28405e15508</a>
</p>
</div>
</div>
</div>
<div id="outline-container-orgff21e1e" class="outline-3">
<h3 id="orgff21e1e"><span class="section-number-3">27.2.</span> docker NVIDIA Container Toolkit</h3>
<div class="outline-text-3" id="text-27-2">
<ul class="org-ul">
<li>on server: NVIDIA CUDA Driver and NVIDIA Container Toolkit
<ul class="org-ul">
<li>nvidia-docker wrapper ("NVIDIA Container Toolkit" package)</li>
<li>NVIDIA Container Runtime (nvidia-container-runtime)</li>
</ul></li>
<li>in container: CUDA Toolkid</li>
</ul>

<p>
CUDA images
</p>
<ul class="org-ul">
<li>base: Includes the CUDA runtime (cudart)
<ul class="org-ul">
<li><a href="https://gitlab.com/nvidia/container-images/cuda/blob/master/dist/11.8.0/ubuntu2004/base/Dockerfile">https://gitlab.com/nvidia/container-images/cuda/blob/master/dist/11.8.0/ubuntu2004/base/Dockerfile</a></li>
</ul></li>
<li>runtime: Builds on the base and includes the CUDA math libraries, and NCCL. A runtime image that also
includes cuDNN is available.</li>
<li>devel: Builds on the runtime and includes headers, development tools for building CUDA images. These images
are particularly useful for multi-stage builds.</li>
</ul>


<p>
links
</p>
<ul class="org-ul">
<li><a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda">https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda</a></li>
<li><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html</a></li>
<li><a href="https://hub.docker.com/r/nvidia/cuda">https://hub.docker.com/r/nvidia/cuda</a></li>
</ul>

<p>
Notes:
</p>
<ul class="org-ul">
<li>отключено обновление apt-mark hold nvidia-utils-525 apt-mark hold nvidia-utils-520</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgf2cfc87" class="outline-2">
<h2 id="orgf2cfc87"><span class="section-number-2">28.</span> Data Roles - Data team</h2>
<div class="outline-text-2" id="text-28">
<ul class="org-ul">
<li>ML Engineer/MLOps Engineer - ML infractructure, ML models, ML workflow pipelines, data Ingestion, monitoring</li>
<li>Data Engineer - data management, data pipeline management</li>
<li>DevOps Engineer - Software engineer and DevOps skills, ML workflow pipeline orchestration, CI/CD pipeline management, monitoring</li>
<li>Software Engineer (bottom) - applies design patterns and coding guidlines</li>
<li>Data Scientist - ML model development</li>
<li>Backend Engineer - ML infractructure management</li>
</ul>
</div>
<div id="outline-container-org27ca940" class="outline-3">
<h3 id="org27ca940"><span class="section-number-3">28.1.</span> Architect -</h3>
<div class="outline-text-3" id="text-28-1">
<ul class="org-ul">
<li>Communication</li>
<li>Modeling</li>
<li>Business Acumen?</li>
</ul>
</div>
</div>
<div id="outline-container-orgfb081db" class="outline-3">
<h3 id="orgfb081db"><span class="section-number-3">28.2.</span> System analyst</h3>
<div class="outline-text-3" id="text-28-2">
<ul class="org-ul">
<li>сбор требований</li>
<li>документация</li>
<li>интеграции и базы данных</li>
<li>проектирование модели данных и решения</li>
<li>SQL запросы</li>
<li>проектирование высоконагруженной системы</li>
<li>выбор технологий</li>
</ul>
</div>
</div>
<div id="outline-container-orgc59b812" class="outline-3">
<h3 id="orgc59b812"><span class="section-number-3">28.3.</span> Data Engineers</h3>
<div class="outline-text-3" id="text-28-3">
<p>
essential:
</p>
<ul class="org-ul">
<li>Data Pipeline</li>
<li>Databases</li>
<li>Data Tools</li>
</ul>

<p>
Architecting and maintaining databases, building pipelines that move the data through different sources and
 systems, and developing tools used by the company for analytics, dashboarding, and, eventually, ML.
</p>
<ul class="org-ul">
<li>programming languages such as SQL and Python</li>
<li>familiar with modern data tools and solutions (Amazon Web Services, Google Cloud Platform, Snowflake,
distributed systems, dbt, Airflow, and more).</li>
</ul>
</div>
</div>
<div id="outline-container-orgf0c68c9" class="outline-3">
<h3 id="orgf0c68c9"><span class="section-number-3">28.4.</span> Data Analysts</h3>
<div class="outline-text-3" id="text-28-4">
<p>
essential:
</p>
<ul class="org-ul">
<li>Storytelling</li>
<li>Data visualization</li>
<li>Business insights</li>
<li>Metrics &amp; Reporting</li>
</ul>

<p>
translating data into analyses and business insights.
</p>
<ul class="org-ul">
<li>descriptive statistics</li>
<li>metrics definition</li>
<li>data visualization</li>
<li>presentations &amp; storytelling</li>
<li>problem solving</li>
<li>product intuition</li>
<li>stakeholder management.</li>
</ul>

<p>
further specialize:
</p>
<ul class="org-ul">
<li>“Data Science, Analyst”</li>
<li>“Product Analyst”</li>
<li>“Business Analysts”</li>
<li>“Business Intelligence Analyst” and more.</li>
</ul>
</div>
</div>

<div id="outline-container-org7e5588a" class="outline-3">
<h3 id="org7e5588a"><span class="section-number-3">28.5.</span> Data Engineer+ Data Analytic</h3>
<div class="outline-text-3" id="text-28-5">
<p>
Руководство данными
</p>
<ul class="org-ul">
<li>Ведение хранилищ и бизнес-аналитика</li>
<li>Хранение и операции с данными - архивирование, восстановление - администрирование</li>
<li>Качество данных - расследование инцидентов с качеством.</li>
<li>Архитектура данных - проектирование</li>
<li>Интеграция и интероперабельность - чтобы данные связывались по ключам и площадки для BI анализа, аналитики</li>
<li>Руководство данными - административная область о том как выработать регламенты</li>
<li>Управление документами и контентом - про документооборот</li>
<li>Безопасность данных</li>
<li>Метаданные - типы данных, объединение полей</li>
<li>Справочные и основные данные - гдето это ведение золотого источника и master данных</li>
<li>Моделирование и проектирование данных - продуктов</li>
</ul>

<p>
Как
</p>
<ol class="org-ol">
<li>Формализация жизненного цикла данных</li>
<li>Создание каталога данных - централизованное и новое описание подтягивается автоматом</li>
<li>Создание системы управления качеством данных</li>
<li>Разработка инструментов построения линяжа(Line edge?) данных</li>
<li>Создание регламентов и нормативов по проектированию данных</li>
</ol>

<p>
Антипаттерны
</p>
<ol class="org-ol">
<li>Описывать данные внешних систем</li>
<li>Организовывать тотальную проверку качества данных. проверка по верхам это уже  5% нагрузка на хранилица - это уже много.</li>
<li>Хранить все данные на всякий слуйчай - оценивать ценность данных. Стоимость владения и время на сопровождение</li>
<li>Создавать системы управления данными исключительно для себя.</li>
</ol>
</div>
</div>
<div id="outline-container-org4eafb7d" class="outline-3">
<h3 id="org4eafb7d"><span class="section-number-3">28.6.</span> Data Scientist</h3>
<div class="outline-text-3" id="text-28-6">
<p>
<b>generate business insights</b>
</p>

<p>
essential:
</p>
<ul class="org-ul">
<li>Stats &amp; ML Modeling</li>
<li>Inference</li>
<li>Experimentation</li>
</ul>

<p>
popular alternative nowadays is “Research Scientist”.
</p>

<p>
apply advanced statistical techniques such
</p>
<ul class="org-ul">
<li>regression</li>
<li>classification</li>
<li>clustering</li>
<li>optimization to automate processes that impact business operations or customer facing products.</li>
</ul>

<p>
They typically partner with
</p>
<ul class="org-ul">
<li>Software Engineers or</li>
<li>ML Engineers for the deployment and monitoring of their models.</li>
</ul>

<p>
A graduate degree in a quantitative field is often desirable for candidates interested in a Data Science position.
</p>

<p>
techs:
</p>
<ul class="org-ul">
<li>Python, SQL, ML, PyTorch</li>
<li>DVC, MLFlow</li>
<li>Spark, Hadoop, Hive or Data Bricks</li>
<li>data leaks</li>
<li>Kafka, Airflow, Snowflake</li>
<li>AWS/Azure/GCP</li>
</ul>

<p>
classic
</p>
<ul class="org-ul">
<li>разрабатывать модели и алгоритмы</li>
<li>развивать внутренние инструменты обучения и дообучения ML-моделей</li>
<li>анализировать и мониторить качество моделей, контролировать их качество и стабильность работы внедрённых моделей</li>
<li>совместно с фрод-аналитиками формулировать гипотезы и проверять их</li>
<li>поддерживать вывод моделей в пром</li>
</ul>
</div>
</div>
<div id="outline-container-orgef16602" class="outline-3">
<h3 id="orgef16602"><span class="section-number-3">28.7.</span> Machine Learning Engineers</h3>
<div class="outline-text-3" id="text-28-7">
<p>
<b>turn data into products</b>
</p>
<ul class="org-ul">
<li>ML Ops</li>
<li>Model Deployment</li>
</ul>

<p>
ability to design efficient algorithms for the proposed solutions, deploy and manage them with ML Ops
 techniques, and monitor their performance over time.
</p>
</div>
</div>
<div id="outline-container-orgfe558e7" class="outline-3">
<h3 id="orgfe558e7"><span class="section-number-3">28.8.</span> Backend Engineer</h3>
<div class="outline-text-3" id="text-28-8">
<p>
Composition API;
Опыт работы с Graphql, PostgreSQL, Flask;
Знание Git;
Опыт работы с Web 3.0
</p>
</div>
</div>
<div id="outline-container-org182e237" class="outline-3">
<h3 id="org182e237"><span class="section-number-3">28.9.</span> Project manager (web3)</h3>
<div class="outline-text-3" id="text-28-9">
<ul class="org-ul">
<li>методологией CJM</li>
<li>проведение Cust Dev и глубинных интервью</li>
<li>проектирование пользовательских интерфейсов и UX</li>
<li>В совершенстве владение всеми инструментами Google Workspace</li>
<li>Свободное владение Miro, Notion, CRM, Tilda, Figma, Jira, MetaMask и др.</li>
<li>Владение гибкими методологиями управления: Scrum, Agile</li>
<li>Опыт работы с различными чат-бот платформами и разработка авто-воронок</li>
<li>Высокий уровень эмоционального интеллекта и эмпатии</li>
</ul>
<p>
otv
</p>
<ul class="org-ul">
<li>P&amp;L (Profit and loss statement), или PNL, — отчёт, показывающий прибыль и убытки компании за определённый период.</li>
<li>Организовывать и координировать еженедельные Sync митинги со всей командой и план/факт</li>
<li>Проводить Daily митинги с командой и приоритизировать задачи</li>
<li>️Фиксировать договорённости в Notion и поддерживать канбан задач в актуальном виде</li>
<li>️Вести общий календарь команды и организовывать встречи</li>
<li>️Описывать документацию и технические требования для команды разработки</li>
<li>️Разрабатывать и актуализировать инвестиционные материалы для Data Room: white paper, pitch deck, токеномика, Agreements</li>
<li>️Проводить Pitch сессии и выступления на английском языке перед венчурными инвесторами, фондами и крипто комьюнити</li>
<li>️Разрабатывать, описывать, оцифровывать и контролировать бизнес-процессы</li>
<li>️Нанимать и онбордить новых людей в команды на RU / ENG языках</li>
<li>️Готовить еженедельные апдэйты для чатов с эдвайзерами, партнерами и инвесторами</li>
<li>️Организовывать и модерировать AMA сессии, Pitch days, прямые эфиры и др. активности</li>
</ul>
</div>
</div>
<div id="outline-container-org5145987" class="outline-3">
<h3 id="org5145987"><span class="section-number-3">28.10.</span> Manager of ML team</h3>
<div class="outline-text-3" id="text-28-10">
<ul class="org-ul">
<li>Участие в совместной разработки продуктов, стратегии компании в части риск составляющей совместно с другими подразделениями.Проведения AB- тестирования в рамках риск-стратегии.</li>
<li>Переговоры с контрагентами, договорная работа, оплата счетов.</li>
<li>Поиск и тестирование новых сервисов, улучшающих работы СПР (больше одобрений – меньше просрочка)</li>
</ul>

<p>
Требования:
</p>
<ul class="org-ul">
<li>Понимание принципов работы систем классического машинного обучения (задачи классификации)</li>
<li>Знания Excel (сводные таблицы) обязательно, SQL (на уровне CUD) обязательно, Python (pandas, numpy) обязательно</li>
<li>Высокий уровень аналитических навыков (логика, реляционная алгебра, теория вероятностей)</li>
<li>Опыт построения систем машинного обучения в финансовой организации</li>
<li>Опыт управления командой (постановка и контроль выполнения задач, найм, onboarding)</li>
</ul>

<p>
Будет приемуществом:
</p>
<ul class="org-ul">
<li>Знание законов и регуляторных требований в области управления кредитным рискомОпыт управления процессами в роли стейкхолдера, в роли оунера</li>
<li>Знание PowerBI, Excel (PowerPivot, PowerQuery), python (sklearn, catboost, matplotlib, sqlalchemy)</li>
<li>Навыки системного и портфельного анализа</li>
</ul>

<p>
Hot to become
</p>
</div>
</div>
<div id="outline-container-org831cd3f" class="outline-3">
<h3 id="org831cd3f"><span class="section-number-3">28.11.</span> MLOps</h3>
<div class="outline-text-3" id="text-28-11">
<p>
а крупный проект требуется Разработчик Python MLOps
</p>

<p>
Обязанности:
</p>

<p>
Разработка рабочего места исследователя данных в составе MLOps платформы, а также решения для serving-a моделей
Разработка системы для автоматического разворачивания рабочих мест дата-специалистов на базе Kubernetes.
Разработка интеграции рабочих мест с Hadoop – стеком.
Разработка решения для автоматизации вывода моделей машинного обучения в продакшн.
Реализация ролевой модели доступа к системе
Реализация логирования событий
Интеграция с системами ИБ
</p>

<p>
Требования:
</p>

<p>
Опыт в разработке MLOps инструментов/платформ
Опыт разработки ML-моделей с помощью Pytorch/tensorflow
Опыт продуктивизации ML-моделей
Опыт создания пайплайнов по обучению ML-моделей
Опыт доработки Jupyterhub и MLFlow (или аналогичных собственных реализаций)
Опыт использования k8s, git, terraform
</p>
</div>
</div>

<div id="outline-container-orgea67ddb" class="outline-3">
<h3 id="orgea67ddb"><span class="section-number-3">28.12.</span> Admin Linux/DevOps</h3>
<div class="outline-text-3" id="text-28-12">
<ul class="org-ul">
<li>Опыт администрирования семейства ОС Astra Linux;</li>
<li>Знания сетевых протоколов HTTP/HTTPS, SMTP, FTP/SFTP, SSL/TLS, SSH;</li>
<li>Уверенные знания ОС Linux:</li>
<li>знание отличие Startup Management (initd) и Service Mgmt (systemd)</li>
<li>уверенное владение командной строкой в Linux для мониторинга процессов (ps, top, htop, atop, lsof), проверок производительности системы (nmon, iostat, sar, vmstat)</li>
<li>отличные знание сетевого стека Linux, уверенное владение утилитами диагностики сетевых подключений (ping, traceroute, mtr, nmap, netstat, tcpdupm, dig, scp), файрволов Linux (ufw/firewalld, iptables/nftables)</li>
<li>навыки разворачивания PKI на базе Linux</li>
<li>опыт настройки и эксплуатации Reverse Proxy, Forward Proxy, Load Balancer, Caching Server;</li>
<li>Опыт администрирования Nginx, Apache с высоконагруженными сервисами;</li>
<li>Опыт работы с базами данных (MySQL, PostgreSQL, др.);</li>
<li>Знания языков bash, python на уровне чтения/написания скриптов;</li>
<li>Опыт работы с Git, GitLab, Jenkins, CI/CD, понимание процессов разработки;</li>
<li>Опыт работы с контейнеризацией (Docker);</li>
<li>Знания и опыт работы с Kubernetes;</li>
<li>Знания протоколов аутентификации SAML 2.0 и OpenID Connect;</li>
<li>Знание и навыки работы с системой резервного копирования Veeam;</li>
<li>Знание и практические навыки работы с системами виртуализации на базе VmWare;</li>
<li>Опыт работы с системами мониторинга (Nagios, Grafana, Zabbix);</li>
<li>Опыт эксплуатации серверного оборудования основных вендоров, систем хранения данных ведущих вендоров;</li>
<li>Опыт работы с системами хранения данных СХД корпоративного уровня;</li>
<li>Желание развиваться в направлении DevOps инженера;</li>
<li>Умение работать в команде;</li>
<li>Внимательность, аккуратность, стрессоустойчивость, коммуникабельность, ответственность, дисциплинированность;</li>
<li>Готовность решать инциденты в любое время;</li>
<li>Английский язык, достаточный для свободного чтения и понимания технической документации, а также переписки на приемлемом уровне</li>
</ul>
</div>
</div>
<div id="outline-container-org64d0f46" class="outline-3">
<h3 id="org64d0f46"><span class="section-number-3">28.13.</span> AI High Performance Computing Engineer</h3>
<div class="outline-text-3" id="text-28-13">
<p>
HPC processes massive amounts of data and solves today’s most complex computing problems in real time or
 near-real time.
</p>
</div>


<div id="outline-container-org475e7fe" class="outline-4">
<h4 id="org475e7fe"><span class="section-number-4">28.13.1.</span> terms</h4>
<div class="outline-text-4" id="text-28-13-1">
<dl class="org-dl">
<dt>Massively parallel computing.</dt><dd>tens of thousands to millions of processors or processor cores.</dd>
<dt>Computer clusters</dt><dd>The computers, called <b>nodes</b> (GPU)</dd>
<dt>High-performance components</dt><dd>are high-speed, high-throughput and low-latency components.</dd>
<dt>Grid computing</dt><dd>widely distributed computer resources. tend to be more heterogeneous. form of distributed
computing.</dd>
<dt>Data Distribution</dt><dd>distributed among the nodes,</dd>
<dt>CPU stepping technologies</dt><dd>Both Intel and AMD offer , which allow the administrator to step up and step
down the CPU frequency at various granularities.</dd>
<dt>inference cluster</dt><dd>simpler hardware with less power than the training cluster but with the lowest latency
possible.</dd>
</dl>
</div>
</div>


<div id="outline-container-orgc836d88" class="outline-4">
<h4 id="orgc836d88"><span class="section-number-4">28.13.2.</span> workloads</h4>
<div class="outline-text-4" id="text-28-13-2">
<dl class="org-dl">
<dt>Healthcare, genomics and life sciences</dt><dd>Genome decoding, drug discovery and design, rapid cancer
diagnosis, and molecular modeling.</dd>
<dt>Financial services</dt><dd>automated trading and fraud detection, Monte Carlo simulation.</dd>
<dt>Government and defense.</dt><dd>weather forcasting and climate modeling, energy research and intelligence work</dd>
<dt>Energy.</dt><dd>seismic data processing, reservoir simulation and modeling, geospatial analytics, wind
simulation and terrain mapping.</dd>
</dl>
</div>
</div>
<div id="outline-container-org6eca746" class="outline-4">
<h4 id="org6eca746"><span class="section-number-4">28.13.3.</span> artcles</h4>
<div class="outline-text-4" id="text-28-13-3">
</div>
<ol class="org-ol">
<li><a id="orgc0f3782"></a>Convergence of artificial intelligence and high performance computing on NSF‑supported cyberinfrastructure<br />
<div class="outline-text-5" id="text-28-13-3-1">
<p>
ImageNet
</p>
<ul class="org-ul">
<li>GPU/Speedup: 8/4, 16/8, 32/12, 64/20</li>
<li>GPU / Total time(sec) epoch: 2/70000, 4/50000, 8/20000, 16/10000, 32/5000
<a href="https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-020-00361-2.pdf">https://journalofbigdata.springeropen.com/counter/pdf/10.1186/s40537-020-00361-2.pdf</a></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org64b9c28" class="outline-4">
<h4 id="org64b9c28"><span class="section-number-4">28.13.4.</span> NVIDIA</h4>
<div class="outline-text-4" id="text-28-13-4">
<p>
<a href="https://resources.nvidia.com/en-us-hpc-ebooks/hpc-for-the-age-of-ai?xs=409135">https://resources.nvidia.com/en-us-hpc-ebooks/hpc-for-the-age-of-ai?xs=409135</a>
</p>
</div>
<ol class="org-ol">
<li><a id="org317b0a7"></a>courses<br />
<div class="outline-text-5" id="text-28-13-4-1">
<p>
course C++ paid <a href="https://courses.nvidia.com/courses/course-v1:DLI+S-AC-08+V1/">https://courses.nvidia.com/courses/course-v1:DLI+S-AC-08+V1/</a>
</p>

<p>
free CUDA <a href="https://courses.nvidia.com/courses/course-v1:DLI+T-AC-01+V1/about">https://courses.nvidia.com/courses/course-v1:DLI+T-AC-01+V1/about</a>
</p>

<p>
free brain <a href="https://courses.nvidia.com/courses/course-v1:DLI+T-FX-01+V1/about">https://courses.nvidia.com/courses/course-v1:DLI+T-FX-01+V1/about</a>
</p>

<p>
free Disaster Risk Monitoring Using Satellite Imagery <a href="https://courses.nvidia.com/courses/course-v1:DLI+S-ES-01+V1/">https://courses.nvidia.com/courses/course-v1:DLI+S-ES-01+V1/</a>
</p>

<p>
free Digital Fingerprinting with MorpheusTM <a href="https://courses.nvidia.com/courses/course-v1:DLI+T-DS-02+V1/about">https://courses.nvidia.com/courses/course-v1:DLI+T-DS-02+V1/about</a>
</p>

<p>
free Generative AI Explained <a href="https://courses.nvidia.com/courses/course-v1:DLI+S-FX-07+V1/">https://courses.nvidia.com/courses/course-v1:DLI+S-FX-07+V1/</a>
</p>

<p>
free Augmenting LLMs using Retrieval Augmented Generation <a href="https://courses.nvidia.com/courses/course-v1:NVIDIA+S-FX-16+v1/">https://courses.nvidia.com/courses/course-v1:NVIDIA+S-FX-16+v1/</a>
</p>

<p>
free Building RAG Agents for LLMs <a href="https://courses.nvidia.com/courses/course-v1:DLI+S-FX-15+V1/">https://courses.nvidia.com/courses/course-v1:DLI+S-FX-15+V1/</a>
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org06e3141" class="outline-4">
<h4 id="org06e3141"><span class="section-number-4">28.13.5.</span> cooling</h4>
<div class="outline-text-4" id="text-28-13-5">
<p>
Water Cooling and Immersion Cooling
</p>

<p>
Power Usage Effectiveness (PUE). - the total energy coming into a data center divided by the power being
 supplied to the servers in that data center.
</p>
<ul class="org-ul">
<li>reduce for cooling, air movement, water pumping, AC to DC conversion, and so on.</li>
</ul>

<p>
types:
</p>
<dl class="org-dl">
<dt>direct water cooling</dt><dd>to the power-hungry parts of a server, such as the CPUs, GPUs, memory, and networking.
<ul class="org-ul">
<li>PUE 1.4 -175</li>
</ul></dd>
<dt>immersion cooling</dt><dd>in which the entire server is immersed in some kind of heat conductive liquid that is electrically insulated
<dl class="org-dl">
<dt>PUE</dt><dd>1.05 - 1.1</dd>
</dl></dd>
<dt>air</dt><dd>.
<dl class="org-dl">
<dt>PUE</dt><dd>1.02 - 1.05</dd>
</dl></dd>
</dl>

<p>
NVIDIA <a href="https://www.grcooling.com/wp-content/uploads/2018/06/grc_analyst_report_the_nsa_does_more_with_less_with_immersion_cooling.pdf">https://www.grcooling.com/wp-content/uploads/2018/06/grc_analyst_report_the_nsa_does_more_with_less_with_immersion_cooling.pdf</a>
</p>
</div>
</div>
<div id="outline-container-org22b9551" class="outline-4">
<h4 id="org22b9551"><span class="section-number-4">28.13.6.</span> blogs</h4>
<div class="outline-text-4" id="text-28-13-6">
<p>
<a href="https://developer.nvidia.com/blog/search-posts/?q=&amp;tags=HPC%20%2F%20Scientific%20Computing&amp;categories">https://developer.nvidia.com/blog/search-posts/?q=&amp;tags=HPC%20%2F%20Scientific%20Computing&amp;categories</a>=
</p>
</div>
</div>
<div id="outline-container-orgc9a8309" class="outline-4">
<h4 id="orgc9a8309"><span class="section-number-4">28.13.7.</span> network</h4>
<div class="outline-text-4" id="text-28-13-7">
<p>
single high-performance network, usually used for both message passing and filesystem data flow.
</p>

<p>
Summit Supercomputer which has 2x Enhanced Data Rate (EDR) 100 GB/s InfiniBand, and the NVIDIA
Selene which has 8x High Dynamic Range (HDR) 200Gb/s InfiniBand.
</p>

<p>
network latency (microsec)
</p>

<p>
impact of bandwith on training time <a href="https://people.csail.mit.edu/ghobadi/papers/sipml_sigcomm_2021.pdf">https://people.csail.mit.edu/ghobadi/papers/sipml_sigcomm_2021.pdf</a>
</p>

<p>
Zero trust TUDO <a href="https://blogs.nvidia.com/blog/what-is-zero-trust/">https://blogs.nvidia.com/blog/what-is-zero-trust/</a>
</p>
</div>
</div>
<div id="outline-container-org4f2c909" class="outline-4">
<h4 id="org4f2c909"><span class="section-number-4">28.13.8.</span> ways to apply AI in HPC</h4>
<div class="outline-text-4" id="text-28-13-8">
<p>
reduce time for each simulation or “design of experiments" DOE -3 -2 -1 0 1 2 3 Reduce number of simulations
</p>
<ul class="org-ul">
<li>-3 - Surrogate Models - Replace the numerical solver with a trainined AI model</li>
<li>-2 - Coarse Model Up-Sampling - Employed a training AI model to up-sapmling fast running course simulations</li>
<li>-1 - AI Assisted Simulation - Employed a training AI model to provide a better numerical starting point</li>
<li>3 - AI Simulation Control - Use a reinforcement ML model to choose simulation paramenters</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orga393f58" class="outline-3">
<h3 id="orga393f58"><span class="section-number-3">28.14.</span> ML infrastructure engineer, ML platform engineer</h3>
<div class="outline-text-3" id="text-28-14">
<ul class="org-ul">
<li>familiarity with parallelism</li>
<li>distributed computing</li>
<li>low-level optimization</li>
</ul>
</div>
</div>
<div id="outline-container-org629d2cf" class="outline-3">
<h3 id="org629d2cf"><span class="section-number-3">28.15.</span> ML accelerator/hardware engineer</h3>
</div>
<div id="outline-container-orgda1238f" class="outline-3">
<h3 id="orgda1238f"><span class="section-number-3">28.16.</span> Product analytic</h3>
<div class="outline-text-3" id="text-28-16">
<ul class="org-ul">
<li>t-test, z-test, chi-square, A/B-test</li>
<li>Aplitude, Mixpanel, GA</li>
</ul>
</div>
</div>
<div id="outline-container-org4bae08a" class="outline-3">
<h3 id="org4bae08a"><span class="section-number-3">28.17.</span> <span class="todo TODO">TODO</span> Operations research ?</h3>
<div class="outline-text-3" id="text-28-17">
<p>
системный подход к поставленной проблеме и анализ.
</p>

<p>
системный подход - Любая задача, которая решается, должна рассматриваться с точки зрения влияния на критерии
 функционирования системы в целом. Для исследования операций характерно то, что при решении каждой проблемы
 могут возникать новые задачи. Важной особенностью исследования операций есть стремление найти оптимальное
 решение поставленной задачи (принцип «оптимальности»).
</p>

<p>
ИО используют в основном крупные западные компании в решении задач планирования производства (контроллинга,
 логистики, маркетинга) и прочих сложных задач. Применение ИО в экономике позволяет понизить затраты или
 повысить продуктивность предприятия.
</p>

<p>
Операционная аналитика
</p>

<p>
Примеры практических задач:
</p>
<ul class="org-ul">
<li>План снабжения предприятий</li>
<li>Постройка участка магистрали</li>
<li>Продажа сезонных товаров</li>
<li>Снегозащита дорог</li>
<li>Противолодочный рейд</li>
<li>Выборочный контроль продукции</li>
<li>Медицинское обследование</li>
<li>Библиотечное обслуживание</li>
</ul>
<p>
Примеры математических (комбинаторных) задач, связанных с ИО:
</p>
<ul class="org-ul">
<li>Задача о ранце,</li>
<li>Задача коммивояжёра,</li>
<li>Транспортная задача,</li>
<li>Задача об упаковке в контейнеры,</li>
<li>Задачи составления расписания, диспетчеризации такие как «расписание открытия магазина»[англ.] (англ. Open-shop scheduling), «задача планирования для поточной линии» (англ. Flow Shop Scheduling Problem), теория расписаний (англ. Job Shop Scheduling) и т. д.a</li>
</ul>
</div>
</div>
<div id="outline-container-orgd53ccbb" class="outline-3">
<h3 id="orgd53ccbb"><span class="section-number-3">28.18.</span> Optimization Modeling Specialist</h3>
<div class="outline-text-3" id="text-28-18">
<p>
NumPy, Pandas, SciPy, PuLP;
</p>
<ul class="org-ul">
<li>Генетический алгоритм</li>
<li>Муравьиный алгоритм</li>
<li>Метод отжига</li>
<li>Tabu search</li>
<li>Large adaptive neighborhood search</li>
<li>Fix and optimize</li>
<li>Greedy</li>
<li>k-Regret</li>
<li>First Come First Serve (FCFS) Scheduling Algorithm</li>
<li>Shortest Job First (SJF) Scheduling Algorithm</li>
<li>Longest Job First (LJF) Scheduling Algorithm</li>
<li>Priority Scheduling Algorithm</li>
<li>Round Robin Scheduling Algorithm</li>
<li>Shortest Remaining Time First (SRTF)</li>
<li>Critical job first</li>
<li>Permurtation Shifting</li>
<li><p>
Bottlneck (Job Shop)
</p>

<p>
Владение методами сведения нелинейных задач к линейным, исследование операций;
Понимание принципов мультипроцессинга, многопоточности;
Умение писать асинхронный код.
</p></li>
</ul>
</div>
</div>

<div id="outline-container-orgb0e680c" class="outline-3">
<h3 id="orgb0e680c"><span class="section-number-3">28.19.</span> links</h3>
<div class="outline-text-3" id="text-28-19">

<div id="orgbc29087" class="figure">
<p><img src="https://images.squarespace-cdn.com/content/61fd85d490d950673294e700/1646607286800-Z48ES2E9L7025SWCPMF3/Radar.png" alt="Radar.png" />
</p>
</div>
<ul class="org-ul">
<li><a href="https://www.datacaptains.com/blog/guide-to-data-roles">https://www.datacaptains.com/blog/guide-to-data-roles</a></li>
<li>2205.02302 exiv</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org01ca6c2" class="outline-2">
<h2 id="org01ca6c2"><span class="section-number-2">29.</span> ML Scientists</h2>
<div class="outline-text-2" id="text-29">
<p>
AndreasMueller  - sklearn
</p>
<ul class="org-ul">
<li><a href="https://github.com/pystruct/pystruct">https://github.com/pystruct/pystruct</a></li>
<li><a href="https://alexanderdyakonov.files.wordpress.com/2015/04/ama2015_scikit.pdf">https://alexanderdyakonov.files.wordpress.com/2015/04/ama2015_scikit.pdf</a></li>
</ul>
<p>
others
</p>
<ul class="org-ul">
<li><a href="https://sassafras13.github.io">https://sassafras13.github.io</a></li>
</ul>

<p>
Andrej Karpathy -  deep learning and computer vision
</p>
<ul class="org-ul">
<li><a href="https://cs.stanford.edu/people/karpathy/advice.html">https://cs.stanford.edu/people/karpathy/advice.html</a></li>
<li><a href="https://karpathy.github.io/2016/09/07/phd/">https://karpathy.github.io/2016/09/07/phd/</a></li>
</ul>

<p>
Krystian Safjan's - Data Scientist | Researcher | Team Leader
<a href="https://safjan.com/mlops-certifications-a-comprehensive-guide/#mlops-certifications-a-comprehensive-guide">https://safjan.com/mlops-certifications-a-comprehensive-guide/#mlops-certifications-a-comprehensive-guide</a>
</p>
</div>
</div>
<div id="outline-container-org91a8441" class="outline-2">
<h2 id="org91a8441"><span class="section-number-2">30.</span> pyannote - audio</h2>
<div class="outline-text-2" id="text-30">
<p>
Official pyannote.audio pipelines (i.e. those under the pyannote organization umbrella) are open-source, but gated.
</p>
</div>
<div id="outline-container-orgd4e3cdf" class="outline-3">
<h3 id="orgd4e3cdf"><span class="section-number-3">30.1.</span> comparizion nvidia and pyannote</h3>
<div class="outline-text-3" id="text-30-1">
<ul class="org-ul">
<li>nvidia <a href="https://arxiv.org/abs/2203.15974">https://arxiv.org/abs/2203.15974</a></li>
<li>pyannote <a href="https://huggingface.co/pyannote/speaker-diarization/blob/main/technical_report_2.1.pdf">https://huggingface.co/pyannote/speaker-diarization/blob/main/technical_report_2.1.pdf</a></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgb8f50b1" class="outline-2">
<h2 id="orgb8f50b1"><span class="section-number-2">31.</span> AI Coding Assistants</h2>
<div class="outline-text-2" id="text-31">
</div>
<div id="outline-container-org295476e" class="outline-3">
<h3 id="org295476e"><span class="section-number-3">31.1.</span> tasks</h3>
<div class="outline-text-3" id="text-31-1">
<ul class="org-ul">
<li>less time creating boilerplate and repetitive code patterns</li>
</ul>
</div>
</div>
<div id="outline-container-orgbc70b7e" class="outline-3">
<h3 id="orgbc70b7e"><span class="section-number-3">31.2.</span> products</h3>
<div class="outline-text-3" id="text-31-2">
<ul class="org-ul">
<li>GitHub Copilot - GitHub, OpenAI, and Microsoft</li>
<li>OpenAI Codex</li>
<li>Tabnine: Tabnine - privacy and security</li>
<li>Replit: Replit AI (Ghostwriter)  -</li>
<li>Amazon: Amazon CodeWhisperer</li>
<li>Snyk: DeepCode AI - for infosec</li>
<li>Codium: Codium - the testing process</li>
<li>AskCodi: AskCodi</li>
<li>GitLab Comparison Chart - web only</li>
</ul>


<ul class="org-ul">
<li>K.Explorer</li>
<li>Cycloid</li>
<li>AiXcoder</li>
<li>Azure DevOps Server</li>
<li>AlphaCode</li>
<li>AccuRev</li>
<li>BLACKBOX AI</li>
<li>Bitbucket</li>

<li>Kodezi (Best for Teams)</li>
<li>Replit Ghostwriter (Best Browser Assistant)</li>
<li>Tabnine (Best Language and IDE Support)</li>
<li>Github Copilot (Most Reputable)</li>
<li>Code Snippets AI (Most Flexible Features)</li>
<li>K.Explorer (Best for Code Completion)</li>
<li>AI Code Reviewer (Best for Simple Code Review)</li>
</ul>
</div>
</div>

<div id="outline-container-org656f5b7" class="outline-3">
<h3 id="org656f5b7"><span class="section-number-3">31.3.</span> wide abilities</h3>
<div class="outline-text-3" id="text-31-3">
<ul class="org-ul">
<li>Autocompletion, Predicting next action, suggest actions.</li>
<li>Error Detection and Correction</li>
<li>Code Refactoring Suggestions, Whole project understanding.</li>
<li>Code Documentation Generation and Update</li>
</ul>
</div>
</div>
<div id="outline-container-org291274b" class="outline-3">
<h3 id="org291274b"><span class="section-number-3">31.4.</span> narrow abilities</h3>
<div class="outline-text-3" id="text-31-4">
<ul class="org-ul">
<li>Writing new blocks of code by guidance</li>
<li>Converting Legacy Code</li>
<li>Integration with Development Environments: execut unit tests, ensuring code quality and reliability: mypy, pylint</li>
<li>create tests.</li>
<li>run security scans and identify potential vulnerabilities</li>
<li>do multisteps tasks as agent.</li>
<li>define functions and ask agent operate with them</li>
</ul>
</div>
</div>
<div id="outline-container-org9c6b7a1" class="outline-3">
<h3 id="org9c6b7a1"><span class="section-number-3">31.5.</span> heavy abilities</h3>
<div class="outline-text-3" id="text-31-5">
<ul class="org-ul">
<li>Error Detection without execution.</li>
<li>next step prediction</li>
<li>voice control</li>
</ul>
</div>
</div>
<div id="outline-container-org4ca455f" class="outline-3">
<h3 id="org4ca455f"><span class="section-number-3">31.6.</span> Approaches: skillsets vs traditional agent</h3>
<div class="outline-text-3" id="text-31-6">
<p>
Skillsets
</p>
<ul class="org-ul">
<li>creating a collection of specialized skills or functionalities that can be combined to perform a wide range
of tasks</li>
<li>for virtual assistants</li>
</ul>

<p>
Traditional Agent
</p>
<ul class="org-ul">
<li>create autonomous entities that perceive their environment and take actions to achieve specific goals</li>
<li>for robotics or autonomous driving</li>
</ul>

<p>
Skillsets are ideal when you want to:
</p>
<ul class="org-ul">
<li>Quickly integrate existing APIs or services without managing AI logic</li>
<li>Focus purely on your service's core functionality</li>
<li>Maintain consistent Copilot-style interactions without extensive development</li>
<li>Get started with minimal infrastructure and setup</li>
</ul>

<p>
Use agents instead if you need:
</p>
<ul class="org-ul">
<li>Complex custom interaction flows</li>
<li>Specific LLM model control (using LLMs that aren't provided by the Copilot API)</li>
<li>Custom prompt crafting</li>
<li>Advanced state management</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgca7a43d" class="outline-2">
<h2 id="orgca7a43d"><span class="section-number-2">32.</span> Generative AI articles</h2>
<div class="outline-text-2" id="text-32">
<ul class="org-ul">
<li>GPT 2, GPT 3 <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf</a></li>
<li>DistillBERT <a href="https://arxiv.org/pdf/1910.01108.pdf%3C/p%3E">https://arxiv.org/pdf/1910.01108.pdf%3C/p%3E</a></li>
<li>Texar <a href="https://arxiv.org/pdf/1809.00794.pdf">https://arxiv.org/pdf/1809.00794.pdf</a></li>
<li>XLM-RoBERTa <a href="https://arxiv.org/pdf/1911.02116.pdf">https://arxiv.org/pdf/1911.02116.pdf</a></li>
<li>DeBERTa <a href="https://arxiv.org/pdf/2006.03654.pdf">https://arxiv.org/pdf/2006.03654.pdf</a></li>
<li>T5 <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html</a></li>
<li>BART <a href="https://arxiv.org/abs/1910.13461">https://arxiv.org/abs/1910.13461</a></li>
<li>Llama <a href="https://arxiv.org/pdf/2302.13971.pdf">https://arxiv.org/pdf/2302.13971.pdf</a></li>
<li>Opt <a href="https://arxiv.org/pdf/2205.01068.pdf">https://arxiv.org/pdf/2205.01068.pdf</a></li>
<li>Vicuna / Falcon</li>
<li>image harmonization <a href="https://arxiv.org/abs/2006.00809">https://arxiv.org/abs/2006.00809</a></li>
<li>StyleGan2 <a href="https://arxiv.org/abs/1912.04958">https://arxiv.org/abs/1912.04958</a></li>
<li>StyleGAN v3 <a href="https://arxiv.org/pdf/2106.12423.pdf">https://arxiv.org/pdf/2106.12423.pdf</a></li>
<li>Multi-style Generative Network for Real-time Transfer <a href="https://arxiv.org/pdf/1703.06953.pdf">https://arxiv.org/pdf/1703.06953.pdf</a></li>
<li>ALAE <a href="https://arxiv.org/pdf/2004.04467.pdf">https://arxiv.org/pdf/2004.04467.pdf</a></li>
<li>NERF <a href="https://arxiv.org/pdf/2003.08934.pdf">https://arxiv.org/pdf/2003.08934.pdf</a>
<ul class="org-ul">
<li>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</li>
</ul></li>
<li>StyleNeRF <a href="https://arxiv.org/pdf/2110.08985.pdf">https://arxiv.org/pdf/2110.08985.pdf</a></li>
<li>LaMa <a href="https://arxiv.org/pdf/2109.07161.pdf">https://arxiv.org/pdf/2109.07161.pdf</a>
<ul class="org-ul">
<li>Resolution-robust Large Mask Inpainting with Fourier Convolutions</li>
</ul></li>
<li>SwinIR <a href="https://arxiv.org/pdf/2108.10257.pdf">https://arxiv.org/pdf/2108.10257.pdf</a></li>
<li>DeepLab v3 <a href="https://arxiv.org/pdf/1706.05587v3.pdf">https://arxiv.org/pdf/1706.05587v3.pdf</a></li>
<li>Knet <a href="https://arxiv.org/pdf/2106.14855.pdf">https://arxiv.org/pdf/2106.14855.pdf</a></li>
<li>FastFCN <a href="https://arxiv.org/pdf/1903.11816.pdf">https://arxiv.org/pdf/1903.11816.pdf</a></li>
<li>SegFormer <a href="https://arxiv.org/pdf/2105.15203.pdf">https://arxiv.org/pdf/2105.15203.pdf</a></li>
<li>Segment Anything <a href="https://arxiv.org/pdf/2304.02643.pdf">https://arxiv.org/pdf/2304.02643.pdf</a></li>
<li>Latent / Stable Diffusion <a href="https://arxiv.org/pdf/2112.10752.pdf">https://arxiv.org/pdf/2112.10752.pdf</a></li>
<li>ControlNet <a href="https://arxiv.org/pdf/2302.05543.pdf">https://arxiv.org/pdf/2302.05543.pdf</a></li>
<li>Dall-E 2 <a href="https://arxiv.org/pdf/2204.06125.pdf">https://arxiv.org/pdf/2204.06125.pdf</a></li>
<li>Imagen <a href="https://arxiv.org/pdf/2205.11487.pdf">https://arxiv.org/pdf/2205.11487.pdf</a></li>
<li>Boosting Monocular depth estimation <a href="https://arxiv.org/pdf/2105.14021v1.pdf">https://arxiv.org/pdf/2105.14021v1.pdf</a></li>
<li>GLPN <a href="https://arxiv.org/pdf/2201.07436v3.pdf">https://arxiv.org/pdf/2201.07436v3.pdf</a></li>
<li>Midas <a href="https://arxiv.org/pdf/1907.01341v3.pdf">https://arxiv.org/pdf/1907.01341v3.pdf</a></li>
<li>LlaVa <a href="https://arxiv.org/pdf/2304.08485.pdf">https://arxiv.org/pdf/2304.08485.pdf</a></li>
<li>Resnet <a href="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a></li>
<li>MobileNet <a href="https://arxiv.org/pdf/1704.04861.pdf">https://arxiv.org/pdf/1704.04861.pdf</a></li>
<li>Transformer <a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></li>
<li>Vision Transformer <a href="https://arxiv.org/pdf/2010.11929.pdf">https://arxiv.org/pdf/2010.11929.pdf</a></li>
<li>Triplet Loss <a href="https://arxiv.org/pdf/1503.03832.pdf">https://arxiv.org/pdf/1503.03832.pdf</a></li>
<li>InstDisc <a href="https://arxiv.org/pdf/1805.01978v1.pdf">https://arxiv.org/pdf/1805.01978v1.pdf</a></li>
<li>SimCLR <a href="https://arxiv.org/pdf/2002.05709.pdf">https://arxiv.org/pdf/2002.05709.pdf</a></li>
<li>NNCLR <a href="https://arxiv.org/pdf/2104.14548.pdf">https://arxiv.org/pdf/2104.14548.pdf</a></li>
</ul>



<p>
Symbols grounding theory 2017 <a href="https://arxiv.org/pdf/1703.04368.pdf">https://arxiv.org/pdf/1703.04368.pdf</a>
</p>
</div>
</div>

<div id="outline-container-org79beed5" class="outline-2">
<h2 id="org79beed5"><span class="section-number-2">33.</span> Miracle webinars</h2>
<div class="outline-text-2" id="text-33">
</div>
<div id="outline-container-orgc7056df" class="outline-3">
<h3 id="orgc7056df"><span class="section-number-3">33.1.</span> Leveraging Explainable AI and GCP for predicting Loan Risk on Vimeo</h3>
<div class="outline-text-3" id="text-33-1">
</div>
<div id="outline-container-org422d253" class="outline-4">
<h4 id="org422d253"><span class="section-number-4">33.1.1.</span> link</h4>
<div class="outline-text-4" id="text-33-1-1">
<p>
<a href="https://player.vimeo.com/video/801154905">https://player.vimeo.com/video/801154905</a>
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgdc08060" class="outline-2">
<h2 id="orgdc08060"><span class="section-number-2">34.</span> semi-supervised learning or week supervision</h2>
<div class="outline-text-2" id="text-34">
</div>
<div id="outline-container-org464b9e9" class="outline-3">
<h3 id="org464b9e9"><span class="section-number-3">34.1.</span> may refer to</h3>
<div class="outline-text-3" id="text-34-1">
<p>
transductive learning - Трансдуктивное обучение. -  is to infer the correct labels for the given unlabeled data
</p>
<ul class="org-ul">
<li>was introduced by Vladimir Vapnik in the 1990s</li>
<li>would label the unlabeled points according to the clusters to which they naturally belong</li>
<li>it builds no predictive model - if add new points need to be repeated with all of the points in order to predict a label.</li>
<li>two categories:
<ul class="org-ul">
<li>those that seek to assign discrete labels to unlabeled points
<ul class="org-ul">
<li>Manifold-learning-based transduction is still a very young field of research.</li>
</ul></li>
<li>those that seek to regress continuous labels for unlabeled points.</li>
</ul></li>
</ul>

<p>
inductive learning - goal of inductive learning is to infer the correct mapping from X to Y.
</p>
<ul class="org-ul">
<li>inductive approach to solving this problem is to use the labeled points to train a supervised learning
algorithm, and then have it predict labels for all of the unlabeled points</li>
</ul>

<p>
Layer Normalization
</p>
</div>
</div>
</div>

<div id="outline-container-orgd55bf63" class="outline-2">
<h2 id="orgd55bf63"><span class="section-number-2">35.</span> Mojo - language</h2>
<div class="outline-text-2" id="text-35">
<ul class="org-ul">
<li><a href="https://docs.modular.com/mojo/why-mojo.html">https://docs.modular.com/mojo/why-mojo.html</a></li>
<li><a href="https://docs.modular.com/mojo/programming-manual.html">https://docs.modular.com/mojo/programming-manual.html</a></li>
</ul>
</div>
</div>

<div id="outline-container-orge45980c" class="outline-2">
<h2 id="orge45980c"><span class="section-number-2">36.</span> интересные AI проекты</h2>
<div class="outline-text-2" id="text-36">
<ul class="org-ul">
<li>Drag Gan</li>
<li>300.ya.ru</li>
</ul>

<p>
<a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a>
</p>
</div>
</div>

<div id="outline-container-orgbeea570" class="outline-2">
<h2 id="orgbeea570"><span class="section-number-2">37.</span> nuancesprog.ru</h2>
<div class="outline-text-2" id="text-37">
</div>
<div id="outline-container-orgc41f3d2" class="outline-3">
<h3 id="orgc41f3d2"><span class="section-number-3">37.1.</span> общепринятая базовая оценка</h3>
<div class="outline-text-3" id="text-37-1">
<p>
Позволяет понять,
</p>
<ol class="org-ol">
<li>можно ли вообще найти зависимость к целевой переменной в данных</li>
<li>нулевая точкая для улучшения предсказания</li>
</ol>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #8ac6f2; font-weight: bold;">from</span> sklearn.dummy <span style="color: #8ac6f2; font-weight: bold;">import</span> DummyClassifier
<span style="color: #cae682;">clf</span> = DummyRegressor().fit(X_train, y_train)
clf.score(X_test, y_test)
</pre>
</div>
</div>
</div>
<div id="outline-container-org33e0f06" class="outline-3">
<h3 id="org33e0f06"><span class="section-number-3">37.2.</span> remove constant columns with VarianceThreshold</h3>
<div class="outline-text-3" id="text-37-2">
<pre class="example">
from sklearn.feature_selection import VarianceThreshold
var_thr = VarianceThreshold(threshold = 0.25) #Removing both constant and quasi-constant
</pre>
</div>
</div>
<div id="outline-container-org791cecc" class="outline-3">
<h3 id="org791cecc"><span class="section-number-3">37.3.</span> sklearn pitfalls</h3>
<div class="outline-text-3" id="text-37-3">
<p>
<a href="https://scikit-learn.org/stable/common_pitfalls.html">https://scikit-learn.org/stable/common_pitfalls.html</a>
</p>
<ul class="org-ul">
<li>controlling-randomness
<ul class="org-ul">
<li>random<sub>state</sub>=None: Sklearn использует глобальный набор сидов (seed set) NumPy с np.random.seed(seed<sub>number</sub>)</li>
<li>or random<sub>state</sub>=integer</li>
</ul></li>
<li>Inconsistent preprocessing - data transformation must be applyed everywhere, include production.</li>
<li>Data leakage:
<ol class="org-ol">
<li>Test data should never be used to make choices about the model.</li>
<li>train and test data subsets should receive the same preprocessing transformation</li>
</ol></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org78bedde" class="outline-2">
<h2 id="org78bedde"><span class="section-number-2">38.</span> NEXT LEVEL</h2>
<div class="outline-text-2" id="text-38">
<ul class="org-ul">
<li>implicit, lightfm</li>
<li>A/B test, Uplift моделирование, churn prediction</li>
<li>Those with a master's degree in a related field or equivalent industry experience</li>
<li>Anyone with experience participating in Recommendation System-related projects</li>
<li>Those with papers from top-tier ML conferences (ICML, ICLR, NeurIPS, CVPR, ECCV, ICCV, ACL, EMNLP, NAACL, KDD, SIGIR, CIKM, RecSys, etc.)</li>
<li>Those who have won awards from AI-related challenges (Kaggle, Hackathon, etc.)</li>
<li>A person with extensive knowledge and experience in Causal Inference</li>
<li>Those who can communicate smoothly in English</li>
<li>приветствуется опыт гибкой разработки (Scrum/Kanban).</li>
<li>Hadoop, Spark</li>
<li>понимание что такое p-value и умение проверять статистические гипотезы;</li>
<li>Построение моделей: • CLTV/LTV/CLV • Next best offer • Отток клиентов • NLP • Кластеризация;</li>
<li>МГУ ВМК
<ul class="org-ul">
<li><a href="http://master.cmc.msu.ru/?q=ru/node/2516">http://master.cmc.msu.ru/?q=ru/node/2516</a></li>
<li>О программе повышения квалификации для риск-менеджмента банков <a href="http://master.cmc.msu.ru/?q=ru/node/3286">http://master.cmc.msu.ru/?q=ru/node/3286</a></li>
</ul></li>
</ul>

<p>
resume and site
</p>
<ul class="org-ul">
<li><a href="https://www.gonsie.com/cv.html">https://www.gonsie.com/cv.html</a></li>
<li><a href="https://alexott.net/en/alexott-cv-en.html">https://alexott.net/en/alexott-cv-en.html</a></li>
</ul>
</div>
<div id="outline-container-org6855d9a" class="outline-3">
<h3 id="org6855d9a"><span class="section-number-3">38.1.</span> learn</h3>
<div class="outline-text-3" id="text-38-1">
<p>
DVC
Optuna
MLFlow
</p>

<p>
 Google Cloud Platform (GCP)
 FastAPI
  Fiber
  Bazel
OpenShift
DeepLarning 4J
Agile
Significant industry certifications (SANS, CISSP, CCIE, etc.)
DAX, M Code
Понимание принципов работы NeRF моделей.
Опыт работы с фреймворком Mindspore.
Опыт работы с ML алгоритмами крайне желателен (деревья решений, доверительные интервалы, рекомендательные системы, регрессия)
DDD, CORS
Theano
transformers, PyTorch, huggingface
</p>

<p>
Experience with AWS services (Glue, Lambda, S3, Redshift, DynamoDB, Terraform)
</p>

<p>
знание методологии шесть сигм;
</p>

<p>
TensorBoard, OnnxRT;
основные метрики в задачах CV
</p>

<p>
Хорошее понимание основ классического компьютерного зрения (базовые морфологические операции, Discrete Fourier Transform, Hough transforms, Sobel derivatives
</p>

<p>
Теоретическая подготовка в области RL и опыт использования алгоритмов RL
</p>

<p>
Что делать если у модели в продакшене исчез признак
</p>

<p>
ML Flow, Kubeflow, Vertex AI, AWS Sagemaker, ML Run, DVC etc.
</p>
</div>
</div>
</div>

<div id="outline-container-org9e02bc4" class="outline-2">
<h2 id="org9e02bc4"><span class="section-number-2">39.</span> sobes, собеседование</h2>
<div class="outline-text-2" id="text-39">
<ul class="org-ul">
<li>middle, senior difference <a href="https://towardsdatascience.com/a-checklist-to-track-your-data-science-progress-bf92e878edf2">https://towardsdatascience.com/a-checklist-to-track-your-data-science-progress-bf92e878edf2</a></li>
<li>questions huyenchip.com/ml-interviews-book/</li>
<li>ML <a href="https://github.com/masmahbubalom/InterviewQuestions/tree/main/ML%20Interview%20Question">https://github.com/masmahbubalom/InterviewQuestions/tree/main/ML%20Interview%20Question</a></li>
<li>NLP <a href="https://github.com/masmahbubalom/InterviewQuestions/tree/main/NLP%20Interview%20Questions">https://github.com/masmahbubalom/InterviewQuestions/tree/main/NLP%20Interview%20Questions</a></li>
</ul>
</div>
<div id="outline-container-org5966497" class="outline-3">
<h3 id="org5966497"><span class="section-number-3">39.1.</span> SQL</h3>
<div class="outline-text-3" id="text-39-1">
<ul class="org-ul">
<li>оконные функции - introduced in SQL:2003 - a way to perform calculations across a set of rows related to the
current row, without the need for self-joins or subqueries.</li>
<li>indexes - faster retrival like operations, slower modification and additional complexity</li>
</ul>
</div>
</div>
<div id="outline-container-orgf2f205c" class="outline-3">
<h3 id="orgf2f205c"><span class="section-number-3">39.2.</span> statistic</h3>
<div class="outline-text-3" id="text-39-2">
<ul class="org-ul">
<li>empirical risk minimization - error function = loss function + regularization. we cannot know exactly how
well an algorithm will work in practice (the true "risk") because we don't know the true distribution of data
that the algorithm will work on, but we can instead measure its performance on a known set of training data</li>
</ul>
</div>
</div>

<div id="outline-container-org1bbab5d" class="outline-3">
<h3 id="org1bbab5d"><span class="section-number-3">39.3.</span> DS</h3>
<div class="outline-text-3" id="text-39-3">
<ul class="org-ul">
<li><p>
types of analysis: EDA clusterization - visualizing data to identify patterns, trends, and anomalies.
</p>
<ul class="org-ul">
<li>Descriptive statistics - mean, median, mode, range, and standard deviation</li>
<li>Categorical - contingency tables, chi-square tests, and logistic regression</li>
<li>Multivariate - has multiple variables or factors - PCA, factor analysis, and discriminant analysis.</li>
<li>Time-series - moving averages, exponential smoothing, and ARIMA models</li>
<li>Survival analysis - time-to-event outcomes - Kaplan-Meier curves and Cox proportional hazards models.</li>
<li>Partition of variance - decomposing the total variation in a dataset into different sources of variation,</li>
</ul>
<p>
useful for understanding the relative importance of different factors in explaining the variation in the
data. Partitioning variance include ANOVA and linear regression.
</p></li>
<li>Random Forest vs Gradient boosting:
<ul class="org-ul">
<li>Random Forest: bagging (Bootstrap Aggregating), ensures that each tree focuses on different aspects of the
data. reduce the variance of the individual decision trees. ,</li>
<li>Gradient boosting: boosting method, add learner to correct the errors of the previous models, reducing the
bias by iteratively adding new base learners that correct the errors of the previous ones, have problem to
recognize ratio relationship between features.</li>
</ul></li>
<li>Mean vs Median vs Mode: Среднее арифметическое хорошо подходит для нормально распределенного величины,
интервылных величин и данных без выбросов. Медиана подходит для данных с выбросами, смещенным распределением
и для порядковых категорий. Мода для категориальных данных, multimodal distributions и для дискретных данных с явным пиком.</li>
</ul>
</div>
</div>
<div id="outline-container-org7d12157" class="outline-3">
<h3 id="org7d12157"><span class="section-number-3">39.4.</span> ML</h3>
<div class="outline-text-3" id="text-39-4">
<ul class="org-ul">
<li>regression vs classification difference -
<ul class="org-ul">
<li>classification to find boundaries between classes (optimize probabilities) , regression to find
relationship function (optimize values)</li>
<li>for classifiction we have n outputs for every class, for regression we have n
outputs for every target</li>
<li>Classificaton loss - cross entropy, regression - MSE</li>
</ul></li>
<li><p>
нормализация - имеет неточный смысл. Это приведение значений к какой-то общей норме (расстоянию), mean
normalization - приведение к mean=0. Чаще всего имеется в виду MinMaxScaling - (x-min)/(max-min) - [0;1]
</p>
<ol class="org-ol">
<li>for each feature contributes approximately proportionately to the final distance. 2) gradient descent</li>
</ol>
<p>
converges much faster with feature scaling than without it
</p></li>
<li>линейные модели - модели состояшие из линейных функий: приращение функции пропорционально приращению
аргумента.</li>
<li>линейная регрессия - model in form of linear combination, Ordinary Least Squares (OLS) parameter estimatiom method frequently used</li>
<li>логистическая регрессия - a logistic model in form of linear combination but inside of logistic function that predict in (0,1)</li>
<li>polynomial regression - relationship modelled as an nth degree polynomial in x. a special case of multiple
linear regression.</li>
<li>logistic regression - for classification task, converts log-odds (-∞,+∞) to probability (0,1). p = 1/(1 +
e<sup>ß0 + ß1*x + ß2*x2 + … + ßn*xn</sup>).</li>
<li><b>переобучение</b> - когда модель показывает плохую обобщающую способность на данных, которые не были использованы
в обучении.</li>
<li><b>недобучение</b> - модель не достаточна сложна и поэтому показывает плохой результат на обучающем датасете</li>
<li><b>как бороться с переобучением</b> - изменением параметров модели, увеличением разнообразности входных данных,
регуляризация, замена модели на более сложную, уменьшить количество признаков во входных данных, борьба с
выбросами, уменьшать каличество параметров в слоях NN, избавиться от колиниарности в зависимых признаках</li>
<li>TODO: как бороться с переобучением в случайных лесах - Увеличение количества деревьев и их глубины,
Использование случайного сэмплирования</li>
<li>Ансамбль — это набор предсказателей, которые вместе дают ответ (например, среднее по всем)</li>
<li>Бэггинг - усреднение (например, взвешенное среднее, голосование большинства или нормальное среднее). Random Forest.</li>
<li>Бустинг - каждая новая модель учится на результатах всех предыдущих моделей.</li>
<li><b>градиентрый бустинг</b> - способ объединять базовых алгоритмов в композицию, последовательное применение
предиктора (предсказателя) таким образом, что каждая последующая модель сводит ошибку предыдущей к
минимуму. Это метод Машинного обучения (ML) для задач Регрессии (Regression) и Классификации
(Classification), который создает прогнозирующую Модель (Model) в форме Ансамбля (Ensemble) слабых алгоритм
прогнозирования, обычно Деревьев решений (Decision Tree). each new model is trained to minimize the residual
error of the previous models, using the negative gradient of the loss function as a guide.</li>
<li><b>K-nearest neighbor</b> - использует евклидову метрику и просто запиминает весь датасет, а потом определяет класс по ближайшим k соседям.</li>
<li>Random forest - бэггинг + feature bagging + randomized node optimization + out-of-bag error as an estimate of the
generalization error + Measuring variable importance through permutation.
<ul class="org-ul">
<li>Bagging is a special case of the model averaging approach.</li>
<li>bootstrap sample - technique of creating copies of dataset by sampling with possible duplications of some items.</li>
</ul></li>
<li>types of ML algorithms: by business problem: classification, regression, forecasting,
segmentation. Algirithm Randomized: Las Vegas vs Monte Carlo; or non-Randomized. Learning process:
Supervised, Unsupervised, Reinforcement, Optimization.</li>
<li>Classification ML algorithms: Naive Bayes, Decision Tree, K-nearest neighbor, logistic regression, SVM, random forest.</li>
<li>low bias, high variance - overfitting ;  high bias, low variance  - underfitting</li>
<li>How Adam works: Combine the adaptive methods (learning rate is adaptively adjusted according to the sum of
the squares of all historical gradients) and the momentum method (accumulating the previous gradient as
momentum and perform the gradient update process with momentum.).</li>
<li>Linerization types:
<ul class="org-ul">
<li>Logarithmic Transformations</li>
<li>decomposition: piecewise linear approximation, Dynamic Mode Decomposition (DMD), feature engineering</li>
<li>Deep Learning and Koopman Operator</li>
<li>Linear Programming Relaxations</li>
<li>simplex method for linear programming</li>
</ul></li>
<li>Gradient Boosting: ∇L(y,Fi​(x))=∂Fi​(x)/∂L - guides the algorithm in determining the direction for the next weak learner
<ul class="org-ul">
<li>L(y,Fi(x)) - loss function, that measure difference between y and predicted values Fi(x)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org0c7c743" class="outline-3">
<h3 id="org0c7c743"><span class="section-number-3">39.5.</span> DL</h3>
<div class="outline-text-3" id="text-39-5">
<ul class="org-ul">
<li>TODO: activation layer</li>
<li>droup out - randomly drop units (along with their connections) from the neural network during training. This
prevents units from co-adapting too much. Therefore, a hidden unit cannot rely on other specific units to
correct its mistakes.</li>
<li>регуляризация - метод, для предотвращения переобучения. например, это переменная, которая увеличивает
функцию потерь так, чтобы уменьшить сложность целевой модели</li>
<li>batch normalization - distribution of each layer’s inputs changes during training, the output of each level
is normalized and used as input of the next level.</li>
<li>normalization - один раз входных данных, batch normalization,</li>
<li>CNN, LSTM,</li>
<li>transformer - Encoder/decoder architecture, token is converted via a word embedding, positional information
of the token is added to the word embedding. has residual connections and layer normalization steps.
<ul class="org-ul">
<li>scaled dot-product attention blocks -</li>
<li>Multi-head attention</li>
<li>Masked attention</li>
</ul></li>
<li>mean(average)=sum(x)/n, median=sorted(x)[n//2], mode=most frequent</li>
<li><p>
l1 и l2 в регуляризации - отличия. It is penalty term added to loss function to restricting the size of coefficient.
</p>
<ul class="org-ul">
<li>l1 good for high number of features</li>
<li>l2 can deal with the multicollinearity.  can be used to estimate the significance of predictors and based</li>
</ul>
<p>
on that it can penalize the insignificant predictors.
</p></li>
<li>TODO: почему batch normalization улучшает обучение</li>
<li>как обучать очень большие модели, которые требуют много времени, чтобы быть уверенным в рещультате: начинать
с небольших кусков данных и валидировать модель после этих кусков, увеличивая их размер.</li>
<li>Tensorflow vs PyTorc differences: CNTK, Caffe2, Theano, and TensorFlow) make use of static graphs, PyTorch
and Chainer) use dynamic graphs.</li>
<li>DL framework: siloed stack of API, graph, and runtime.</li>
<li>dataflow graphs - computation flow that serves as an Intermediate Representation (IR), and is conducive for optimization and translation to run on specific devices (CPU, GPU, FPGA, etc.).</li>
</ul>
</div>
</div>
<div id="outline-container-org7042c4f" class="outline-3">
<h3 id="org7042c4f"><span class="section-number-3">39.6.</span> Python</h3>
<div class="outline-text-3" id="text-39-6">
<ul class="org-ul">
<li><p>
dict - collection which is ordered*, changeable and do not allow duplicates. one of implementation is hash
table: hashes of keys point to data buckets
</p>
<ul class="org-ul">
<li>pros : the average number of instructions that are necessary to lookup an element of the table is</li>
</ul>
<p>
independent of the number of elements stored in the table itself
</p>
<ul class="org-ul">
<li>collision resolution in hash table? common strategies:
<ul class="org-ul">
<li>open addressing - add value next to first one</li>
<li>separate chaining - create nested index structure in occupied bucket</li>
</ul></li>
</ul></li>
<li>Polymorphism concept in functional and object-oriented programming languages: in OOP often achieved through
inheritance and method overriding, in functionl achieved through parametric polymorphism or ad hoc
polymorphism. Parametric polymorphism allows functions to be written generically so that they can operate on
a wide range of data types without specifying the exact types in advance. Ad hoc polymorphism, on the other
hand, involves using type classes or interfaces to define common behavior for different types.</li>
<li>множественнае наследование - supported</li>
<li>сборщик мусора - combination of reference counting(efficient memory management) and generational garbage(for
reference cycles, dividing objects into three generations based on their lifetime. G1: new, G23
surviced. Collecting short-lived objects more frequently = reduces the overhead of gc)</li>
<li>double __ and single _ underscore in names</li>
</ul>
<div class="org-src-container">
<pre class="src src-text">

1) sigle leading - weak &#8220;internal use&#8221; indicator (means non-public).

2) single_trailing_underscore_: used by convention to avoid conflicts with Python keyword

3) __double_leading_underscore: when naming a class attribute, invokes name mangling (inside class Foo,
 __boo becomes Foo._Foo__boo).
4) double before and after name - is &#8220;magic&#8221; objects or attributes that live in user-controlled namespaces (or Special Attributes).

Mangling &#1082;&#1072;&#1083;&#1077;&#1095;&#1080;&#1090;&#1100; - Python&#8217;s name mangling rules - To avoid name clashes with subclasses.

There is no attribute is really private in Python. Private term is not correct. __boo may play role of private.
</pre>
</div>
</div>
</div>

<div id="outline-container-orgca5246c" class="outline-3">
<h3 id="orgca5246c"><span class="section-number-3">39.7.</span> NLP</h3>
<div class="outline-text-3" id="text-39-7">
<ul class="org-ul">
<li>мешок слов, bag of words - way of extracting features from text - 1) vocabulary of known words, 2) measure
of the presence of known words</li>
<li>tf-idf - TFIDF(t,D) = произведение TF (Term Frequency) на IDF (Inversed Document Frequency) - показывает
специфичность данной фразы t по отношению к остальным фразам документа D. TF*IDF score for a term in a
document, where TF how ofter term occurs in this docuent, IDF how rare a term is across the entire corpus.
<ul class="org-ul">
<li>to rank documents based on their relevance to a query</li>
<li>features: identify key terms that distinguish between different classes or categories of text</li>
</ul></li>
<li>step by step explanation of Transformer: Tokenization (outside) -&gt; Embedding (within)-&gt; Positional Encoding -&gt; attention
scores between all pairs of tokens -&gt; activation functions -&gt; Layer normalization -&gt; probability distribution
over the vocabulary for the next token in the prompt (softmax)</li>
<li>levels of text splitting: Characters / Tokens, Recursive Character, Semantic Chunker, Document structure, Agent-like Splitting</li>
<li><p>
Понимание различий между - задач, которые они решают. Architecture, datasets, performance metrics, number of
parameters, finetuning and training simplicity.
</p>
<ul class="org-ul">
<li>BERT - bidirectional transformer model, encoder-only model, which considers both left and right context when making</li>
</ul>
<p>
predictions, best for sentiment analysis or natural language understanding (NLU) tasks.  3TB of data. 340 million parameters
</p>
<ul class="org-ul">
<li>GPT - decoder-only setup, GPT-3 only considers the left context when making predictions. 45TB of data. 1.5
billion parameters. pros: Text Generation, Language Modeling. cons: no bidirectional context, may require</li>
</ul>
<p>
extensive fine-tuning for specific NLP tasks
</p>
<ul class="org-ul">
<li>T5 - encoder-decoder setup. tasks framed as text-to-text transformations. pros: large corpus diverse</li>
</ul>
<p>
linguistic patterns, Versatility, Scalability. cons: Computationally Intensive - large number of parameters,
not easy Fine-Tuning.
</p>
<ul class="org-ul">
<li>Switch - Mixture of Experts (MoE) model trained on Masked Language Modeling (MLM) task. that combines</li>
</ul>
<p>
multiple transformer models specialized in different tasks. beneficial for tasks that require handling
diverse and complex inputs.
</p>
<ul class="org-ul">
<li>Switch Transformers - activate a sparse subgraph of the network. enables faster training (scaling</li>
</ul>
<p>
properties) while being better than T5 on fine-tuned tasks.
</p>
<ul class="org-ul">
<li>Meena - designed for open-domain dialogue. large number of parameters. for conversational applications and
chatbots where maintaining engaging and contextually relevant conversations is crucial. pros: Large Model</li>
</ul>
<p>
Size - capture conversational nuances. cons: Resource Intensive - large size, Lack of Task Specificity.
</p></li>

<li><p>
tasks
</p>
<ul class="org-ul">
<li>токенизация tokenization - Byte Pair Encoding (BPE) or SentencePiece
<ul class="org-ul">
<li>Byte Pair Encoding (BPE) - subword-based tokenization method</li>
<li>SentencePiece - subword tokenizer and detokenizer</li>
</ul></li>
<li>лемматизация Lemmatization - reducing words to their canonical form or lemma, which represents the dictionary form of a</li>
</ul>
<p>
word. It may be better to incorporate lemmatization and stemming more directly into the model architecture.
</p>
<ul class="org-ul">
<li>стемминг stemming - technique used to reduce inflected words to their base or root form, known as the
"stem." by removing their affixes .</li>
<li>lemmatization and stemming - potentially leading to better performance of LLMs in tasks such as text</li>
</ul>
<p>
generation, sentiment analysis, question answering, and more.
</p>
<ul class="org-ul">
<li>извлечение сущностей Named entity recognition (NER) - information extraction task - find and classify</li>
<li>классификация текста - text classification</li>
</ul></li>
<li><p>
tools
</p>
<ul class="org-ul">
<li>word2vec - embeddings, NN-based, semantic relationships, two archs:
<ul class="org-ul">
<li>Continuous Bag of Words (CBOW) - capture meaning based on context</li>
<li>Skip-gram - predict context for word</li>
</ul></li>
<li>doc2vec - embeddings, Google too, two impl: Distributed Memory (DM) and Distributed Bag of Words (DBOW)</li>
<li>GloVe - embeddings, unsupervised learning algorithm - matrix factorization. good for word analogy, word</li>
</ul>
<p>
similarity, and sentiment analysis.
</p>
<ul class="org-ul">
<li>FastText</li>
<li>BERT</li>
<li>LSTM in NLP - is type of RNN. Bi-directional LSTMs - improves the model's ability to understand the</li>
</ul>
<p>
context of words. Attention Mechanism: Attention mechanisms can be integrated with LSTMs to focus on relevant
parts of the input sequence when making predictions.
</p>
<ul class="org-ul">
<li>CNN in NLP. - to capture local patterns and hierarchies in data. Multi-channel CNNs - set with different</li>
</ul>
<p>
kernel sizes, for text classification and sentiment analysis;
</p>
<ul class="org-ul">
<li>NLTK - toolbox, as an education and research tool. string input-output. general-purpose. has better support for English</li>
<li>spaCy - for specific tasks. object-oriented approach</li>
<li>Gensim - focuses on topic modeling and document similarity tasks. simplicity and ease. has integration</li>
<li>Stanford’s CoreNLP - Java library with Python wrappers. It’s in many existing production systems due to its speed.</li>
</ul>
<p>
with popular deep learning frameworks
</p></li>
<li>scores: perplexity - in NLP indicating how well LM can predict the next word in a sequence of text. lower -
higher certainty in its predictions.</li>
<li>scores: BLEU score</li>
<li>score: WER - Word Error Rate (WER) - measure of the number of words that were incorrectly recognized -
calculated as the sum of substitutions (S), deletions (D), and insertions (I) divided by the total number of
words in the reference sequence (N).</li>
<li>How RAG works? retriever system - it fetch relevant document for question from vector data storage and use
as a context for a question.</li>
<li><p>
достоинства и недостатки Decoder and Encoder:
</p>
<ul class="org-ul">
<li>Decoder-based LLMs are generates output in autoregressive manner. Good for generating text and completing</li>
</ul>
<p>
sequences but have limited contextual understanding and slower inference.
</p>
<ul class="org-ul">
<li>Encoder-based LLMs are limited by input and output length of datata and excel at understanding context and</li>
</ul>
<p>
are great for tasks like text classification (sentiment and NLP), can be computationally intensive for large texts.
</p>

<p>
understanding and analyzing text, while Decoder-based LLMs are better suited for generating text and completing sequences.
</p></li>
</ul>

<p>
Базовые понимания работы генеративных моделей
Понимание и Работа с Embeddings (использование распространенных моделей)
Векторные Базы Данных (Faiss и аналоги)
Контекстный и Семантический Поиск (пригодится для поиска по данным)
Понимание того как работают AI агенты (фреймворк LangChain)
Обработка и анализ данных: Навыки работы с большими данными, их предобработка и анализ.
Разработка веб-сервисов и микро сервисов: Способность разрабатывать масштабируемые веб-сервисы.
</p>

<p>
Требования:
</p>

<p>
Планируемые интеграция с AI-сервисами, которые вам предстоит реализовывать:
</p>

<p>
OpenAI API, YandexGPT API, Gigachat API: Для работы с продвинутыми моделями генерации текста и других медиа.
Yandex SpeachKIT и аналоги: текст в речь и обратно
Сервисы обработки и генерации изображений
Сервисы обработки и генерации видео и 3D моделей
</p>
</div>

<div id="outline-container-org0796dc0" class="outline-4">
<h4 id="org0796dc0"><span class="section-number-4">39.7.1.</span> <a href="https://github.com/masmahbubalom/InterviewQuestions/blob/main/NLP%20Interview%20Questions/README.md">https://github.com/masmahbubalom/InterviewQuestions/blob/main/NLP%20Interview%20Questions/README.md</a></h4>
<div class="outline-text-4" id="text-39-7-1">
<ul class="org-ul">
<li>What is NLP, and why is it important?</li>

<li>Explain the difference between NLP and NLU (Natural Language Understanding).</li>
<li>What are some common applications of NLP?</li>
<li>Describe tokenization in NLP.</li>
<li>What is stemming, and how does it differ from lemmatization?</li>
<li>Explain the concept of stop words in NLP.
<ul class="org-ul">
<li>Considered to be of little value in terms of extracting meaning:  "the," "is," "in," "and," "a," and "to."</li>
</ul></li>
<li><p>
What is POS tagging, and why is it used?
</p>
<ul class="org-ul">
<li>Part of Speech (POS) tagging is the process of assigning grammatical tags to words: noun, verb, adjective,</li>
</ul>
<p>
adverb, etc
</p></li>
<li><p>
How does named entity recognition (NER) work?
</p>
<ul class="org-ul">
<li>identify and classify named entities within a text into predefined categories such as names of persons,</li>
</ul>
<p>
organizations, locations, dates, and so on. Large Language Models (LLMs) used.
</p></li>
<li>What is TF-IDF, and what is its significance in NLP?</li>
<li>Explain the concept of word embeddings.</li>
<li>What are some popular word embedding techniques?
<ul class="org-ul">
<li>Word2Vec, GloVe, ELMo (Embeddings from Language Models) and BERT (Bidirectional Encoder Representations from Transformers)</li>
</ul></li>
<li>What is Word2Vec, and how does it work?</li>
<li>Describe the difference between CBOW and Skip-gram models in Word2Vec.</li>
<li>What is GloVe (Global Vectors for Word Representation)?</li>
<li>Explain the concept of language modeling.</li>
<li><p>
What is perplexity in language modeling?
</p>
<ul class="org-ul">
<li>measure of how well a probabilistic model predicts a sample.  ability to predict uniformly among the set</li>
</ul>
<p>
of specified tokens in a corpus.  measures the degree of uncertainty of a language model when it generates a
new token. sensitive to linguistic features and sentence length.
</p></li>
<li>How does a recurrent neural network (RNN) differ from a feedforward neural network?
<ul class="org-ul">
<li>maintain a hidden state, which captures information from previous inputs</li>
</ul></li>
<li>What are some limitations of traditional RNNs?</li>
<li>What is the vanishing gradient problem in RNNs?</li>
<li>Describe the structure and purpose of Long Short-Term Memory (LSTM) networks.</li>
<li>What is attention mechanism in NLP? allowing the model to focus on relevant information by computing
similarity scores with dot product by creating matrix of similarity scores.</li>
<li><p>
Explain the transformer architecture.
</p>
<ul class="org-ul">
<li>Deep learning architecture, based on the multi-head attention mechanism (2017). Originally have</li>
</ul>
<p>
autoregressive text generation. Tokenization by word.
</p></li>
<li>What are the advantages of transformers over RNNs and LSTMs?</li>
<li>Describe the encoder-decoder architecture in sequence-to-sequence models.</li>
<li>What is beam search in the context of sequence generation?</li>
<li>Explain the concept of machine translation and some popular methods for it.</li>
<li>How does sentiment analysis work?
<ul class="org-ul">
<li>the task of determining the emotional tone of a piece of text</li>
</ul></li>
<li>What are some techniques for feature extraction in sentiment analysis?</li>
<li>What is topic modeling, and how is it useful in NLP?</li>
<li>Explain the Latent Dirichlet Allocation (LDA) algorithm.</li>
<li>Describe the bag-of-words (BoW) model.</li>
<li>What is dependency parsing?</li>
<li>How does dependency parsing differ from constituency parsing?</li>
<li>Explain the concept of named entity recognition (NER).</li>
<li>What are some challenges faced in named entity recognition?</li>
<li>Describe the BIO tagging scheme used in NER.</li>
<li>What is sequence labeling, and why is it important in NLP?</li>
<li>Explain the concept of sequence-to-sequence learning.</li>
<li>What are some popular frameworks or libraries used in NLP?</li>
<li>Describe some common evaluation metrics used in NLP tasks.</li>
<li>What is the BLEU score, and how is it used in NLP evaluation?</li>
<li>Explain the concept of cross-entropy loss in NLP.</li>
<li>How do you handle out-of-vocabulary words in NLP models?</li>
<li>What is transfer learning, and how is it applied in NLP?</li>
<li>Describe some pre-trained language models, such as BERT, GPT, or RoBERTa.</li>
<li>How do you fine-tune a pre-trained language model for a specific task?</li>
<li>What is text generation, and what are some challenges associated with it?</li>
<li>How do you deal with imbalanced datasets in NLP tasks?</li>
<li>Explain the concept of word sense disambiguation.</li>
<li>What are some ethical considerations in NLP research and applications?</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org0d4c428" class="outline-3">
<h3 id="org0d4c428"><span class="section-number-3">39.8.</span> CV:</h3>
<div class="outline-text-3" id="text-39-8">
<ul class="org-ul">
<li>object detection frameworks: Two-stage/proposal (Region based detectors) region</li>
</ul>
<p>
proposal, (ii) feature extraction, and (iii) classification ; Single Stage Detectors: as a
regression problem of the bounding box coordinates.
</p>
<ul class="org-ul">
<li>How YOLO works?  the algorithm divides the image into S×S grids and checks whether the center of each
object lies within a grid cell, the matched grid cell will regress the bounding box of the selected object in
the grid. Finally, the overlapping bounding boxes are merged to produce the most plausible bounding boxes.</li>
<li>YOLO versions: 2015 first YOLO nearby objects was difficult to detect. YOLOv2 batch normalization, high
resolution classifiers, the use of anchor boxes instead of fully connected layers, and the use of clustering
to determine the bounding box sizes as priors. YOLOv3: A multi-scale prediction o estimate bounding boxes at
three different scales, softmax has been replaced by logistic classifiers. YOLOv4
weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization
(CmBN), Self-adversarial-training (SAT) and Mish-activation to improve the performance.  Net: convolution
layers 1x1 of stride 1, and max-pooling layers.</li>
<li>How SSD works? single-stage detector, determine the category scores and box offsets for a set of predefined
bounding boxes using small convolutional filters on top of the feature maps.</li>
<li>RetinaNet works? (same to FPN) information from different resolution used, at each level two subnets: class
and box. New focal loss focusing on hard examples was proposed by adding a multiplicative factor to the
cross-entropy loss.</li>
</ul>
</div>
</div>

<div id="outline-container-org86b8c56" class="outline-3">
<h3 id="org86b8c56"><span class="section-number-3">39.9.</span> СберМаркет</h3>
<div class="outline-text-3" id="text-39-9">
<ul class="org-ul">
<li>скалярное произведение. Ответ: это метрика расстояния векторов и определяется произвольно, должно
удовлетворять аксиомам</li>
<li>bagging boosting для паралельной обработки</li>
<li>L1 L2 для выбора признаков - L1 regularization can be helpful in features selection by eradicating the
unimportant features, whereas, L2 regularization is not recommended for feature selection.</li>
<li>если модель константа, что для нее будет важнее bias or varience - Ответ: если константа, то у нее нет
variance, а значит важнее bias</li>
</ul>
</div>
</div>

<div id="outline-container-org6f32947" class="outline-3">
<h3 id="org6f32947"><span class="section-number-3">39.10.</span> MLOps:</h3>
<div class="outline-text-3" id="text-39-10">
<ul class="org-ul">
<li>What is MLOps? MLOps is the intersection of Machine Learning and DevOps principles. + data + perform A/B test.</li>
<li>main steps of ML Lifecycle. <a href="#orgf7b4d45">23.4</a> <a href="#org561c8af">23.1</a></li>
<li>MLOps vs DevOps - data changes rapidly and the up-gradation of models has to happen more frequently than typical software application code.</li>
<li>How do you create infrastructure for MLOps? The core responsibility typically lies outside of the scope of
an MLOps engineer. For example, if the enterprise has a predominantly AWS-based infrastructure, then it
becomes easy to implement MLOps pipelines utilizing AWS Sagemaker framework in conjunction with services like
Sagemaker pipelines, Cloudformation, Lambdas for orchestration and Infrastructure as Code. If the enterprise
is open, then the best platform for most modern software development firms is leaning towards a Kubernetes
(k8s) powered infrastructure. This also enables the ML engineer to adopt Kubeflow which is quickly becoming
the de facto MLOps framework of choice for many ML practitioners.</li>
<li>How to create CI/CD pipelines for machine learning? building code, running tests and deploying new versions
of model/application when there are updates/revisions.  including data in addition to code. AWS driven,
Sagemaker pipelines or Kubeflow pipelines or traditional tools like Jenkins or even Github actions to build.
CI/CD pipelines.</li>
<li>Model drift, or Training-serving skew, or concept drift, occurs when the model performance during the
inference phase (using real-world data) degrades when compared to its performance during the training phase
(using historical, labeled data). It is also known as train/serve skew as the performance of the model is
skewed when compared with the training and serving phases. <b>Data Drift</b> is a condition where the inference data
on which predictions are expected do not follow the same distribution as the training data.
<ul class="org-ul">
<li>A discrepancy between how you handle data in the training and serving pipelines.</li>
<li>A change in the data between when you train and when you serve.</li>
<li>A feedback loop between your model and your algorithm. -  addressed by proper ML system design</li>
<li>Training happened on a limited number of categories but a recent environmental change happened which added another category</li>
<li>In NLP problems the real world data has significantly more number of tokens that are different from training data</li>
</ul></li>
<li>train/serve skew and some potential ways to avoid them. If the prediction data differs significantly from
the training data then it can be argued that there is a train/serve skew.</li>
</ul>
</div>
</div>

<div id="outline-container-org24b7497" class="outline-3">
<h3 id="org24b7497"><span class="section-number-3">39.11.</span> DevOps</h3>
<div class="outline-text-3" id="text-39-11">
<ul class="org-ul">
<li>Containerization pros: 1) virtualization - no dependency on hardware, 2) share resources among virtual
machines 3) easier to build software with service-oriented architecture, like microservices. cons: 1) ability
to scale quickly and to change parts of the application 2) raise complexity</li>
</ul>
</div>
</div>

<div id="outline-container-orgf43c695" class="outline-3">
<h3 id="orgf43c695"><span class="section-number-3">39.12.</span> Docker</h3>
<div class="outline-text-3" id="text-39-12">
<ul class="org-ul">
<li>Какие типы сетей есть в docker? types:
<ul class="org-ul">
<li>&#x2013;ingress network,</li>
<li>"predefined networks",</li>
<li>"swarm network",</li>
<li>bridge: The default network driver.</li>
<li>host</li>
<li>overlay</li>
<li>ipvlan</li>
<li>macvlan</li>
<li>none</li>
<li>network plugins</li>
</ul></li>
<li>Как узнать метрики потребления ресурсов у контейнера? Сколько потребляет дискового пространства контейнер?
<ul class="org-ul">
<li>docker stats &#x2013;all &#x2013;no-stream &#x2013;no-trunc # memory, cpu</li>
<li>docker system df -v</li>
<li>docker status container<sub>ID</sub> #to check single container resources</li>
</ul></li>
<li>В чем разница между ARG и ENV?
<ul class="org-ul">
<li>ARG is only available during the build of a Docker imag</li>
<li>ENV values are available to containers, but also RUN-style commands during the Docker build
starting with the line where they are introduced. If you set an environment variable in an
intermediate container using bash (RUN export VARI=5 &amp;&amp; …) it will not persist in the next
command.</li>
</ul></li>
<li><p>
Что знаете про distroless образы? Делали ли свои? (если да, то отдельно спросить под какую задачу)
</p>
<ul class="org-ul">
<li>Images contain only your application and its runtime dependencies - statically compiled and</li>
</ul>
<p>
self-contained. "FROM scratch" or cleared without OS package manager.
</p></li>
<li>Каким образом можно ограничить потребляемую память или количество cpu у контейнера?
<ul class="org-ul">
<li>docker info - to check if kernel support capability</li>
<li>memory: hard limits and soft. ex: &#x2013;memory=10M for hard limit. Add &#x2013;memory-reservation to make it soft.</li>
<li>CPU: &#x2013;cpus="1.5" for one and half at most CPUs will be used</li>
<li>There is no access to GPU by default, to add GPU: &#x2013;gpus.</li>
<li><a href="https://docs.docker.com/config/containers/resource_constraints/">https://docs.docker.com/config/containers/resource_constraints/</a></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org0a790db" class="outline-3">
<h3 id="org0a790db"><span class="section-number-3">39.13.</span> prompt engineer</h3>
<div class="outline-text-3" id="text-39-13">
<p>
What limitations of language models do you know, and how can you get around them?
</p>
<ul class="org-ul">
<li>Reasoning and Logical Inference: use knowledge graphs, prompt techniques, consider finetuning.</li>
<li>Knowledge and Expertise:  clear and detailed prompts, Use domain-specific models or fine-tune LLMs.</li>
<li>Understanding and Context: detailed in-context learning, multi-step inference, Implement feedback mechanisms
where users can correct or clarify the model's responses.</li>
<li>Planning and Execution: human verification before execution</li>
<li>Ethical and Misinformation: ethical guidelines and feedback mechanisms, Reinforcement Learning from Human
Feedback (RLHF) to fine-tune the model's responses based on human ratings and feedback.</li>

<li>Опишите ваш опыт работы с инструментами генеративного ИИ (например, ChatGPT, Bard, Claude). Как вы их использовали?
<ul class="org-ul">
<li>По HTTP API и веб формы.</li>
</ul></li>
<li>Какие ограничения языковых моделей вы знаете, и как их можно обойти?
<ul class="org-ul">
<li>Рассуждение и Логическое Выведение: использовать знание графики, техники промптинга, рассмотреть дообучение.</li>
<li>Знания и Экспертиза: четкие и подробные промпты, использовать модели, специализированные для конкретной области, или дообучать БЯМ (Большие Языковые Модели).</li>
<li>Понимание и Контекст: подробное обучение в контексте, многошаговое вывод, реализовать механизмы обратной связи, где пользователи могут исправить или уточнить ответы модели.</li>
<li>Планирование и Исполнение: верификация человеком перед исполнением.</li>
<li>Этические и Дезинформационные Вопросы: этические руководства и механизмы обратной связи, обучение с подкреплением от человеческой обратной связи (RLHF) для дообучения ответов модели на основе оценок и обратной связи от людей.</li>
</ul></li>
<li>Какие параметры и подходы вы используете для A/B тестирования промптов?
<ul class="org-ul">
<li>Я использовал только multi-armed bandit RL подход.</li>
</ul></li>
<li>Вы использовали OpenAI API с помощью Pyphon или аналогичные инструменты? Если да, то для каких задач?
<ul class="org-ul">
<li>Для задачи генерации отзывов, я использоал YandexGPT, скрипт Bash на cURL и jq.</li>
</ul></li>
<li>Как вы оцениваете качество работы модели? Какие метрики или подходы вы используете?
<ul class="org-ul">
<li>NLP метрики в зависимости от решаемой задачи, человеческую обратную связь, banchmark-и, онлайн-мониторинг.</li>
</ul></li>
<li>Есть ли опыт работы с финтех/крипто проектами? Если да, то какой. Если нет, то использовали ли ранее крипту для личных целей?
<ul class="org-ul">
<li>Нет. Да использую, для опланы интернет сервисов.</li>
</ul></li>
<li>уточните пожалуйста ваши ожидания по заработной плате, чтобы мы понимали попадает ли это в рамки наших возможностей.
<ul class="org-ul">
<li>от 200 000 RUB чистыми минимум</li>
</ul></li>
<li>уточните пожалуйста ваш уровень письменного и устного английского.
<ul class="org-ul">
<li>B2, B2 - свободно слушаю, читаю, пишу и говорю.</li>
</ul></li>
</ul>

<p>
Best Practices for Evaluation LLM:
</p>
<ul class="org-ul">
<li>Multiple Evaluation Metrics</li>
<li>Enhanced Human Evaluation - Improve consistency and objectivity through clear guidelines, standardized
criteria, and inter-rater reliability checks</li>
<li>Benchmark Selection</li>
<li>Continuous Monitoring</li>
</ul>
</div>
</div>
<div id="outline-container-orge65e0cd" class="outline-3">
<h3 id="orge65e0cd"><span class="section-number-3">39.14.</span> Общие вопросы:</h3>
<div class="outline-text-3" id="text-39-14">
<ul class="org-ul">
<li>Перечислите используемые Вами методологии, паттерны, принципы написания кода
<ul class="org-ul">
<li>я не помню их очень много и используются интуитивно, это вопрос на целую лекцию</li>
</ul></li>
<li>Как называется объект, имеющий аналогичный интерфейс с некоторым объектом, но эмулирующий его работу?
Известны ли Вам python-фраемворки помогающие в имплементации таких объектов?
<ul class="org-ul">
<li>mock объект?</li>
<li>большинство библиотек для тестирования кода</li>
</ul></li>
<li>Как откатить два последних коммита, но оставить их изменения ?
<ul class="org-ul">
<li>docker checkout ^<sup>HEAD</sup> ?</li>
</ul></li>
</ul>

<p>
Increase value by 20% and decrease by 20%: 120 - 0.2 * 120 = 120 - 240/10 = 96 = -4%
</p>
</div>
</div>
<div id="outline-container-org4b0c049" class="outline-3">
<h3 id="org4b0c049"><span class="section-number-3">39.15.</span> Поведение</h3>
<div class="outline-text-3" id="text-39-15">
<p>
Руководитель - ждет подчинение или просто хочет получить консультацию.
</p>

<p>
Тех лид - хочет развлечься.
</p>

<p>
HR - ждет адекватную быструю связь.
</p>

<p>
Самое важное - подстроиться под монеру общения собеседника.
</p>

<p>
На первой лини как правило “астролог повидения”.
</p>

<p>
Женжины любят рабов-джентельменов с чувством собственного достоинства.
</p>

<p>
Мужчины любят собак улыбак, шутливых мудрунов, roaming bright.
</p>
</div>
</div>
<div id="outline-container-orgf97632c" class="outline-3">
<h3 id="orgf97632c"><span class="section-number-3">39.16.</span> Секция Linux:</h3>
<div class="outline-text-3" id="text-39-16">
<ul class="org-ul">
<li>Какое ограничение на количество открытых файлов для одного процесса в дефолтной конфигурации linux? Как
изменить\посмотреть?
<ul class="org-ul">
<li>По умолчанию ядра при запуске или компиляции выбирает максимальное значение.</li>
<li>For Red Hat Linux: 4096</li>
<li>cat /proc/sys/fs/file-max - max number of file handlers, that can be opened</li>
<li>ulimit -Hn -  hard limit. ulimit -Hn - soft limit</li>
<li>to set system wide: sysctl -w fs.file-max=500000</li>
<li>to set user level: ?</li>
</ul></li>
<li>Как проверить доступность порта на удаленной машине?
<ul class="org-ul">
<li>nmap -n -Pn 192.168.1.0/24 -p80,8080</li>
</ul></li>
<li>Как в командной строке узнать адрес текущей машины
<ul class="org-ul">
<li>ip a</li>
</ul></li>
<li>Команда в Linux чтобы для файла задать следующие права - владельцу все, группе чтение, остальным ничего
<ul class="org-ul">
<li>chmod u=rwx,g=r,o= file</li>
</ul></li>
<li>Как посмотреть пид процесса, использующего известный вам порт?
<ul class="org-ul">
<li>netstat -pl | grep :80</li>
</ul></li>
<li>Как передать данные между двумя процессами в Linux
<ul class="org-ul">
<li>file</li>
<li>signals</li>
<li>network sockets</li>
<li>Unix domain socket</li>
<li>POSIX message queue: mount  -t mqueue none  /dev/mqueue</li>
<li>Named, Anonymous pipe (FIFO) - os.pipe()</li>
<li>Shared memory multiprocessing.shared<sub>memory</sub> by name</li>
<li>Memory-mapped file (tmpfs)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org6ea4e9a" class="outline-3">
<h3 id="org6ea4e9a"><span class="section-number-3">39.17.</span> Секция Network:</h3>
<div class="outline-text-3" id="text-39-17">
<ul class="org-ul">
<li>Как в питоне собрать пакет начиная от канального уровня модели OSI и отправить не дожидаясь ответа?
<ul class="org-ul">
<li>socket.socket(socket.AF<sub>INET</sub>, socket.SOCK<sub>DGRAM</sub>).sendto(bytes(MESSAGE, "utf-8"), (UDP<sub>IP</sub>, UDP<sub>PORT</sub>))</li>
</ul></li>
<li>Что такое DNS (днс) сервер?
<ul class="org-ul">
<li>Domain Name System - a system used to convert a computer's host name into an IP address on the Internet</li>
</ul></li>
<li><p>
Что такое NAT (нат)
</p>
<ul class="org-ul">
<li>Network address translation (NAT) - is a method of mapping an IP address space into another by modifying</li>
</ul>
<p>
network address information in the IP header of packets while they are in transit across a traffic routing
device.
</p></li>
<li>Как сделать icmp запрос?
<ul class="org-ul">
<li>ping google.com</li>
</ul></li>
<li>Какой протокол транспортного уровня модели OSI используется DHCP сервером?
<ul class="org-ul">
<li>UDP</li>
</ul></li>
<li>Какой диапазон ип адресов входит в подсеть: 192.168.4.4/30 ?
<ul class="org-ul">
<li>Subnet Mask:255.255.255.252, Wildcard Mask:0.0.0.3,  192.168.4.5 - 192.168.4.6</li>
</ul></li>
<li>Как с некоторой долей вероятности определить операционную систему по ip-адресу?
<ul class="org-ul">
<li>nmap -O &lt;target&gt;</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgc33ddac" class="outline-2">
<h2 id="orgc33ddac"><span class="section-number-2">40.</span> articles</h2>
<div class="outline-text-2" id="text-40">
<p>
<a href="theory#MissingReference">theory#MissingReference</a>
</p>
</div>
<div id="outline-container-org8077a36" class="outline-3">
<h3 id="org8077a36"><span class="section-number-3">40.1.</span> 2019 A Survey of Optimization Methods from a Machine Learning Perspective</h3>
<div class="outline-text-3" id="text-40-1">
<p>
<a href="https://arxiv.org/abs/1906.06821">https://arxiv.org/abs/1906.06821</a>
</p>

<p>
Optimization tools for machine learning applications seek to minimize the finite sum:
</p>
<ul class="org-ul">
<li>min f(x) = 1/n ∑fi(x) , sum for fi(x) is loss associated with sample i.</li>
</ul>

<p>
variance reduction techniques - by carefully blending large and small batch gradients. Most machine learning
 problems, once formulated, can be solved as optimization problems.
</p>
</div>

<div id="outline-container-org1f523e1" class="outline-4">
<h4 id="org1f523e1"><span class="section-number-4">40.1.1.</span> applications</h4>
<div class="outline-text-4" id="text-40-1-1">
<p>
Reinforcement learning (RL) is a branch of machine learning, for which an agent interacts with the environment
 by trial-and-error mechanism and learns an optimal policy by maximizing cumulative rewards.
</p>

<p>
Meta learning has recently become very popular in the field of machine learning. The goal of meta learning is
 to design a model that can efficiently adapt to the new environment with as few samples as possible.  can
 solve the few-shot learning problems.
</p>
<ul class="org-ul">
<li>types: metric-based methods, model-based methods and optimization-based methods.</li>
</ul>
</div>
</div>
<div id="outline-container-org8d1c054" class="outline-4">
<h4 id="org8d1c054"><span class="section-number-4">40.1.2.</span> categories of methods:</h4>
<div class="outline-text-4" id="text-40-1-2">
<ul class="org-ul">
<li>first-order optimization methods - stochastic gradient methods</li>
<li>high-order optimization methods - Newton’s method
<ul class="org-ul">
<li>converge at a faster speed in which the curvature information makes the search direction
more effective</li>
</ul></li>
<li>heuristic derivative-free optimization methods - the coordinate descent method.
<ul class="org-ul">
<li>used in the case that the derivative of the objective function may not exist or be difficult to calculate</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org61b984e" class="outline-4">
<h4 id="org61b984e"><span class="section-number-4">40.1.3.</span> problems</h4>
<div class="outline-text-4" id="text-40-1-3">
<p>
<b>sparse</b> If data are sparse and features occur at different frequencies, it is not expected to update the
 corresponding variables with the same learning rate. A higher learning rate is often expected for less
 frequently occurring features.
</p>

<p>
stochastic gradient-based algorithms
</p>
<ul class="org-ul">
<li>the learning rate will be oscillating in the later training stage of some adaptive methods, which may lead
to the problem of non-converging.</li>
</ul>
</div>
</div>
<div id="outline-container-orgfa4a18b" class="outline-4">
<h4 id="orgfa4a18b"><span class="section-number-4">40.1.4.</span> 1)</h4>
<div class="outline-text-4" id="text-40-1-4">
<ol class="org-ol">
<li>describe the optimization problems</li>
<li>the principles and progresses of commonly used optimization methods</li>
<li>applications and developments of optimization methods in fields</li>
<li>open problems for the optimization</li>
</ol>
</div>
</div>
<div id="outline-container-orgec62c84" class="outline-4">
<h4 id="orgec62c84"><span class="section-number-4">40.1.5.</span> Summary of First-Order Optimization Methods <a id="orgd376911"></a></h4>
<div class="outline-text-4" id="text-40-1-5">
<p>
GD
</p>
<ul class="org-ul">
<li>Solve the optimal value along the direction of the gradient descent. The method converges at a linear rate.</li>
<li>The solution is global optimal when the objective function is convex.</li>
<li>In each parameter update, gradients of total samples need to be calculated, so the calculation cost is high.</li>
</ul>

<p>
SGD
</p>
<ul class="org-ul">
<li>The update parameters are calculated using a randomly sampled mini-batch.  The method converges at a
sublinear rate.</li>
<li>The calculation time for each update does not depend on the total number of training samples, and a lot of
calculation cost is saved.</li>
<li>It is difficult to choose an appropriate learning rate, and using the same learning rate for all parameters
is not appropriate. The solution may be trapped at the saddle point in some cases.</li>
</ul>

<p>
NAG
</p>
<ul class="org-ul">
<li>Accelerate the current gradient descent by accumulating the previous gradient as momentum and perform the</li>
</ul>
<p>
gradient update process with momentum.
</p>
<ul class="org-ul">
<li>When the gradient direction changes, the momentum can slow the update speed and reduce the oscillation; when
the gradient direction remains, the momentum can accelerate the parameter update. Momentum helps to jump
out of locally optimal solution.</li>
<li>It is difficult to choose a suitable learning rate.</li>
</ul>

<p>
AdaGrad
</p>
<ul class="org-ul">
<li>The learning rate is adaptively adjusted according to the sum of the squares of all historical gradients.</li>
<li>In the early stage of training, the cumu- lative gradient is smaller, the learning rate is larger, and
learning speed is faster. The method is suitable for dealing with sparse gradient problems.  The learning
rate of each parameter adjusts adaptively.</li>
<li>As the training time increases, the accumulated gradient will become larger and larger, making the learning
rate tend to zero, resulting in ineffective parameter updates. A manual learning rate is still needed. It is
not suitable for dealing with non-convex problems.</li>
</ul>

<p>
AdaDelta/ RMSProp
</p>
<ul class="org-ul">
<li>Change the way of total gradient accumulation to exponential moving average.</li>
<li>Improve the ineffective learning problem in the late stage of AdaGrad. It is suitable for optimizing
non-stationary and non-convex problems.</li>
<li>In the late training stage, the update process may be repeated around the local minimum.</li>
</ul>

<p>
Adam
</p>
<ul class="org-ul">
<li>Combine the adaptive methods and the momentum method. Use the first-order moment estimation and the second-
order moment estimation of the gradient to dynamically adjust the learning rate of each parameter. Add the
bias correction.</li>
<li>The gradient descent process is rela- tively stable. It is suitable for most non-convex optimization
problems with large data sets and high dimensional space.</li>
<li>The method may not converge in some cases.</li>
</ul>

<p>
SAG
</p>
<ul class="org-ul">
<li>The old gradient of each sample and the summation of gradients over all samples are maintained in
memory. For each update, one sample is randomly selected and the gradient sum is recalculated and used as the
update direction.</li>
<li>The method is a linear convergence algorithm, which is much faster than SGD.</li>
<li>The method is only applicable to smooth and convex functions and needs to store the gradient of each
sample. It is inconvenient to be applied in non- convex neural networks.</li>
</ul>

<p>
SVRG
</p>
<ul class="org-ul">
<li>Instead of saving the gradient of each sample, the average gradient is saved at regular intervals. The
gradient sum is updated at each iteration by calculating the gradients with respect to the old parameters and
the current parameters for the randomly selected samples.</li>
<li>The method does not need to maintain all gradients in memory, which saves memory resources. It is a linear
con- vergence algorithm.</li>
<li>To apply it to larger/deeper neural nets whose training cost is a critical issue, further investigation is
still needed.</li>
</ul>

<p>
ADMM
</p>
<ul class="org-ul">
<li>The method solves optimization prob- lems with linear constraints by adding a penalty term to the objective
and separating variables into sub-problems which can be solved iteratively.</li>
<li>The method uses the separable op- erators in the convex optimization problem to divide a large problem into
multiple small problems that can be solved in a distributed manner. The framework is practical in most large-
scale optimization problems.</li>
<li>The original residuals and dual resid- uals are both related to the penalty parameter whose value is
difficult to determine.</li>
</ul>

<p>
Frank-Wolfe
</p>
<ul class="org-ul">
<li>The method approximates the objec- tive function with a linear function, solves the linear programming to
find the feasible descending direction, and makes a one-dimensional search along the direction in the
feasible domain.</li>
<li>The method can solve optimization problems with linear constraints, whose convergence speed is fast in early
iterations.</li>
<li>The method converges slowly in later phases. When the iterative point is close to the optimal solution, the
search di- rection and the gradient of the objective function tend to be orthogonal. Such a direction is not
the best downward direction.</li>
</ul>
</div>
</div>
<div id="outline-container-org58546d3" class="outline-4">
<h4 id="org58546d3"><span class="section-number-4">40.1.6.</span> Summary of High-Order Optimization Methods</h4>
<div class="outline-text-4" id="text-40-1-6">
<p>
Conjugate Gradient
</p>
<ul class="org-ul">
<li>It is an optimization method between the first-order and second-order gra- dient methods. It constructs a
set of conjugated directions using the gradient of known points, and searches along the conjugated direction
to find the mini- mum points of the objective function.</li>
<li>CG method only calculates the first or- der gradient but has faster convergence than the steepest descent
method.</li>
<li>Compared with the first-order gradient</li>
</ul>
<p>
method, the calculation of the conjugate gradient is more complex.
</p>

<p>
Newton’s Method
</p>
<ul class="org-ul">
<li>Newton’s method calculates the inverse matrix of the Hessian matrix to obtain faster convergence than the
first-order gradient descent method.</li>
<li>Newton’s method uses second-order gradient information which has faster convergence than the first-order
gra- dient method. Newton’s method has quadratic convergence under certain conditions.</li>
<li>It needs long computing time and large storage space to calculate and store the inverse matrix of the
Hessian matrix at each iteration.</li>
</ul>

<p>
Quasi-Newton Method
</p>
<ul class="org-ul">
<li>Quasi-Newton method uses an approx- imate matrix to approximate the the Hessian matrix or its inverse
matrix. Popular quasi-Newton methods include DFP, BFGS and LBFGS.</li>
<li>Quasi-Newton method does not need to calculate the inverse matrix of the Hessian matrix, which reduces the
com- puting time. In general cases, quasi- Newton method can achieve superlinear convergence.</li>
<li>Quasi-Newton method needs a large storage space, which is not suitable for handling the optimization of
large-scale problems.</li>
</ul>

<p>
Sochastic Quasi-Newton Method
</p>
<ul class="org-ul">
<li>Stochastic quasi-Newton method em- ploys techniques of stochastic opti- mization. Representative methods are
online-LBFGS [124] and SQN [125].</li>
<li>Stochastic quasi-Newton method can deal with large-scale machine learning problems.</li>
<li>Compared with the stochastic gradient method, the calculation of stochastic quasi-Newton method is more
complex.</li>
</ul>

<p>
Hessian Free Method [7]
</p>
<ul class="org-ul">
<li>HF method performs a sub- optimization using the conjugate gradient, which avoids the expensive computation
of inverse Hessian matrix. HF method can employ the second-</li>
<li>order gradient information but does not need to directly calculate Hessian matrices. Thus, it is suitable
for high dimensional optimization.</li>
<li>The cost of computation for the matrix- vector product in HF method increases linearly with the increase of
training data. It does not work well for large- scale problems. Sub-sampled</li>
</ul>

<p>
Hessian Free Method [147]
</p>
<ul class="org-ul">
<li>Sup-sampled Hessian free method uses stochastic gradient and sub-sampled Hessian-vector during the process
of updating.</li>
<li>The sub-sampled HF method can deal with large-scale machine learning opti- mization problems.</li>
<li>Compared with the stochastic gradient method, the calculation is more com- plex and needs more computing
time in each iteration.</li>
</ul>

<p>
Natural Gradient
</p>
<ul class="org-ul">
<li>The basic idea of the natural gradient is to construct the gradient descent algorithm in the predictive
function space rather than the parametric space.</li>
<li>The natural gradient uses the Riemann structure of the parametric space to adjust the update direction,
which is more suitable for finding the extremum of the objective function.</li>
<li>In the natural gradient method, the calculation of the Fisher information matrix is complex</li>
</ul>
</div>
</div>
<div id="outline-container-orgb56e56b" class="outline-4">
<h4 id="orgb56e56b"><span class="section-number-4">40.1.7.</span> Available Toolkits for Optimization</h4>
<div class="outline-text-4" id="text-40-1-7">
<p>
CVX [166] Matlab CVX is a matlab-based modeling system for convex optimization but cannot handle large-scale
 problems. <a href="http://cvxr.com/cvx/download/">http://cvxr.com/cvx/download/</a>
</p>

<p>
CVXPY [167] Python CVXPY is a python package developed by Stanford University Convex Optimization Group for
 solving convex optimization problems. <a href="http://www.cvxpy.org/">http://www.cvxpy.org/</a>
</p>

<p>
CVXOPT [168] Python CVXOPT can be used for handling convex optimization. It is developed by Martin Andersen,
 Joachim Dahl, and Lieven Vandenberghe. <a href="http://cvxopt.org/">http://cvxopt.org/</a>
</p>

<p>
APM [169] Python APM python is suitable for large-scale optimization and can solve the problems of linear
 programming, quadratic programming, integer programming, nonlinear optimization and so
 on. <a href="http://apmonitor.com/wiki/index.php/Main/PythonApp">http://apmonitor.com/wiki/index.php/Main/PythonApp</a>
</p>

<p>
SPAMS [123] C++ SPAMS is an optimization toolbox for solving various sparse estimation problems, which is
 developed and maintained by Julien Mairal. Available interfaces include matlab, R, python and
 C++. <a href="http://spams-devel.gforge.inria.fr/">http://spams-devel.gforge.inria.fr/</a>
</p>

<p>
minConf Matlab minConf can be used for optimizing differentiable multi- variate functions which subject to
 simple constraints on parameters. It is a set of matlab functions, in which there are many methods to choose
 from. <a href="https://www.cs.ubc.ca/%E2%88%BCschmidtm/Software/minConf.html">https://www.cs.ubc.ca/%E2%88%BCschmidtm/Software/minConf.html</a>
</p>

<p>
tf.train.optimizer [170] Python; C++; CUDA The basic optimization class, which is usually not called directly
 and its subclasses are often used. It includes classic optimization algorithms such as gradient descent and
 AdaGrad. <a href="https://www.tensorflow.org/api">https://www.tensorflow.org/api</a> guides/python/train
</p>
</div>
</div>
</div>
<div id="outline-container-orge5573d4" class="outline-3">
<h3 id="orge5573d4"><span class="section-number-3">40.2.</span> 2023 A Survey on Machine Learning from Few Samples</h3>
<div class="outline-text-3" id="text-40-2">
<p>
<a href="https://arxiv.org/pdf/2009.02653.pdf">https://arxiv.org/pdf/2009.02653.pdf</a>
</p>

<p>
Few sample learning (FSL)
</p>

<p>
most cutting-edge machine learning algorithms are data-hungry
</p>
</div>
</div>
<div id="outline-container-org95b6809" class="outline-3">
<h3 id="org95b6809"><span class="section-number-3">40.3.</span> <span class="todo TODO">TODO</span> DPO Direct Performance Optimization - training on pairs</h3>
<div class="outline-text-3" id="text-40-3">
<p>
good/bad allow to speed up data labeling. <a href="https://arxiv.org/pdf/2305.18290">https://arxiv.org/pdf/2305.18290</a>
</p>
</div>
</div>
</div>

<div id="outline-container-orgbff88b1" class="outline-2">
<h2 id="orgbff88b1"><span class="section-number-2">41.</span> hardware</h2>
<div class="outline-text-2" id="text-41">
<p>
processors:
</p>
<ul class="org-ul">
<li>CPU - architecuters: ARM/ARM64, instructions: RISC</li>
<li>GPU</li>
<li>NPU</li>
<li>FPGA - field-programmable gate array</li>
<li>Intel GNA</li>
</ul>

<p>
companies:
</p>
<ul class="org-ul">
<li>Nvidia</li>
<li>Intel</li>
<li>Amd</li>
<li>Huawai</li>
<li>Amazon</li>
</ul>
</div>
<div id="outline-container-orgf782cac" class="outline-3">
<h3 id="orgf782cac"><span class="section-number-3">41.1.</span> embedded networks</h3>
<div class="outline-text-3" id="text-41-1">
<p>
Library for Training Binarized Neural Networks  <a href="https://github.com/larq/larq">https://github.com/larq/larq</a>
</p>
</div>
</div>
</div>
<div id="outline-container-org419e325" class="outline-2">
<h2 id="org419e325"><span class="section-number-2">42.</span> formats</h2>
<div class="outline-text-2" id="text-42">
<ul class="org-ul">
<li>HDF5 (Hierarchical Data Format) sci-libs/hdf5 dev-python/h5py</li>
<li>Apache Parquet - columnar storage format for Hadoop <a href="https://github.com/apache/parquet-mr">https://github.com/apache/parquet-mr</a> <a href="https://parquet.apache.org/">https://parquet.apache.org/</a></li>
</ul>
</div>
</div>
<div id="outline-container-org557b012" class="outline-2">
<h2 id="org557b012"><span class="section-number-2">43.</span> Free Courses</h2>
<div class="outline-text-2" id="text-43">
<p>
I've put together the leading Free AI courses.
</p>
</div>

<div id="outline-container-org6629b3e" class="outline-3">
<h3 id="org6629b3e"><span class="section-number-3">43.1.</span> Beginer</h3>
<div class="outline-text-3" id="text-43-1">
<ol class="org-ol">
<li>Introduction to AI – IBM</li>
</ol>
<p>
<a href="https://www.coursera.org/learn/introduction-to-ai">https://www.coursera.org/learn/introduction-to-ai</a>
</p>

<ol class="org-ol">
<li>Introduction to Responsible AI – Google</li>
</ol>
<p>
<a href="https://www.cloudskillsboost.google/course_templates/554">https://www.cloudskillsboost.google/course_templates/554</a>
</p>

<ol class="org-ol">
<li>Machine Learning Specialization – Stanford University</li>
</ol>
<p>
<a href="https://www.coursera.org/specializations/machine-learning-introduction">https://www.coursera.org/specializations/machine-learning-introduction</a>
</p>

<ol class="org-ol">
<li>Prompt Engineering Specialization – Vanderbilt University</li>
</ol>
<p>
<a href="https://www.coursera.org/specializations/prompt-engineering">https://www.coursera.org/specializations/prompt-engineering</a>
</p>
</div>
</div>

<div id="outline-container-org35fa6cd" class="outline-3">
<h3 id="org35fa6cd"><span class="section-number-3">43.2.</span> Intermediate Level:</h3>
<div class="outline-text-3" id="text-43-2">
<ol class="org-ol">
<li>Advanced Learning Algorithms – Stanford University</li>
</ol>
<p>
<a href="https://www.coursera.org/learn/advanced-learning-algorithms">https://www.coursera.org/learn/advanced-learning-algorithms</a>
</p>

<ol class="org-ol">
<li>Machine Learning Engineer Certification – Google Cloud</li>
</ol>
<p>
<a href="https://www.coursera.org/professional-certificates/preparing-for-google-cloud-machine-learning-engineer-professional-certificate">https://www.coursera.org/professional-certificates/preparing-for-google-cloud-machine-learning-engineer-professional-certificate</a>
</p>

<ol class="org-ol">
<li>Data Science: Machine Learning – Harvard University</li>
</ol>
<p>
<a href="https://pll.harvard.edu/course/data-science-machine-learning">https://pll.harvard.edu/course/data-science-machine-learning</a>
</p>
</div>
</div>

<div id="outline-container-org19222bd" class="outline-3">
<h3 id="org19222bd"><span class="section-number-3">43.3.</span> Advanced Level:</h3>
<div class="outline-text-3" id="text-43-3">
<ol class="org-ol">
<li>Amazon CodeWhisperer – Amazon</li>
</ol>
<p>
<a href="https://explore.skillbuilder.aws/learn/course/external/view/elearning/16405/amazon-codewhisperer-getting-started">https://explore.skillbuilder.aws/learn/course/external/view/elearning/16405/amazon-codewhisperer-getting-started</a>
</p>
</div>
</div>
</div>

<div id="outline-container-org68cb82c" class="outline-2">
<h2 id="org68cb82c"><span class="section-number-2">44.</span> <span class="todo TODO">TODO</span> Model compression - smaller</h2>
<div class="outline-text-2" id="text-44">
<ul class="org-ul">
<li>Low Rank Factorization - replace metrics/layers of NN to lower dimensionality</li>
</ul>
</div>
</div>
<div id="outline-container-org805fa0b" class="outline-2">
<h2 id="org805fa0b"><span class="section-number-2">45.</span> <span class="todo TODO">TODO</span> fusion operator optimization</h2>
</div>
<div id="outline-container-org450512d" class="outline-2">
<h2 id="org450512d"><span class="section-number-2">46.</span> SAS (Statistical analysis system)</h2>
<div class="outline-text-2" id="text-46">
<p>
by SAS Institute
</p>
<ul class="org-ul">
<li>Written in C</li>
<li>Proprietary</li>
<li>www.sas.com</li>
</ul>
<p>
Links
</p>
<ul class="org-ul">
<li><a href="https://sascrunch.com/18-free-resources-to-help-you-learn-sas/">https://sascrunch.com/18-free-resources-to-help-you-learn-sas/</a></li>
<li><a href="https://www.listendata.com/p/sas-tutorials.html">https://www.listendata.com/p/sas-tutorials.html</a></li>
<li><a href="https://data-flair.training/blogs/sas-tutorial/">https://data-flair.training/blogs/sas-tutorial/</a></li>
<li><a href="https://support.sas.com/en/documentation.html">https://support.sas.com/en/documentation.html</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Created: 2025-01-14 Tue 12:40</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
